## Comprehensive Guide to Building AI Agents in Daytona Sandboxes

This directory contains complete implementations and tutorials for building autonomous coding agents and AI-powered applications using Daytona's isolated sandbox environments.

### Claude Agent SDK Integration

**Two-Agent System**: Project Manager Agent (local, Claude Sonnet 4) plans tasks and delegates via `<developer_task>` XML tags to Developer Agent (in Daytona sandbox, Claude Agent SDK). Developer executes and streams results back. Setup: clone repo, set `DAYTONA_API_KEY` and `ANTHROPIC_API_KEY`, Node.js 18+, `npm install && npm run start`. Architecture uses Anthropic SDK locally with `claude-sonnet-4-20250514`, Claude Agent SDK in sandbox, communication via XML tags. Developer can host web apps with preview links. Example: "make a lunar lander web app" → Project Manager delegates → Developer creates HTML/CSS/JavaScript game with physics, gravity, thrust, keyboard controls, landing detection, fuel management → hosted on port 80 with preview URL.

**Single Agent System**: Autonomous agent using Claude Agent SDK inside Daytona sandbox. Main Node.js program creates sandbox and initializes Python agent. Sandbox agent uses Claude Agent SDK with tools: Read, Edit, Glob, Grep, Bash. System prompt includes workspace directory (`/home/daytona`) and preview URL format. User interaction: CLI prompt → passed to agent via `coding_agent.run_query_sync()` → agent executes → output streamed back. Example: "Build a Zelda-like game where I can move around and talk to famous programmers" → Agent creates top-down 2D game with grid-based movement, 5 NPC programmers, arrow keys/WASD controls, SPACE to talk → hosted with preview link.

**Running Claude Code via CLI**: Install Daytona CLI (`brew install daytonaio/cli/daytona`), authenticate (`daytona login --api-key=YOUR_API_KEY`), create sandbox (`daytona sandbox create --name claude-sandbox`), SSH in (`daytona ssh claude-sandbox`), run Claude (`claude` → browser auth → paste code back).

**Running Claude Code Programmatically**: Execute Claude Code tasks with real-time PTY-based output streaming. Python (AsyncDaytona recommended): create sandbox, install Claude Code globally, create PTY session with `on_data` callback, send command with `ANTHROPIC_API_KEY` env var, wait for completion. TypeScript: similar pattern using `Daytona` SDK, `createPty()`, `sendInput()`, `onData` callback. PTY sessions enable real-time streaming; pass API key via environment variable in command; call `sandbox.delete()` for cleanup.

### Python RLM Guides

**DSPy RLMs with DaytonaInterpreter**: Run DSPy's recursive language models (RLMs) with code execution in isolated Daytona sandboxes. RLM execution is iterative REPL loop: LLM sends task inputs → LLM responds with Python code → code runs in Daytona sandbox with `llm_query()` calls bridged back to host → repeat until `SUBMIT()` called. Setup: clone repo, create venv, `pip install -e .`, set `DAYTONA_API_KEY` and `OPENROUTER_API_KEY` in `.env`. Basic usage: configure LM with DSPy, create DaytonaInterpreter, define RLM with signature, call with inputs, shutdown interpreter. Bridging mechanism: DaytonaInterpreter launches Flask broker server inside sandbox, injects wrapper functions. Wrappers POST requests to broker and block until results arrive. Host polling loop picks up pending requests, executes them (calls LLM API or runs tool functions), posts results back. Built-in functions in sandbox: `llm_query(prompt)` — send single prompt to LLM, get string back; `llm_query_batched(prompts)` — send multiple prompts concurrently, get list of strings back. State persists across iterations: variables, imports, function definitions carry over. Example: `demo.py` analyzes _The Count of Monte Cristo_ (117 chapters) tracking wealth trajectories of five characters. RLM batches chapters, fans out with `llm_query_batched()` sending prompts like "Extract wealth events from these chapters as JSON", parses JSON, accumulates results, iterates for next batch, calls `SUBMIT()` when done. Plots results as smoothed time series.

**Recursive Language Models (RLM) with Daytona Sandboxes**: Build agent systems where agents spawn sub-agents in isolated sandboxes, enabling unlimited recursion depth and parallel exploration. Agents run in iteration loop: LLM call → extract Python code → execute in REPL. Code can call `rlm_query()` to spawn sub-agents, each with own sandbox and fresh repository clone. Results propagate back up tree. Setup: clone repo, create venv, `pip install -e .`, set `DAYTONA_API_KEY` and `LLM_API_KEY` in `.env`. Running: `python run.py <repo_url> -p "<task_prompt>" [-b branch] [--commit sha] [-c config.yaml] [-o output.patch]`. Example: `python run.py https://github.com/scikit-learn/scikit-learn -p "Investigate TODO comments..."`. Agent execution loop: each iteration builds prompt with context from previous execution, gets LLM completion, extracts and executes Python code blocks in REPL, checks if agent called `FINAL()` to submit results, formats output for next iteration. Sub-agent spawning: single sub-agent with `rlm_query(task)`, parallel spawning with `rlm_query_batched([tasks])`. Agent code interface (available in REPL): `rlm_query(task)` — spawn single sub-agent, returns result string; `rlm_query_batched(tasks)` — spawn multiple sub-agents in parallel; `FINAL(answer)` — submit final result (root: triggers patch extraction); `FINAL_VAR(var_name)` — submit variable value as result; `edit_file(path, old, new)` — edit file with syntax validation. Example: scikit-learn TODO investigation. Root agent spawns 25 depth-1 sub-agents to explore different sklearn modules in parallel. Some depth-1 agents spawn depth-2 sub-agents for large modules. Results propagate back up. Root agent synthesizes findings, identifies easiest TODO, fixes it. Generated patch example shows completion of `__all__` list. Execution: 40 agents total (25 depth-1, 15 depth-2), completed in 316 seconds. Configuration (`config.yaml`): model name, max_sandboxes, max_iterations, global_timeout, result_truncation_limit. Sandbox budget tracks total sandboxes created over lifetime; sub-agent sandboxes deleted immediately after completion. Viewing results: results saved to `results/` as JSON. View with `python -m http.server 8000` and open `http://localhost:8000/viewer/`. Viewer shows interactive agent hierarchy tree, iteration details with code/output, statistics (agent count, max depth, total iterations).

### AgentKit Integration

Build autonomous coding agent using AgentKit framework that performs software development tasks in Daytona sandbox. Agent can create web apps, run tests, execute scripts, automate multi-step workflows based on natural language prompts. Workflow: user provides natural language prompt → agent reasons about request, plans steps, executes them securely in Daytona sandbox. Setup: clone repo, get API keys from Daytona Dashboard and Anthropic Console, copy `.env.example` to `.env` with `DAYTONA_API_KEY` and `ANTHROPIC_API_KEY`. Local usage (Node.js 18+): `npm install && npm run start`. Docker: `docker buildx build . -t coding-agent && docker run --rm -it coding-agent`. Configuration: edit main prompt in `network.run(...)` in `src/index.ts` to change agent's task; set `enableDebugLogs=true` for detailed agent flow tracking. Example usage: default prompt creates React Notes app. Terminal output shows app ready with preview URL. Agent execution flow: uses LLM with access to specialized tools for Daytona sandbox operations. Project initialization with `shellTool` runs `npm create vite@latest notes -- --template react`. Install dependencies with `shellTool` runs `cd notes && npm install`. Create components with `uploadFilesTool` uploads App.jsx and App.css. Start dev server with `startDevServerTool` runs `cd notes && npm run dev`. Health check with `checkDevServerHealthTool` verifies dev server is running. Summary: agent outputs `DEV_SERVER_PORT=5173` and `TASK_COMPLETED` signal. `DEV_SERVER_PORT` is auto-detected and used to generate preview link. `TASK_COMPLETED` signals task completion for agent routing logic. Key advantages: secure isolated execution in Daytona sandboxes, multi-language support, auto-detects dev server, starts it, generates preview link, detailed debug logs for agent actions.

### OpenAI Codex SDK Integration

Build autonomous coding agent using OpenAI Codex inside Daytona sandbox. Agent can develop full-stack web apps, write code in any language, install dependencies, run scripts, start dev servers, generate preview links. Workflow: user interacts via CLI → main program sends prompts to agent in sandbox → agent executes and returns results. Agent automatically detects when to host web apps and generates preview links using Daytona Preview Links feature. Example workflow: create 3D animated lunar lander game → agent creates HTML/CSS/JavaScript with physics → hosted on port 8080 with preview link. Setup: clone repo, get API keys from Daytona Dashboard and OpenAI Developer Platform, copy `.env.example` to `.env` with `DAYTONA_API_KEY` and `SANDBOX_OPENAI_API_KEY`. ⚠️ `SANDBOX_OPENAI_API_KEY` is passed into sandbox and accessible to all code executed inside. Requires Node.js 18+. Install and run: `npm install && npm run start`. Architecture: two components. Main program (`src/index.ts`): Node.js script on local machine using Daytona SDK to create/manage sandbox and provide CLI. Sandbox agent (`agent/index.ts`): Node.js script inside sandbox using Codex SDK. Initialization: main program creates Daytona sandbox with OpenAI API key in environment, configures Codex system prompt via `.codex/config.toml` in sandbox, uploads agent package to sandbox, runs `npm install` in agent directory, waits for user input and runs agent asynchronously per prompt. System prompt configuration: set developer instructions with `/home/daytona` directory and preview URL pattern. User input loop with async agent execution: create session, execute OpenCode command with `PROMPT` env var, get session command logs with callbacks, delete session. Sandbox agent code: initialize Codex with custom options (workingDirectory: `/home/daytona`, skipGitRepoCheck: true, sandboxMode: `danger-full-access`). Maintain thread state across requests: read thread ID from file, resume existing thread or start new one. Key advantages: secure isolated execution in Daytona sandboxes, direct terminal communication with agent, automatic dev server detection and live preview links, multi-language and full-stack support, thread persistence across multiple requests, simple setup with automatic cleanup.

### AI Data Analysis with Agentic Loop

Run AI-generated code in Daytona Sandbox to analyze data: user provides dataset (CSV or other formats) → LLM generates code (usually Python) based on user's data → sandbox executes the code and returns results → LLM receives feedback and iterates to refine code if needed → display final results to user. Example: analyze vehicle valuation dataset, identify price relation to manufacturing year, generate visualizations using Claude and Daytona's secure sandbox with agentic loop for iterative refinement. Setup: `pip install daytona anthropic python-dotenv` (Python) or `npm install @daytonaio/sdk @anthropic-ai/sdk dotenv` (TypeScript). Configure `.env` with `DAYTONA_API_KEY` and `ANTHROPIC_API_KEY`. Dataset preparation: download vehicle valuation dataset from `https://download.daytona.io/dataset.csv` and save as `dataset.csv`. Create sandbox and upload dataset: `daytona = Daytona(); sandbox = daytona.create(); sandbox.fs.upload_file("dataset.csv", "/home/daytona/dataset.csv")`. Code execution handler: function to execute code and extract charts. Returns ExecutionResult with stdout, exit_code, charts. If execution has artifacts with charts, save PNG files. Analysis prompt: define what Claude should analyze (CSV file location, columns, analysis task, visualization requirements). Tool definition: define tool for Claude to execute Python code with input schema (code: string). Agentic loop: iterative loop allowing Claude to refine code based on execution feedback. Send initial prompt to Claude with tool definition. For each iteration (max 10): Claude generates response with optional tool calls. If tool calls exist, execute Python code in sandbox. Send execution results back to Claude (errors or success). Claude refines code based on feedback. Loop ends when Claude signals no more tool calls or max iterations reached. Loop workflow: send initial prompt to Claude with tool definition. For each iteration (max 10): Claude generates response with optional tool calls. If tool calls exist, execute Python code in sandbox. Send execution results back to Claude (errors or success). Claude refines code based on feedback. Loop ends when Claude signals no more tool calls or max iterations reached. Advantages: secure execution in isolated sandboxes, automatic artifact capture (charts, tables, outputs), built-in error detection and logging, language agnostic (Python used here, but Daytona supports multiple languages). Running the analysis: `python data-analysis.py` (Python) or `npx tsx data-analysis.ts` (TypeScript). Generates chart saved to `chart-0.png` showing vehicle valuation by manufacturing year.

### Google ADK Code Generator Agent

Build code generator agent using Google ADK that generates, tests, and verifies code in Daytona sandboxes. Agent takes natural language descriptions, generates implementations in Python/JavaScript/TypeScript, creates and executes tests, iterates on failures, returns verified working code. Setup: clone repo, `pip install -U google-adk daytona-adk python-dotenv`, configure `.env` with `DAYTONA_API_KEY` and `GOOGLE_API_KEY`. Core components: Google ADK provides Agent (AI model wrapper), App (container bundling agents with plugins), InMemoryRunner (execution engine). DaytonaPlugin provides tools to execute code (Python/JavaScript/TypeScript), run shell commands, upload/read files, start background processes in isolated sandboxes. Implementation: load environment, extract final response from ADK events, define agent instruction enforcing test-driven workflow (write function → write tests → execute code in sandbox to verify → if execution fails, fix and re-execute until tests pass → once verified, respond with only the function), configure plugin with labels, create agent with Gemini model and instruction, bundle and run with InMemoryRunner. Execution flow: with logging.DEBUG enabled, see sandbox creation, plugin registration, code generation, execution, iteration (if tests fail, agent fixes and re-executes), response (once tests pass, agent returns verified code), cleanup (sandbox automatically deleted when context exits). Output control: by default, agent returns only the working function. To include tests in response, add to prompt. Complete example provided with all setup and execution code. Run with `python main.py`. API reference: see daytona-adk documentation for complete API reference of available tools and configuration options.

### LangChain Data Analysis Integration

`DaytonaDataAnalysisTool` is a LangChain tool integration enabling agents to perform secure Python data analysis in sandboxed environments. Agents receive natural language prompts, reason about the task, generate Python code, execute it securely in Daytona sandbox, process results. Setup: `pip install -U langchain langchain-anthropic langchain-daytona-data-analysis python-dotenv`. Requires Python 3.10+. Environment configuration: `DAYTONA_API_KEY` and `ANTHROPIC_API_KEY`. Initialize model: `ChatAnthropic(model_name="claude-sonnet-4-5-20250929", temperature=0, timeout=None, max_retries=2)`. Define result handler: function to process ExecutionArtifacts, print stdout, save PNG charts to files. Initialize tool and upload data: `DaytonaDataAnalysisTool(on_result=process_data_analysis_result)`, upload file with description explaining CSV structure and analysis requirements. Create and run agent: `create_agent(model, tools=[DataAnalysisTool], debug=True)`, invoke with natural language request, close tool when finished. Execution flow: agent receives natural language request → agent determines need for DaytonaDataAnalysisTool → agent generates Python code for analysis → code executes securely in Daytona sandbox → results processed by handler function → charts saved to local directory → sandbox resources cleaned up. Agent typically explores dataset first (shape, columns, data types), then generates detailed analysis code with data cleaning, outlier removal, calculations, visualizations. API reference: `upload_file(file: IO, description: str)` uploads file to sandbox at `/home/daytona/`, description explains file purpose and data structure; `download_file(remote_path: str)` downloads file from sandbox by remote path; `remove_uploaded_file(uploaded_file: SandboxUploadedFile)` removes previously uploaded file from sandbox; `get_sandbox()` returns current sandbox instance for inspecting properties and metadata; `install_python_packages(package_names: str | list[str])` installs Python packages in sandbox using pip; `close()` closes and deletes sandbox environment. Data structures: SandboxUploadedFile (name, remote_path, description), Sandbox (represents Daytona sandbox instance). Example: vehicle price analysis analyzing vehicle valuations dataset with complete working example code.

### Letta Code Integration

Run autonomous coding agent based on Letta Code inside Daytona sandbox. Agent develops web apps, writes code in any language, installs dependencies, runs scripts. Letta Code uses stateful agents with persistent memory across sessions. Workflow: launch main script to create Daytona sandbox and install Letta Code → agent configured with custom Daytona-aware system prompt → interactive CLI interface for chatting with agent and issuing commands → agent hosts web apps and provides preview links via Daytona Preview Links feature → sandbox automatically deleted on exit. Example interaction: create markdown editor with live preview → agent creates app, hosts on port 8080, provides preview link. Setup: clone repo, get API keys from Daytona Dashboard and Letta Platform, create `.env` with `DAYTONA_API_KEY` and `SANDBOX_LETTA_API_KEY`. Install and run (Node.js 18+): `npm install && npm run start`. Security note: Letta API key is passed into sandbox environment and may be accessible to code executed within it. Architecture: two main TypeScript files. index.ts: creates sandbox, installs Letta Code, configures system prompt, provides interactive CLI. letta-session.ts: manages PTY-based bidirectional communication with Letta Code, handles JSON message streaming and response parsing. Initialization process: create Daytona sandbox with Letta API key in environment variables, install Letta Code globally via process execution, create PTY session for bidirectional communication, launch Letta Code in bidirectional headless mode with stream-json format. Flags: `--system-custom` pass custom system prompt with Daytona-specific instructions and URL pattern for preview links; `--input-format stream-json --output-format stream-json` enable JSON message streaming for real-time communication; `--yolo` allow agent to run shell commands without explicit approval. Message handling: send prompts via `processPrompt()` method, which formats user input as JSON and sends through PTY. User message format: `{"type": "user", "message": {"role": "user", "content": "create a simple web server"}}`. Agent responds with streaming JSON messages. Tool calls arrive as fragments. `handleParsedMessage()` method parses JSON fragments, combines consecutive fragments for same tool call, formats and displays results. Key advantages: secure isolated execution in Daytona sandboxes, stateful agents with persistent memory across sessions, full Letta Code capabilities (file operations, shell commands), agents viewable in Letta's Agent Development Environment, automatic preview link generation for hosted services, multi-language and full-stack support, automatic cleanup on exit.

### Mastra Coding Agent Integration

Integrate Mastra coding agent with Daytona sandboxes to execute AI-powered coding tasks in secure, isolated environments. Use Mastra Studio for ChatGPT-like interface with human-in-the-loop workflows. Requirements: Node.js 20+, OpenAI API key (or other LLM provider), Daytona API key from Daytona Dashboard. Setup: clone template repo, create `.env` file with `OPENAI_API_KEY`, `MODEL=openai/gpt-4o-mini`, `DAYTONA_API_KEY`. Install dependencies: `pnpm install`. Running the agent: `pnpm run dev`, access Mastra Studio at `http://localhost:4111`. Interface provides: conversation history organized in threads, visual debugging of agent execution steps and tool calls, model switching between different AI providers, real-time tool inspection. Tool calls and execution: agent uses several tools to interact with Daytona sandboxes. `createSandbox`: provisions new sandbox with name, language, labels, envVars. Returns sandboxId. `writeFiles`: create multiple files in sandbox with path and data. Returns success and list of file paths. `runCommand`: execute commands in sandbox with command, envs, workingDirectory, timeoutSeconds, captureOutput. Returns success, exitCode, stdout, command, executionTime. Terminal logging: tool calls and results logged with full visibility including arguments, results, token usage with caching metrics, unique identifiers for debugging. Sandbox management: active sandboxes appear in Daytona Dashboard. Clean up resources when finished unless sandbox needs to remain active for preview URLs or ongoing development. Key advantages: secure isolation (all operations run in isolated Daytona sandboxes), multi-language support (execute code across different programming languages), enhanced debugging (visualize and debug agent workflows in Mastra Studio), scalable execution (leverage Daytona's cloud infrastructure).

### OpenClaw in Daytona Sandbox

Running OpenClaw in Daytona sandbox provides isolation, security, 24/7 uptime without consuming local machine resources. Prerequisites: Daytona account and API key from Daytona Dashboard, local terminal (macOS, Linux, or Windows). Install CLI and authenticate: install Daytona CLI (`brew install daytonaio/cli/daytona` for Mac/Linux, PowerShell script for Windows), verify version is 0.135.0 or higher (`daytona --version`), authenticate (`daytona login --api-key=YOUR_API_KEY`). Create and connect to sandbox: create sandbox with OpenClaw preinstalled (`daytona sandbox create --name openclaw --snapshot daytona-medium --auto-stop 0`). `daytona-medium` snapshot is required (minimum 2GB memory for OpenClaw gateway). `--auto-stop 0` flag keeps sandbox running indefinitely. SSH into sandbox (`daytona ssh openclaw`). Onboard OpenClaw: start onboarding (`openclaw onboard`), follow prompts: accept security acknowledgment, select Quickstart mode, select Anthropic as model provider, select Anthropic API key auth method, paste Anthropic API key, keep default model (`anthropic/claude-opus-4-5`), skip channel configuration (configure later), skip skills configuration, skip hooks configuration, skip gateway service installation (already installed). Onboarding output displays dashboard link with gateway token in URL (after `?token=`). Save this token for dashboard authentication. Start gateway and access dashboard: start gateway in background (`nohup openclaw gateway run > /tmp/gateway.log 2>&1 &`), generate preview URL from local terminal (not SSH session) (`daytona preview-url openclaw --port 18789`). Creates signed preview URL that expires after 1 hour (customizable with `--expires` flag). Open URL in browser, go to Overview, paste gateway token in Gateway Token field, click Connect. Device pairing: OpenClaw requires device approval for security. List pending requests (`openclaw devices list`), approve device (`openclaw devices approve REQUEST_ID`). Click Connect again in dashboard. Green status indicator confirms OpenClaw is ready. Security layers: preview URL (time-limited access to dashboard port), gateway token (required for dashboard authentication), device approval (only approved devices can control assistant). Keep gateway token and preview URL secret. Configure Telegram: create bot via @BotFather in Telegram (send `/start`, then `/newbot`, enter bot name and username, copy bot token). Configure OpenClaw: `openclaw config set channels.telegram.enabled true`, `openclaw config set channels.telegram.botToken YOUR_BOT_TOKEN`, `openclaw config get channels.telegram`. Restart gateway: `openclaw gateway stop`, `nohup openclaw gateway run > /tmp/gateway.log 2>&1 &`. Complete verification in Telegram: open bot chat and click Start, copy pairing code and approve (`openclaw pairing approve telegram PAIRING_CODE`). Configure WhatsApp: run configuration (`openclaw config --section channels`). When prompted: select Local (this machine) for gateway location, choose Configure/link, select WhatsApp (QR link), select Yes for "Link WhatsApp now (QR)?". Scan QR code in WhatsApp: Settings → Linked Devices → Link a Device. Select This is my personal phone number and enter phone number when prompted. When prompted for another channel, choose Finished. Start chatting: send message to yourself in WhatsApp and OpenClaw responds. To allow other users, add their phone numbers to Allow From list in Channels → WhatsApp in dashboard.

### OpenCode Web Agent Integration

Run OpenCode coding agent inside Daytona sandbox using web interface. Agent can develop web apps, write code in any language, install dependencies, run scripts, supports 75+ LLM providers with live preview links. Workflow: launch main script to create Daytona sandbox with OpenCode installed → access web interface via preview link → create and interact with agent sessions → agent automatically generates preview links for hosted web apps → press Ctrl+C to delete sandbox automatically. Setup: clone repo, get API key from Daytona Dashboard, create `.env` with `DAYTONA_API_KEY`. Requires Node.js 18+. Install and run: `npm install && npm run start`. Models and API providers: OpenCode supports 75+ LLM providers with free default. Change providers in web interface menu. To persist API keys between runs, add them to sandbox environment: `sandbox = await daytona.create({ envVars: { ANTHROPIC_API_KEY: process.env.ANTHROPIC_API_KEY || '', } })`. Script implementation: initialization steps: create new Daytona sandbox, install OpenCode globally via npm using process execution, upload custom agent configuration with Daytona-specific system prompt, start OpenCode web server on port 3000, replace localhost URL with Daytona preview link. Main script code: execute OpenCode as async command (`sandbox.process.executeSessionCommand(sessionId, { command: `${envVar} opencode web --port ${OPENCODE_PORT}`, runAsync: true })`). Parse OpenCode's output and replace localhost with preview link. Agent configuration: custom system prompt passed as JSON via `OPENCODE_CONFIG_CONTENT` environment variable. Prompt includes Daytona sandbox awareness, `/home/daytona` directory usage, preview URL pattern, instructions to start servers in background with `&`. Cleanup: automatic sandbox deletion on Ctrl+C. Key advantages: secure isolated execution in Daytona sandboxes, OpenCode Web interface accessible via browser, support for 75+ LLM providers, all agent code execution happens inside sandbox, automatic preview link generation for deployed services, custom agent configuration for Daytona-specific workflows, clean resource management with automatic sandbox cleanup.

### Reinforcement Learning Training with TRL and GRPO

Train code-generating LLMs using TRL's GRPOTrainer with parallel Daytona sandboxes for safe, concurrent code evaluation. Workflow with 500 parallel sandboxes: generate many code completions per prompt (e.g., 250 per prompt per step) → evaluate each completion in its own sandbox against test suite → compute rewards (0-1 for test pass rate, -1 for errors/banned patterns) → GRPO reinforces completions scoring above group average. Sandboxes spawned once at start, reused throughout training, cleaned up after completion. Setup: Python 3.10+, 80GB+ VRAM GPU (adjustable via `per_device_train_batch_size`). Clone repo, create venv, `pip install -e .`. Create `.env` file with Daytona API key from dashboard. Task definition: tasks define prompts with test cases and validation rules. Each task includes: prompt (code context model continues from, completion mode not QA), func_name (function name being implemented), banned_patterns (patterns disqualifying completion, prevents cheating with built-ins), tests (test inputs for verification), reference (reference implementation for comparison). Completion processing: sanitization extracts only indented lines forming function body from model output. Model output with extra content (comments, examples) gets trimmed to just function body. Banned pattern detection checks before sandbox execution. Banned patterns trigger -1.0 reward without execution. Test harness assembly: `build_test_harness` combines prompt, completion, and test runner into executable Python. Assembled code executes in sandbox and prints JSON results: `{"results": [true, true, false, true, true]}`. Sandbox pool management: create and reuse pool throughout training. Pool size (500) matches effective batch size to ensure all completions evaluate in parallel. Code evaluation: main evaluation function ties everything together. Sanitize completion, check for banned patterns, build test harness, execute in sandbox with timeout, parse JSON results from stdout, compute reward (0-1 for test pass rate, -1 for errors). Parallel batch evaluation: distribute completions across sandbox pool with round-robin. Reward function: compute scalar rewards from sandbox evaluation results. Reward scheme: -1.0 (error, timeout, or banned pattern), 0.0 (no tests present), 0.0 to 1.0 (fraction of tests passed). Sync/async bridging: TRL's GRPOTrainer expects synchronous reward function; Daytona SDK uses async. Bridge with event loop. Training configuration: GRPOConfig with per_device_train_batch_size, gradient_accumulation_steps, num_generations, max_prompt_length, max_completion_length, learning_rate, num_train_epochs, logging_steps, max_steps, bf16, use_vllm, vllm_mode, vllm_gpu_memory_utilization, gradient_checkpointing, loss_type, beta. Key alignment: `per_device_train_batch_size (20) × gradient_accumulation_steps (25) = 500` equals `EFFECTIVE_BATCH_SIZE` for perfect parallelism. vLLM colocate mode: runs inference on same GPU as training, using 15% GPU memory for generation, rest for training. Running training: `python train.py`. Output shows sandbox creation and parallel evaluation progress. After completion, metrics saved to `training_results/metrics.jsonl` and model to `training_results/checkpoint-8`. Example evaluation walkthrough: model generates completion with function body → sanitize_completion extracts indented lines only → has_banned_pattern checks for disqualifying patterns → build_test_harness assembles full executable script → sandbox.code_interpreter.run_code executes in sandbox with timeout → parse JSON results from stdout: `{"results": [true, true, true, true, true]}` → compute reward: `5 / 5 = 1.0` (all tests passed). Adding custom tasks: extend TASKS dictionary with new task definition (prompt, func_name, banned_patterns, tests, reference). Reference function must be defined in test harness. Configuration parameters: EFFECTIVE_BATCH_SIZE (500, parallel sandboxes count), MAX_TIMEOUT_SECONDS (1, timeout per code execution), MODEL_NAME (Qwen/Qwen3-1.7B-Base, base model to train). Scaling tips: keep `per_device_train_batch_size * gradient_accumulation_steps = EFFECTIVE_BATCH_SIZE` for optimal parallelism, increase MAX_TIMEOUT_SECONDS for complex algorithmic tasks, reduce per_device_train_batch_size for GPUs with less VRAM, increase gradient_accumulation_steps proportionally. Results: training achieves near-perfect performance after 8 steps with 500-completion batch size, showing rapid improvement from initial random completions to correct implementations.