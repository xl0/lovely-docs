{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vercel/ai\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_docs.settings import settings, GitSource\n",
    "from lovely_docs.git import clone_repo\n",
    "from lovely_docs.docs import (\n",
    "    build_markdown_doc_tree,\n",
    "    process_tree_depth_first,\n",
    "    calculate_total_usage,\n",
    "    calculate_cost,\n",
    "    save_processed_documents\n",
    ")\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64: 3850.0/3850.0 completed with 306 local objects."
     ]
    }
   ],
   "source": [
    "source =  GitSource(\n",
    "    name=\"vercel/ai\",\n",
    "    doc_dir=\"content/docs\",\n",
    "    repo=\"https://github.com/vercel/ai\",\n",
    "    commit=\"main\",\n",
    "    ecosystems=[\"js\"])\n",
    "\n",
    "\n",
    "commit, clone_dir = clone_repo(source)\n",
    "source.commit = commit # Replace the literal commit (master) with the hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocItem(origPath=Path('.'), name='', displayName='', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('00-introduction/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK by Vercel\\ndescription: The AI SDK is the TypeScript toolkit for building AI applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.\\n---\\n\\n# AI SDK\\n\\nThe AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.\\n\\n## Why use the AI SDK?\\n\\nIntegrating large language models (LLMs) into applications is complicated and heavily dependent on the specific model provider you use.\\n\\nThe AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.\\n\\nFor example, here’s how you can generate text with various models using the AI SDK:\\n\\n<PreviewSwitchProviders />\\n\\nThe AI SDK has two main libraries:\\n\\n- **[AI SDK Core](/docs/ai-sdk-core):** A unified API for generating text, structured objects, tool calls, and building agents with LLMs.\\n- **[AI SDK UI](/docs/ai-sdk-ui):** A set of framework-agnostic hooks for quickly building chat and generative user interface.\\n\\n## Model Providers\\n\\nThe AI SDK supports [multiple model providers](/providers).\\n\\n<OfficialModelCards />\\n\\n## Templates\\n\\nWe\\'ve built some [templates](https://vercel.com/templates?type=ai) that include AI SDK integrations for different use cases, providers, and frameworks. You can use these templates to get started with your AI-powered application.\\n\\n### Starter Kits\\n\\n<Templates type=\"starter-kits\" />\\n\\n### Feature Exploration\\n\\n<Templates type=\"feature-exploration\" />\\n\\n### Frameworks\\n\\n<Templates type=\"frameworks\" />\\n\\n### Generative UI\\n\\n<Templates type=\"generative-ui\" />\\n\\n### Security\\n\\n<Templates type=\"security\" />\\n\\n## Join our Community\\n\\nIf you have questions about anything related to the AI SDK, you\\'re always welcome to ask our community on [the Vercel Community](https://community.vercel.com/c/ai-sdk/62).\\n\\n## `llms.txt` (for Cursor, Windsurf, Copilot, Claude etc.)\\n\\nYou can access the entire AI SDK documentation in Markdown format at [ai-sdk.dev/llms.txt](/llms.txt). This can be used to ask any LLM (assuming it has a big enough context window) questions about the AI SDK based on the most up-to-date documentation.\\n\\n### Example Usage\\n\\nFor instance, to prompt an LLM with questions about the AI SDK:\\n\\n1. Copy the documentation contents from [ai-sdk.dev/llms.txt](/llms.txt)\\n2. Use the following prompt format:\\n\\n```prompt\\nDocumentation:\\n{paste documentation here}\\n---\\nBased on the above documentation, answer the following:\\n{your question}\\n```\\n', children=[]), DocItem(origPath=Path('01-announcing-ai-sdk-6-beta/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK 6 Beta\\ndescription: Get started with the Beta version of AI SDK 6.\\n---\\n\\n# Announcing AI SDK 6 Beta\\n\\n<Note type=\"warning\">\\n  AI SDK 6 is in beta — while more stable than alpha, AI SDK 6 is still in\\n  active development and APIs may still change. Pin to specific versions as\\n  breaking changes may occur in patch releases.\\n</Note>\\n\\n## Why AI SDK 6?\\n\\nAI SDK 6 is a **major version** due to the introduction of the **v3 Language Model Specification** that powers new capabilities like agents and tool approval. However, unlike AI SDK 5, **this release is not expected to have major breaking changes** for most users.\\n\\nThe version bump reflects improvements to the specification, not a complete redesign of the SDK. If you\\'re using AI SDK 5, migrating to v6 should be straightforward with minimal code changes.\\n\\n## Beta Version Guidance\\n\\nThe AI SDK 6 Beta is intended for:\\n\\n- **Trying out new features** and giving us feedback on the developer experience\\n- **Experimenting with agents** and tool approval workflows\\n\\nYour feedback during this beta phase directly shapes the final stable release. Share your experiences through [GitHub issues](https://github.com/vercel/ai/issues/new/choose).\\n\\n## Installation\\n\\nTo install the AI SDK 6 Beta, run the following command:\\n\\n```bash\\nnpm install ai@beta @ai-sdk/openai@beta @ai-sdk/react@beta\\n```\\n\\n<Note type=\"warning\">\\n  APIs may still change during beta. Pin to specific versions as breaking\\n  changes may occur in patch releases.\\n</Note>\\n\\n## What\\'s New in AI SDK 6?\\n\\nAI SDK 6 introduces several features (with more to come soon!):\\n\\n### Agent Abstraction\\n\\nA new unified interface for building agents with full control over execution flow, tool loops, and state management.\\n\\n### Tool Execution Approval\\n\\nRequest user confirmation before executing tools, enabling native human-in-the-loop patterns.\\n\\n### Structured Output (Stable)\\n\\nGenerate structured data alongside tool calling with `generateText` and `streamText` - now stable and production-ready.\\n\\n### Reranking Support\\n\\nImprove search relevance by reordering documents based on their relationship to a query using specialized reranking models.\\n\\n### Image Editing Support\\n\\nNative support for image editing (coming soon).\\n\\n## Agent Abstraction\\n\\nAI SDK 6 introduces a powerful new `Agent` interface that provides a standardized way to build agents.\\n\\n### Default Implementation: ToolLoopAgent\\n\\nThe `ToolLoopAgent` class provides a default implementation out of the box:\\n\\n```typescript\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { weatherTool } from \\'@/tool/weather\\';\\n\\nexport const weatherAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful weather assistant.\\',\\n  tools: {\\n    weather: weatherTool,\\n  },\\n});\\n\\n// Use the agent\\nconst result = await weatherAgent.generate({\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\nThe agent automatically handles the tool execution loop:\\n\\n1. Calls the LLM with your prompt\\n2. Executes any requested tool calls\\n3. Adds results back to the conversation\\n4. Repeats until complete (default `stopWhen: stepCountIs(20)`)\\n\\n### Configuring Call Options\\n\\nCall options let you pass type-safe runtime inputs to dynamically configure your agents. Use them to inject retrieved documents for RAG, select models based on request complexity, customize tool behavior per request, or adjust any agent setting based on context.\\n\\nWithout call options, you\\'d need to create multiple agents or handle configuration logic outside the agent. With call options, you define a schema once and modify agent behavior at runtime:\\n\\n```typescript\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst supportAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    userId: z.string(),\\n    accountType: z.enum([\\'free\\', \\'pro\\', \\'enterprise\\']),\\n  }),\\n  instructions: \\'You are a helpful customer support agent.\\',\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    instructions:\\n      settings.instructions +\\n      `\\\\nUser context:\\n- Account type: ${options.accountType}\\n- User ID: ${options.userId}\\n\\nAdjust your response based on the user\\'s account level.`,\\n  }),\\n});\\n\\n// Pass options when calling the agent\\nconst result = await supportAgent.generate({\\n  prompt: \\'How do I upgrade my account?\\',\\n  options: {\\n    userId: \\'user_123\\',\\n    accountType: \\'free\\',\\n  },\\n});\\n```\\n\\nThe `options` parameter is type-safe and will error if you don\\'t provide it or pass incorrect types.\\n\\nCall options enable dynamic agent configuration for several scenarios:\\n\\n- **RAG**: Fetch relevant documents and inject them into prompts at runtime\\n- **Dynamic model selection**: Choose faster or more capable models based on request complexity\\n- **Tool configuration**: Adjust tools per request\\n- **Provider options**: Set reasoning effort, temperature, or other provider-specific settings dynamically\\n\\nLearn more in the [Configuring Call Options](/docs/agents/configuring-call-options) documentation.\\n\\n### UI Integration\\n\\nAgents integrate seamlessly with React and other UI frameworks:\\n\\n```typescript\\n// Server-side API route\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: weatherAgent,\\n    messages,\\n  });\\n}\\n```\\n\\n```typescript\\n// Client-side with type safety\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { InferAgentUIMessage } from \\'ai\\';\\nimport { weatherAgent } from \\'@/agent/weather-agent\\';\\n\\ntype WeatherAgentUIMessage = InferAgentUIMessage<typeof weatherAgent>;\\n\\nconst { messages, sendMessage } = useChat<WeatherAgentUIMessage>();\\n```\\n\\n### Custom Agent Implementations\\n\\nIn AI SDK 6, `Agent` is an interface rather than a concrete class. While `ToolLoopAgent` provides a solid default implementation for most use cases, you can implement the `Agent` interface to build custom agent architectures:\\n\\n```typescript\\nimport { Agent } from \\'ai\\';\\n\\n// Build your own multi-agent orchestrator that delegates to specialists\\nclass Orchestrator implements Agent {\\n  constructor(private subAgents: Record<string, Agent>) {\\n    /* Implementation */\\n  }\\n}\\n\\nconst orchestrator = new Orchestrator({\\n  subAgents: {\\n    // your subagents\\n  },\\n});\\n```\\n\\nThis approach enables you to experiment with orchestrators, memory layers, custom stop conditions, and agent patterns tailored to your specific use case.\\n\\n## Tool Execution Approval\\n\\nAI SDK 6 introduces a tool approval system that gives you control over when tools are executed.\\n\\nEnable approval for a tool by setting `needsApproval`:\\n\\n```typescript\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const weatherTool = tool({\\n  description: \\'Get the weather in a location\\',\\n  inputSchema: z.object({\\n    city: z.string(),\\n  }),\\n  needsApproval: true, // Require user approval\\n  execute: async ({ city }) => {\\n    const weather = await fetchWeather(city);\\n    return weather;\\n  },\\n});\\n```\\n\\n### Dynamic Approval\\n\\nMake approval decisions based on tool input:\\n\\n```typescript\\nexport const paymentTool = tool({\\n  description: \\'Process a payment\\',\\n  inputSchema: z.object({\\n    amount: z.number(),\\n    recipient: z.string(),\\n  }),\\n  // Only require approval for large transactions\\n  needsApproval: async ({ amount }) => amount > 1000,\\n  execute: async ({ amount, recipient }) => {\\n    return await processPayment(amount, recipient);\\n  },\\n});\\n```\\n\\n### Client-Side Approval UI\\n\\nHandle approval requests in your UI:\\n\\n```tsx\\nexport function WeatherToolView({ invocation, addToolApprovalResponse }) {\\n  if (invocation.state === \\'approval-requested\\') {\\n    return (\\n      <div>\\n        <p>Can I retrieve the weather for {invocation.input.city}?</p>\\n        <button\\n          onClick={() =>\\n            addToolApprovalResponse({\\n              id: invocation.approval.id,\\n              approved: true,\\n            })\\n          }\\n        >\\n          Approve\\n        </button>\\n        <button\\n          onClick={() =>\\n            addToolApprovalResponse({\\n              id: invocation.approval.id,\\n              approved: false,\\n            })\\n          }\\n        >\\n          Deny\\n        </button>\\n      </div>\\n    );\\n  }\\n\\n  if (invocation.state === \\'output-available\\') {\\n    return (\\n      <div>\\n        Weather: {invocation.output.weather}\\n        Temperature: {invocation.output.temperature}°F\\n      </div>\\n    );\\n  }\\n\\n  // Handle other states...\\n}\\n```\\n\\n### Auto-Submit After Approvals\\n\\nAutomatically continue the conversation once approvals are handled:\\n\\n```typescript\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { lastAssistantMessageIsCompleteWithApprovalResponses } from \\'ai\\';\\n\\nconst { messages, addToolApprovalResponse } = useChat({\\n  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithApprovalResponses,\\n});\\n```\\n\\n## Structured Output (Stable)\\n\\nAI SDK 6 stabilizes structured output support for agents, enabling you to generate structured data alongside multi-step tool calling.\\n\\nPreviously, you could only generate structured outputs with `generateObject` and `streamObject`, which didn\\'t support tool calling. Now `ToolLoopAgent` (and `generateText` / `streamText`) can combine both capabilities using the `output` parameter:\\n\\n```typescript\\nimport { Output, ToolLoopAgent, tool } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        city: z.string(),\\n      }),\\n      execute: async ({ city }) => {\\n        return { temperature: 72, condition: \\'sunny\\' };\\n      },\\n    }),\\n  },\\n  output: Output.object({\\n    schema: z.object({\\n      summary: z.string(),\\n      temperature: z.number(),\\n      recommendation: z.string(),\\n    }),\\n  }),\\n});\\n\\nconst { output } = await agent.generate({\\n  prompt: \\'What is the weather in San Francisco and what should I wear?\\',\\n});\\n// The agent calls the weather tool AND returns structured output\\nconsole.log(output);\\n// {\\n//   summary: \"It\\'s sunny in San Francisco\",\\n//   temperature: 72,\\n//   recommendation: \"Wear light clothing and sunglasses\"\\n// }\\n```\\n\\n### Output Types\\n\\nThe `Output` object provides multiple strategies for structured generation:\\n\\n- **`Output.object()`**: Generate structured objects with Zod schemas\\n- **`Output.array()`**: Generate arrays of structured objects\\n- **`Output.choice()`**: Select from a specific set of options\\n- **`Output.text()`**: Generate plain text (default behavior)\\n\\n### Streaming Structured Output\\n\\nUse `agent.stream()` to stream structured output as it\\'s being generated:\\n\\n```typescript\\nimport { ToolLoopAgent, Output } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst profileAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'Generate realistic person profiles.\\',\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number(),\\n      occupation: z.string(),\\n    }),\\n  }),\\n});\\n\\nconst { partialOutputStream } = await profileAgent.stream({\\n  prompt: \\'Generate a person profile.\\',\\n});\\n\\nfor await (const partial of partialOutputStream) {\\n  console.log(partial);\\n  // { name: \"John\" }\\n  // { name: \"John\", age: 30 }\\n  // { name: \"John\", age: 30, occupation: \"Engineer\" }\\n}\\n```\\n\\n### Support in `generateText` and `streamText`\\n\\nStructured outputs are also supported in `generateText` and `streamText` functions, allowing you to use this feature outside of agents when needed.\\n\\n<Note>\\n  When using structured output with `generateText` or `streamText`, you must\\n  configure multiple steps with `stopWhen` because generating the structured\\n  output is itself a step. For example: `stopWhen: stepCountIs(2)` to allow tool\\n  calling and output generation.\\n</Note>\\n\\n## Reranking Support\\n\\nAI SDK 6 introduces native support for reranking, a technique that improves search relevance by reordering documents based on their relationship to a query.\\n\\nUnlike embedding-based similarity search, reranking models are specifically trained to understand query-document relationships, producing more accurate relevance scores:\\n\\n```typescript\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  \\'sunny day at the beach\\',\\n  \\'rainy afternoon in the city\\',\\n  \\'snowy night in the mountains\\',\\n];\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'talk about rain\\',\\n  topN: 2,\\n});\\n\\nconsole.log(ranking);\\n// [\\n//   { originalIndex: 1, score: 0.9, document: \\'rainy afternoon in the city\\' },\\n//   { originalIndex: 0, score: 0.3, document: \\'sunny day at the beach\\' }\\n// ]\\n```\\n\\n### Structured Document Reranking\\n\\nReranking also supports structured documents, making it ideal for searching through databases, emails, or other structured content:\\n\\n```typescript\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  {\\n    from: \\'Paul Doe\\',\\n    subject: \\'Follow-up\\',\\n    text: \\'We are happy to give you a discount of 20% on your next order.\\',\\n  },\\n  {\\n    from: \\'John McGill\\',\\n    subject: \\'Missing Info\\',\\n    text: \\'Sorry, but here is the pricing information from Oracle: $5000/month\\',\\n  },\\n];\\n\\nconst { rerankedDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'Which pricing did we get from Oracle?\\',\\n  topN: 1,\\n});\\n\\nconsole.log(rerankedDocuments[0]);\\n// { from: \\'John McGill\\', subject: \\'Missing Info\\', text: \\'...\\' }\\n```\\n\\n### Supported Providers\\n\\nSeveral providers offer reranking models:\\n\\n- [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)\\n- [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models)\\n- [Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models)\\n\\n## Image Editing Support\\n\\nNative support for image editing and generation workflows is coming soon. This will enable:\\n\\n- Image-to-image transformations\\n- Multi-modal editing with text prompts\\n\\n## Migration from AI SDK 5.x\\n\\nAI SDK 6 is expected to have minimal breaking changes. The version bump is due to the v3 Language Model Specification, but most AI SDK 5 code will work with little or no modification.\\n\\n## Timeline\\n\\n**AI SDK 6 Beta**: Available now\\n\\n**Stable Release**: End of 2025\\n', children=[]), DocItem(origPath=Path('02-foundations'), name='02-foundations', displayName='02-foundations', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('02-foundations/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of foundational concepts critical to understanding the AI SDK\\n---\\n\\n# Overview\\n\\n<Note>\\n  This page is a beginner-friendly introduction to high-level artificial\\n  intelligence (AI) concepts. To dive right into implementing the AI SDK, feel\\n  free to skip ahead to our [quickstarts](/docs/getting-started) or learn about\\n  our [supported models and providers](/docs/foundations/providers-and-models).\\n</Note>\\n\\nThe AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.\\n\\nFor example, here’s how you can generate text with various models using the AI SDK:\\n\\n<PreviewSwitchProviders />\\n\\nTo effectively leverage the AI SDK, it helps to familiarize yourself with the following concepts:\\n\\n## Generative Artificial Intelligence\\n\\n**Generative artificial intelligence** refers to models that predict and generate various types of outputs (such as text, images, or audio) based on what’s statistically likely, pulling from patterns they’ve learned from their training data. For example:\\n\\n- Given a photo, a generative model can generate a caption.\\n- Given an audio file, a generative model can generate a transcription.\\n- Given a text description, a generative model can generate an image.\\n\\n## Large Language Models\\n\\nA **large language model (LLM)** is a subset of generative models focused primarily on **text**. An LLM takes a sequence of words as input and aims to predict the most likely sequence to follow. It assigns probabilities to potential next sequences and then selects one. The model continues to generate sequences until it meets a specified stopping criterion.\\n\\nLLMs learn by training on massive collections of written text, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well.\\n\\nHowever, it\\'s crucial to understand LLMs\\' limitations. When asked about less known or absent information, like the birthday of a personal relative, LLMs might \"hallucinate\" or make up information. It\\'s essential to consider how well-represented the information you need is in the model.\\n\\n## Embedding Models\\n\\nAn **embedding model** is used to convert complex data (like words or images) into a dense vector (a list of numbers) representation, known as an embedding. Unlike generative models, embedding models do not generate new text or data. Instead, they provide representations of semantic and syntactic relationships between entities that can be used as input for other models or other natural language processing tasks.\\n\\nIn the next section, you will learn about the difference between models providers and models, and which ones are available in the AI SDK.\\n', children=[]), DocItem(origPath=Path('02-foundations/02-providers-and-models.mdx'), name='02-providers-and-models.mdx', displayName='02-providers-and-models.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Providers and Models\\ndescription: Learn about the providers and models available in the AI SDK.\\n---\\n\\n# Providers and Models\\n\\nCompanies such as OpenAI and Anthropic (providers) offer access to a range of large language models (LLMs) with differing strengths and capabilities through their own APIs.\\n\\nEach provider typically has its own unique method for interfacing with their models, complicating the process of switching providers and increasing the risk of vendor lock-in.\\n\\nTo solve these challenges, AI SDK Core offers a standardized approach to interacting with LLMs through a [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v2) that abstracts differences between providers. This unified interface allows you to switch between providers with ease while using the same API for all providers.\\n\\nHere is an overview of the AI SDK Provider Architecture:\\n\\n<MDXImage\\n  srcLight=\"/images/ai-sdk-diagram.png\"\\n  srcDark=\"/images/ai-sdk-diagram-dark.png\"\\n  width={800}\\n  height={800}\\n/>\\n\\n## AI SDK Providers\\n\\nThe AI SDK comes with a wide range of providers that you can use to interact with different language models:\\n\\n- [xAI Grok Provider](/providers/ai-sdk-providers/xai) (`@ai-sdk/xai`)\\n- [OpenAI Provider](/providers/ai-sdk-providers/openai) (`@ai-sdk/openai`)\\n- [Azure OpenAI Provider](/providers/ai-sdk-providers/azure) (`@ai-sdk/azure`)\\n- [Anthropic Provider](/providers/ai-sdk-providers/anthropic) (`@ai-sdk/anthropic`)\\n- [Amazon Bedrock Provider](/providers/ai-sdk-providers/amazon-bedrock) (`@ai-sdk/amazon-bedrock`)\\n- [Google Generative AI Provider](/providers/ai-sdk-providers/google-generative-ai) (`@ai-sdk/google`)\\n- [Google Vertex Provider](/providers/ai-sdk-providers/google-vertex) (`@ai-sdk/google-vertex`)\\n- [Mistral Provider](/providers/ai-sdk-providers/mistral) (`@ai-sdk/mistral`)\\n- [Together.ai Provider](/providers/ai-sdk-providers/togetherai) (`@ai-sdk/togetherai`)\\n- [Cohere Provider](/providers/ai-sdk-providers/cohere) (`@ai-sdk/cohere`)\\n- [Fireworks Provider](/providers/ai-sdk-providers/fireworks) (`@ai-sdk/fireworks`)\\n- [DeepInfra Provider](/providers/ai-sdk-providers/deepinfra) (`@ai-sdk/deepinfra`)\\n- [DeepSeek Provider](/providers/ai-sdk-providers/deepseek) (`@ai-sdk/deepseek`)\\n- [Cerebras Provider](/providers/ai-sdk-providers/cerebras) (`@ai-sdk/cerebras`)\\n- [Groq Provider](/providers/ai-sdk-providers/groq) (`@ai-sdk/groq`)\\n- [Perplexity Provider](/providers/ai-sdk-providers/perplexity) (`@ai-sdk/perplexity`)\\n- [ElevenLabs Provider](/providers/ai-sdk-providers/elevenlabs) (`@ai-sdk/elevenlabs`)\\n- [LMNT Provider](/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)\\n- [Hume Provider](/providers/ai-sdk-providers/hume) (`@ai-sdk/hume`)\\n- [Rev.ai Provider](/providers/ai-sdk-providers/revai) (`@ai-sdk/revai`)\\n- [Deepgram Provider](/providers/ai-sdk-providers/deepgram) (`@ai-sdk/deepgram`)\\n- [Gladia Provider](/providers/ai-sdk-providers/gladia) (`@ai-sdk/gladia`)\\n- [LMNT Provider](/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)\\n- [AssemblyAI Provider](/providers/ai-sdk-providers/assemblyai) (`@ai-sdk/assemblyai`)\\n- [Baseten Provider](/providers/ai-sdk-providers/baseten) (`@ai-sdk/baseten`)\\n\\nYou can also use the [OpenAI Compatible provider](/providers/openai-compatible-providers) with OpenAI-compatible APIs:\\n\\n- [LM Studio](/providers/openai-compatible-providers/lmstudio)\\n- [Heroku](/providers/openai-compatible-providers/heroku)\\n\\nOur [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v2) is published as an open-source package, which you can use to create [custom providers](/providers/community-providers/custom-providers).\\n\\nThe open-source community has created the following providers:\\n\\n- [Ollama Provider](/providers/community-providers/ollama) (`ollama-ai-provider`)\\n- [FriendliAI Provider](/providers/community-providers/friendliai) (`@friendliai/ai-provider`)\\n- [Portkey Provider](/providers/community-providers/portkey) (`@portkey-ai/vercel-provider`)\\n- [Cloudflare Workers AI Provider](/providers/community-providers/cloudflare-workers-ai) (`workers-ai-provider`)\\n- [OpenRouter Provider](/providers/community-providers/openrouter) (`@openrouter/ai-sdk-provider`)\\n- [Aihubmix Provider](/providers/community-providers/aihubmix) (`@aihubmix/ai-sdk-provider`)\\n- [Requesty Provider](/providers/community-providers/requesty) (`@requesty/ai-sdk`)\\n- [Crosshatch Provider](/providers/community-providers/crosshatch) (`@crosshatch/ai-provider`)\\n- [Mixedbread Provider](/providers/community-providers/mixedbread) (`mixedbread-ai-provider`)\\n- [Voyage AI Provider](/providers/community-providers/voyage-ai) (`voyage-ai-provider`)\\n- [Mem0 Provider](/providers/community-providers/mem0)(`@mem0/vercel-ai-provider`)\\n- [Letta Provider](/providers/community-providers/letta)(`@letta-ai/vercel-ai-sdk-provider`)\\n- [Supermemory Provider](/providers/community-providers/supermemory)(`@supermemory/tools`)\\n- [Spark Provider](/providers/community-providers/spark) (`spark-ai-provider`)\\n- [AnthropicVertex Provider](/providers/community-providers/anthropic-vertex-ai) (`anthropic-vertex-ai`)\\n- [LangDB Provider](/providers/community-providers/langdb) (`@langdb/vercel-provider`)\\n- [Dify Provider](/providers/community-providers/dify) (`dify-ai-provider`)\\n- [Sarvam Provider](/providers/community-providers/sarvam) (`sarvam-ai-provider`)\\n- [Claude Code Provider](/providers/community-providers/claude-code) (`ai-sdk-provider-claude-code`)\\n- [Built-in AI Provider](/providers/community-providers/built-in-ai) (`built-in-ai`)\\n- [Gemini CLI Provider](/providers/community-providers/gemini-cli) (`ai-sdk-provider-gemini-cli`)\\n- [A2A Provider](/providers/community-providers/a2a) (`a2a-ai-provider`)\\n- [SAP-AI Provider](/providers/community-providers/sap-ai) (`@mymediset/sap-ai-provider`)\\n- [AI/ML API Provider](/providers/community-providers/aimlapi) (`@ai-ml.api/aimlapi-vercel-ai`)\\n- [MCP Sampling Provider](/providers/community-providers/mcp-sampling) (`@mcpc-tech/mcp-sampling-ai-provider`)\\n- [ACP Provider](/providers/community-providers/acp) (`@mcpc-tech/acp-ai-provider`)\\n\\n## Self-Hosted Models\\n\\nYou can access self-hosted models with the following providers:\\n\\n- [Ollama Provider](/providers/community-providers/ollama)\\n- [LM Studio](/providers/openai-compatible-providers/lmstudio)\\n- [Baseten](/providers/ai-sdk-providers/baseten)\\n- [Built-in AI](/providers/community-providers/built-in-ai)\\n\\nAdditionally, any self-hosted provider that supports the OpenAI specification can be used with the [OpenAI Compatible Provider](/providers/openai-compatible-providers).\\n\\n## Model Capabilities\\n\\nThe AI providers support different language models with various capabilities.\\nHere are the capabilities of popular models:\\n\\n| Provider                                                                 | Model                                       | Image Input         | Object Generation   | Tool Usage          | Tool Streaming      |\\n| ------------------------------------------------------------------------ | ------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-4`                                    | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3`                                    | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3-fast`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3-mini`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3-mini-fast`                          | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-2-1212`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-2-vision-1212`                        | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-beta`                                 | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-vision-beta`                          | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |\\n| [Vercel](/providers/ai-sdk-providers/vercel)                             | `v0-1.0-md`                                 | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5`                                     | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-mini`                                | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-nano`                                | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1-chat-latest`                       | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1-codex-mini`                        | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1-codex`                             | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1`                                   | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-codex`                               | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-chat-latest`                         | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-opus-4-5`                           | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-opus-4-1`                           | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-opus-4-0`                           | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-sonnet-4-0`                         | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-3-7-sonnet-latest`                  | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-3-5-haiku-latest`                   | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `pixtral-large-latest`                      | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-large-latest`                      | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-medium-latest`                     | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-medium-2505`                       | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-small-latest`                      | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `pixtral-12b-2409`                          | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai) | `gemini-2.0-flash-exp`                      | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai) | `gemini-1.5-flash`                          | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai) | `gemini-1.5-pro`                            | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex)               | `gemini-2.0-flash-exp`                      | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex)               | `gemini-1.5-flash`                          | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex)               | `gemini-1.5-pro`                            | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [DeepSeek](/providers/ai-sdk-providers/deepseek)                         | `deepseek-chat`                             | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [DeepSeek](/providers/ai-sdk-providers/deepseek)                         | `deepseek-reasoner`                         | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |\\n| [Cerebras](/providers/ai-sdk-providers/cerebras)                         | `llama3.1-8b`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Cerebras](/providers/ai-sdk-providers/cerebras)                         | `llama3.1-70b`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Cerebras](/providers/ai-sdk-providers/cerebras)                         | `llama3.3-70b`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `meta-llama/llama-4-scout-17b-16e-instruct` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `llama-3.3-70b-versatile`                   | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `llama-3.1-8b-instant`                      | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `mixtral-8x7b-32768`                        | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `gemma2-9b-it`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n\\n<Note>\\n  This table is not exhaustive. Additional models can be found in the provider\\n  documentation pages and on the provider websites.\\n</Note>\\n', children=[]), DocItem(origPath=Path('02-foundations/03-prompts.mdx'), name='03-prompts.mdx', displayName='03-prompts.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Prompts\\ndescription: Learn about the Prompt structure used in the AI SDK.\\n---\\n\\n# Prompts\\n\\nPrompts are instructions that you give a [large language model (LLM)](/docs/foundations/overview#large-language-models) to tell it what to do.\\nIt\\'s like when you ask someone for directions; the clearer your question, the better the directions you\\'ll get.\\n\\nMany LLM providers offer complex interfaces for specifying prompts. They involve different roles and message types.\\nWhile these interfaces are powerful, they can be hard to use and understand.\\n\\nIn order to simplify prompting, the AI SDK supports text, message, and system prompts.\\n\\n## Text Prompts\\n\\nText prompts are strings.\\nThey are ideal for simple generation use cases,\\ne.g. repeatedly generating content for variants of the same prompt text.\\n\\nYou can set text prompts using the `prompt` property made available by AI SDK functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text) or [`generateObject`](/docs/reference/ai-sdk-core/generate-object).\\nYou can structure the text in any way and inject variables, e.g. using a template literal.\\n\\n```ts highlight=\"3\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\nYou can also use template literals to provide dynamic data to your prompt.\\n\\n```ts highlight=\"3-5\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt:\\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\\n    `Please suggest the best tourist activities for me to do.`,\\n});\\n```\\n\\n## System Prompts\\n\\nSystem prompts are the initial set of instructions given to models that help guide and constrain the models\\' behaviors and responses.\\nYou can set system prompts using the `system` property.\\nSystem prompts work with both the `prompt` and the `messages` properties.\\n\\n```ts highlight=\"3-6\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system:\\n    `You help planning travel itineraries. ` +\\n    `Respond to the users\\' request with a list ` +\\n    `of the best stops to make in their destination.`,\\n  prompt:\\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\\n    `Please suggest the best tourist activities for me to do.`,\\n});\\n```\\n\\n<Note>\\n  When you use a message prompt, you can also use system messages instead of a\\n  system prompt.\\n</Note>\\n\\n## Message Prompts\\n\\nA message prompt is an array of user, assistant, and tool messages.\\nThey are great for chat interfaces and more complex, multi-modal prompts.\\nYou can use the `messages` property to set message prompts.\\n\\nEach message has a `role` and a `content` property. The content can either be text (for user and assistant messages), or an array of relevant parts (data) for that message type.\\n\\n```ts highlight=\"3-7\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Hi!\\' },\\n    { role: \\'assistant\\', content: \\'Hello, how can I help?\\' },\\n    { role: \\'user\\', content: \\'Where can I buy the best Currywurst in Berlin?\\' },\\n  ],\\n});\\n```\\n\\nInstead of sending a text in the `content` property, you can send an array of parts that includes a mix of text and other content parts.\\n\\n<Note type=\"warning\">\\n  Not all language models support all message and content types. For example,\\n  some models might not be capable of handling multi-modal inputs or tool\\n  messages. [Learn more about the capabilities of select\\n  models](./providers-and-models#model-capabilities).\\n</Note>\\n\\n### Provider Options\\n\\nYou can pass through additional provider-specific metadata to enable provider-specific functionality at 3 levels.\\n\\n#### Function Call Level\\n\\nFunctions like [`streamText`](/docs/reference/ai-sdk-core/stream-text#provider-options) or [`generateText`](/docs/reference/ai-sdk-core/generate-text#provider-options) accept a `providerOptions` property.\\n\\nAdding provider options at the function call level should be used when you do not need granular control over where the provider options are applied.\\n\\n```ts\\nconst { text } = await generateText({\\n  model: azure(\\'your-deployment-name\\'),\\n  providerOptions: {\\n    openai: {\\n      reasoningEffort: \\'low\\',\\n    },\\n  },\\n});\\n```\\n\\n#### Message Level\\n\\nFor granular control over applying provider options at the message level, you can pass `providerOptions` to the message object:\\n\\n```ts\\nimport { ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [\\n  {\\n    role: \\'system\\',\\n    content: \\'Cached system message\\',\\n    providerOptions: {\\n      // Sets a cache control breakpoint on the system message\\n      anthropic: { cacheControl: { type: \\'ephemeral\\' } },\\n    },\\n  },\\n];\\n```\\n\\n#### Message Part Level\\n\\nCertain provider-specific options require configuration at the message part level:\\n\\n```ts\\nimport { ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [\\n  {\\n    role: \\'user\\',\\n    content: [\\n      {\\n        type: \\'text\\',\\n        text: \\'Describe the image in detail.\\',\\n        providerOptions: {\\n          openai: { imageDetail: \\'low\\' },\\n        },\\n      },\\n      {\\n        type: \\'image\\',\\n        image:\\n          \\'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true\\',\\n        // Sets image detail configuration for image part:\\n        providerOptions: {\\n          openai: { imageDetail: \\'low\\' },\\n        },\\n      },\\n    ],\\n  },\\n];\\n```\\n\\n<Note type=\"warning\">\\n  AI SDK UI hooks like [`useChat`](/docs/reference/ai-sdk-ui/use-chat) return\\n  arrays of `UIMessage` objects, which do not support provider options. We\\n  recommend using the\\n  [`convertToModelMessages`](/docs/reference/ai-sdk-ui/convert-to-core-messages)\\n  function to convert `UIMessage` objects to\\n  [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) objects before\\n  applying or appending message(s) or message parts with `providerOptions`.\\n</Note>\\n\\n### User Messages\\n\\n#### Text Parts\\n\\nText content is the most common type of content. It is a string that is passed to the model.\\n\\nIf you only need to send text content in a message, the `content` property can be a string,\\nbut you can also use it to send multiple content parts.\\n\\n```ts highlight=\"7-10\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        {\\n          type: \\'text\\',\\n          text: \\'Where can I buy the best Currywurst in Berlin?\\',\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Image Parts\\n\\nUser messages can include image parts. An image can be one of the following:\\n\\n- base64-encoded image:\\n  - `string` with base-64 encoded content\\n  - data URL `string`, e.g. `data:image/png;base64,...`\\n- binary image:\\n  - `ArrayBuffer`\\n  - `Uint8Array`\\n  - `Buffer`\\n- URL:\\n  - http(s) URL `string`, e.g. `https://example.com/image.png`\\n  - `URL` object, e.g. `new URL(\\'https://example.com/image.png\\')`\\n\\n##### Example: Binary image (Buffer)\\n\\n```ts highlight=\"8-11\"\\nconst result = await generateText({\\n  model,\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'Describe the image in detail.\\' },\\n        {\\n          type: \\'image\\',\\n          image: fs.readFileSync(\\'./data/comic-cat.png\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n##### Example: Base-64 encoded image (string)\\n\\n```ts highlight=\"8-11\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'Describe the image in detail.\\' },\\n        {\\n          type: \\'image\\',\\n          image: fs.readFileSync(\\'./data/comic-cat.png\\').toString(\\'base64\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n##### Example: Image URL (string)\\n\\n```ts highlight=\"8-12\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'Describe the image in detail.\\' },\\n        {\\n          type: \\'image\\',\\n          image:\\n            \\'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true\\',\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### File Parts\\n\\n<Note type=\"warning\">\\n  Only a few providers and models currently support file parts: [Google\\n  Generative AI](/providers/ai-sdk-providers/google-generative-ai), [Google\\n  Vertex AI](/providers/ai-sdk-providers/google-vertex),\\n  [OpenAI](/providers/ai-sdk-providers/openai) (for `wav` and `mp3` audio with\\n  `gpt-4o-audio-preview`), [Anthropic](/providers/ai-sdk-providers/anthropic),\\n  [OpenAI](/providers/ai-sdk-providers/openai) (for `pdf`).\\n</Note>\\n\\nUser messages can include file parts. A file can be one of the following:\\n\\n- base64-encoded file:\\n  - `string` with base-64 encoded content\\n  - data URL `string`, e.g. `data:image/png;base64,...`\\n- binary data:\\n  - `ArrayBuffer`\\n  - `Uint8Array`\\n  - `Buffer`\\n- URL:\\n  - http(s) URL `string`, e.g. `https://example.com/some.pdf`\\n  - `URL` object, e.g. `new URL(\\'https://example.com/some.pdf\\')`\\n\\nYou need to specify the MIME type of the file you are sending.\\n\\n##### Example: PDF file from Buffer\\n\\n```ts highlight=\"12-15\"\\nimport { google } from \\'@ai-sdk/google\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: google(\\'gemini-1.5-flash\\'),\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'What is the file about?\\' },\\n        {\\n          type: \\'file\\',\\n          mediaType: \\'application/pdf\\',\\n          data: fs.readFileSync(\\'./data/example.pdf\\'),\\n          filename: \\'example.pdf\\', // optional, not used by all providers\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n##### Example: mp3 audio file from Buffer\\n\\n```ts highlight=\"12-14\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: openai(\\'gpt-4o-audio-preview\\'),\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'What is the audio saying?\\' },\\n        {\\n          type: \\'file\\',\\n          mediaType: \\'audio/mpeg\\',\\n          data: fs.readFileSync(\\'./data/galileo.mp3\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Custom Download Function (Experimental)\\n\\nYou can use custom download functions to implement throttling, retries, authentication, caching, and more.\\n\\nThe default download implementation automatically downloads files in parallel when they are not supported by the model.\\n\\nCustom download function can be passed via the `experimental_download` property:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  experimental_download: async (\\n    requestedDownloads: Array<{\\n      url: URL;\\n      isUrlSupportedByModel: boolean;\\n    }>,\\n  ): PromiseLike<\\n    Array<{\\n      data: Uint8Array;\\n      mediaType: string | undefined;\\n    } | null>\\n  > => {\\n    // ... download the files and return an array with similar order\\n  },\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        {\\n          type: \\'file\\',\\n          data: new URL(\\'https://api.company.com/private/document.pdf\\'),\\n          mediaType: \\'application/pdf\\',\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n<Note>\\n  The `experimental_download` option is experimental and may change in future\\n  releases.\\n</Note>\\n\\n### Assistant Messages\\n\\nAssistant messages are messages that have a role of `assistant`.\\nThey are typically previous responses from the assistant\\nand can contain text, reasoning, and tool call parts.\\n\\n#### Example: Assistant message with text content\\n\\n```ts highlight=\"5\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Hi!\\' },\\n    { role: \\'assistant\\', content: \\'Hello, how can I help?\\' },\\n  ],\\n});\\n```\\n\\n#### Example: Assistant message with text content in array\\n\\n```ts highlight=\"7\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Hi!\\' },\\n    {\\n      role: \\'assistant\\',\\n      content: [{ type: \\'text\\', text: \\'Hello, how can I help?\\' }],\\n    },\\n  ],\\n});\\n```\\n\\n#### Example: Assistant message with tool call content\\n\\n```ts highlight=\"7-14\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'How many calories are in this block of cheese?\\' },\\n    {\\n      role: \\'assistant\\',\\n      content: [\\n        {\\n          type: \\'tool-call\\',\\n          toolCallId: \\'12345\\',\\n          toolName: \\'get-nutrition-data\\',\\n          input: { cheese: \\'Roquefort\\' },\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Example: Assistant message with file content\\n\\n<Note>\\n  This content part is for model-generated files. Only a few models support\\n  this, and only for file types that they can generate.\\n</Note>\\n\\n```ts highlight=\"9-11\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Generate an image of a roquefort cheese!\\' },\\n    {\\n      role: \\'assistant\\',\\n      content: [\\n        {\\n          type: \\'file\\',\\n          mediaType: \\'image/png\\',\\n          data: fs.readFileSync(\\'./data/roquefort.jpg\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n### Tool messages\\n\\n<Note>\\n  [Tools](/docs/foundations/tools) (also known as function calling) are programs\\n  that you can provide an LLM to extend its built-in functionality. This can be\\n  anything from calling an external API to calling functions within your UI.\\n  Learn more about Tools in [the next section](/docs/foundations/tools).\\n</Note>\\n\\nFor models that support [tool](/docs/foundations/tools) calls, assistant messages can contain tool call parts, and tool messages can contain tool output parts.\\nA single assistant message can call multiple tools, and a single tool message can contain multiple tool results.\\n\\n```ts highlight=\"14-42\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        {\\n          type: \\'text\\',\\n          text: \\'How many calories are in this block of cheese?\\',\\n        },\\n        { type: \\'image\\', image: fs.readFileSync(\\'./data/roquefort.jpg\\') },\\n      ],\\n    },\\n    {\\n      role: \\'assistant\\',\\n      content: [\\n        {\\n          type: \\'tool-call\\',\\n          toolCallId: \\'12345\\',\\n          toolName: \\'get-nutrition-data\\',\\n          input: { cheese: \\'Roquefort\\' },\\n        },\\n        // there could be more tool calls here (parallel calling)\\n      ],\\n    },\\n    {\\n      role: \\'tool\\',\\n      content: [\\n        {\\n          type: \\'tool-result\\',\\n          toolCallId: \\'12345\\', // needs to match the tool call id\\n          toolName: \\'get-nutrition-data\\',\\n          output: {\\n            type: \\'json\\',\\n            value: {\\n              name: \\'Cheese, roquefort\\',\\n              calories: 369,\\n              fat: 31,\\n              protein: 22,\\n            },\\n          },\\n        },\\n        // there could be more tool results here (parallel calling)\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Multi-modal Tool Results\\n\\n<Note type=\"warning\">\\n  Multi-part tool results are experimental and only supported by Anthropic.\\n</Note>\\n\\nTool results can be multi-part and multi-modal, e.g. a text and an image.\\nYou can use the `experimental_content` property on tool parts to specify multi-part tool results.\\n\\n```ts highlight=\"24-46\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    // ...\\n    {\\n      role: \\'tool\\',\\n      content: [\\n        {\\n          type: \\'tool-result\\',\\n          toolCallId: \\'12345\\', // needs to match the tool call id\\n          toolName: \\'get-nutrition-data\\',\\n          // for models that do not support multi-part tool results,\\n          // you can include a regular output part:\\n          output: {\\n            type: \\'json\\',\\n            value: {\\n              name: \\'Cheese, roquefort\\',\\n              calories: 369,\\n              fat: 31,\\n              protein: 22,\\n            },\\n          },\\n        },\\n        {\\n          type: \\'tool-result\\',\\n          toolCallId: \\'12345\\', // needs to match the tool call id\\n          toolName: \\'get-nutrition-data\\',\\n          // for models that support multi-part tool results,\\n          // you can include a multi-part content part:\\n          output: {\\n            type: \\'content\\',\\n            value: [\\n              {\\n                type: \\'text\\',\\n                text: \\'Here is an image of the nutrition data for the cheese:\\',\\n              },\\n              {\\n                type: \\'media\\',\\n                data: fs\\n                  .readFileSync(\\'./data/roquefort-nutrition-data.png\\')\\n                  .toString(\\'base64\\'),\\n                mediaType: \\'image/png\\',\\n              },\\n            ],\\n          },\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n### System Messages\\n\\nSystem messages are messages that are sent to the model before the user messages to guide the assistant\\'s behavior.\\nYou can alternatively use the `system` property.\\n\\n```ts highlight=\"4\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'system\\', content: \\'You help planning travel itineraries.\\' },\\n    {\\n      role: \\'user\\',\\n      content:\\n        \\'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.\\',\\n    },\\n  ],\\n});\\n```\\n', children=[]), DocItem(origPath=Path('02-foundations/04-tools.mdx'), name='04-tools.mdx', displayName='04-tools.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Tools\\ndescription: Learn about tools with the AI SDK.\\n---\\n\\n# Tools\\n\\nWhile [large language models (LLMs)](/docs/foundations/overview#large-language-models) have incredible generation capabilities,\\nthey struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather).\\n\\nTools are actions that an LLM can invoke.\\nThe results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, when you ask an LLM for the \"weather in London\", and there is a weather tool available, it could call a tool\\nwith London as the argument. The tool would then fetch the weather data and return it to the LLM. The LLM can then use this\\ninformation in its response.\\n\\n## What is a tool?\\n\\nA tool is an object that can be called by the model to perform a specific task.\\nYou can use tools with [`generateText`](/docs/reference/ai-sdk-core/generate-text)\\nand [`streamText`](/docs/reference/ai-sdk-core/stream-text) by passing one or more tools to the `tools` parameter.\\n\\nA tool consists of three properties:\\n\\n- **`description`**: An optional description of the tool that can influence when the tool is picked.\\n- **`inputSchema`**: A [Zod schema](/docs/foundations/tools#schema-specification-and-validation-with-zod) or a [JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the input required for the tool to run. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\\n- **`execute`**: An optional async function that is called with the arguments from the tool call.\\n\\n<Note>\\n  `streamUI` uses UI generator tools with a `generate` function that can return\\n  React components.\\n</Note>\\n\\nIf the LLM decides to use a tool, it will generate a tool call.\\nTools with an `execute` function are run automatically when these calls are generated.\\nThe output of the tool calls are returned using tool result objects.\\n\\nYou can automatically pass tool results back to the LLM\\nusing [multi-step calls](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) with `streamText` and `generateText`.\\n\\n## Schemas\\n\\nSchemas are used to define the parameters for tools and to validate the [tool calls](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\nThe AI SDK supports both raw JSON schemas (using the [`jsonSchema` function](/docs/reference/ai-sdk-core/json-schema))\\nand [Zod](https://zod.dev/) schemas (either directly or using the [`zodSchema` function](/docs/reference/ai-sdk-core/zod-schema)).\\n\\n[Zod](https://zod.dev/) is a popular TypeScript schema validation library.\\nYou can install it with:\\n\\n<Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n  <Tab>\\n    <Snippet text=\"pnpm add zod\" dark />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"npm install zod\" dark />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"yarn add zod\" dark />\\n  </Tab>\\n\\n  <Tab>\\n    <Snippet text=\"bun add zod\" dark />\\n  </Tab>\\n</Tabs>\\n\\nYou can then specify a Zod schema, for example:\\n\\n```ts\\nimport z from \\'zod\\';\\n\\nconst recipeSchema = z.object({\\n  recipe: z.object({\\n    name: z.string(),\\n    ingredients: z.array(\\n      z.object({\\n        name: z.string(),\\n        amount: z.string(),\\n      }),\\n    ),\\n    steps: z.array(z.string()),\\n  }),\\n});\\n```\\n\\n<Note>\\n  You can also use schemas for structured output generation with\\n  [`generateObject`](/docs/reference/ai-sdk-core/generate-object) and\\n  [`streamObject`](/docs/reference/ai-sdk-core/stream-object).\\n</Note>\\n\\n## Tool Packages\\n\\nGiven tools are JavaScript objects, they can be packaged and distributed through npm like any other library. This makes it easy to share reusable tools across projects and with the community.\\n\\n### Using Ready-Made Tool Packages\\n\\nInstall a tool package and import the tools you need:\\n\\n```bash\\npnpm add some-tool-package\\n```\\n\\nThen pass them directly to `generateText`, `streamText`, or your agent definition:\\n\\n```ts highlight=\"2, 8\"\\nimport { generateText, stepCountIs } from \\'ai\\';\\nimport { searchTool } from \\'some-tool-package\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-haiku-4.5\\',\\n  prompt: \\'When was Vercel Ship AI?\\',\\n  tools: {\\n    webSearch: searchTool,\\n  },\\n  stopWhen: stepCountIs(10),\\n});\\n```\\n\\n### Publishing Your Own Tools\\n\\nYou can publish your own tool packages to npm for others to use. Simply export your tool objects from your package:\\n\\n```ts\\n// my-tools/index.ts\\nexport const myTool = {\\n  description: \\'A helpful tool\\',\\n  inputSchema: z.object({\\n    query: z.string(),\\n  }),\\n  execute: async ({ query }) => {\\n    // your tool logic\\n    return result;\\n  },\\n};\\n```\\n\\nAnyone can then install and use your tools by importing them.\\n\\nTo get started, you can use the [AI SDK Tool Package Template](https://github.com/vercel-labs/ai-sdk-tool-as-package-template) which provides a ready-to-use starting point for publishing your own tools.\\n\\n## Toolsets\\n\\nWhen you work with tools, you typically need a mix of application-specific tools and general-purpose tools. The community has created various toolsets and resources to help you build and use tools.\\n\\n### Ready-to-Use Tool Packages\\n\\nThese packages provide pre-built tools you can install and use immediately:\\n\\n- **[@exalabs/ai-sdk](https://www.npmjs.com/package/@exalabs/ai-sdk)** - Web search tool that lets AI search the web and get real-time information.\\n- **[@parallel-web/ai-sdk-tools](https://www.npmjs.com/package/@parallel-web/ai-sdk-tools)** - Web search and extract tools powered by Parallel Web API for real-time information and content extraction.\\n- **[Stripe agent tools](https://docs.stripe.com/agents?framework=vercel)** - Tools for interacting with Stripe.\\n- **[StackOne ToolSet](https://docs.stackone.com/agents/typescript/frameworks/vercel-ai-sdk)** - Agentic integrations for hundreds of [enterprise SaaS](https://www.stackone.com/integrations) platforms.\\n- **[agentic](https://docs.agentic.so/marketplace/ts-sdks/ai-sdk)** - A collection of 20+ tools that connect to external APIs such as [Exa](https://exa.ai/) or [E2B](https://e2b.dev/).\\n- **[AWS Bedrock AgentCore](https://github.com/aws/bedrock-agentcore-sdk-typescript)** - Fully managed AI agent services including [**Browser**](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html) (a fast and secure cloud-based browser runtime to enable agents to interact with web applications, fill forms, navigate websites, and extract information) and [**Code Interpreter**](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html) (an isolated sandbox environment for agents to execute code in Python, JavaScript, and TypeScript, enhancing accuracy and expanding ability to solve complex end-to-end tasks).\\n- **[Composio](https://docs.composio.dev/providers/vercel)** - 250+ tools like GitHub, Gmail, Salesforce and [more](https://composio.dev/tools).\\n- **[JigsawStack](http://www.jigsawstack.com/docs/integration/vercel)** - Over 30+ small custom fine-tuned models available for specific uses.\\n- **[AI Tools Registry](https://ai-tools-registry.vercel.app)** - A Shadcn-compatible tool definitions and components registry for the AI SDK.\\n- **[Toolhouse](https://docs.toolhouse.ai/toolhouse/toolhouse-sdk/using-vercel-ai)** - AI function-calling in 3 lines of code for over 25 different actions.\\n\\n### MCP Tools\\n\\nThese are pre-built tools available as MCP servers:\\n\\n- **[Smithery](https://smithery.ai/docs/integrations/vercel_ai_sdk)** - An open marketplace of 6,000+ MCPs, including [Browserbase](https://browserbase.com/) and [Exa](https://exa.ai/).\\n- **[Pipedream](https://pipedream.com/docs/connect/mcp/ai-frameworks/vercel-ai-sdk)** - Developer toolkit that lets you easily add 3,000+ integrations to your app or AI agent.\\n- **[Apify](https://docs.apify.com/platform/integrations/vercel-ai-sdk)** - Apify provides a [marketplace](https://apify.com/store) of thousands of tools for web scraping, data extraction, and browser automation.\\n\\n### Tool Building Tutorials\\n\\nThese tutorials and guides help you build your own tools that integrate with specific services:\\n\\n- **[browserbase](https://docs.browserbase.com/integrations/vercel/introduction#vercel-ai-integration)** - Tutorial for building browser tools that run a headless browser.\\n- **[browserless](https://docs.browserless.io/ai-integrations/vercel-ai-sdk)** - Guide for integrating browser automation (self-hosted or cloud-based).\\n- **[AI Tool Maker](https://github.com/nihaocami/ai-tool-maker)** - A CLI utility to generate AI SDK tools from OpenAPI specs.\\n- **[Interlify](https://www.interlify.com/docs/integrate-with-vercel-ai)** - Guide for converting APIs into tools.\\n- **[DeepAgent](https://deepagent.amardeep.space/docs/vercel-ai-sdk)** - A suite of 50+ AI tools and integrations, seamlessly connecting with APIs like Tavily, E2B, Airtable and [more](https://deepagent.amardeep.space/docs).\\n\\n<Note>\\n  Do you have open source tools or tool libraries that are compatible with the\\n  AI SDK? Please [file a pull request](https://github.com/vercel/ai/pulls) to\\n  add them to this list.\\n</Note>\\n\\n## Learn more\\n\\nThe AI SDK Core [Tool Calling](/docs/ai-sdk-core/tools-and-tool-calling)\\nand [Agents](/docs/foundations/agents) documentation has more information about tools and tool calling.\\n', children=[]), DocItem(origPath=Path('02-foundations/05-streaming.mdx'), name='05-streaming.mdx', displayName='05-streaming.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming\\ndescription: Why use streaming for AI applications?\\n---\\n\\n# Streaming\\n\\nStreaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.\\n\\n[Large language models (LLMs)](/docs/foundations/overview#large-language-models) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you\\'re likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by **displaying parts of the response as they become available**.\\n\\n<div className=\"grid lg:grid-cols-2 grid-cols-1 gap-4 mt-8\">\\n  <Card\\n    title=\"Blocking UI\"\\n    description=\"Blocking responses wait until the full response is available before displaying it.\"\\n  >\\n    <BrowserIllustration highlight blocking />\\n  </Card>\\n  <Card\\n    title=\"Streaming UI\"\\n    description=\"Streaming responses can transmit parts of the response as they become available.\"\\n  >\\n    <BrowserIllustration highlight />\\n  </Card>\\n</div>\\n\\n## Real-world Examples\\n\\nHere are 2 examples that illustrate how streaming UIs can improve user experiences in a real-world setting –\\xa0the first uses a blocking UI, while the second uses a streaming UI.\\n\\n### Blocking UI\\n\\n<InlinePrompt\\n  initialInput=\"Come up with the first 200 characters of the first book in the Harry Potter series.\"\\n  blocking\\n/>\\n\\n### Streaming UI\\n\\n<InlinePrompt initialInput=\"Come up with the first 200 characters of the first book in the Harry Potter series.\" />\\n\\nAs you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.\\n\\nWhile streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren\\'t always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.\\n\\nHowever, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible. In the example below, we stream text generation from OpenAI\\'s `gpt-4.1` in under 10 lines of code using the SDK\\'s [`streamText`](/docs/reference/ai-sdk-core/stream-text) function:\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\n\\nconst { textStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a poem about embedding models.\\',\\n});\\n\\nfor await (const textPart of textStream) {\\n  console.log(textPart);\\n}\\n```\\n\\nFor an introduction to streaming UIs and the AI SDK, check out our [Getting Started guides](/docs/getting-started).\\n', children=[]), DocItem(origPath=Path('02-foundations/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Foundations\\ndescription: A section that covers foundational knowledge around LLMs and concepts crucial to the AI SDK\\n---\\n\\n# Foundations\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description: 'Learn about foundational concepts around AI and LLMs.',\\n      href: '/docs/foundations/overview',\\n    },\\n    {\\n      title: 'Providers and Models',\\n      description:\\n        'Learn about the providers and models that you can use with the AI SDK.',\\n      href: '/docs/foundations/providers-and-models',\\n    },\\n    {\\n      title: 'Prompts',\\n      description:\\n        'Learn about how Prompts are used and defined in the AI SDK.',\\n      href: '/docs/foundations/prompts',\\n    },\\n    {\\n      title: 'Tools',\\n      description: 'Learn about tools in the AI SDK.',\\n      href: '/docs/foundations/tools',\\n    },\\n    {\\n      title: 'Streaming',\\n      description: 'Learn why streaming is used for AI applications.',\\n      href: '/docs/foundations/streaming',\\n    },\\n    {\\n      title: 'Agents',\\n      description: 'Learn how to build agents with the AI SDK.',\\n      href: '/docs/foundations/agents',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('02-getting-started'), name='02-getting-started', displayName='02-getting-started', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('02-getting-started/01-navigating-the-library.mdx'), name='01-navigating-the-library.mdx', displayName='01-navigating-the-library.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Navigating the Library\\ndescription: Learn how to navigate the AI SDK.\\n---\\n\\n# Navigating the Library\\n\\nThe AI SDK is a powerful toolkit for building AI applications. This page will help you pick the right tools for your requirements.\\n\\nLet’s start with a quick overview of the AI SDK, which is comprised of three parts:\\n\\n- **[AI SDK Core](/docs/ai-sdk-core/overview):**\\xa0A unified, provider agnostic API for generating text, structured objects, and tool calls with LLMs.\\n- **[AI SDK UI](/docs/ai-sdk-ui/overview):**\\xa0A set of framework-agnostic hooks for building chat and generative user interfaces.\\n- [AI SDK RSC](/docs/ai-sdk-rsc/overview):\\xa0Stream generative user interfaces with React Server Components (RSC). Development is currently experimental and we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview).\\n\\n## Choosing the Right Tool for Your Environment\\n\\nWhen deciding which part of the AI SDK to use, your first consideration should be the environment and existing stack you are working with. Different components of the SDK are tailored to specific frameworks and environments.\\n\\n| Library                                   | Purpose                                                                                                                                                                                                  | Environment Compatibility                                          |\\n| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |\\n| [AI SDK Core](/docs/ai-sdk-core/overview) | Call any LLM with unified API (e.g. [generateText](/docs/reference/ai-sdk-core/generate-text) and [generateObject](/docs/reference/ai-sdk-core/generate-object))                                         | Any JS environment (e.g. Node.js, Deno, Browser)                   |\\n| [AI SDK UI](/docs/ai-sdk-ui/overview)     | Build streaming chat and generative UIs (e.g. [useChat](/docs/reference/ai-sdk-ui/use-chat))                                                                                                             | React & Next.js, Vue & Nuxt, Svelte & SvelteKit                    |\\n| [AI SDK RSC](/docs/ai-sdk-rsc/overview)   | Stream generative UIs from Server to Client (e.g. [streamUI](/docs/reference/ai-sdk-rsc/stream-ui)). Development is currently experimental and we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview). | Any framework that supports React Server Components (e.g. Next.js) |\\n\\n## Environment Compatibility\\n\\nThese tools have been designed to work seamlessly with each other and it\\'s likely that you will be using them together. Let\\'s look at how you could decide which libraries to use based on your application environment, existing stack, and requirements.\\n\\nThe following table outlines AI SDK compatibility based on environment:\\n\\n| Environment           | [AI SDK Core](/docs/ai-sdk-core/overview) | [AI SDK UI](/docs/ai-sdk-ui/overview) | [AI SDK RSC](/docs/ai-sdk-rsc/overview) |\\n| --------------------- | ----------------------------------------- | ------------------------------------- | --------------------------------------- |\\n| None / Node.js / Deno | <Check size={18} />                       | <Cross size={18} />                   | <Cross size={18} />                     |\\n| Vue / Nuxt            | <Check size={18} />                       | <Check size={18} />                   | <Cross size={18} />                     |\\n| Svelte / SvelteKit    | <Check size={18} />                       | <Check size={18} />                   | <Cross size={18} />                     |\\n| Next.js Pages Router  | <Check size={18} />                       | <Check size={18} />                   | <Cross size={18} />                     |\\n| Next.js App Router    | <Check size={18} />                       | <Check size={18} />                   | <Check size={18} />                     |\\n\\n## When to use AI SDK UI\\n\\nAI SDK UI provides a set of framework-agnostic hooks for quickly building **production-ready AI-native applications**. It offers:\\n\\n- Full support for streaming chat and client-side generative UI\\n- Utilities for handling common AI interaction patterns (i.e. chat, completion, assistant)\\n- Production-tested reliability and performance\\n- Compatibility across popular frameworks\\n\\n## AI SDK UI Framework Compatibility\\n\\nAI SDK UI supports the following frameworks:\\xa0[React](https://react.dev/),\\xa0[Svelte](https://svelte.dev/),\\xa0and [Vue.js](https://vuejs.org/). Here is a comparison of the supported functions across these frameworks:\\n\\n| Function                                                   | React               | Svelte              | Vue.js              |\\n| ---------------------------------------------------------- | ------------------- | ------------------- | ------------------- |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)              | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat) tool calling | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |\\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion)  | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [useObject](/docs/reference/ai-sdk-ui/use-object)          | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |\\n\\n<Note>\\n  [Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\\n  welcome to implement missing features for non-React frameworks.\\n</Note>\\n\\n## When to use AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components)\\n(RSCs) provide a new approach to building React applications that allow components\\nto render on the server, fetch data directly, and stream the results to the client,\\nreducing bundle size and improving performance. They also introduce a new way to\\ncall server-side functions from anywhere in your application called [Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations).\\n\\nAI SDK RSC provides a number of utilities that allow you to stream values and UI directly from the server to the client. However, **it\\'s important to be aware of current limitations**:\\n\\n- **Cancellation**: currently, it is not possible to abort a stream using Server Actions. This will be improved in future releases of React and Next.js.\\n- **Increased Data Transfer**: using [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) can lead to quadratic data transfer (quadratic to the length of generated text). You can avoid this using [ `createStreamableValue` ](/docs/reference/ai-sdk-rsc/create-streamable-value) instead, and rendering the component client-side.\\n- **Re-mounting Issue During Streaming**: when using `createStreamableUI`, components re-mount on `.done()`, causing [flickering](https://github.com/vercel/ai/issues/2232).\\n\\nGiven these limitations, **we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production applications**.\\n', children=[]), DocItem(origPath=Path('02-getting-started/02-nextjs-app-router.mdx'), name='02-nextjs-app-router.mdx', displayName='02-nextjs-app-router.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Next.js App Router\\ndescription: Learn how to build your first agent with the AI SDK and Next.js App Router.\\n---\\n\\n# Next.js App Router Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Create Your Application\\n\\nStart by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.\\n\\n<div className=\"mb-4\">\\n  <Note>\\n    Be sure to select yes when prompted to use the App Router and Tailwind CSS.\\n    If you are looking for the Next.js Pages Router quickstart guide, you can\\n    find it [here](/docs/getting-started/nextjs-pages-router).\\n  </Note>\\n</div>\\n\\n<Snippet text=\"pnpm create next-app@latest my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK\\'s React hooks. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable.\\n</Note>\\n\\n## Create a Route Handler\\n\\nCreate a route handler, `app/api/chat/route.ts` and add the following code:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n2. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n3. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\\n4. Finally, return the result to the client to stream the response.\\n\\nThis Route Handler creates a POST request endpoint at `/api/chat`.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have a Route Handler that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`app/page.tsx`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n<Note>\\n  Make sure you add the `\"use client\"` directive to the top of your file. This\\n  allows you to add interactivity with Javascript.\\n</Note>\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:3000. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your Route Handler\\n\\nModify your `app/api/chat/route.ts` file to include the new weather tool:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"2,13-27\"\\nimport { streamText, UIMessage, convertToModelMessages, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocation in your UI, update your `app/page.tsx` file:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"16-21\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your Route Handler\\n\\nModify your `app/api/chat/route.ts` file to include the `stopWhen` condition:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You set `stopWhen` to be when `stepCountIs` 5, allowing the model to use up to 5 \"steps\" for any given generation.\\n2. You add an `onStepFinish` callback to log any `toolResults` from each step of the interaction, helping you understand the model\\'s tool usage. This means we can also delete the `toolCall` and `toolResult` `console.log` statements from the previous example.\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\\n\\n### Add another tool\\n\\nUpdate your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"34-47\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Update Your Frontend\\n\\nupdate your `app/page.tsx` file to render the new temperature conversion tool:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"21\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n              case \\'tool-convertFahrenheitToCelsius\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/03-nextjs-pages-router.mdx'), name='03-nextjs-pages-router.mdx', displayName='03-nextjs-pages-router.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Next.js Pages Router\\ndescription: Learn how to build your first agent with the AI SDK and Next.js Pages Router.\\n---\\n\\n# Next.js Pages Router Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Setup Your Application\\n\\nStart by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.\\n\\n<Note>\\n  Be sure to select no when prompted to use the App Router. If you are looking\\n  for the Next.js App Router quickstart guide, you can find it\\n  [here](/docs/getting-started/nextjs-app-router).\\n</Note>\\n\\n<Snippet text=\"pnpm create next-app@latest my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK\\'s React hooks. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable.\\n</Note>\\n\\n## Create a Route Handler\\n\\n<Note>\\n  As long as you are on Next.js 13+, you can use Route Handlers (using the App\\n  Router) alongside the Pages Router. This is recommended to enable you to use\\n  the Web APIs interface/signature and to better support streaming.\\n</Note>\\n\\nCreate a Route Handler (`app/api/chat/route.ts`) and add the following code:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n2. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n3. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\\n4. Finally, return the result to the client to stream the response.\\n\\nThis Route Handler creates a POST request endpoint at `/api/chat`.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui) package abstract the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`pages/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```tsx filename=\"pages/index.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:3000. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\n### Update Your Route Handler\\n\\nLet\\'s start by giving your chatbot a weather tool. Update your Route Handler (`app/api/chat/route.ts`):\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { streamText, UIMessage, convertToModelMessages, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocations in your UI, update your `pages/index.tsx` file:\\n\\n```tsx filename=\"pages/index.tsx\" highlight=\"16-21\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your Route Handler\\n\\nModify your `app/api/chat/route.ts` file to include the `stopWhen` condition:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\\n\\n### Add another tool\\n\\nUpdate your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"26-39\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Update Your Frontend\\n\\nUpdate your `pages/index.tsx` file to render the new temperature conversion tool:\\n\\n```tsx filename=\"pages/index.tsx\" highlight=\"21\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n              case \\'tool-convertFahrenheitToCelsius\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/04-svelte.mdx'), name='04-svelte.mdx', displayName='04-svelte.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Svelte\\ndescription: Learn how to build your first agent with the AI SDK and Svelte.\\n---\\n\\n# Svelte Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Set Up Your Application\\n\\nStart by creating a new SvelteKit application. This command will create a new directory named `my-ai-app` and set up a basic SvelteKit application inside it.\\n\\n<Snippet text=\"npx sv create my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install Dependencies\\n\\nInstall `ai` and `@ai-sdk/svelte`, the AI package and AI SDK\\'s Svelte bindings. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add -D ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install -D ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add -D ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"bun add -d ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable. Vite does not automatically load\\n  environment variables onto `process.env`, so you\\'ll need to import\\n  `AI_GATEWAY_API_KEY` from `$env/static/private` in your code (see below).\\n</Note>\\n\\n## Create an API route\\n\\nCreate a SvelteKit Endpoint, `src/routes/api/chat/+server.ts` and add the following code:\\n\\n```tsx filename=\"src/routes/api/chat/+server.ts\"\\nimport {\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  createGateway,\\n} from \\'ai\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n<Note>\\n  If you see type errors with `AI_GATEWAY_API_KEY` or your `POST` function, run\\n  the dev server.\\n</Note>\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Create a gateway provider instance with the `createGateway` function from the `ai` package.\\n2. Define a `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n3. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n4. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\\n5. Return the result to the client to stream the response.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to set up your frontend. The AI SDK\\'s [UI](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one class, `Chat`.\\nIts properties and API are largely the same as React\\'s [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`src/routes/+page.svelte`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```svelte filename=\"src/routes/+page.svelte\"\\n<script lang=\"ts\">\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let input = \\'\\';\\n  const chat = new Chat({});\\n\\n  function handleSubmit(event: SubmitEvent) {\\n    event.preventDefault();\\n    chat.sendMessage({ text: input });\\n    input = \\'\\';\\n  }\\n</script>\\n\\n<main>\\n  <ul>\\n    {#each chat.messages as message, messageIndex (messageIndex)}\\n      <li>\\n        <div>{message.role}</div>\\n        <div>\\n          {#each message.parts as part, partIndex (partIndex)}\\n            {#if part.type === \\'text\\'}\\n              <div>{part.text}</div>\\n            {/if}\\n          {/each}\\n        </div>\\n      </li>\\n    {/each}\\n  </ul>\\n  <form onsubmit={handleSubmit}>\\n    <input bind:value={input} />\\n    <button type=\"submit\">Send</button>\\n  </form>\\n</main>\\n```\\n\\nThis page utilizes the `Chat` class, which will, by default, use the `POST` route handler you created earlier. The class provides functions and state for handling user input and form submission. The `Chat` class provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:5173. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Svelte.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your API Route\\n\\nModify your `src/routes/api/chat/+server.ts` file to include the new weather tool:\\n\\n```tsx filename=\"src/routes/api/chat/+server.ts\" highlight=\"2,3,17-31\"\\nimport {\\n  createGateway,\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocation in your UI, update your `src/routes/+page.svelte` file:\\n\\n```svelte filename=\"src/routes/+page.svelte\"\\n<script lang=\"ts\">\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let input = \\'\\';\\n  const chat = new Chat({});\\n\\n  function handleSubmit(event: SubmitEvent) {\\n    event.preventDefault();\\n    chat.sendMessage({ text: input });\\n    input = \\'\\';\\n  }\\n</script>\\n\\n<main>\\n  <ul>\\n    {#each chat.messages as message, messageIndex (messageIndex)}\\n      <li>\\n        <div>{message.role}</div>\\n        <div>\\n          {#each message.parts as part, partIndex (partIndex)}\\n            {#if part.type === \\'text\\'}\\n              <div>{part.text}</div>\\n            {:else if part.type === \\'tool-weather\\'}\\n              <pre>{JSON.stringify(part, null, 2)}</pre>\\n            {/if}\\n          {/each}\\n        </div>\\n      </li>\\n    {/each}\\n  </ul>\\n  <form onsubmit={handleSubmit}>\\n    <input bind:value={input} />\\n    <button type=\"submit\">Send</button>\\n  </form>\\n</main>\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your API Route\\n\\nModify your `src/routes/api/chat/+server.ts` file to include the `stopWhen` condition:\\n\\n```ts filename=\"src/routes/api/chat/+server.ts\" highlight=\"15\"\\nimport {\\n  createGateway,\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\\n\\n### Add another tool\\n\\nUpdate your `src/routes/api/chat/+server.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"src/routes/api/chat/+server.ts\" highlight=\"32-45\"\\nimport {\\n  createGateway,\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Update Your Frontend\\n\\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\\n\\n```svelte filename=\"src/routes/+page.svelte\" highlight=\"17\"\\n<script lang=\"ts\">\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let input = \\'\\';\\n  const chat = new Chat({});\\n\\n  function handleSubmit(event: SubmitEvent) {\\n    event.preventDefault();\\n    chat.sendMessage({ text: input });\\n    input = \\'\\';\\n  }\\n</script>\\n\\n<main>\\n  <ul>\\n    {#each chat.messages as message, messageIndex (messageIndex)}\\n      <li>\\n        <div>{message.role}</div>\\n        <div>\\n          {#each message.parts as part, partIndex (partIndex)}\\n            {#if part.type === \\'text\\'}\\n              <div>{part.text}</div>\\n            {:else if part.type === \\'tool-weather\\' || part.type === \\'tool-convertFahrenheitToCelsius\\'}\\n              <pre>{JSON.stringify(part, null, 2)}</pre>\\n            {/if}\\n          {/each}\\n        </div>\\n      </li>\\n    {/each}\\n  </ul>\\n  <form onsubmit={handleSubmit}>\\n    <input bind:value={input} />\\n    <button type=\"submit\">Send</button>\\n  </form>\\n</main>\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## How does `@ai-sdk/svelte` differ from `@ai-sdk/react`?\\n\\nThe surface-level difference is that Svelte uses classes to manage state, whereas React uses hooks, so `useChat` in React is `Chat` in Svelte. Other than that, there are a few things to keep in mind:\\n\\n### 1. Arguments to classes aren\\'t reactive by default\\n\\nUnlike in React, where hooks are rerun any time their containing component is invalidated, code in the `script` block of a Svelte component is only run once when the component is created.\\nThis means that, if you want arguments to your class to be reactive, you need to make sure you pass a _reference_ into the class, rather than a value:\\n\\n```svelte\\n<script>\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let { id } = $props();\\n\\n  // won\\'t work; the class instance will be created once, `id` will be copied by value, and won\\'t update when $props.id changes\\n  let chat = new Chat({ id });\\n\\n  // will work; passes `id` by reference, so `Chat` always has the latest value\\n  let chat = new Chat({\\n    get id() {\\n      return id;\\n    },\\n  });\\n</script>\\n```\\n\\nKeep in mind that this normally doesn\\'t matter; most parameters you\\'ll pass into the Chat class are static (for example, you typically wouldn\\'t expect your `onError` handler to change).\\n\\n### 2. You can\\'t destructure class properties\\n\\nIn vanilla JavaScript, destructuring class properties copies them by value and \"disconnects\" them from their class instance:\\n\\n```js\\nconst classInstance = new Whatever();\\nclassInstance.foo = \\'bar\\';\\nconst { foo } = classInstance;\\nclassInstance.foo = \\'baz\\';\\n\\nconsole.log(foo); // \\'bar\\'\\n```\\n\\nThe same is true of classes in Svelte:\\n\\n```svelte\\n<script>\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  const chat = new Chat({});\\n  let { messages } = chat;\\n\\n  chat.append({ content: \\'Hello, world!\\', role: \\'user\\' }).then(() => {\\n    console.log(messages); // []\\n    console.log(chat.messages); // [{ content: \\'Hello, world!\\', role: \\'user\\' }] (plus some other stuff)\\n  });\\n</script>\\n```\\n\\n### 3. Instance synchronization requires context\\n\\nIn React, hook instances with the same `id` are synchronized -- so two instances of `useChat` will have the same `messages`, `status`, etc. if they have the same `id`.\\nFor most use cases, you probably don\\'t need this behavior -- but if you do, you can create a context in your root layout file using `createAIContext`:\\n\\n```svelte\\n<script>\\n  import { createAIContext } from \\'@ai-sdk/svelte\\';\\n\\n  let { children } = $props();\\n\\n  createAIContext();\\n  // all hooks created after this or in components that are children of this component\\n  // will have synchronized state\\n</script>\\n\\n{@render children()}\\n```\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n- To learn more about Svelte, check out the [official documentation](https://svelte.dev/docs/svelte).\\n', children=[]), DocItem(origPath=Path('02-getting-started/05-nuxt.mdx'), name='05-nuxt.mdx', displayName='05-nuxt.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Vue.js (Nuxt)\\ndescription: Learn how to build your first agent with the AI SDK and Vue.js (Nuxt).\\n---\\n\\n# Vue.js (Nuxt) Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Setup Your Application\\n\\nStart by creating a new Nuxt application. This command will create a new directory named `my-ai-app` and set up a basic Nuxt application inside it.\\n\\n<Snippet text=\"pnpm create nuxt my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/vue`. The Vercel AI Gateway provider ships with the `ai` package.\\n\\n<Note>\\n  The AI SDK is designed to be a unified interface to interact with any large\\n  language model. This means that you can change model and providers with just\\n  one line of code! Learn more about [available providers](/providers) and\\n  [building custom providers](/providers/community-providers/custom-providers)\\n  in the [providers](/providers) section.\\n</Note>\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n### Configure Vercel AI Gateway API key\\n\\nCreate a `.env` file in your project root and add your Vercel AI Gateway API Key. This key is used to authenticate your application with the Vercel AI Gateway service.\\n\\n<Snippet text=\"touch .env\" />\\n\\nEdit the `.env` file:\\n\\n```env filename=\".env\"\\nNUXT_AI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key and configure the environment variable in `nuxt.config.ts`:\\n\\n```ts filename=\"nuxt.config.ts\"\\nexport default defineNuxtConfig({\\n  // rest of your nuxt config\\n  runtimeConfig: {\\n    aiGatewayApiKey: \\'\\',\\n  },\\n});\\n```\\n\\n<Note className=\"mb-4\">\\n  This guide uses Nuxt\\'s runtime config to manage the API key. The `NUXT_`\\n  prefix in the environment variable allows Nuxt to automatically load it into\\n  the runtime config. While the AI Gateway Provider also supports a default\\n  `AI_GATEWAY_API_KEY` environment variable, this approach provides better\\n  integration with Nuxt\\'s configuration system.\\n</Note>\\n\\n## Create an API route\\n\\nCreate an API route, `server/api/chat.ts` and add the following code:\\n\\n```typescript filename=\"server/api/chat.ts\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  createGateway,\\n} from \\'ai\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Create a gateway provider instance with the `createGateway` function from the `ai` package.\\n2. Define an Event Handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n3. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n4. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\\n5. Return the result to the client to stream the response.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui/overview) package abstract the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`pages/index.vue`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```typescript filename=\"pages/index.vue\"\\n<script setup lang=\"ts\">\\nimport { Chat } from \"@ai-sdk/vue\";\\nimport { ref } from \"vue\";\\n\\nconst input = ref(\"\");\\nconst chat = new Chat({});\\n\\nconst handleSubmit = (e: Event) => {\\n    e.preventDefault();\\n    chat.sendMessage({ text: input.value });\\n    input.value = \"\";\\n};\\n</script>\\n\\n<template>\\n    <div>\\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\\n            <div\\n                v-for=\"(part, index) in m.parts\"\\n                :key=\"`${m.id}-${part.type}-${index}`\"\\n            >\\n                <div v-if=\"part.type === \\'text\\'\">{{ part.text }}</div>\\n            </div>\\n        </div>\\n\\n        <form @submit=\"handleSubmit\">\\n            <input v-model=\"input\" placeholder=\"Say something...\" />\\n        </form>\\n    </div>\\n</template>\\n```\\n\\n<Note>\\n  If your project has `app.vue` instead of `pages/index.vue`, delete the\\n  `app.vue` file and create a new `pages/index.vue` file with the code above.\\n</Note>\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`ref`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:3000. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Nuxt.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your API Route\\n\\nModify your `server/api/chat.ts` file to include the new weather tool:\\n\\n```typescript filename=\"server/api/chat.ts\" highlight=\"1,16-32\"\\nimport {\\n  createGateway,\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocation in your UI, update your `pages/index.vue` file:\\n\\n```typescript filename=\"pages/index.vue\" highlight=\"16-18\"\\n<script setup lang=\"ts\">\\nimport { Chat } from \"@ai-sdk/vue\";\\nimport { ref } from \"vue\";\\n\\nconst input = ref(\"\");\\nconst chat = new Chat({});\\n\\nconst handleSubmit = (e: Event) => {\\n    e.preventDefault();\\n    chat.sendMessage({ text: input.value });\\n    input.value = \"\";\\n};\\n</script>\\n\\n<template>\\n    <div>\\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\\n            <div\\n                v-for=\"(part, index) in m.parts\"\\n                :key=\"`${m.id}-${part.type}-${index}`\"\\n            >\\n                <div v-if=\"part.type === \\'text\\'\">{{ part.text }}</div>\\n                <pre v-if=\"part.type === \\'tool-weather\\'\">{{ JSON.stringify(part, null, 2) }}</pre>\\n            </div>\\n        </div>\\n\\n        <form @submit=\"handleSubmit\">\\n            <input v-model=\"input\" placeholder=\"Say something...\" />\\n        </form>\\n    </div>\\n</template>\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your API Route\\n\\nModify your `server/api/chat.ts` file to include the `stopWhen` condition:\\n\\n```typescript filename=\"server/api/chat.ts\" highlight=\"22\"\\nimport {\\n  createGateway,\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n      stopWhen: stepCountIs(5),\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\\n\\n### Add another tool\\n\\nUpdate your `server/api/chat.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```typescript filename=\"server/api/chat.ts\" highlight=\"32-45\"\\nimport {\\n  createGateway,\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n      stopWhen: stepCountIs(5),\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n        convertFahrenheitToCelsius: tool({\\n          description: \\'Convert a temperature in fahrenheit to celsius\\',\\n          inputSchema: z.object({\\n            temperature: z\\n              .number()\\n              .describe(\\'The temperature in fahrenheit to convert\\'),\\n          }),\\n          execute: async ({ temperature }) => {\\n            const celsius = Math.round((temperature - 32) * (5 / 9));\\n            return {\\n              celsius,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\n### Update Your Frontend\\n\\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\\n\\n```typescript filename=\"pages/index.vue\" highlight=\"24\"\\n<script setup lang=\"ts\">\\nimport { Chat } from \"@ai-sdk/vue\";\\nimport { ref } from \"vue\";\\n\\nconst input = ref(\"\");\\nconst chat = new Chat({});\\n\\nconst handleSubmit = (e: Event) => {\\n    e.preventDefault();\\n    chat.sendMessage({ text: input.value });\\n    input.value = \"\";\\n};\\n</script>\\n\\n<template>\\n    <div>\\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\\n            <div\\n                v-for=\"(part, index) in m.parts\"\\n                :key=\"`${m.id}-${part.type}-${index}`\"\\n            >\\n                <div v-if=\"part.type === \\'text\\'\">{{ part.text }}</div>\\n                <pre\\n                    v-if=\"\\n                        part.type === \\'tool-weather\\' ||\\n                        part.type === \\'tool-convertFahrenheitToCelsius\\'\\n                    \"\\n                    >{{ JSON.stringify(part, null, 2) }}</pre\\n                >\\n            </div>\\n        </div>\\n\\n        <form @submit=\"handleSubmit\">\\n            <input v-model=\"input\" placeholder=\"Say something...\" />\\n        </form>\\n    </div>\\n</template>\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/06-nodejs.mdx'), name='06-nodejs.mdx', displayName='06-nodejs.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Node.js\\ndescription: Learn how to build your first agent with the AI SDK and Node.js.\\n---\\n\\n# Node.js Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Setup Your Application\\n\\nStart by creating a new directory using the `mkdir` command. Change into your new directory and then run the `pnpm init` command. This will create a `package.json` in your new directory.\\n\\n```bash\\nmkdir my-ai-app\\ncd my-ai-app\\npnpm init\\n```\\n\\n### Install Dependencies\\n\\nInstall `ai`, the AI SDK, along with other necessary dependencies.\\n\\n<Note>\\n  The AI SDK is designed to be a unified interface to interact with any large\\n  language model. This means that you can change model and providers with just\\n  one line of code! Learn more about [available providers](/providers) and\\n  [building custom providers](/providers/community-providers/custom-providers)\\n  in the [providers](/providers) section.\\n</Note>\\n\\n```bash\\npnpm add ai@beta zod dotenv\\npnpm add -D @types/node tsx typescript\\n```\\n\\nThe `ai` package contains the AI SDK. You will use `zod` to define type-safe schemas that you will pass to the large language model (LLM). You will use `dotenv` to access environment variables (your Vercel AI Gateway key) within your application. There are also three development dependencies, installed with the `-D` flag, that are necessary to run your Typescript code.\\n\\n### Configure Vercel AI Gateway API key\\n\\nCreate a `.env` file in your project\\'s root directory and add your Vercel AI Gateway API Key. This key is used to authenticate your application with the Vercel AI Gateway service.\\n\\n<Snippet text=\"touch .env\" />\\n\\nEdit the `.env` file:\\n\\n```env filename=\".env\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK will use the `AI_GATEWAY_API_KEY` environment variable to\\n  authenticate with Vercel AI Gateway.\\n</Note>\\n\\n## Create Your Application\\n\\nCreate an `index.ts` file in the root of your project and add the following code:\\n\\n```ts filename=\"index.ts\"\\nimport { ModelMessage, streamText } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Set up a readline interface to take input from the terminal, enabling interactive sessions directly from the command line.\\n2. Initialize an array called `messages` to store the history of your conversation. This history allows the agent to maintain context in ongoing dialogues.\\n3. In the `main` function:\\n\\n- Prompt for and capture user input, storing it in `userInput`.\\n- Add user input to the `messages` array as a user message.\\n- Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages`.\\n- Iterate over the text stream returned by the `streamText` function (`result.textStream`) and print the contents of the stream to the terminal.\\n- Add the assistant\\'s response to the `messages` array.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your agent! To start your application, use the command:\\n\\n<Snippet text=\"pnpm tsx index.ts\" />\\n\\nYou should see a prompt in your terminal. Test it out by entering a message and see the AI agent respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Node.js.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n## Enhance Your Agent with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the agent would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your agent by adding a simple weather tool.\\n\\n### Update Your Application\\n\\nModify your `index.ts` file to include the new weather tool:\\n\\n```ts filename=\"index.ts\" highlight=\"2,4,24-37\"\\nimport { ModelMessage, streamText, tool } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the agent understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The agent will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your agent can \"fetch\" weather information for any location the user asks about. When the agent determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and the results will be used by the agent to generate its response.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the agent uses the new tool.\\n\\nNotice the blank \"assistant\" response? This is because instead of generating a text response, the agent generated a tool call. You can access the tool call and subsequent tool result in the `toolCall` and `toolResult` keys of the result object.\\n\\n```typescript highlight=\"46-47\"\\nimport { ModelMessage, streamText, tool } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    console.log(await result.toolCalls);\\n    console.log(await result.toolResults);\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool results are visible in the chat interface, the agent isn\\'t using this information to answer your original query. This is because once the agent generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. This feature will automatically send tool results back to the agent to trigger an additional generation until the stopping condition you define is met. In this case, you want the agent to answer your question using the results from the weather tool.\\n\\n### Update Your Application\\n\\nModify your `index.ts` file to configure stopping conditions with `stopWhen`:\\n\\n```ts filename=\"index.ts\" highlight=\"38-41\"\\nimport { ModelMessage, streamText, tool, stepCountIs } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n      stopWhen: stepCountIs(5),\\n      onStepFinish: async ({ toolResults }) => {\\n        if (toolResults.length) {\\n          console.log(JSON.stringify(toolResults, null, 2));\\n        }\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nIn this updated code:\\n\\n1. You set `stopWhen` to be when `stepCountIs` 5, allowing the agent to use up to 5 \"steps\" for any given generation.\\n2. You add an `onStepFinish` callback to log any `toolResults` from each step of the interaction, helping you understand the agent\\'s tool usage. This means we can also delete the `toolCall` and `toolResult` `console.log` statements from the previous example.\\n\\nNow, when you ask about the weather in a location, you should see the agent using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the agent to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the agent to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\\n\\n### Adding a second tool\\n\\nUpdate your `index.ts` file to add a new tool to convert the temperature from Celsius to Fahrenheit:\\n\\n```ts filename=\"index.ts\" highlight=\"37-48\"\\nimport { ModelMessage, streamText, tool, stepCountIs } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n        convertFahrenheitToCelsius: tool({\\n          description: \\'Convert a temperature in fahrenheit to celsius\\',\\n          inputSchema: z.object({\\n            temperature: z\\n              .number()\\n              .describe(\\'The temperature in fahrenheit to convert\\'),\\n          }),\\n          execute: async ({ temperature }) => {\\n            const celsius = Math.round((temperature - 32) * (5 / 9));\\n            return {\\n              celsius,\\n            };\\n          },\\n        }),\\n      },\\n      stopWhen: stepCountIs(5),\\n      onStepFinish: async ({ toolResults }) => {\\n        if (toolResults.length) {\\n          console.log(JSON.stringify(toolResults, null, 2));\\n        }\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The agent will call the weather tool for New York.\\n2. You\\'ll see the tool result logged.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The agent will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the agent to gather information and use it to provide more accurate and contextual responses, making your agent considerably more useful.\\n\\nThis example demonstrates how tools can expand your agent\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the agent to access and process real-world data in real-time and perform actions that interact with the outside world. Tools bridge the gap between the agent\\'s knowledge cutoff and current information, while also enabling it to take meaningful actions beyond just generating text responses.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI agent using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/07-expo.mdx'), name='07-expo.mdx', displayName='07-expo.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Expo\\ndescription: Learn how to build your first agent with the AI SDK and Expo.\\n---\\n\\n# Expo Quickstart\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface with [Expo](https://expo.dev/). Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Create Your Application\\n\\nStart by creating a new Expo application. This command will create a new directory named `my-ai-app` and set up a basic Expo application inside it.\\n\\n<Snippet text=\"pnpm create expo-app@latest my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n<Note>This guide requires Expo 52 or higher.</Note>\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK\\'s React hooks. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable.\\n</Note>\\n\\n## Create an API Route\\n\\nCreate a route handler, `app/api/chat+api.ts` and add the following code:\\n\\n```tsx filename=\"app/api/chat+api.ts\"\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.\\n2. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `ai`) and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour.\\n3. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\\n4. Finally, return the result to the client to stream the response.\\n\\nThis API route creates a POST request endpoint at `/api/chat`.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`app/(tabs)/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```tsx filename=\"app/(tabs)/index.tsx\"\\nimport { generateAPIUrl } from \\'@/utils\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { fetch as expoFetch } from \\'expo/fetch\\';\\nimport { useState } from \\'react\\';\\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from \\'react-native\\';\\n\\nexport default function App() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, error, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\\n      api: generateAPIUrl(\\'/api/chat\\'),\\n    }),\\n    onError: error => console.error(error, \\'ERROR\\'),\\n  });\\n\\n  if (error) return <Text>{error.message}</Text>;\\n\\n  return (\\n    <SafeAreaView style={{ height: \\'100%\\' }}>\\n      <View\\n        style={{\\n          height: \\'95%\\',\\n          display: \\'flex\\',\\n          flexDirection: \\'column\\',\\n          paddingHorizontal: 8,\\n        }}\\n      >\\n        <ScrollView style={{ flex: 1 }}>\\n          {messages.map(m => (\\n            <View key={m.id} style={{ marginVertical: 8 }}>\\n              <View>\\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\\n                {m.parts.map((part, i) => {\\n                  switch (part.type) {\\n                    case \\'text\\':\\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\\n                  }\\n                })}\\n              </View>\\n            </View>\\n          ))}\\n        </ScrollView>\\n\\n        <View style={{ marginTop: 8 }}>\\n          <TextInput\\n            style={{ backgroundColor: \\'white\\', padding: 8 }}\\n            placeholder=\"Say something...\"\\n            value={input}\\n            onChange={e => setInput(e.nativeEvent.text)}\\n            onSubmitEditing={e => {\\n              e.preventDefault();\\n              sendMessage({ text: input });\\n              setInput(\\'\\');\\n            }}\\n            autoFocus={true}\\n          />\\n        </View>\\n      </View>\\n    </SafeAreaView>\\n  );\\n}\\n```\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n<Note>\\n  You use the expo/fetch function instead of the native node fetch to enable\\n  streaming of chat responses. This requires Expo 52 or higher.\\n</Note>\\n\\n### Create the API URL Generator\\n\\nBecause you\\'re using expo/fetch for streaming responses instead of the native fetch function, you\\'ll need an API URL generator to ensure you are using the correct base url and format depending on the client environment (e.g. web or mobile). Create a new file called `utils.ts` in the root of your project and add the following code:\\n\\n```ts filename=\"utils.ts\"\\nimport Constants from \\'expo-constants\\';\\n\\nexport const generateAPIUrl = (relativePath: string) => {\\n  const origin = Constants.experienceUrl.replace(\\'exp://\\', \\'http://\\');\\n\\n  const path = relativePath.startsWith(\\'/\\') ? relativePath : `/${relativePath}`;\\n\\n  if (process.env.NODE_ENV === \\'development\\') {\\n    return origin.concat(path);\\n  }\\n\\n  if (!process.env.EXPO_PUBLIC_API_BASE_URL) {\\n    throw new Error(\\n      \\'EXPO_PUBLIC_API_BASE_URL environment variable is not defined\\',\\n    );\\n  }\\n\\n  return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);\\n};\\n```\\n\\nThis utility function handles URL generation for both development and production environments, ensuring your API calls work correctly across different devices and configurations.\\n\\n<Note>\\n  Before deploying to production, you must set the `EXPO_PUBLIC_API_BASE_URL`\\n  environment variable in your production environment. This variable should\\n  point to the base URL of your API server.\\n</Note>\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm expo\" />\\n\\nHead to your browser and open http://localhost:8081. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Expo.\\n\\n<Note>\\n  If you experience \"Property `structuredClone` doesn\\'t exist\" errors on mobile,\\n  add the [polyfills described below](#polyfills).\\n</Note>\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your API route\\n\\nModify your `app/api/chat+api.ts` file to include the new weather tool:\\n\\n```tsx filename=\"app/api/chat+api.ts\" highlight=\"2,11-25\"\\nimport { streamText, UIMessage, convertToModelMessages, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the weather tool invocation in your UI, update your `app/(tabs)/index.tsx` file:\\n\\n```tsx filename=\"app/(tabs)/index.tsx\" highlight=\"31-35\"\\nimport { generateAPIUrl } from \\'@/utils\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { fetch as expoFetch } from \\'expo/fetch\\';\\nimport { useState } from \\'react\\';\\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from \\'react-native\\';\\n\\nexport default function App() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, error, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\\n      api: generateAPIUrl(\\'/api/chat\\'),\\n    }),\\n    onError: error => console.error(error, \\'ERROR\\'),\\n  });\\n\\n  if (error) return <Text>{error.message}</Text>;\\n\\n  return (\\n    <SafeAreaView style={{ height: \\'100%\\' }}>\\n      <View\\n        style={{\\n          height: \\'95%\\',\\n          display: \\'flex\\',\\n          flexDirection: \\'column\\',\\n          paddingHorizontal: 8,\\n        }}\\n      >\\n        <ScrollView style={{ flex: 1 }}>\\n          {messages.map(m => (\\n            <View key={m.id} style={{ marginVertical: 8 }}>\\n              <View>\\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\\n                {m.parts.map((part, i) => {\\n                  switch (part.type) {\\n                    case \\'text\\':\\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\\n                    case \\'tool-weather\\':\\n                      return (\\n                        <Text key={`${m.id}-${i}`}>\\n                          {JSON.stringify(part, null, 2)}\\n                        </Text>\\n                      );\\n                  }\\n                })}\\n              </View>\\n            </View>\\n          ))}\\n        </ScrollView>\\n\\n        <View style={{ marginTop: 8 }}>\\n          <TextInput\\n            style={{ backgroundColor: \\'white\\', padding: 8 }}\\n            placeholder=\"Say something...\"\\n            value={input}\\n            onChange={e => setInput(e.nativeEvent.text)}\\n            onSubmitEditing={e => {\\n              e.preventDefault();\\n              sendMessage({ text: input });\\n              setInput(\\'\\');\\n            }}\\n            autoFocus={true}\\n          />\\n        </View>\\n      </View>\\n    </SafeAreaView>\\n  );\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool results are visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your API Route\\n\\nModify your `app/api/chat+api.ts` file to include the `stopWhen` condition:\\n\\n```tsx filename=\"app/api/chat+api.ts\" highlight=\"10\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nHead back to the Expo app and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\\n\\n### Add More Tools\\n\\nUpdate your `app/api/chat+api.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"app/api/chat+api.ts\" highlight=\"28-41\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\n### Update the UI for the new tool\\n\\nTo display the temperature conversion tool invocation in your UI, update your `app/(tabs)/index.tsx` file to handle the new tool part:\\n\\n```tsx filename=\"app/(tabs)/index.tsx\" highlight=\"37-42\"\\nimport { generateAPIUrl } from \\'@/utils\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { fetch as expoFetch } from \\'expo/fetch\\';\\nimport { useState } from \\'react\\';\\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from \\'react-native\\';\\n\\nexport default function App() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, error, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\\n      api: generateAPIUrl(\\'/api/chat\\'),\\n    }),\\n    onError: error => console.error(error, \\'ERROR\\'),\\n  });\\n\\n  if (error) return <Text>{error.message}</Text>;\\n\\n  return (\\n    <SafeAreaView style={{ height: \\'100%\\' }}>\\n      <View\\n        style={{\\n          height: \\'95%\\',\\n          display: \\'flex\\',\\n          flexDirection: \\'column\\',\\n          paddingHorizontal: 8,\\n        }}\\n      >\\n        <ScrollView style={{ flex: 1 }}>\\n          {messages.map(m => (\\n            <View key={m.id} style={{ marginVertical: 8 }}>\\n              <View>\\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\\n                {m.parts.map((part, i) => {\\n                  switch (part.type) {\\n                    case \\'text\\':\\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\\n                    case \\'tool-weather\\':\\n                    case \\'tool-convertFahrenheitToCelsius\\':\\n                      return (\\n                        <Text key={`${m.id}-${i}`}>\\n                          {JSON.stringify(part, null, 2)}\\n                        </Text>\\n                      );\\n                  }\\n                })}\\n              </View>\\n            </View>\\n          ))}\\n        </ScrollView>\\n\\n        <View style={{ marginTop: 8 }}>\\n          <TextInput\\n            style={{ backgroundColor: \\'white\\', padding: 8 }}\\n            placeholder=\"Say something...\"\\n            value={input}\\n            onChange={e => setInput(e.nativeEvent.text)}\\n            onSubmitEditing={e => {\\n              e.preventDefault();\\n              sendMessage({ text: input });\\n              setInput(\\'\\');\\n            }}\\n            autoFocus={true}\\n          />\\n        </View>\\n      </View>\\n    </SafeAreaView>\\n  );\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool result displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Polyfills\\n\\nSeveral functions that are internally used by the AI SDK might not available in the Expo runtime depending on your configuration and the target platform.\\n\\nFirst, install the following packages:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet\\n        text=\"pnpm add @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n    <Tab>\\n      <Snippet\\n        text=\"npm install @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n    <Tab>\\n      <Snippet\\n        text=\"yarn add @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n    <Tab>\\n      <Snippet\\n        text=\"bun add @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n  </Tabs>\\n</div>\\n\\nThen create a new file in the root of your project with the following polyfills:\\n\\n```ts filename=\"polyfills.js\"\\nimport { Platform } from \\'react-native\\';\\nimport structuredClone from \\'@ungap/structured-clone\\';\\n\\nif (Platform.OS !== \\'web\\') {\\n  const setupPolyfills = async () => {\\n    const { polyfillGlobal } = await import(\\n      \\'react-native/Libraries/Utilities/PolyfillFunctions\\'\\n    );\\n\\n    const { TextEncoderStream, TextDecoderStream } = await import(\\n      \\'@stardazed/streams-text-encoding\\'\\n    );\\n\\n    if (!(\\'structuredClone\\' in global)) {\\n      polyfillGlobal(\\'structuredClone\\', () => structuredClone);\\n    }\\n\\n    polyfillGlobal(\\'TextEncoderStream\\', () => TextEncoderStream);\\n    polyfillGlobal(\\'TextDecoderStream\\', () => TextDecoderStream);\\n  };\\n\\n  setupPolyfills();\\n}\\n\\nexport {};\\n```\\n\\nFinally, import the polyfills in your root `_layout.tsx`:\\n\\n```ts filename=\"_layout.tsx\"\\nimport \\'@/polyfills\\';\\n```\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Getting Started\\ndescription: Welcome to the AI SDK documentation!\\n---\\n\\n# Getting Started\\n\\nThe following guides are intended to provide you with an introduction to some of the core features provided by the AI SDK.\\n\\n<QuickstartFrameworkCards />\\n\\n## Backend Framework Examples\\n\\nYou can also use [AI SDK Core](/docs/ai-sdk-core/overview) and [AI SDK UI](/docs/ai-sdk-ui/overview) with the following backend frameworks:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Node.js HTTP Server',\\n      description: 'Send AI responses from a Node.js HTTP server.',\\n      href: '/examples/api-servers/node-js-http-server',\\n    },\\n    {\\n      title: 'Express',\\n      description: 'Send AI responses from an Express server.',\\n      href: '/examples/api-servers/express',\\n    },\\n    {\\n      title: 'Hono',\\n      description: 'Send AI responses from a Hono server.',\\n      href: '/examples/api-servers/hono',\\n    },\\n    {\\n      title: 'Fastify',\\n      description: 'Send AI responses from a Fastify server.',\\n      href: '/examples/api-servers/fastify',\\n    },\\n    {\\n      title: 'Nest.js',\\n      description: 'Send AI responses from a Nest.js server.',\\n      href: '/examples/api-servers/nest',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('03-agents'), name='03-agents', displayName='03-agents', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('03-agents/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Overview\\ndescription: Learn how to build agents with the AI SDK.\\n---\\n\\n# Agents\\n\\nAgents are **large language models (LLMs)** that use **tools** in a **loop** to accomplish tasks.\\n\\nThese components work together:\\n\\n- **LLMs** process input and decide the next action\\n- **Tools** extend capabilities beyond text generation (reading files, calling APIs, writing to databases)\\n- **Loop** orchestrates execution through:\\n  - **Context management** - Maintaining conversation history and deciding what the model sees (input) at each step\\n  - **Stopping conditions** - Determining when the loop (task) is complete\\n\\n## ToolLoopAgent Class\\n\\nThe ToolLoopAgent class handles these three components. Here's an agent that uses multiple tools in a loop to accomplish a task:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs, tool } from 'ai';\\nimport { z } from 'zod';\\n\\nconst weatherAgent = new ToolLoopAgent({\\n  model: 'anthropic/claude-sonnet-4.5',\\n  tools: {\\n    weather: tool({\\n      description: 'Get the weather in a location (in Fahrenheit)',\\n      inputSchema: z.object({\\n        location: z.string().describe('The location to get the weather for'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n    convertFahrenheitToCelsius: tool({\\n      description: 'Convert temperature from Fahrenheit to Celsius',\\n      inputSchema: z.object({\\n        temperature: z.number().describe('Temperature in Fahrenheit'),\\n      }),\\n      execute: async ({ temperature }) => {\\n        const celsius = Math.round((temperature - 32) * (5 / 9));\\n        return { celsius };\\n      },\\n    }),\\n  },\\n  // Agent's default behavior is to stop after a maximum of 20 steps\\n  // stopWhen: stepCountIs(20),\\n});\\n\\nconst result = await weatherAgent.generate({\\n  prompt: 'What is the weather in San Francisco in celsius?',\\n});\\n\\nconsole.log(result.text); // agent's final answer\\nconsole.log(result.steps); // steps taken by the agent\\n```\\n\\nThe agent automatically:\\n\\n1. Calls the `weather` tool to get the temperature in Fahrenheit\\n2. Calls `convertFahrenheitToCelsius` to convert it\\n3. Generates a final text response with the result\\n\\nThe Agent class handles the loop, context management, and stopping conditions.\\n\\n## Why Use the Agent Class?\\n\\nThe Agent class is the recommended approach for building agents with the AI SDK because it:\\n\\n- **Reduces boilerplate** - Manages loops and message arrays\\n- **Improves reusability** - Define once, use throughout your application\\n- **Simplifies maintenance** - Single place to update agent configuration\\n\\nFor most use cases, start with the Agent class. Use core functions (`generateText`, `streamText`) when you need explicit control over each step for complex structured workflows.\\n\\n## Structured Workflows\\n\\nAgents are flexible and powerful, but non-deterministic. When you need reliable, repeatable outcomes with explicit control flow, use core functions with structured workflow patterns combining:\\n\\n- Conditional statements for explicit branching\\n- Standard functions for reusable logic\\n- Error handling for robustness\\n- Explicit control flow for predictability\\n\\n[Explore workflow patterns](/docs/agents/workflows) to learn more about building structured, reliable systems.\\n\\n## Next Steps\\n\\n- **[Building Agents](/docs/agents/building-agents)** - Guide to creating agents with the Agent class\\n- **[Workflow Patterns](/docs/agents/workflows)** - Structured patterns using core functions for complex workflows\\n- **[Loop Control](/docs/agents/loop-control)** - Execution control with stopWhen and prepareStep\\n\", children=[]), DocItem(origPath=Path('03-agents/02-building-agents.mdx'), name='02-building-agents.mdx', displayName='02-building-agents.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Building Agents\\ndescription: Complete guide to creating agents with the Agent class.\\n---\\n\\n# Building Agents\\n\\nThe Agent class provides a structured way to encapsulate LLM configuration, tools, and behavior into reusable components. It handles the agent loop for you, allowing the LLM to call tools multiple times in sequence to accomplish complex tasks. Define agents once and use them across your application.\\n\\n## Why Use the ToolLoopAgent Class?\\n\\nWhen building AI applications, you often need to:\\n\\n- **Reuse configurations** - Same model settings, tools, and prompts across different parts of your application\\n- **Maintain consistency** - Ensure the same behavior and capabilities throughout your codebase\\n- **Simplify API routes** - Reduce boilerplate in your endpoints\\n- **Type safety** - Get full TypeScript support for your agent\\'s tools and outputs\\n\\nThe ToolLoopAgent class provides a single place to define your agent\\'s behavior.\\n\\n## Creating an Agent\\n\\nDefine an agent by instantiating the ToolLoopAgent class with your desired configuration:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst myAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: {\\n    // Your tools here\\n  },\\n});\\n```\\n\\n## Configuration Options\\n\\nThe Agent class accepts all the same settings as `generateText` and `streamText`. Configure:\\n\\n### Model and System Instructions\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are an expert software engineer.\\',\\n});\\n```\\n\\n### Tools\\n\\nProvide tools that the agent can use to accomplish tasks:\\n\\n```ts\\nimport { ToolLoopAgent, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst codeAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    runCode: tool({\\n      description: \\'Execute Python code\\',\\n      inputSchema: z.object({\\n        code: z.string(),\\n      }),\\n      execute: async ({ code }) => {\\n        // Execute code and return result\\n        return { output: \\'Code executed successfully\\' };\\n      },\\n    }),\\n  },\\n});\\n```\\n\\n### Loop Control\\n\\nBy default, agents run for 20 steps (`stopWhen: stepCountIs(20)`). In each step, the model either generates text or calls a tool. If it generates text, the agent completes. If it calls a tool, the AI SDK executes that tool.\\n\\nTo let agents call multiple tools in sequence, configure `stopWhen` to allow more steps. After each tool execution, the agent triggers a new generation where the model can call another tool or generate text:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  stopWhen: stepCountIs(20), // Allow up to 20 steps\\n});\\n```\\n\\nEach step represents one generation (which results in either text or a tool call). The loop continues until:\\n\\n- A finish reasoning other than tool-calls is returned, or\\n- A tool that is invoked does not have an execute function, or\\n- A tool call needs approval, or\\n- A stop condition is met\\n\\nYou can combine multiple conditions:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  stopWhen: [\\n    stepCountIs(20), // Maximum 20 steps\\n    yourCustomCondition(), // Custom logic for when to stop\\n  ],\\n});\\n```\\n\\nLearn more about [loop control and stop conditions](/docs/agents/loop-control).\\n\\n### Tool Choice\\n\\nControl how the agent uses tools:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools here\\n  },\\n  toolChoice: \\'required\\', // Force tool use\\n  // or toolChoice: \\'none\\' to disable tools\\n  // or toolChoice: \\'auto\\' (default) to let the model decide\\n});\\n```\\n\\nYou can also force the use of a specific tool:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: weatherTool,\\n    cityAttractions: attractionsTool,\\n  },\\n  toolChoice: {\\n    type: \\'tool\\',\\n    toolName: \\'weather\\', // Force the weather tool to be used\\n  },\\n});\\n```\\n\\n### Structured Output\\n\\nDefine structured output schemas:\\n\\n```ts\\nimport { ToolLoopAgent, Output, stepCountIs } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst analysisAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: Output.object({\\n    schema: z.object({\\n      sentiment: z.enum([\\'positive\\', \\'neutral\\', \\'negative\\']),\\n      summary: z.string(),\\n      keyPoints: z.array(z.string()),\\n    }),\\n  }),\\n  stopWhen: stepCountIs(10),\\n});\\n\\nconst { output } = await analysisAgent.generate({\\n  prompt: \\'Analyze customer feedback from the last quarter\\',\\n});\\n```\\n\\n## Define Agent Behavior with System Instructions\\n\\nSystem instructions define your agent\\'s behavior, personality, and constraints. They set the context for all interactions and guide how the agent responds to user queries and uses tools.\\n\\n### Basic System Instructions\\n\\nSet the agent\\'s role and expertise:\\n\\n```ts\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions:\\n    \\'You are an expert data analyst. You provide clear insights from complex data.\\',\\n});\\n```\\n\\n### Detailed Behavioral Instructions\\n\\nProvide specific guidelines for agent behavior:\\n\\n```ts\\nconst codeReviewAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a senior software engineer conducting code reviews.\\n\\n  Your approach:\\n  - Focus on security vulnerabilities first\\n  - Identify performance bottlenecks\\n  - Suggest improvements for readability and maintainability\\n  - Be constructive and educational in your feedback\\n  - Always explain why something is an issue and how to fix it`,\\n});\\n```\\n\\n### Constrain Agent Behavior\\n\\nSet boundaries and ensure consistent behavior:\\n\\n```ts\\nconst customerSupportAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a customer support specialist for an e-commerce platform.\\n\\n  Rules:\\n  - Never make promises about refunds without checking the policy\\n  - Always be empathetic and professional\\n  - If you don\\'t know something, say so and offer to escalate\\n  - Keep responses concise and actionable\\n  - Never share internal company information`,\\n  tools: {\\n    checkOrderStatus,\\n    lookupPolicy,\\n    createTicket,\\n  },\\n});\\n```\\n\\n### Tool Usage Instructions\\n\\nGuide how the agent should use available tools:\\n\\n```ts\\nconst researchAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a research assistant with access to search and document tools.\\n\\n  When researching:\\n  1. Always start with a broad search to understand the topic\\n  2. Use document analysis for detailed information\\n  3. Cross-reference multiple sources before drawing conclusions\\n  4. Cite your sources when presenting information\\n  5. If information conflicts, present both viewpoints`,\\n  tools: {\\n    webSearch,\\n    analyzeDocument,\\n    extractQuotes,\\n  },\\n});\\n```\\n\\n### Format and Style Instructions\\n\\nControl the output format and communication style:\\n\\n```ts\\nconst technicalWriterAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a technical documentation writer.\\n\\n  Writing style:\\n  - Use clear, simple language\\n  - Avoid jargon unless necessary\\n  - Structure information with headers and bullet points\\n  - Include code examples where relevant\\n  - Write in second person (\"you\" instead of \"the user\")\\n\\n  Always format responses in Markdown.`,\\n});\\n```\\n\\n## Using an Agent\\n\\nOnce defined, you can use your agent in three ways:\\n\\n### Generate Text\\n\\nUse `generate()` for one-time text generation:\\n\\n```ts\\nconst result = await myAgent.generate({\\n  prompt: \\'What is the weather like?\\',\\n});\\n\\nconsole.log(result.text);\\n```\\n\\n### Stream Text\\n\\nUse `stream()` for streaming responses:\\n\\n```ts\\nconst stream = myAgent.stream({\\n  prompt: \\'Tell me a story\\',\\n});\\n\\nfor await (const chunk of stream.textStream) {\\n  console.log(chunk);\\n}\\n```\\n\\n### Respond to UI Messages\\n\\nUse `createAgentUIStreamResponse()` to create API responses for client applications:\\n\\n```ts\\n// In your API route (e.g., app/api/chat/route.ts)\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: myAgent,\\n    messages,\\n  });\\n}\\n```\\n\\n## End-to-end Type Safety\\n\\nYou can infer types for your agent\\'s `UIMessage`s:\\n\\n```ts\\nimport { ToolLoopAgent, InferAgentUIMessage } from \\'ai\\';\\n\\nconst myAgent = new ToolLoopAgent({\\n  // ... configuration\\n});\\n\\n// Infer the UIMessage type for UI components or persistence\\nexport type MyAgentUIMessage = InferAgentUIMessage<typeof myAgent>;\\n```\\n\\nUse this type in your client components with `useChat`:\\n\\n```tsx filename=\"components/chat.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyAgentUIMessage } from \\'@/agent/my-agent\\';\\n\\nexport function Chat() {\\n  const { messages } = useChat<MyAgentUIMessage>();\\n  // Full type safety for your messages and tools\\n}\\n```\\n\\n## Next Steps\\n\\nNow that you understand building agents, you can:\\n\\n- Explore [workflow patterns](/docs/agents/workflows) for structured patterns using core functions\\n- Learn about [loop control](/docs/agents/loop-control) for advanced execution control\\n- See [manual loop examples](/cookbook/node/manual-agent-loop) for custom workflow implementations\\n', children=[]), DocItem(origPath=Path('03-agents/03-workflows.mdx'), name='03-workflows.mdx', displayName='03-workflows.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Workflow Patterns\\ndescription: Learn workflow patterns for building reliable agents with the AI SDK.\\n---\\n\\n# Workflow Patterns\\n\\nCombine the building blocks from the [overview](/docs/agents/overview) with these patterns to add structure and reliability to your agents:\\n\\n- [Sequential Processing](#sequential-processing-chains) - Steps executed in order\\n- [Parallel Processing](#parallel-processing) - Independent tasks run simultaneously\\n- [Evaluation/Feedback Loops](#evaluator-optimizer) - Results checked and improved iteratively\\n- [Orchestration](#orchestrator-worker) - Coordinating multiple components\\n- [Routing](#routing) - Directing work based on context\\n\\n## Choose Your Approach\\n\\nConsider these key factors:\\n\\n- **Flexibility vs Control** - How much freedom does the LLM need vs how tightly you must constrain its actions?\\n- **Error Tolerance** - What are the consequences of mistakes in your use case?\\n- **Cost Considerations** - More complex systems typically mean more LLM calls and higher costs\\n- **Maintenance** - Simpler architectures are easier to debug and modify\\n\\n**Start with the simplest approach that meets your needs**. Add complexity only when required by:\\n\\n1. Breaking down tasks into clear steps\\n2. Adding tools for specific capabilities\\n3. Implementing feedback loops for quality control\\n4. Introducing multiple agents for complex workflows\\n\\nLet's look at examples of these patterns in action.\\n\\n## Patterns with Examples\\n\\nThese patterns, adapted from [Anthropic's guide on building effective agents](https://www.anthropic.com/research/building-effective-agents), serve as building blocks you can combine to create comprehensive workflows. Each pattern addresses specific aspects of task execution. Combine them thoughtfully to build reliable solutions for complex problems.\\n\\n## Sequential Processing (Chains)\\n\\nThe simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. Use this pattern for tasks with well-defined sequences, like content generation pipelines or data transformation processes.\\n\\n```ts\\nimport { generateText, generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function generateMarketingCopy(input: string) {\\n  const model = 'openai/gpt-4o';\\n\\n  // First step: Generate marketing copy\\n  const { text: copy } = await generateText({\\n    model,\\n    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,\\n  });\\n\\n  // Perform quality check on copy\\n  const { object: qualityMetrics } = await generateObject({\\n    model,\\n    schema: z.object({\\n      hasCallToAction: z.boolean(),\\n      emotionalAppeal: z.number().min(1).max(10),\\n      clarity: z.number().min(1).max(10),\\n    }),\\n    prompt: `Evaluate this marketing copy for:\\n    1. Presence of call to action (true/false)\\n    2. Emotional appeal (1-10)\\n    3. Clarity (1-10)\\n\\n    Copy to evaluate: ${copy}`,\\n  });\\n\\n  // If quality check fails, regenerate with more specific instructions\\n  if (\\n    !qualityMetrics.hasCallToAction ||\\n    qualityMetrics.emotionalAppeal < 7 ||\\n    qualityMetrics.clarity < 7\\n  ) {\\n    const { text: improvedCopy } = await generateText({\\n      model,\\n      prompt: `Rewrite this marketing copy with:\\n      ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}\\n      ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}\\n      ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}\\n\\n      Original copy: ${copy}`,\\n    });\\n    return { copy: improvedCopy, qualityMetrics };\\n  }\\n\\n  return { copy, qualityMetrics };\\n}\\n```\\n\\n## Routing\\n\\nThis pattern lets the model decide which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. Use this when handling varied inputs that require different processing approaches. In the example below, the first LLM call's results determine the second call's model size and system prompt.\\n\\n```ts\\nimport { generateObject, generateText } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function handleCustomerQuery(query: string) {\\n  const model = 'openai/gpt-4o';\\n\\n  // First step: Classify the query type\\n  const { object: classification } = await generateObject({\\n    model,\\n    schema: z.object({\\n      reasoning: z.string(),\\n      type: z.enum(['general', 'refund', 'technical']),\\n      complexity: z.enum(['simple', 'complex']),\\n    }),\\n    prompt: `Classify this customer query:\\n    ${query}\\n\\n    Determine:\\n    1. Query type (general, refund, or technical)\\n    2. Complexity (simple or complex)\\n    3. Brief reasoning for classification`,\\n  });\\n\\n  // Route based on classification\\n  // Set model and system prompt based on query type and complexity\\n  const { text: response } = await generateText({\\n    model:\\n      classification.complexity === 'simple'\\n        ? 'openai/gpt-4o-mini'\\n        : 'openai/o4-mini',\\n    system: {\\n      general:\\n        'You are an expert customer service agent handling general inquiries.',\\n      refund:\\n        'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',\\n      technical:\\n        'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',\\n    }[classification.type],\\n    prompt: query,\\n  });\\n\\n  return { response, classification };\\n}\\n```\\n\\n## Parallel Processing\\n\\nBreak down tasks into independent subtasks that execute simultaneously. This pattern uses parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyze multiple documents or process different aspects of a single input concurrently (like code review).\\n\\n```ts\\nimport { generateText, generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\n// Example: Parallel code review with multiple specialized reviewers\\nasync function parallelCodeReview(code: string) {\\n  const model = 'openai/gpt-4o';\\n\\n  // Run parallel reviews\\n  const [securityReview, performanceReview, maintainabilityReview] =\\n    await Promise.all([\\n      generateObject({\\n        model,\\n        system:\\n          'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',\\n        schema: z.object({\\n          vulnerabilities: z.array(z.string()),\\n          riskLevel: z.enum(['low', 'medium', 'high']),\\n          suggestions: z.array(z.string()),\\n        }),\\n        prompt: `Review this code:\\n      ${code}`,\\n      }),\\n\\n      generateObject({\\n        model,\\n        system:\\n          'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',\\n        schema: z.object({\\n          issues: z.array(z.string()),\\n          impact: z.enum(['low', 'medium', 'high']),\\n          optimizations: z.array(z.string()),\\n        }),\\n        prompt: `Review this code:\\n      ${code}`,\\n      }),\\n\\n      generateObject({\\n        model,\\n        system:\\n          'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',\\n        schema: z.object({\\n          concerns: z.array(z.string()),\\n          qualityScore: z.number().min(1).max(10),\\n          recommendations: z.array(z.string()),\\n        }),\\n        prompt: `Review this code:\\n      ${code}`,\\n      }),\\n    ]);\\n\\n  const reviews = [\\n    { ...securityReview.object, type: 'security' },\\n    { ...performanceReview.object, type: 'performance' },\\n    { ...maintainabilityReview.object, type: 'maintainability' },\\n  ];\\n\\n  // Aggregate results using another model instance\\n  const { text: summary } = await generateText({\\n    model,\\n    system: 'You are a technical lead summarizing multiple code reviews.',\\n    prompt: `Synthesize these code review results into a concise summary with key actions:\\n    ${JSON.stringify(reviews, null, 2)}`,\\n  });\\n\\n  return { reviews, summary };\\n}\\n```\\n\\n## Orchestrator-Worker\\n\\nA primary model (orchestrator) coordinates the execution of specialized workers. Each worker optimizes for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.\\n\\n```ts\\nimport { generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function implementFeature(featureRequest: string) {\\n  // Orchestrator: Plan the implementation\\n  const { object: implementationPlan } = await generateObject({\\n    model: 'openai/o4-mini',\\n    schema: z.object({\\n      files: z.array(\\n        z.object({\\n          purpose: z.string(),\\n          filePath: z.string(),\\n          changeType: z.enum(['create', 'modify', 'delete']),\\n        }),\\n      ),\\n      estimatedComplexity: z.enum(['low', 'medium', 'high']),\\n    }),\\n    system:\\n      'You are a senior software architect planning feature implementations.',\\n    prompt: `Analyze this feature request and create an implementation plan:\\n    ${featureRequest}`,\\n  });\\n\\n  // Workers: Execute the planned changes\\n  const fileChanges = await Promise.all(\\n    implementationPlan.files.map(async file => {\\n      // Each worker is specialized for the type of change\\n      const workerSystemPrompt = {\\n        create:\\n          'You are an expert at implementing new files following best practices and project patterns.',\\n        modify:\\n          'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',\\n        delete:\\n          'You are an expert at safely removing code while ensuring no breaking changes.',\\n      }[file.changeType];\\n\\n      const { object: change } = await generateObject({\\n        model: 'anthropic/claude-sonnet-4.5',\\n        schema: z.object({\\n          explanation: z.string(),\\n          code: z.string(),\\n        }),\\n        system: workerSystemPrompt,\\n        prompt: `Implement the changes for ${file.filePath} to support:\\n        ${file.purpose}\\n\\n        Consider the overall feature context:\\n        ${featureRequest}`,\\n      });\\n\\n      return {\\n        file,\\n        implementation: change,\\n      };\\n    }),\\n  );\\n\\n  return {\\n    plan: implementationPlan,\\n    changes: fileChanges,\\n  };\\n}\\n```\\n\\n## Evaluator-Optimizer\\n\\nAdd quality control to workflows with dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow proceeds, retries with adjusted parameters, or takes corrective action. This creates robust workflows capable of self-improvement and error recovery.\\n\\n```ts\\nimport { generateText, generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function translateWithFeedback(text: string, targetLanguage: string) {\\n  let currentTranslation = '';\\n  let iterations = 0;\\n  const MAX_ITERATIONS = 3;\\n\\n  // Initial translation\\n  const { text: translation } = await generateText({\\n    model: 'openai/gpt-4o-mini', // use small model for first attempt\\n    system: 'You are an expert literary translator.',\\n    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:\\n    ${text}`,\\n  });\\n\\n  currentTranslation = translation;\\n\\n  // Evaluation-optimization loop\\n  while (iterations < MAX_ITERATIONS) {\\n    // Evaluate current translation\\n    const { object: evaluation } = await generateObject({\\n      model: 'anthropic/claude-sonnet-4.5', // use a larger model to evaluate\\n      schema: z.object({\\n        qualityScore: z.number().min(1).max(10),\\n        preservesTone: z.boolean(),\\n        preservesNuance: z.boolean(),\\n        culturallyAccurate: z.boolean(),\\n        specificIssues: z.array(z.string()),\\n        improvementSuggestions: z.array(z.string()),\\n      }),\\n      system: 'You are an expert in evaluating literary translations.',\\n      prompt: `Evaluate this translation:\\n\\n      Original: ${text}\\n      Translation: ${currentTranslation}\\n\\n      Consider:\\n      1. Overall quality\\n      2. Preservation of tone\\n      3. Preservation of nuance\\n      4. Cultural accuracy`,\\n    });\\n\\n    // Check if quality meets threshold\\n    if (\\n      evaluation.qualityScore >= 8 &&\\n      evaluation.preservesTone &&\\n      evaluation.preservesNuance &&\\n      evaluation.culturallyAccurate\\n    ) {\\n      break;\\n    }\\n\\n    // Generate improved translation based on feedback\\n    const { text: improvedTranslation } = await generateText({\\n      model: 'anthropic/claude-sonnet-4.5', // use a larger model\\n      system: 'You are an expert literary translator.',\\n      prompt: `Improve this translation based on the following feedback:\\n      ${evaluation.specificIssues.join('\\\\n')}\\n      ${evaluation.improvementSuggestions.join('\\\\n')}\\n\\n      Original: ${text}\\n      Current Translation: ${currentTranslation}`,\\n    });\\n\\n    currentTranslation = improvedTranslation;\\n    iterations++;\\n  }\\n\\n  return {\\n    finalTranslation: currentTranslation,\\n    iterationsRequired: iterations,\\n  };\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('03-agents/04-loop-control.mdx'), name='04-loop-control.mdx', displayName='04-loop-control.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Loop Control\\ndescription: Control agent execution with built-in loop management using stopWhen and prepareStep\\n---\\n\\n# Loop Control\\n\\nYou can control both the execution flow and the settings at each step of the agent loop. The loop continues until:\\n\\n- A finish reasoning other than tool-calls is returned, or\\n- A tool that is invoked does not have an execute function, or\\n- A tool call needs approval, or\\n- A stop condition is met\\n\\nThe AI SDK provides built-in loop control through two parameters: `stopWhen` for defining stopping conditions and `prepareStep` for modifying settings (model, tools, messages, and more) between steps.\\n\\n## Stop Conditions\\n\\nThe `stopWhen` parameter controls when to stop execution when there are tool results in the last step. By default, agents stop after 20 steps using `stepCountIs(20)`.\\n\\nWhen you provide `stopWhen`, the agent continues executing after tool calls until a stopping condition is met. When the condition is an array, execution stops when any of the conditions are met.\\n\\n### Use Built-in Conditions\\n\\nThe AI SDK provides several built-in stopping conditions:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  stopWhen: stepCountIs(20), // Default state: stop after 20 steps maximum\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Analyze this dataset and create a summary report\\',\\n});\\n```\\n\\n### Combine Multiple Conditions\\n\\nCombine multiple stopping conditions. The loop stops when it meets any condition:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs, hasToolCall } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  stopWhen: [\\n    stepCountIs(20), // Maximum 20 steps\\n    hasToolCall(\\'someTool\\'), // Stop after calling \\'someTool\\'\\n  ],\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Research and analyze the topic\\',\\n});\\n```\\n\\n### Create Custom Conditions\\n\\nBuild custom stopping conditions for specific requirements:\\n\\n```ts\\nimport { ToolLoopAgent, StopCondition, ToolSet } from \\'ai\\';\\n\\nconst tools = {\\n  // your tools\\n} satisfies ToolSet;\\n\\nconst hasAnswer: StopCondition<typeof tools> = ({ steps }) => {\\n  // Stop when the model generates text containing \"ANSWER:\"\\n  return steps.some(step => step.text?.includes(\\'ANSWER:\\')) ?? false;\\n};\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools,\\n  stopWhen: hasAnswer,\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Find the answer and respond with \"ANSWER: [your answer]\"\\',\\n});\\n```\\n\\nCustom conditions receive step information across all steps:\\n\\n```ts\\nconst budgetExceeded: StopCondition<typeof tools> = ({ steps }) => {\\n  const totalUsage = steps.reduce(\\n    (acc, step) => ({\\n      inputTokens: acc.inputTokens + (step.usage?.inputTokens ?? 0),\\n      outputTokens: acc.outputTokens + (step.usage?.outputTokens ?? 0),\\n    }),\\n    { inputTokens: 0, outputTokens: 0 },\\n  );\\n\\n  const costEstimate =\\n    (totalUsage.inputTokens * 0.01 + totalUsage.outputTokens * 0.03) / 1000;\\n  return costEstimate > 0.5; // Stop if cost exceeds $0.50\\n};\\n```\\n\\n## Prepare Step\\n\\nThe `prepareStep` callback runs before each step in the loop and defaults to the initial settings if you don\\'t return any changes. Use it to modify settings, manage context, or implement dynamic behavior based on execution history.\\n\\n### Dynamic Model Selection\\n\\nSwitch models based on step requirements:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/gpt-4o-mini\\', // Default model\\n  tools: {\\n    // your tools\\n  },\\n  prepareStep: async ({ stepNumber, messages }) => {\\n    // Use a stronger model for complex reasoning after initial steps\\n    if (stepNumber > 2 && messages.length > 10) {\\n      return {\\n        model: \\'anthropic/claude-sonnet-4.5\\',\\n      };\\n    }\\n    // Continue with default settings\\n    return {};\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\n### Context Management\\n\\nManage growing conversation history in long-running loops:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  prepareStep: async ({ messages }) => {\\n    // Keep only recent messages to stay within context limits\\n    if (messages.length > 20) {\\n      return {\\n        messages: [\\n          messages[0], // Keep system instructions\\n          ...messages.slice(-10), // Keep last 10 messages\\n        ],\\n      };\\n    }\\n    return {};\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\n### Tool Selection\\n\\nControl which tools are available at each step:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    search: searchTool,\\n    analyze: analyzeTool,\\n    summarize: summarizeTool,\\n  },\\n  prepareStep: async ({ stepNumber, steps }) => {\\n    // Search phase (steps 0-2)\\n    if (stepNumber <= 2) {\\n      return {\\n        activeTools: [\\'search\\'],\\n        toolChoice: \\'required\\',\\n      };\\n    }\\n\\n    // Analysis phase (steps 3-5)\\n    if (stepNumber <= 5) {\\n      return {\\n        activeTools: [\\'analyze\\'],\\n      };\\n    }\\n\\n    // Summary phase (step 6+)\\n    return {\\n      activeTools: [\\'summarize\\'],\\n      toolChoice: \\'required\\',\\n    };\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\nYou can also force a specific tool to be used:\\n\\n```ts\\nprepareStep: async ({ stepNumber }) => {\\n  if (stepNumber === 0) {\\n    // Force the search tool to be used first\\n    return {\\n      toolChoice: { type: \\'tool\\', toolName: \\'search\\' },\\n    };\\n  }\\n\\n  if (stepNumber === 5) {\\n    // Force the summarize tool after analysis\\n    return {\\n      toolChoice: { type: \\'tool\\', toolName: \\'summarize\\' },\\n    };\\n  }\\n\\n  return {};\\n};\\n```\\n\\n### Message Modification\\n\\nTransform messages before sending them to the model:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  prepareStep: async ({ messages, stepNumber }) => {\\n    // Summarize tool results to reduce token usage\\n    const processedMessages = messages.map(msg => {\\n      if (msg.role === \\'tool\\' && msg.content.length > 1000) {\\n        return {\\n          ...msg,\\n          content: summarizeToolResult(msg.content),\\n        };\\n      }\\n      return msg;\\n    });\\n\\n    return { messages: processedMessages };\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\n## Access Step Information\\n\\nBoth `stopWhen` and `prepareStep` receive detailed information about the current execution:\\n\\n```ts\\nprepareStep: async ({\\n  model, // Current model configuration\\n  stepNumber, // Current step number (0-indexed)\\n  steps, // All previous steps with their results\\n  messages, // Messages to be sent to the model\\n}) => {\\n  // Access previous tool calls and results\\n  const previousToolCalls = steps.flatMap(step => step.toolCalls);\\n  const previousResults = steps.flatMap(step => step.toolResults);\\n\\n  // Make decisions based on execution history\\n  if (previousToolCalls.some(call => call.toolName === \\'dataAnalysis\\')) {\\n    return {\\n      toolChoice: { type: \\'tool\\', toolName: \\'reportGenerator\\' },\\n    };\\n  }\\n\\n  return {};\\n},\\n```\\n\\n## Manual Loop Control\\n\\nFor scenarios requiring complete control over the agent loop, you can use AI SDK Core functions (`generateText` and `streamText`) to implement your own loop management instead of using `stopWhen` and `prepareStep`. This approach provides maximum flexibility for complex workflows.\\n\\n### Implementing a Manual Loop\\n\\nBuild your own agent loop when you need full control over execution:\\n\\n```ts\\nimport { generateText, ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [{ role: \\'user\\', content: \\'...\\' }];\\n\\nlet step = 0;\\nconst maxSteps = 10;\\n\\nwhile (step < maxSteps) {\\n  const result = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages,\\n    tools: {\\n      // your tools here\\n    },\\n  });\\n\\n  messages.push(...result.response.messages);\\n\\n  if (result.text) {\\n    break; // Stop when model generates text\\n  }\\n\\n  step++;\\n}\\n```\\n\\nThis manual approach gives you complete control over:\\n\\n- Message history management\\n- Step-by-step decision making\\n- Custom stopping conditions\\n- Dynamic tool and model selection\\n- Error handling and recovery\\n\\n[Learn more about manual agent loops in the cookbook](/cookbook/node/manual-agent-loop).\\n', children=[]), DocItem(origPath=Path('03-agents/05-configuring-call-options.mdx'), name='05-configuring-call-options.mdx', displayName='05-configuring-call-options.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Configuring Call Options\\ndescription: Pass type-safe runtime inputs to dynamically configure agent behavior.\\n---\\n\\n# Configuring Call Options\\n\\nCall options allow you to pass type-safe structured inputs to your agent. Use them to dynamically modify any agent setting based on the specific request.\\n\\n## Why Use Call Options?\\n\\nWhen you need agent behavior to change based on runtime context:\\n\\n- **Add dynamic context** - Inject retrieved documents, user preferences, or session data into prompts\\n- **Select models dynamically** - Choose faster or more capable models based on request complexity\\n- **Configure tools per request** - Pass user location to search tools or adjust tool behavior\\n- **Customize provider options** - Set reasoning effort, temperature, or other provider-specific settings\\n\\nWithout call options, you\\'d need to create multiple agents or handle configuration logic outside the agent.\\n\\n## How It Works\\n\\nDefine call options in three steps:\\n\\n1. **Define the schema** - Specify what inputs you accept using `callOptionsSchema`\\n2. **Configure with `prepareCall`** - Use those inputs to modify agent settings\\n3. **Pass options at runtime** - Provide the options when calling `generate()` or `stream()`\\n\\n## Basic Example\\n\\nAdd user context to your agent\\'s prompt at runtime:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst supportAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    userId: z.string(),\\n    accountType: z.enum([\\'free\\', \\'pro\\', \\'enterprise\\']),\\n  }),\\n  instructions: \\'You are a helpful customer support agent.\\',\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    instructions:\\n      settings.instructions +\\n      `\\\\nUser context:\\n- Account type: ${options.accountType}\\n- User ID: ${options.userId}\\n\\nAdjust your response based on the user\\'s account level.`,\\n  }),\\n});\\n\\n// Call the agent with specific user context\\nconst result = await supportAgent.generate({\\n  prompt: \\'How do I upgrade my account?\\',\\n  options: {\\n    userId: \\'user_123\\',\\n    accountType: \\'free\\',\\n  },\\n});\\n```\\n\\nThe `options` parameter is now required and type-checked. If you don\\'t provide it or pass incorrect types, TypeScript will error.\\n\\n## Modifying Agent Settings\\n\\nUse `prepareCall` to modify any agent setting. Return only the settings you want to change.\\n\\n### Dynamic Model Selection\\n\\nChoose models based on request characteristics:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/gpt-4o-mini\\', // Default model\\n  callOptionsSchema: z.object({\\n    complexity: z.enum([\\'simple\\', \\'complex\\']),\\n  }),\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    model:\\n      options.complexity === \\'simple\\' ? \\'openai/gpt-4o-mini\\' : \\'openai/o1-mini\\',\\n  }),\\n});\\n\\n// Use faster model for simple queries\\nawait agent.generate({\\n  prompt: \\'What is 2+2?\\',\\n  options: { complexity: \\'simple\\' },\\n});\\n\\n// Use more capable model for complex reasoning\\nawait agent.generate({\\n  prompt: \\'Explain quantum entanglement\\',\\n  options: { complexity: \\'complex\\' },\\n});\\n```\\n\\n### Dynamic Tool Configuration\\n\\nConfigure tools based on runtime context:\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst newsAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    userCity: z.string().optional(),\\n    userRegion: z.string().optional(),\\n  }),\\n  tools: {\\n    web_search: openai.tools.webSearch(),\\n  },\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    tools: {\\n      web_search: openai.tools.webSearch({\\n        searchContextSize: \\'low\\',\\n        userLocation: {\\n          type: \\'approximate\\',\\n          city: options.userCity,\\n          region: options.userRegion,\\n          country: \\'US\\',\\n        },\\n      }),\\n    },\\n  }),\\n});\\n\\nawait newsAgent.generate({\\n  prompt: \\'What are the top local news stories?\\',\\n  options: {\\n    userCity: \\'San Francisco\\',\\n    userRegion: \\'California\\',\\n  },\\n});\\n```\\n\\n### Provider-Specific Options\\n\\nConfigure provider settings dynamically:\\n\\n```ts\\nimport { openai, OpenAIProviderOptions } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/o1-mini\\',\\n  callOptionsSchema: z.object({\\n    taskDifficulty: z.enum([\\'low\\', \\'medium\\', \\'high\\']),\\n  }),\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    providerOptions: {\\n      openai: {\\n        reasoningEffort: options.taskDifficulty,\\n      } satisfies OpenAIProviderOptions,\\n    },\\n  }),\\n});\\n\\nawait agent.generate({\\n  prompt: \\'Analyze this complex scenario...\\',\\n  options: { taskDifficulty: \\'high\\' },\\n});\\n```\\n\\n## Advanced Patterns\\n\\n### Retrieval Augmented Generation (RAG)\\n\\nFetch relevant context and inject it into your prompt:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst ragAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    query: z.string(),\\n  }),\\n  prepareCall: async ({ options, ...settings }) => {\\n    // Fetch relevant documents (this can be async)\\n    const documents = await vectorSearch(options.query);\\n\\n    return {\\n      ...settings,\\n      instructions: `Answer questions using the following context:\\n\\n${documents.map(doc => doc.content).join(\\'\\\\n\\\\n\\')}`,\\n    };\\n  },\\n});\\n\\nawait ragAgent.generate({\\n  prompt: \\'What is our refund policy?\\',\\n  options: { query: \\'refund policy\\' },\\n});\\n```\\n\\nThe `prepareCall` function can be async, enabling you to fetch data before configuring the agent.\\n\\n### Combining Multiple Modifications\\n\\nModify multiple settings together:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/gpt-5-nano\\',\\n  callOptionsSchema: z.object({\\n    userRole: z.enum([\\'admin\\', \\'user\\']),\\n    urgency: z.enum([\\'low\\', \\'high\\']),\\n  }),\\n  tools: {\\n    readDatabase: readDatabaseTool,\\n    writeDatabase: writeDatabaseTool,\\n  },\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    // Upgrade model for urgent requests\\n    model:\\n      options.urgency === \\'high\\'\\n        ? \\'anthropic/claude-sonnet-4.5\\'\\n        : settings.model,\\n    // Limit tools based on user role\\n    activeTools:\\n      options.userRole === \\'admin\\'\\n        ? [\\'readDatabase\\', \\'writeDatabase\\']\\n        : [\\'readDatabase\\'],\\n    // Adjust instructions\\n    instructions: `You are a ${options.userRole} assistant.\\n${options.userRole === \\'admin\\' ? \\'You have full database access.\\' : \\'You have read-only access.\\'}`,\\n  }),\\n});\\n\\nawait agent.generate({\\n  prompt: \\'Update the user record\\',\\n  options: {\\n    userRole: \\'admin\\',\\n    urgency: \\'high\\',\\n  },\\n});\\n```\\n\\n## Using with createAgentUIStreamResponse\\n\\nPass call options through API routes to your agent:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\nimport { myAgent } from \\'@/ai/agents/my-agent\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages, userId, accountType } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: myAgent,\\n    messages,\\n    options: {\\n      userId,\\n      accountType,\\n    },\\n  });\\n}\\n```\\n\\n## Next Steps\\n\\n- Learn about [loop control](/docs/agents/loop-control) for execution management\\n- Explore [workflow patterns](/docs/agents/workflows) for complex multi-step processes\\n', children=[]), DocItem(origPath=Path('03-agents/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Agents\\ndescription: An overview of building agents with the AI SDK.\\n---\\n\\n# Agents\\n\\nThe following section show you how to build agents with the AI SDK - systems where large language models (LLMs) use tools in a loop to accomplish tasks.\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description: 'Learn what agents are and why to use the Agent class.',\\n      href: '/docs/agents/overview',\\n    },\\n    {\\n      title: 'Building Agents',\\n      description: 'Complete guide to creating agents with the Agent class.',\\n      href: '/docs/agents/building-agents',\\n    },\\n    {\\n      title: 'Workflow Patterns',\\n      description:\\n        'Structured patterns using core functions for complex workflows.',\\n      href: '/docs/agents/workflows',\\n    },\\n    {\\n      title: 'Loop Control',\\n      description: 'Advanced execution control with stopWhen and prepareStep.',\\n      href: '/docs/agents/loop-control',\\n    },\\n    {\\n      title: 'Configuring Call Options',\\n      description:\\n        'Pass type-safe runtime inputs to dynamically configure agent behavior.',\\n      href: '/docs/agents/configuring-call-options',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('03-ai-sdk-core'), name='03-ai-sdk-core', displayName='03-ai-sdk-core', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('03-ai-sdk-core/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of AI SDK Core.\\n---\\n\\n# AI SDK Core\\n\\nLarge Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale.\\nThey are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.\\n\\nAI SDK Core **simplifies working with LLMs by offering a standardized way of integrating them into your app** - so you can focus on building great AI applications for your users, not waste time on technical details.\\n\\nFor example, here’s how you can generate text with various models using the AI SDK:\\n\\n<PreviewSwitchProviders />\\n\\n## AI SDK Core Functions\\n\\nAI SDK Core has various functions designed for [text generation](./generating-text), [structured data generation](./generating-structured-data), and [tool usage](./tools-and-tool-calling).\\nThese functions take a standardized approach to setting up [prompts](./prompts) and [settings](./settings), making it easier to work with different models.\\n\\n- [`generateText`](/docs/ai-sdk-core/generating-text): Generates text and [tool calls](./tools-and-tool-calling).\\n  This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\\n- [`streamText`](/docs/ai-sdk-core/generating-text): Stream text and tool calls.\\n  You can use the `streamText` function for interactive use cases such as [chat bots](/docs/ai-sdk-ui/chatbot) and [content streaming](/docs/ai-sdk-ui/completion).\\n- [`generateObject`](/docs/ai-sdk-core/generating-structured-data): Generates a typed, structured object that matches a [Zod](https://zod.dev/) schema.\\n  You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\\n- [`streamObject`](/docs/ai-sdk-core/generating-structured-data): Stream a structured object that matches a Zod schema.\\n  You can use this function to [stream generated UIs](/docs/ai-sdk-ui/object-generation).\\n\\n## API Reference\\n\\nPlease check out the [AI SDK Core API Reference](/docs/reference/ai-sdk-core) for more details on each function.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/05-generating-text.mdx'), name='05-generating-text.mdx', displayName='05-generating-text.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Generating Text\\ndescription: Learn how to generate text with the AI SDK.\\n---\\n\\n# Generating and Streaming Text\\n\\nLarge language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process.\\nFor example, you can ask a model to come up with a recipe, draft an email, or summarize a document.\\n\\nThe AI SDK Core provides two functions to generate text and stream it from LLMs:\\n\\n- [`generateText`](#generatetext): Generates text for a given prompt and model.\\n- [`streamText`](#streamtext): Streams text from a given prompt and model.\\n\\nAdvanced LLM features such as [tool calling](./tools-and-tool-calling) and [structured data generation](./generating-structured-data) are built on top of text generation.\\n\\n## `generateText`\\n\\nYou can generate text using the [`generateText`](/docs/reference/ai-sdk-core/generate-text) function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\\n\\n```tsx\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n});\\n```\\n\\nYou can use more [advanced prompts](./prompts) to generate text with more complex instructions and content:\\n\\n```tsx\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system:\\n    \\'You are a professional writer. \\' +\\n    \\'You write simple, clear, and concise content.\\',\\n  prompt: `Summarize the following article in 3-5 sentences: ${article}`,\\n});\\n```\\n\\nThe result object of `generateText` contains several promises that resolve when all required data is available:\\n\\n- `result.content`: The content that was generated in the last step.\\n- `result.text`: The generated text.\\n- `result.reasoning`: The full reasoning that the model has generated in the last step.\\n- `result.reasoningText`: The reasoning text of the model (only available for some models).\\n- `result.files`: The files that were generated in the last step.\\n- `result.sources`: Sources that have been used as references in the last step (only available for some models).\\n- `result.toolCalls`: The tool calls that were made in the last step.\\n- `result.toolResults`: The results of the tool calls from the last step.\\n- `result.finishReason`: The reason the model finished generating text.\\n- `result.usage`: The usage of the model during the final step of text generation.\\n- `result.totalUsage`: The total usage across all steps (for multi-step generations).\\n- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).\\n- `result.request`: Additional request information.\\n- `result.response`: Additional response information, including response messages and body.\\n- `result.providerMetadata`: Additional provider-specific metadata.\\n- `result.steps`: Details for all steps, useful for getting information about intermediate steps.\\n- `result.output`: The generated structured output using the `output` specification.\\n\\n### Accessing response headers & body\\n\\nSometimes you need access to the full response from the model provider,\\ne.g. to access some provider-specific headers or body content.\\n\\nYou can access the raw response headers and body using the `response` property:\\n\\n```ts\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n});\\n\\nconsole.log(JSON.stringify(result.response.headers, null, 2));\\nconsole.log(JSON.stringify(result.response.body, null, 2));\\n```\\n\\n### `onFinish` callback\\n\\nWhen using `generateText`, you can provide an `onFinish` callback that is triggered after the last step is finished (\\n[API Reference](/docs/reference/ai-sdk-core/generate-text#on-finish)\\n).\\nIt contains the text, usage information, finish reason, messages, steps, total usage, and more:\\n\\n```tsx highlight=\"6-8\"\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\\n    // your own logic, e.g. for saving the chat history or recording usage\\n\\n    const messages = response.messages; // messages that were generated\\n  },\\n});\\n```\\n\\n## `streamText`\\n\\nDepending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.\\n\\nAI SDK Core provides the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function which simplifies streaming text from LLMs:\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n\\n// example: use textStream as an async iterable\\nfor await (const textPart of result.textStream) {\\n  console.log(textPart);\\n}\\n```\\n\\n<Note>\\n  `result.textStream` is both a `ReadableStream` and an `AsyncIterable`.\\n</Note>\\n\\n<Note type=\"warning\">\\n  `streamText` immediately starts streaming and suppresses errors to prevent\\n  server crashes. Use the `onError` callback to log errors.\\n</Note>\\n\\nYou can use `streamText` on its own or in combination with [AI SDK\\nUI](/examples/next-pages/basics/streaming-text-generation) and [AI SDK\\nRSC](/examples/next-app/basics/streaming-text-generation).\\nThe result object contains several helper functions to make the integration into [AI SDK UI](/docs/ai-sdk-ui) easier:\\n\\n- `result.toUIMessageStreamResponse()`: Creates a UI Message stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.\\n- `result.pipeUIMessageStreamToResponse()`: Writes UI Message stream delta output to a Node.js response-like object.\\n- `result.toTextStreamResponse()`: Creates a simple text stream HTTP response.\\n- `result.pipeTextStreamToResponse()`: Writes text delta output to a Node.js response-like object.\\n\\n<Note>\\n  `streamText` is using backpressure and only generates tokens as they are\\n  requested. You need to consume the stream in order for it to finish.\\n</Note>\\n\\nIt also provides several promises that resolve when the stream is finished:\\n\\n- `result.content`: The content that was generated in the last step.\\n- `result.text`: The generated text.\\n- `result.reasoning`: The full reasoning that the model has generated.\\n- `result.reasoningText`: The reasoning text of the model (only available for some models).\\n- `result.files`: Files that have been generated by the model in the last step.\\n- `result.sources`: Sources that have been used as references in the last step (only available for some models).\\n- `result.toolCalls`: The tool calls that have been executed in the last step.\\n- `result.toolResults`: The tool results that have been generated in the last step.\\n- `result.finishReason`: The reason the model finished generating text.\\n- `result.usage`: The usage of the model during the final step of text generation.\\n- `result.totalUsage`: The total usage across all steps (for multi-step generations).\\n- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).\\n- `result.steps`: Details for all steps, useful for getting information about intermediate steps.\\n- `result.request`: Additional request information from the last step.\\n- `result.response`: Additional response information from the last step.\\n- `result.providerMetadata`: Additional provider-specific metadata from the last step.\\n\\n### `onError` callback\\n\\n`streamText` immediately starts streaming to enable sending data without waiting for the model.\\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\\n\\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\\n\\n```tsx highlight=\"6-8\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onError({ error }) {\\n    console.error(error); // your error logging logic here\\n  },\\n});\\n```\\n\\n### `onChunk` callback\\n\\nWhen using `streamText`, you can provide an `onChunk` callback that is triggered for each chunk of the stream.\\n\\nIt receives the following chunk types:\\n\\n- `text`\\n- `reasoning`\\n- `source`\\n- `tool-call`\\n- `tool-input-start`\\n- `tool-input-delta`\\n- `tool-result`\\n- `raw`\\n\\n```tsx highlight=\"6-11\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onChunk({ chunk }) {\\n    // implement your own logic here, e.g.:\\n    if (chunk.type === \\'text\\') {\\n      console.log(chunk.text);\\n    }\\n  },\\n});\\n```\\n\\n### `onFinish` callback\\n\\nWhen using `streamText`, you can provide an `onFinish` callback that is triggered when the stream is finished (\\n[API Reference](/docs/reference/ai-sdk-core/stream-text#on-finish)\\n).\\nIt contains the text, usage information, finish reason, messages, steps, total usage, and more:\\n\\n```tsx highlight=\"6-8\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\\n    // your own logic, e.g. for saving the chat history or recording usage\\n\\n    const messages = response.messages; // messages that were generated\\n  },\\n});\\n```\\n\\n### `fullStream` property\\n\\nYou can read a stream with all events using the `fullStream` property.\\nThis can be useful if you want to implement your own UI or handle the stream in a different way.\\nHere is an example of how to use the `fullStream` property:\\n\\n```tsx\\nimport { streamText } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    cityAttractions: {\\n      inputSchema: z.object({ city: z.string() }),\\n      execute: async ({ city }) => ({\\n        attractions: [\\'attraction1\\', \\'attraction2\\', \\'attraction3\\'],\\n      }),\\n    },\\n  },\\n  prompt: \\'What are some San Francisco tourist attractions?\\',\\n});\\n\\nfor await (const part of result.fullStream) {\\n  switch (part.type) {\\n    case \\'start\\': {\\n      // handle start of stream\\n      break;\\n    }\\n    case \\'start-step\\': {\\n      // handle start of step\\n      break;\\n    }\\n    case \\'text-start\\': {\\n      // handle text start\\n      break;\\n    }\\n    case \\'text-delta\\': {\\n      // handle text delta here\\n      break;\\n    }\\n    case \\'text-end\\': {\\n      // handle text end\\n      break;\\n    }\\n    case \\'reasoning-start\\': {\\n      // handle reasoning start\\n      break;\\n    }\\n    case \\'reasoning-delta\\': {\\n      // handle reasoning delta here\\n      break;\\n    }\\n    case \\'reasoning-end\\': {\\n      // handle reasoning end\\n      break;\\n    }\\n    case \\'source\\': {\\n      // handle source here\\n      break;\\n    }\\n    case \\'file\\': {\\n      // handle file here\\n      break;\\n    }\\n    case \\'tool-call\\': {\\n      switch (part.toolName) {\\n        case \\'cityAttractions\\': {\\n          // handle tool call here\\n          break;\\n        }\\n      }\\n      break;\\n    }\\n    case \\'tool-input-start\\': {\\n      // handle tool input start\\n      break;\\n    }\\n    case \\'tool-input-delta\\': {\\n      // handle tool input delta\\n      break;\\n    }\\n    case \\'tool-input-end\\': {\\n      // handle tool input end\\n      break;\\n    }\\n    case \\'tool-result\\': {\\n      switch (part.toolName) {\\n        case \\'cityAttractions\\': {\\n          // handle tool result here\\n          break;\\n        }\\n      }\\n      break;\\n    }\\n    case \\'tool-error\\': {\\n      // handle tool error\\n      break;\\n    }\\n    case \\'finish-step\\': {\\n      // handle finish step\\n      break;\\n    }\\n    case \\'finish\\': {\\n      // handle finish here\\n      break;\\n    }\\n    case \\'error\\': {\\n      // handle error here\\n      break;\\n    }\\n    case \\'raw\\': {\\n      // handle raw value\\n      break;\\n    }\\n  }\\n}\\n```\\n\\n### Stream transformation\\n\\nYou can use the `experimental_transform` option to transform the stream.\\nThis is useful for e.g. filtering, changing, or smoothing the text stream.\\n\\nThe transformations are applied before the callbacks are invoked and the promises are resolved.\\nIf you e.g. have a transformation that changes all text to uppercase, the `onFinish` callback will receive the transformed text.\\n\\n#### Smoothing streams\\n\\nThe AI SDK Core provides a [`smoothStream` function](/docs/reference/ai-sdk-core/smooth-stream) that\\ncan be used to smooth out text streaming.\\n\\n```tsx highlight=\"6\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: smoothStream(),\\n});\\n```\\n\\n#### Custom transformations\\n\\nYou can also implement your own custom transformations.\\nThe transformation function receives the tools that are available to the model,\\nand returns a function that is used to transform the stream.\\nTools can either be generic or limited to the tools that you are using.\\n\\nHere is an example of how to implement a custom transformation that converts\\nall text to uppercase:\\n\\n```ts\\nconst upperCaseTransform =\\n  <TOOLS extends ToolSet>() =>\\n  (options: { tools: TOOLS; stopStream: () => void }) =>\\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\\n      transform(chunk, controller) {\\n        controller.enqueue(\\n          // for text chunks, convert the text to uppercase:\\n          chunk.type === \\'text\\'\\n            ? { ...chunk, text: chunk.text.toUpperCase() }\\n            : chunk,\\n        );\\n      },\\n    });\\n```\\n\\nYou can also stop the stream using the `stopStream` function.\\nThis is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.\\n\\nWhen you invoke `stopStream`, it is important to simulate the `step-finish` and `finish` events to guarantee that a well-formed stream is returned\\nand all callbacks are invoked.\\n\\n```ts\\nconst stopWordTransform =\\n  <TOOLS extends ToolSet>() =>\\n  ({ stopStream }: { stopStream: () => void }) =>\\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\\n      // note: this is a simplified transformation for testing;\\n      // in a real-world version more there would need to be\\n      // stream buffering and scanning to correctly emit prior text\\n      // and to detect all STOP occurrences.\\n      transform(chunk, controller) {\\n        if (chunk.type !== \\'text\\') {\\n          controller.enqueue(chunk);\\n          return;\\n        }\\n\\n        if (chunk.text.includes(\\'STOP\\')) {\\n          // stop the stream\\n          stopStream();\\n\\n          // simulate the finish-step event\\n          controller.enqueue({\\n            type: \\'finish-step\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: {\\n              completionTokens: NaN,\\n              promptTokens: NaN,\\n              totalTokens: NaN,\\n            },\\n            request: {},\\n            response: {\\n              id: \\'response-id\\',\\n              modelId: \\'mock-model-id\\',\\n              timestamp: new Date(0),\\n            },\\n            warnings: [],\\n            isContinued: false,\\n          });\\n\\n          // simulate the finish event\\n          controller.enqueue({\\n            type: \\'finish\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: {\\n              completionTokens: NaN,\\n              promptTokens: NaN,\\n              totalTokens: NaN,\\n            },\\n            response: {\\n              id: \\'response-id\\',\\n              modelId: \\'mock-model-id\\',\\n              timestamp: new Date(0),\\n            },\\n          });\\n\\n          return;\\n        }\\n\\n        controller.enqueue(chunk);\\n      },\\n    });\\n```\\n\\n#### Multiple transformations\\n\\nYou can also provide multiple transformations. They are applied in the order they are provided.\\n\\n```tsx highlight=\"4\"\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: [firstTransform, secondTransform],\\n});\\n```\\n\\n## Sources\\n\\nSome providers such as [Perplexity](/providers/ai-sdk-providers/perplexity#sources) and\\n[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.\\n\\nCurrently sources are limited to web pages that ground the response.\\nYou can access them using the `sources` property of the result.\\n\\nEach `url` source contains the following properties:\\n\\n- `id`: The ID of the source.\\n- `url`: The URL of the source.\\n- `title`: The optional title of the source.\\n- `providerMetadata`: Provider metadata for the source.\\n\\nWhen you use `generateText`, you can access the sources using the `sources` property:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'google/gemini-2.5-flash\\',\\n  tools: {\\n    google_search: google.tools.googleSearch({}),\\n  },\\n  prompt: \\'List the top 5 San Francisco news from the past week.\\',\\n});\\n\\nfor (const source of result.sources) {\\n  if (source.sourceType === \\'url\\') {\\n    console.log(\\'ID:\\', source.id);\\n    console.log(\\'Title:\\', source.title);\\n    console.log(\\'URL:\\', source.url);\\n    console.log(\\'Provider metadata:\\', source.providerMetadata);\\n    console.log();\\n  }\\n}\\n```\\n\\nWhen you use `streamText`, you can access the sources using the `fullStream` property:\\n\\n```tsx\\nconst result = streamText({\\n  model: \\'google/gemini-2.5-flash\\',\\n  tools: {\\n    google_search: google.tools.googleSearch({}),\\n  },\\n  prompt: \\'List the top 5 San Francisco news from the past week.\\',\\n});\\n\\nfor await (const part of result.fullStream) {\\n  if (part.type === \\'source\\' && part.sourceType === \\'url\\') {\\n    console.log(\\'ID:\\', part.id);\\n    console.log(\\'Title:\\', part.title);\\n    console.log(\\'URL:\\', part.url);\\n    console.log(\\'Provider metadata:\\', part.providerMetadata);\\n    console.log();\\n  }\\n}\\n```\\n\\nThe sources are also available in the `result.sources` promise.\\n\\n## Examples\\n\\nYou can see `generateText` and `streamText` in action using various frameworks in the following examples:\\n\\n### `generateText`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to generate text in Node.js\\',\\n      link: \\'/examples/node/generating-text/generate-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate text in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/generating-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate text in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/generating-text\\',\\n    },\\n  ]}\\n/>\\n\\n### `streamText`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to stream text in Node.js\\',\\n      link: \\'/examples/node/generating-text/stream-text\\',\\n    },\\n    {\\n      title: \\'Learn to stream text in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/streaming-text-generation\\',\\n    },\\n    {\\n      title: \\'Learn to stream text in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/streaming-text-generation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/10-generating-structured-data.mdx'), name='10-generating-structured-data.mdx', displayName='10-generating-structured-data.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Generating Structured Data\\ndescription: Learn how to generate structured data with the AI SDK.\\n---\\n\\n# Generating Structured Data\\n\\nWhile text generation can be useful, your use case will likely call for generating structured data.\\nFor example, you might want to extract information from text, classify data, or generate synthetic data.\\n\\nMany language models are capable of generating structured data, often defined as using \"JSON modes\" or \"tools\".\\nHowever, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.\\n\\nThe AI SDK standardises structured object generation across model providers\\nwith the [`generateObject`](/docs/reference/ai-sdk-core/generate-object)\\nand [`streamObject`](/docs/reference/ai-sdk-core/stream-object) functions.\\nYou can use both functions with different output strategies, e.g. `array`, `object`, `enum`, or `no-schema`,\\nand with different generation modes, e.g. `auto`, `tool`, or `json`.\\nYou can use [Zod schemas](/docs/reference/ai-sdk-core/zod-schema), [Valibot](/docs/reference/ai-sdk-core/valibot-schema), or [JSON schemas](/docs/reference/ai-sdk-core/json-schema) to specify the shape of the data that you want,\\nand the AI model will generate data that conforms to that structure.\\n\\n<Note>\\n  You can pass Zod objects directly to the AI SDK functions or use the\\n  `zodSchema` helper function.\\n</Note>\\n\\n## Generate Object\\n\\nThe `generateObject` generates structured data from a prompt.\\nThe schema is also used to validate the generated data, ensuring type safety and correctness.\\n\\n```ts\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\n<Note>\\n  See `generateObject` in action with [these examples](#more-examples)\\n</Note>\\n\\n### Accessing response headers & body\\n\\nSometimes you need access to the full response from the model provider,\\ne.g. to access some provider-specific headers or body content.\\n\\nYou can access the raw response headers and body using the `response` property:\\n\\n```ts\\nimport { generateObject } from \\'ai\\';\\n\\nconst result = await generateObject({\\n  // ...\\n});\\n\\nconsole.log(JSON.stringify(result.response.headers, null, 2));\\nconsole.log(JSON.stringify(result.response.body, null, 2));\\n```\\n\\n## Stream Object\\n\\nGiven the added complexity of returning structured data, model response time can be unacceptable for your interactive use case.\\nWith the [`streamObject`](/docs/reference/ai-sdk-core/stream-object) function, you can stream the model\\'s response as it is generated.\\n\\n```ts\\nimport { streamObject } from \\'ai\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  // ...\\n});\\n\\n// use partialObjectStream as an async iterable\\nfor await (const partialObject of partialObjectStream) {\\n  console.log(partialObject);\\n}\\n```\\n\\nYou can use `streamObject` to stream generated UIs in combination with React Server Components (see [Generative UI](../ai-sdk-rsc))) or the [`useObject`](/docs/reference/ai-sdk-ui/use-object) hook.\\n\\n<Note>See `streamObject` in action with [these examples](#more-examples)</Note>\\n\\n### `onError` callback\\n\\n`streamObject` immediately starts streaming.\\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\\n\\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\\n\\n```tsx highlight=\"5-7\"\\nimport { streamObject } from \\'ai\\';\\n\\nconst result = streamObject({\\n  // ...\\n  onError({ error }) {\\n    console.error(error); // your error logging logic here\\n  },\\n});\\n```\\n\\n## Output Strategy\\n\\nYou can use both functions with different output strategies, e.g. `array`, `object`, `enum`, or `no-schema`.\\n\\n### Object\\n\\nThe default output strategy is `object`, which returns the generated data as an object.\\nYou don\\'t need to specify the output strategy if you want to use the default.\\n\\n### Array\\n\\nIf you want to generate an array of objects, you can set the output strategy to `array`.\\nWhen you use the `array` output strategy, the schema specifies the shape of an array element.\\nWith `streamObject`, you can also stream the generated array elements using `elementStream`.\\n\\n```ts highlight=\"7,18\"\\nimport { streamObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { elementStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'array\\',\\n  schema: z.object({\\n    name: z.string(),\\n    class: z\\n      .string()\\n      .describe(\\'Character class, e.g. warrior, mage, or thief.\\'),\\n    description: z.string(),\\n  }),\\n  prompt: \\'Generate 3 hero descriptions for a fantasy role playing game.\\',\\n});\\n\\nfor await (const hero of elementStream) {\\n  console.log(hero);\\n}\\n```\\n\\n### Enum\\n\\nIf you want to generate a specific enum value, e.g. for classification tasks,\\nyou can set the output strategy to `enum`\\nand provide a list of possible values in the `enum` parameter.\\n\\n<Note>Enum output is only available with `generateObject`.</Note>\\n\\n```ts highlight=\"5-6\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'enum\\',\\n  enum: [\\'action\\', \\'comedy\\', \\'drama\\', \\'horror\\', \\'sci-fi\\'],\\n  prompt:\\n    \\'Classify the genre of this movie plot: \\' +\\n    \\'\"A group of astronauts travel through a wormhole in search of a \\' +\\n    \\'new habitable planet for humanity.\"\\',\\n});\\n```\\n\\n### No Schema\\n\\nIn some cases, you might not want to use a schema,\\nfor example when the data is a dynamic user request.\\nYou can use the `output` setting to set the output format to `no-schema` in those cases\\nand omit the schema parameter.\\n\\n```ts highlight=\"6\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'no-schema\\',\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\n## Schema Name and Description\\n\\nYou can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.\\n\\n```ts highlight=\"6-7\"\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schemaName: \\'Recipe\\',\\n  schemaDescription: \\'A recipe for a dish.\\',\\n  schema: z.object({\\n    name: z.string(),\\n    ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\\n    steps: z.array(z.string()),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\n## Accessing Reasoning\\n\\nYou can access the reasoning used by the language model to generate the object via the `reasoning` property on the result. This property contains a string with the model\\'s thought process, if available.\\n\\n```ts\\nimport { OpenAIResponsesProviderOptions } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = await generateObject({\\n  model: \\'openai/gpt-5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(\\n        z.object({\\n          name: z.string(),\\n          amount: z.string(),\\n        }),\\n      ),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n  providerOptions: {\\n    openai: {\\n      strictJsonSchema: true,\\n      reasoningSummary: \\'detailed\\',\\n    } satisfies OpenAIResponsesProviderOptions,\\n  },\\n});\\n\\nconsole.log(result.reasoning);\\n```\\n\\n## Error Handling\\n\\nWhen `generateObject` cannot generate a valid object, it throws a [`AI_NoObjectGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-object-generated-error).\\n\\nThis error occurs when the AI provider fails to generate a parsable object that conforms to the schema.\\nIt can arise due to the following reasons:\\n\\n- The model failed to generate a response.\\n- The model generated a response that could not be parsed.\\n- The model generated a response that could not be validated against the schema.\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.\\n- `response`: Metadata about the language model response, including response id, timestamp, and model.\\n- `usage`: Request token usage.\\n- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.\\n\\n```ts\\nimport { generateObject, NoObjectGeneratedError } from \\'ai\\';\\n\\ntry {\\n  await generateObject({ model, schema, prompt });\\n} catch (error) {\\n  if (NoObjectGeneratedError.isInstance(error)) {\\n    console.log(\\'NoObjectGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Text:\\', error.text);\\n    console.log(\\'Response:\\', error.response);\\n    console.log(\\'Usage:\\', error.usage);\\n  }\\n}\\n```\\n\\n## Repairing Invalid or Malformed JSON\\n\\n<Note type=\"warning\">\\n  The `repairText` function is experimental and may change in the future.\\n</Note>\\n\\nSometimes the model will generate invalid or malformed JSON.\\nYou can use the `repairText` function to attempt to repair the JSON.\\n\\nIt receives the error, either a `JSONParseError` or a `TypeValidationError`,\\nand the text that was generated by the model.\\nYou can then attempt to repair the text and return the repaired text.\\n\\n```ts highlight=\"7-10\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model,\\n  schema,\\n  prompt,\\n  experimental_repairText: async ({ text, error }) => {\\n    // example: add a closing brace to the text\\n    return text + \\'}\\';\\n  },\\n});\\n```\\n\\n## Structured outputs with `generateText` and `streamText`\\n\\nYou can generate structured data with `generateText` and `streamText` by using the `output` setting.\\n\\n<Note>\\n  Some models, e.g. those by OpenAI, support structured outputs and tool calling\\n  at the same time. This is only possible with `generateText` and `streamText`.\\n</Note>\\n\\n### `generateText`\\n\\n```ts highlight=\"2,4-18\"\\n// output is a structured object that matches the schema:\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number().nullable().describe(\\'Age of the person.\\'),\\n      contact: z.object({\\n        type: z.literal(\\'email\\'),\\n        value: z.string(),\\n      }),\\n      occupation: z.object({\\n        type: z.literal(\\'employed\\'),\\n        company: z.string(),\\n        position: z.string(),\\n      }),\\n    }),\\n  }),\\n  prompt: \\'Generate an example person for testing.\\',\\n});\\n```\\n\\n### `streamText`\\n\\n```ts highlight=\"2,4-18\"\\n// partialOutputStream contains generated partial objects:\\nconst { partialOutputStream } = await streamText({\\n  // ...\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number().nullable().describe(\\'Age of the person.\\'),\\n      contact: z.object({\\n        type: z.literal(\\'email\\'),\\n        value: z.string(),\\n      }),\\n      occupation: z.object({\\n        type: z.literal(\\'employed\\'),\\n        company: z.string(),\\n        position: z.string(),\\n      }),\\n    }),\\n  }),\\n  prompt: \\'Generate an example person for testing.\\',\\n});\\n```\\n\\n### Output Types\\n\\nThe AI SDK supports multiple ways of specifying the expected structure of generated data via the `Output` object. You can select from various strategies for structured/text generation and validation.\\n\\n#### `Output.text()`\\n\\nUse `Output.text()` to generate plain text from a model. This option doesn\\'t enforce any schema on the result: you simply receive the model\\'s text as a string.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.text(),\\n  prompt: \\'Tell me a joke.\\',\\n});\\n// output will be a string (the joke)\\n```\\n\\n#### `Output.object()`\\n\\nUse `Output.object({ schema })` to generate a structured object based on a schema (for example, a Zod schema). The output is type-validated to ensure the returned result matches the schema.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number().nullable(),\\n      labels: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate information for a test user.\\',\\n});\\n// output will be an object matching the schema above\\n```\\n\\nPartial outputs (e.g. with `streamText`) are also validated against your provided schema, as much as possible.\\n\\n#### `Output.array()`\\n\\nUse `Output.array({ element })` to specify that you expect an array of typed objects from the model, where each element should conform to a schema.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.array({\\n    element: z.object({\\n      location: z.string(),\\n      temperature: z.number(),\\n      condition: z.string(),\\n    }),\\n  }),\\n  prompt: \\'List the weather for San Francisco and Paris.\\',\\n});\\n// output will be an array of objects like:\\n// [\\n//   { location: \\'San Francisco\\', temperature: 70, condition: \\'Sunny\\' },\\n//   { location: \\'Paris\\', temperature: 65, condition: \\'Cloudy\\' },\\n// ]\\n```\\n\\nWith `Output.array`, the model is guided to return an object with an `elements` property that is an array; the SDK then extracts and validates this array for you.\\n\\nFor more advanced validation or different structures, see [the Output API reference](/docs/reference/ai-sdk-core/output).\\n\\n#### `Output.choice()`\\n\\nUse `Output.choice({ options })` when you expect the model to choose from a specific set of string options, such as for classification or fixed-enum answers.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.choice({\\n    options: [\\'sunny\\', \\'rainy\\', \\'snowy\\'],\\n  }),\\n  prompt: \\'Is the weather sunny, rainy, or snowy today?\\',\\n});\\n// output will be one of: \\'sunny\\', \\'rainy\\', or \\'snowy\\'\\n```\\n\\nYou can provide any set of string options, and the output will always be a single string value that matches one of the specified options. The SDK validates that the result matches one of your options, and will throw if the model returns something invalid.\\n\\nThis is especially useful for making classification-style generations or forcing valid values for API compatibility.\\n\\n#### `Output.json()`\\n\\nUse `Output.json()` when you want to generate and parse unstructured JSON values from the model, without enforcing a specific schema. This is useful if you want to capture arbitrary objects, flexible structures, or when you want to rely on the model\\'s natural output rather than rigid validation.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.json(),\\n  prompt:\\n    \\'For each city, return the current temperature and weather condition as a JSON object.\\',\\n});\\n\\n// output could be any valid JSON, for example:\\n// {\\n//   \"San Francisco\": { \"temperature\": 70, \"condition\": \"Sunny\" },\\n//   \"Paris\": { \"temperature\": 65, \"condition\": \"Cloudy\" }\\n// }\\n```\\n\\nWith `Output.json`, the SDK only checks that the response is valid JSON; it doesn\\'t validate the structure or types of the values. If you need schema validation, use the `.object` or `.array` outputs instead.\\n\\nThis makes `Output.json()` ideal for custom or dynamic data structures, prototyping, working with unpredictable model output, or extracting information when you don\\'t have a strict schema.\\n\\n## More Examples\\n\\nYou can see `generateObject` and `streamObject` in action using various frameworks in the following examples:\\n\\n### `generateObject`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to generate objects in Node.js\\',\\n      link: \\'/examples/node/generating-structured-data/generate-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate objects in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/generating-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate objects in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/generating-object\\',\\n    },\\n  ]}\\n/>\\n\\n### `streamObject`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to stream objects in Node.js\\',\\n      link: \\'/examples/node/streaming-structured-data/stream-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream objects in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/streaming-object-generation\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream objects in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/streaming-object-generation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/15-tools-and-tool-calling.mdx'), name='15-tools-and-tool-calling.mdx', displayName='15-tools-and-tool-calling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Tool Calling\\ndescription: Learn about tool calling and multi-step calls (using stopWhen) with AI SDK Core.\\n---\\n\\n# Tool Calling\\n\\nAs covered under Foundations, [tools](/docs/foundations/tools) are objects that can be called by the model to perform a specific task.\\nAI SDK Core tools contain three elements:\\n\\n- **`description`**: An optional description of the tool that can influence when the tool is picked.\\n- **`inputSchema`**: A [Zod schema](/docs/foundations/tools#schemas) or a [JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the input parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\\n- **`execute`**: An optional async function that is called with the inputs from the tool call. It produces a value of type `RESULT` (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.\\n\\n<Note className=\"mb-2\">\\n  You can use the [`tool`](/docs/reference/ai-sdk-core/tool) helper function to\\n  infer the types of the `execute` parameters.\\n</Note>\\n\\nThe `tools` parameter of `generateText` and `streamText` is an object that has the tool names as keys and the tools as values:\\n\\n```ts highlight=\"6-17\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n  },\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n<Note>\\n  When a model uses a tool, it is called a \"tool call\" and the output of the\\n  tool is called a \"tool result\".\\n</Note>\\n\\nTool calling is not restricted to only text generation.\\nYou can also use it to render user interfaces (Generative UI).\\n\\n## Multi-Step Calls (using stopWhen)\\n\\nWith the `stopWhen` setting, you can enable multi-step calls in `generateText` and `streamText`. When `stopWhen` is set and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the stopping condition is met.\\n\\n<Note>\\n  The `stopWhen` conditions are only evaluated when the last step contains tool\\n  results.\\n</Note>\\n\\nBy default, when you use `generateText` or `streamText`, it triggers a single generation. This works well for many use cases where you can rely on the model\\'s training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, its generation is complete and that step is finished.\\n\\nYou may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.\\n\\nYou can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model\\'s training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.\\n\\n### Example\\n\\nIn the following example, there are two steps:\\n\\n1. **Step 1**\\n   1. The prompt `\\'What is the weather in San Francisco?\\'` is sent to the model.\\n   1. The model generates a tool call.\\n   1. The tool call is executed.\\n1. **Step 2**\\n   1. The tool result is sent to the model.\\n   1. The model generates a response considering the tool result.\\n\\n```ts highlight=\"18-19\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool, stepCountIs } from \\'ai\\';\\n\\nconst { text, steps } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n  },\\n  stopWhen: stepCountIs(5), // stop after a maximum of 5 steps if tools were called\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n<Note>You can use `streamText` in a similar way.</Note>\\n\\n### Steps\\n\\nTo access intermediate tool calls and results, you can use the `steps` property in the result object\\nor the `streamText` `onFinish` callback.\\nIt contains all the text, tool calls, tool results, and more from each step.\\n\\n#### Example: Extract tool results from all steps\\n\\n```ts highlight=\"3,9-10\"\\nimport { generateText } from \\'ai\\';\\n\\nconst { steps } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  stopWhen: stepCountIs(10),\\n  // ...\\n});\\n\\n// extract all tool calls from the steps:\\nconst allToolCalls = steps.flatMap(step => step.toolCalls);\\n```\\n\\n### `onStepFinish` callback\\n\\nWhen using `generateText` or `streamText`, you can provide an `onStepFinish` callback that\\nis triggered when a step is finished,\\ni.e. all text deltas, tool calls, and tool results for the step are available.\\nWhen you have multiple steps, the callback is triggered for each step.\\n\\n```tsx highlight=\"5-7\"\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {\\n    // your own logic, e.g. for saving the chat history or recording usage\\n  },\\n});\\n```\\n\\n### `prepareStep` callback\\n\\nThe `prepareStep` callback is called before a step is started.\\n\\nIt is called with the following parameters:\\n\\n- `model`: The model that was passed into `generateText`.\\n- `stopWhen`: The stopping condition that was passed into `generateText`.\\n- `stepNumber`: The number of the step that is being executed.\\n- `steps`: The steps that have been executed so far.\\n- `messages`: The messages that will be sent to the model for the current step.\\n\\nYou can use it to provide different settings for a step, including modifying the input messages.\\n\\n```tsx highlight=\"5-7\"\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n  prepareStep: async ({ model, stepNumber, steps, messages }) => {\\n    if (stepNumber === 0) {\\n      return {\\n        // use a different model for this step:\\n        model: modelForThisParticularStep,\\n        // force a tool choice for this step:\\n        toolChoice: { type: \\'tool\\', toolName: \\'tool1\\' },\\n        // limit the tools that are available for this step:\\n        activeTools: [\\'tool1\\'],\\n      };\\n    }\\n\\n    // when nothing is returned, the default settings are used\\n  },\\n});\\n```\\n\\n#### Message Modification for Longer Agentic Loops\\n\\nIn longer agentic loops, you can use the `messages` parameter to modify the input messages for each step. This is particularly useful for prompt compression:\\n\\n```tsx\\nprepareStep: async ({ stepNumber, steps, messages }) => {\\n  // Compress conversation history for longer loops\\n  if (messages.length > 20) {\\n    return {\\n      messages: messages.slice(-10),\\n    };\\n  }\\n\\n  return {};\\n},\\n```\\n\\n## Response Messages\\n\\nAdding the generated assistant and tool messages to your conversation history is a common task,\\nespecially if you are using multi-step tool calls.\\n\\nBoth `generateText` and `streamText` have a `response.messages` property that you can use to\\nadd the assistant and tool messages to your conversation history.\\nIt is also available in the `onFinish` callback of `streamText`.\\n\\nThe `response.messages` property contains an array of `ModelMessage` objects that you can add to your conversation history:\\n\\n```ts\\nimport { generateText, ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [\\n  // ...\\n];\\n\\nconst { response } = await generateText({\\n  // ...\\n  messages,\\n});\\n\\n// add the response messages to your conversation history:\\nmessages.push(...response.messages); // streamText: ...((await response).messages)\\n```\\n\\n## Dynamic Tools\\n\\nAI SDK Core supports dynamic tools for scenarios where tool schemas are not known at compile time. This is useful for:\\n\\n- MCP (Model Context Protocol) tools without schemas\\n- User-defined functions at runtime\\n- Tools loaded from external sources\\n\\n### Using dynamicTool\\n\\nThe `dynamicTool` helper creates tools with unknown input/output types:\\n\\n```ts\\nimport { dynamicTool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst customTool = dynamicTool({\\n  description: \\'Execute a custom function\\',\\n  inputSchema: z.object({}),\\n  execute: async input => {\\n    // input is typed as \\'unknown\\'\\n    // You need to validate/cast it at runtime\\n    const { action, parameters } = input as any;\\n\\n    // Execute your dynamic logic\\n    return { result: `Executed ${action}` };\\n  },\\n});\\n```\\n\\n### Type-Safe Handling\\n\\nWhen using both static and dynamic tools, use the `dynamic` flag for type narrowing:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // Static tool with known types\\n    weather: weatherTool,\\n    // Dynamic tool\\n    custom: dynamicTool({\\n      /* ... */\\n    }),\\n  },\\n  onStepFinish: ({ toolCalls, toolResults }) => {\\n    // Type-safe iteration\\n    for (const toolCall of toolCalls) {\\n      if (toolCall.dynamic) {\\n        // Dynamic tool: input is \\'unknown\\'\\n        console.log(\\'Dynamic:\\', toolCall.toolName, toolCall.input);\\n        continue;\\n      }\\n\\n      // Static tool: full type inference\\n      switch (toolCall.toolName) {\\n        case \\'weather\\':\\n          console.log(toolCall.input.location); // typed as string\\n          break;\\n      }\\n    }\\n  },\\n});\\n```\\n\\n## Preliminary Tool Results\\n\\nYou can return an `AsyncIterable` over multiple results.\\nIn this case, the last value from the iterable is the final tool result.\\n\\nThis can be used in combination with generator functions to e.g. stream status information\\nduring the tool execution:\\n\\n```ts\\ntool({\\n  description: \\'Get the current weather.\\',\\n  inputSchema: z.object({\\n    location: z.string(),\\n  }),\\n  async *execute({ location }) {\\n    yield {\\n      status: \\'loading\\' as const,\\n      text: `Getting weather for ${location}`,\\n      weather: undefined,\\n    };\\n\\n    await new Promise(resolve => setTimeout(resolve, 3000));\\n\\n    const temperature = 72 + Math.floor(Math.random() * 21) - 10;\\n\\n    yield {\\n      status: \\'success\\' as const,\\n      text: `The weather in ${location} is ${temperature}°F`,\\n      temperature,\\n    };\\n  },\\n});\\n```\\n\\n## Tool Choice\\n\\nYou can use the `toolChoice` setting to influence when a tool is selected.\\nIt supports the following settings:\\n\\n- `auto` (default): the model can choose whether and which tools to call.\\n- `required`: the model must call a tool. It can choose which tool to call.\\n- `none`: the model must not call tools\\n- `{ type: \\'tool\\', toolName: string (typed) }`: the model must call the specified tool\\n\\n```ts highlight=\"18\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n  },\\n  toolChoice: \\'required\\', // force the model to call a tool\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n## Tool Execution Options\\n\\nWhen tools are called, they receive additional options as a second parameter.\\n\\n### Tool Call ID\\n\\nThe ID of the tool call is forwarded to the tool execution.\\nYou can use it e.g. when sending tool-call related information with stream data.\\n\\n```ts highlight=\"14-20\"\\nimport {\\n  streamText,\\n  tool,\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n} from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const stream = createUIMessageStream({\\n    execute: ({ writer }) => {\\n      const result = streamText({\\n        // ...\\n        messages,\\n        tools: {\\n          myTool: tool({\\n            // ...\\n            execute: async (args, { toolCallId }) => {\\n              // return e.g. custom status for tool call\\n              writer.write({\\n                type: \\'data-tool-status\\',\\n                id: toolCallId,\\n                data: {\\n                  name: \\'myTool\\',\\n                  status: \\'in-progress\\',\\n                },\\n              });\\n              // ...\\n            },\\n          }),\\n        },\\n      });\\n\\n      writer.merge(result.toUIMessageStream());\\n    },\\n  });\\n\\n  return createUIMessageStreamResponse({ stream });\\n}\\n```\\n\\n### Messages\\n\\nThe messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution.\\nYou can access them in the second parameter of the `execute` function.\\nIn multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.\\n\\n```ts highlight=\"8-9\"\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n  tools: {\\n    myTool: tool({\\n      // ...\\n      execute: async (args, { messages }) => {\\n        // use the message history in e.g. calls to other language models\\n        return { ... };\\n      },\\n    }),\\n  },\\n});\\n```\\n\\n### Abort Signals\\n\\nThe abort signals from `generateText` and `streamText` are forwarded to the tool execution.\\nYou can access them in the second parameter of the `execute` function and e.g. abort long-running computations or forward them to fetch calls inside tools.\\n\\n```ts highlight=\"6,11,14\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  abortSignal: myAbortSignal, // signal that will be forwarded to tools\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({ location: z.string() }),\\n      execute: async ({ location }, { abortSignal }) => {\\n        return fetch(\\n          `https://api.weatherapi.com/v1/current.json?q=${location}`,\\n          { signal: abortSignal }, // forward the abort signal to fetch\\n        );\\n      },\\n    }),\\n  },\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n### Context (experimental)\\n\\nYou can pass in arbitrary context from `generateText` or `streamText` via the `experimental_context` setting.\\nThis context is available in the `experimental_context` tool execution option.\\n\\n```ts\\nconst result = await generateText({\\n  // ...\\n  tools: {\\n    someTool: tool({\\n      // ...\\n      execute: async (input, { experimental_context: context }) => {\\n        const typedContext = context as { example: string }; // or use type validation library\\n        // ...\\n      },\\n    }),\\n  },\\n  experimental_context: { example: \\'123\\' },\\n});\\n```\\n\\n## Tool Input Lifecycle Hooks\\n\\nThe following tool input lifecycle hooks are available:\\n\\n- **`onInputStart`**: Called when the model starts generating the input (arguments) for the tool call\\n- **`onInputDelta`**: Called for each chunk of text as the input is streamed\\n- **`onInputAvailable`**: Called when the complete input is available and validated\\n\\n`onInputStart` and `onInputDelta` are only called in streaming contexts (when using `streamText`). They are not called when using `generateText`.\\n\\n### Example\\n\\n```ts highlight=\"15-23\"\\nimport { streamText, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    getWeather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n      onInputStart: () => {\\n        console.log(\\'Tool call starting\\');\\n      },\\n      onInputDelta: ({ inputTextDelta }) => {\\n        console.log(\\'Received input chunk:\\', inputTextDelta);\\n      },\\n      onInputAvailable: ({ input }) => {\\n        console.log(\\'Complete input:\\', input);\\n      },\\n    }),\\n  },\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n## Types\\n\\nModularizing your code often requires defining types to ensure type safety and reusability.\\nTo enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.\\n\\nYou can use them to strongly type your variables, function parameters, and return types\\nin parts of the code that are not directly related to `streamText` or `generateText`.\\n\\nEach tool call is typed with `ToolCall<NAME extends string, ARGS>`, depending\\non the tool that has been invoked.\\nSimilarly, the tool results are typed with `ToolResult<NAME extends string, ARGS, RESULT>`.\\n\\nThe tools in `streamText` and `generateText` are defined as a `ToolSet`.\\nThe type inference helpers `TypedToolCall<TOOLS extends ToolSet>`\\nand `TypedToolResult<TOOLS extends ToolSet>` can be used to\\nextract the tool call and tool result types from the tools.\\n\\n```ts highlight=\"18-19,23-24\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { TypedToolCall, TypedToolResult, generateText, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst myToolSet = {\\n  firstTool: tool({\\n    description: \\'Greets the user\\',\\n    inputSchema: z.object({ name: z.string() }),\\n    execute: async ({ name }) => `Hello, ${name}!`,\\n  }),\\n  secondTool: tool({\\n    description: \\'Tells the user their age\\',\\n    inputSchema: z.object({ age: z.number() }),\\n    execute: async ({ age }) => `You are ${age} years old!`,\\n  }),\\n};\\n\\ntype MyToolCall = TypedToolCall<typeof myToolSet>;\\ntype MyToolResult = TypedToolResult<typeof myToolSet>;\\n\\nasync function generateSomething(prompt: string): Promise<{\\n  text: string;\\n  toolCalls: Array<MyToolCall>; // typed tool calls\\n  toolResults: Array<MyToolResult>; // typed tool results\\n}> {\\n  return generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    tools: myToolSet,\\n    prompt,\\n  });\\n}\\n```\\n\\n## Handling Errors\\n\\nThe AI SDK has three tool-call related errors:\\n\\n- [`NoSuchToolError`](/docs/reference/ai-sdk-errors/ai-no-such-tool-error): the model tries to call a tool that is not defined in the tools object\\n- [`InvalidToolInputError`](/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error): the model calls a tool with inputs that do not match the tool\\'s input schema\\n- [`ToolCallRepairError`](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error): an error that occurred during tool call repair\\n\\nWhen tool execution fails (errors thrown by your tool\\'s `execute` function), the AI SDK adds them as `tool-error` content parts to enable automated LLM roundtrips in multi-step scenarios.\\n\\n### `generateText`\\n\\n`generateText` throws errors for tool schema validation issues and other errors, and can be handled using a `try`/`catch` block. Tool execution errors appear as `tool-error` parts in the result steps:\\n\\n```ts\\ntry {\\n  const result = await generateText({\\n    //...\\n  });\\n} catch (error) {\\n  if (NoSuchToolError.isInstance(error)) {\\n    // handle the no such tool error\\n  } else if (InvalidToolInputError.isInstance(error)) {\\n    // handle the invalid tool inputs error\\n  } else {\\n    // handle other errors\\n  }\\n}\\n```\\n\\nTool execution errors are available in the result steps:\\n\\n```ts\\nconst { steps } = await generateText({\\n  // ...\\n});\\n\\n// check for tool errors in the steps\\nconst toolErrors = steps.flatMap(step =>\\n  step.content.filter(part => part.type === \\'tool-error\\'),\\n);\\n\\ntoolErrors.forEach(toolError => {\\n  console.log(\\'Tool error:\\', toolError.error);\\n  console.log(\\'Tool name:\\', toolError.toolName);\\n  console.log(\\'Tool input:\\', toolError.input);\\n});\\n```\\n\\n### `streamText`\\n\\n`streamText` sends errors as part of the full stream. Tool execution errors appear as `tool-error` parts, while other errors appear as `error` parts.\\n\\nWhen using `toUIMessageStreamResponse`, you can pass an `onError` function to extract the error message from the error part and forward it as part of the stream response:\\n\\n```ts\\nconst result = streamText({\\n  // ...\\n});\\n\\nreturn result.toUIMessageStreamResponse({\\n  onError: error => {\\n    if (NoSuchToolError.isInstance(error)) {\\n      return \\'The model tried to call a unknown tool.\\';\\n    } else if (InvalidToolInputError.isInstance(error)) {\\n      return \\'The model called a tool with invalid inputs.\\';\\n    } else {\\n      return \\'An unknown error occurred.\\';\\n    }\\n  },\\n});\\n```\\n\\n## Tool Call Repair\\n\\n<Note type=\"warning\">\\n  The tool call repair feature is experimental and may change in the future.\\n</Note>\\n\\nLanguage models sometimes fail to generate valid tool calls,\\nespecially when the input schema is complex or the model is smaller.\\n\\nIf you use multiple steps, those failed tool calls will be sent back to the LLM\\nin the next step to give it an opportunity to fix it.\\nHowever, you may want to control how invalid tool calls are repaired without requiring\\nadditional steps that pollute the message history.\\n\\nYou can use the `experimental_repairToolCall` function to attempt to repair the tool call\\nwith a custom function.\\n\\nYou can use different strategies to repair the tool call:\\n\\n- Use a model with structured outputs to generate the inputs.\\n- Send the messages, system prompt, and tool schema to a stronger model to generate the inputs.\\n- Provide more specific repair instructions based on which tool was called.\\n\\n### Example: Use a model with structured outputs for repair\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject, generateText, NoSuchToolError, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model,\\n  tools,\\n  prompt,\\n\\n  experimental_repairToolCall: async ({\\n    toolCall,\\n    tools,\\n    inputSchema,\\n    error,\\n  }) => {\\n    if (NoSuchToolError.isInstance(error)) {\\n      return null; // do not attempt to fix invalid tool names\\n    }\\n\\n    const tool = tools[toolCall.toolName as keyof typeof tools];\\n\\n    const { object: repairedArgs } = await generateObject({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      schema: tool.inputSchema,\\n      prompt: [\\n        `The model tried to call the tool \"${toolCall.toolName}\"` +\\n          ` with the following inputs:`,\\n        JSON.stringify(toolCall.input),\\n        `The tool accepts the following schema:`,\\n        JSON.stringify(inputSchema(toolCall)),\\n        \\'Please fix the inputs.\\',\\n      ].join(\\'\\\\n\\'),\\n    });\\n\\n    return { ...toolCall, input: JSON.stringify(repairedArgs) };\\n  },\\n});\\n```\\n\\n### Example: Use the re-ask strategy for repair\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject, generateText, NoSuchToolError, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model,\\n  tools,\\n  prompt,\\n\\n  experimental_repairToolCall: async ({\\n    toolCall,\\n    tools,\\n    error,\\n    messages,\\n    system,\\n  }) => {\\n    const result = await generateText({\\n      model,\\n      system,\\n      messages: [\\n        ...messages,\\n        {\\n          role: \\'assistant\\',\\n          content: [\\n            {\\n              type: \\'tool-call\\',\\n              toolCallId: toolCall.toolCallId,\\n              toolName: toolCall.toolName,\\n              input: toolCall.input,\\n            },\\n          ],\\n        },\\n        {\\n          role: \\'tool\\' as const,\\n          content: [\\n            {\\n              type: \\'tool-result\\',\\n              toolCallId: toolCall.toolCallId,\\n              toolName: toolCall.toolName,\\n              output: error.message,\\n            },\\n          ],\\n        },\\n      ],\\n      tools,\\n    });\\n\\n    const newToolCall = result.toolCalls.find(\\n      newToolCall => newToolCall.toolName === toolCall.toolName,\\n    );\\n\\n    return newToolCall != null\\n      ? {\\n          toolCallType: \\'function\\' as const,\\n          toolCallId: toolCall.toolCallId,\\n          toolName: toolCall.toolName,\\n          input: JSON.stringify(newToolCall.input),\\n        }\\n      : null;\\n  },\\n});\\n```\\n\\n## Active Tools\\n\\nLanguage models can only handle a limited number of tools at a time, depending on the model.\\nTo allow for static typing using a large number of tools and limiting the available tools to the model at the same time,\\nthe AI SDK provides the `activeTools` property.\\n\\nIt is an array of tool names that are currently active.\\nBy default, the value is `undefined` and all tools are active.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: myToolSet,\\n  activeTools: [\\'firstTool\\'],\\n});\\n```\\n\\n## Multi-modal Tool Results\\n\\n<Note type=\"warning\">\\n  Multi-modal tool results are experimental and only supported by Anthropic and\\n  OpenAI.\\n</Note>\\n\\nIn order to send multi-modal tool results, e.g. screenshots, back to the model,\\nthey need to be converted into a specific format.\\n\\nAI SDK Core tools have an optional `toModelOutput` function\\nthat converts the tool result into a content part.\\n\\nHere is an example for converting a screenshot into a content part:\\n\\n```ts highlight=\"22-27\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    computer: anthropic.tools.computer_20241022({\\n      // ...\\n      async execute({ action, coordinate, text }) {\\n        switch (action) {\\n          case \\'screenshot\\': {\\n            return {\\n              type: \\'image\\',\\n              data: fs\\n                .readFileSync(\\'./data/screenshot-editor.png\\')\\n                .toString(\\'base64\\'),\\n            };\\n          }\\n          default: {\\n            return `executed ${action}`;\\n          }\\n        }\\n      },\\n\\n      // map to tool result content for LLM consumption:\\n      toModelOutput(result) {\\n        return {\\n          type: \\'content\\',\\n          value:\\n            typeof result === \\'string\\'\\n              ? [{ type: \\'text\\', text: result }]\\n              : [{ type: \\'media\\', data: result.data, mediaType: \\'image/png\\' }],\\n        };\\n      },\\n    }),\\n  },\\n  // ...\\n});\\n```\\n\\n## Extracting Tools\\n\\nOnce you start having many tools, you might want to extract them into separate files.\\nThe `tool` helper function is crucial for this, because it ensures correct type inference.\\n\\nHere is an example of an extracted tool:\\n\\n```ts filename=\"tools/weather-tool.ts\" highlight=\"1,4-5\"\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// the `tool` helper function ensures correct type inference:\\nexport const weatherTool = tool({\\n  description: \\'Get the weather in a location\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The location to get the weather for\\'),\\n  }),\\n  execute: async ({ location }) => ({\\n    location,\\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n  }),\\n});\\n```\\n\\n## MCP Tools\\n\\nThe AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools.\\nMCP enables your AI applications to discover and use tools across various services through a standardized interface.\\n\\nFor detailed information about MCP tools, including initialization, transport options, and usage patterns, see the [MCP Tools documentation](/docs/ai-sdk-core/mcp-tools).\\n\\n### AI SDK Tools vs MCP Tools\\n\\nIn most cases, you should define your own AI SDK tools for production applications. They provide full control, type safety, and optimal performance. MCP tools are best suited for rapid development iteration and scenarios where users bring their own tools.\\n\\n| Aspect                 | AI SDK Tools                                              | MCP Tools                                             |\\n| ---------------------- | --------------------------------------------------------- | ----------------------------------------------------- |\\n| **Type Safety**        | Full static typing end-to-end                             | Dynamic discovery at runtime                          |\\n| **Execution**          | Same process as your request (low latency)                | Separate server (network overhead)                    |\\n| **Prompt Control**     | Full control over descriptions and schemas                | Controlled by MCP server owner                        |\\n| **Schema Control**     | You define and optimize for your model                    | Controlled by MCP server owner                        |\\n| **Version Management** | Full visibility over updates                              | Can update independently (version skew risk)          |\\n| **Authentication**     | Same process, no additional auth required                 | Separate server introduces additional auth complexity |\\n| **Best For**           | Production applications requiring control and performance | Development iteration, user-provided tools            |\\n\\n## Examples\\n\\nYou can see tools in action using various frameworks in the following examples:\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to use tools in Node.js\\',\\n      link: \\'/cookbook/node/call-tools\\',\\n    },\\n    {\\n      title: \\'Learn to use tools in Next.js with Route Handlers\\',\\n      link: \\'/cookbook/next/call-tools\\',\\n    },\\n    {\\n      title: \\'Learn to use MCP tools in Node.js\\',\\n      link: \\'/cookbook/node/mcp-tools\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/16-mcp-tools.mdx'), name='16-mcp-tools.mdx', displayName='16-mcp-tools.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Model Context Protocol (MCP)\\ndescription: Learn how to connect to Model Context Protocol (MCP) servers and use their tools with AI SDK Core.\\n---\\n\\n# Model Context Protocol (MCP)\\n\\n<Note type=\"warning\">\\n  The MCP tools feature is experimental and may change in the future.\\n</Note>\\n\\nThe AI SDK supports connecting to [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers to access their tools, resources, and prompts.\\nThis enables your AI applications to discover and use capabilities across various services through a standardized interface.\\n\\n<Note>\\n  If you\\'re using OpenAI\\'s Responses API, you can also use the built-in\\n  `openai.tools.mcp` tool, which provides direct MCP server integration without\\n  needing to convert tools. See the [OpenAI provider\\n  documentation](/providers/ai-sdk-providers/openai#mcp-tool) for details.\\n</Note>\\n\\n## Initializing an MCP Client\\n\\nWe recommend using HTTP transport (like `StreamableHTTPClientTransport`) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.\\n\\nCreate an MCP client using one of the following transport options:\\n\\n- **HTTP transport (Recommended)**: Either configure HTTP directly via the client using `transport: { type: \\'http\\', ... }`, or use MCP\\'s official TypeScript SDK `StreamableHTTPClientTransport`\\n- SSE (Server-Sent Events): An alternative HTTP-based transport\\n- `stdio`: For local development only. Uses standard input/output streams for local MCP servers\\n\\n### HTTP Transport (Recommended)\\n\\nFor production deployments, we recommend using the HTTP transport. You can configure it directly on the client:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\n\\nconst mcpClient = await createMCPClient({\\n  transport: {\\n    type: \\'http\\',\\n    url: \\'https://your-server.com/mcp\\',\\n\\n    // optional: configure HTTP headers\\n    headers: { Authorization: \\'Bearer my-api-key\\' },\\n\\n    // optional: provide an OAuth client provider for automatic authorization\\n    authProvider: myOAuthClientProvider,\\n  },\\n});\\n```\\n\\nAlternatively, you can use `StreamableHTTPClientTransport` from MCP\\'s official TypeScript SDK:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\nimport { StreamableHTTPClientTransport } from \\'@modelcontextprotocol/sdk/client/streamableHttp.js\\';\\n\\nconst url = new URL(\\'https://your-server.com/mcp\\');\\nconst mcpClient = await createMCPClient({\\n  transport: new StreamableHTTPClientTransport(url, {\\n    sessionId: \\'session_123\\',\\n  }),\\n});\\n```\\n\\n### SSE Transport\\n\\nSSE provides an alternative HTTP-based transport option. Configure it with a `type` and `url` property. You can also provide an `authProvider` for OAuth:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\n\\nconst mcpClient = await createMCPClient({\\n  transport: {\\n    type: \\'sse\\',\\n    url: \\'https://my-server.com/sse\\',\\n\\n    // optional: configure HTTP headers\\n    headers: { Authorization: \\'Bearer my-api-key\\' },\\n\\n    // optional: provide an OAuth client provider for automatic authorization\\n    authProvider: myOAuthClientProvider,\\n  },\\n});\\n```\\n\\n### Stdio Transport (Local Servers)\\n\\n<Note type=\"warning\">\\n  The stdio transport should only be used for local servers.\\n</Note>\\n\\nThe Stdio transport can be imported from either the MCP SDK or the AI SDK:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\nimport { StdioClientTransport } from \\'@modelcontextprotocol/sdk/client/stdio.js\\';\\n// Or use the AI SDK\\'s stdio transport:\\n// import { Experimental_StdioMCPTransport as StdioClientTransport } from \\'@ai-sdk/mcp/mcp-stdio\\';\\n\\nconst mcpClient = await createMCPClient({\\n  transport: new StdioClientTransport({\\n    command: \\'node\\',\\n    args: [\\'src/stdio/dist/server.js\\'],\\n  }),\\n});\\n```\\n\\n### Custom Transport\\n\\nYou can also bring your own transport by implementing the `MCPTransport` interface for specific requirements not covered by the standard transports.\\n\\n<Note>\\n  The client returned by the `experimental_createMCPClient` function is a\\n  lightweight client intended for use in tool conversion. It currently does not\\n  support all features of the full MCP client, such as: session\\n  management, resumable streams, and receiving notifications.\\n\\nAuthorization via OAuth is supported when using the AI SDK MCP HTTP or SSE\\ntransports by providing an `authProvider`.\\n\\n</Note>\\n\\n### Closing the MCP Client\\n\\nAfter initialization, you should close the MCP client based on your usage pattern:\\n\\n- For short-lived usage (e.g., single requests), close the client when the response is finished\\n- For long-running clients (e.g., command line apps), keep the client open but ensure it\\'s closed when the application terminates\\n\\nWhen streaming responses, you can close the client when the LLM response has finished. For example, when using `streamText`, you should use the `onFinish` callback:\\n\\n```typescript\\nconst mcpClient = await experimental_createMCPClient({\\n  // ...\\n});\\n\\nconst tools = await mcpClient.tools();\\n\\nconst result = await streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools,\\n  prompt: \\'What is the weather in Brooklyn, New York?\\',\\n  onFinish: async () => {\\n    await mcpClient.close();\\n  },\\n});\\n```\\n\\nWhen generating responses without streaming, you can use try/finally or cleanup functions in your framework:\\n\\n```typescript\\nlet mcpClient: MCPClient | undefined;\\n\\ntry {\\n  mcpClient = await experimental_createMCPClient({\\n    // ...\\n  });\\n} finally {\\n  await mcpClient?.close();\\n}\\n```\\n\\n## Using MCP Tools\\n\\nThe client\\'s `tools` method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:\\n\\n### Schema Discovery\\n\\nWith schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:\\n\\n```typescript\\nconst tools = await mcpClient.tools();\\n```\\n\\nThis approach is simpler to implement and automatically stays in sync with server changes. However, you won\\'t have TypeScript type safety during development, and all tools from the server will be loaded\\n\\n### Schema Definition\\n\\nFor better type safety and control, you can define the tools and their input schemas explicitly in your client code:\\n\\n```typescript\\nimport { z } from \\'zod\\';\\n\\nconst tools = await mcpClient.tools({\\n  schemas: {\\n    \\'get-data\\': {\\n      inputSchema: z.object({\\n        query: z.string().describe(\\'The data query\\'),\\n        format: z.enum([\\'json\\', \\'text\\']).optional(),\\n      }),\\n    },\\n    // For tools with zero inputs, you should use an empty object:\\n    \\'tool-with-no-args\\': {\\n      inputSchema: z.object({}),\\n    },\\n  },\\n});\\n```\\n\\nThis approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define `schemas`, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs\\n\\n## Using MCP Resources\\n\\nAccording to the [MCP specification](https://modelcontextprotocol.io/docs/learn/server-concepts#resources), resources are **application-driven** data sources that provide context to the model. Unlike tools (which are model-controlled), your application decides when to fetch and pass resources as context.\\n\\nThe MCP client provides three methods for working with resources:\\n\\n### Listing Resources\\n\\nList all available resources from the MCP server:\\n\\n```typescript\\nconst resources = await mcpClient.listResources();\\n```\\n\\n### Reading Resource Contents\\n\\nRead the contents of a specific resource by its URI:\\n\\n```typescript\\nconst resourceData = await mcpClient.readResource({\\n  uri: \\'file:///example/document.txt\\',\\n});\\n```\\n\\n### Listing Resource Templates\\n\\nResource templates are dynamic URI patterns that allow flexible queries. List all available templates:\\n\\n```typescript\\nconst templates = await mcpClient.listResourceTemplates();\\n```\\n\\n## Using MCP Prompts\\n\\nAccording to the MCP specification, prompts are user-controlled templates that servers expose for clients to list and retrieve with optional arguments.\\n\\n### Listing Prompts\\n\\n```typescript\\nconst prompts = await mcpClient.listPrompts();\\n```\\n\\n### Getting a Prompt\\n\\nRetrieve prompt messages, optionally passing arguments defined by the server:\\n\\n```typescript\\nconst prompt = await mcpClient.getPrompt({\\n  name: \\'code_review\\',\\n  arguments: { code: \\'function add(a, b) { return a + b; }\\' },\\n});\\n```\\n\\n## Handling Elicitation Requests\\n\\nElicitation is a mechanism where MCP servers can request additional information from the client during tool execution. For example, a server might need user input to complete a registration form or confirmation for a sensitive operation.\\n\\n<Note type=\"warning\">\\n  It is up to the client application to handle elicitation requests properly.\\n  The MCP client simply surfaces these requests from the server to your\\n  application code.\\n</Note>\\n\\n### Enabling Elicitation Support\\n\\nTo enable elicitation, you need to advertise the capability when creating the MCP client:\\n\\n```typescript\\nconst mcpClient = await experimental_createMCPClient({\\n  transport: {\\n    type: \\'sse\\',\\n    url: \\'https://your-server.com/sse\\',\\n  },\\n  capabilities: {\\n    elicitation: {},\\n  },\\n});\\n```\\n\\n### Registering an Elicitation Handler\\n\\nUse the `onElicitationRequest` method to register a handler that will be called when the server requests input:\\n\\n```typescript\\nimport { ElicitationRequestSchema } from \\'@ai-sdk/mcp\\';\\n\\nmcpClient.onElicitationRequest(ElicitationRequestSchema, async request => {\\n  // request.params.message: A message describing what input is needed\\n  // request.params.requestedSchema: JSON schema defining the expected input structure\\n\\n  // Get input from the user (implement according to your application\\'s needs)\\n  const userInput = await getInputFromUser(\\n    request.params.message,\\n    request.params.requestedSchema,\\n  );\\n\\n  // Return the result with one of three actions:\\n  return {\\n    action: \\'accept\\', // or \\'decline\\' or \\'cancel\\'\\n    content: userInput, // only required when action is \\'accept\\'\\n  };\\n});\\n```\\n\\n### Elicitation Response Actions\\n\\nYour handler must return an object with an `action` field that can be one of:\\n\\n- `\\'accept\\'`: User provided the requested information. Must include `content` with the data.\\n- `\\'decline\\'`: User chose not to provide the information.\\n- `\\'cancel\\'`: User cancelled the operation entirely.\\n\\n## Examples\\n\\nYou can see MCP in action in the following examples:\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to use MCP tools in Node.js\\',\\n      link: \\'/cookbook/node/mcp-tools\\',\\n    },\\n    {\\n      title: \\'Learn to handle MCP elicitation requests in Node.js\\',\\n      link: \\'/cookbook/node/mcp-elicitation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/20-prompt-engineering.mdx'), name='20-prompt-engineering.mdx', displayName='20-prompt-engineering.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Prompt Engineering\\ndescription: Learn how to develop prompts with AI SDK Core.\\n---\\n\\n# Prompt Engineering\\n\\n## Tips\\n\\n### Prompts for Tools\\n\\nWhen you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.\\n\\nHere are a few tips to help you get the best results:\\n\\n1. Use a model that is strong at tool calling, such as `gpt-5` or `gpt-4.1`. Weaker models will often struggle to call tools effectively and flawlessly.\\n1. Keep the number of tools low, e.g. to 5 or less.\\n1. Keep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.\\n1. Use semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.\\n1. Add `.describe(\"...\")` to your Zod schema properties to give the model hints about what a particular property is for.\\n1. When the output of a tool might be unclear to the model and there are dependencies between tools, use the `description` field of a tool to provide information about the output of the tool execution.\\n1. You can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.\\n\\nIn general, the goal should be to give the model all information it needs in a clear way.\\n\\n### Tool & Structured Data Schemas\\n\\nThe mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.\\n\\n#### Zod Dates\\n\\nZod expects JavaScript Date objects, but models return dates as strings.\\nYou can specify and validate the date format using `z.string().datetime()` or `z.string().date()`,\\nand then use a Zod transformer to convert the string to a Date object.\\n\\n```ts highlight=\"7-10\"\\nconst result = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    events: z.array(\\n      z.object({\\n        event: z.string(),\\n        date: z\\n          .string()\\n          .date()\\n          .transform(value => new Date(value)),\\n      }),\\n    ),\\n  }),\\n  prompt: \\'List 5 important events from the year 2000.\\',\\n});\\n```\\n\\n#### Optional Parameters\\n\\nWhen working with tools that have optional parameters, you may encounter compatibility issues with certain providers that use strict schema validation.\\n\\n<Note>\\n  This is particularly relevant for OpenAI models with structured outputs\\n  (strict mode).\\n</Note>\\n\\nFor maximum compatibility, optional parameters should use `.nullable()` instead of `.optional()`:\\n\\n```ts highlight=\"6,7,16,17\"\\n// This may fail with strict schema validation\\nconst failingTool = tool({\\n  description: \\'Execute a command\\',\\n  inputSchema: z.object({\\n    command: z.string(),\\n    workdir: z.string().optional(), // This can cause errors\\n    timeout: z.string().optional(),\\n  }),\\n});\\n\\n// This works with strict schema validation\\nconst workingTool = tool({\\n  description: \\'Execute a command\\',\\n  inputSchema: z.object({\\n    command: z.string(),\\n    workdir: z.string().nullable(), // Use nullable instead\\n    timeout: z.string().nullable(),\\n  }),\\n});\\n```\\n\\n#### Temperature Settings\\n\\nFor tool calls and object generation, it\\'s recommended to use `temperature: 0` to ensure deterministic and consistent results:\\n\\n```ts highlight=\"3\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  temperature: 0, // Recommended for tool calls\\n  tools: {\\n    myTool: tool({\\n      description: \\'Execute a command\\',\\n      inputSchema: z.object({\\n        command: z.string(),\\n      }),\\n    }),\\n  },\\n  prompt: \\'Execute the ls command\\',\\n});\\n```\\n\\nLower temperature values reduce randomness in model outputs, which is particularly important when the model needs to:\\n\\n- Generate structured data with specific formats\\n- Make precise tool calls with correct parameters\\n- Follow strict schemas consistently\\n\\n## Debugging\\n\\n### Inspecting Warnings\\n\\nNot all providers support all AI SDK features.\\nProviders either throw exceptions or return warnings when they do not support a feature.\\nTo check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Hello, world!\\',\\n});\\n\\nconsole.log(result.warnings);\\n```\\n\\n### HTTP Request Bodies\\n\\nYou can inspect the raw HTTP request bodies for models that expose them, e.g. [OpenAI](/providers/ai-sdk-providers/openai).\\nThis allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.\\n\\nRequest bodies are available via the `request.body` property of the response:\\n\\n```ts highlight=\"6\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Hello, world!\\',\\n});\\n\\nconsole.log(result.request.body);\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/25-settings.mdx'), name='25-settings.mdx', displayName='25-settings.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Settings\\ndescription: Learn how to configure the AI SDK.\\n---\\n\\n# Settings\\n\\nLarge language models (LLMs) typically provide settings to augment their output.\\n\\nAll AI SDK functions support the following common settings in addition to the model, the [prompt](./prompts), and additional provider-specific settings:\\n\\n```ts highlight=\"3-5\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  maxOutputTokens: 512,\\n  temperature: 0.3,\\n  maxRetries: 5,\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\n<Note>\\n  Some providers do not support all common settings. If you use a setting with a\\n  provider that does not support it, a warning will be generated. You can check\\n  the `warnings` property in the result object to see if any warnings were\\n  generated.\\n</Note>\\n\\n### `maxOutputTokens`\\n\\nMaximum number of tokens to generate.\\n\\n### `temperature`\\n\\nTemperature setting.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, `0` means almost deterministic results, and higher values mean more randomness.\\n\\nIt is recommended to set either `temperature` or `topP`, but not both.\\n\\n<Note>In AI SDK 5.0, temperature is no longer set to `0` by default.</Note>\\n\\n### `topP`\\n\\nNucleus sampling.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, nucleus sampling is a number between 0 and 1.\\nE.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.\\n\\nIt is recommended to set either `temperature` or `topP`, but not both.\\n\\n### `topK`\\n\\nOnly sample from the top K options for each subsequent token.\\n\\nUsed to remove \"long tail\" low probability responses.\\nRecommended for advanced use cases only. You usually only need to use `temperature`.\\n\\n### `presencePenalty`\\n\\nThe presence penalty affects the likelihood of the model to repeat information that is already in the prompt.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, `0` means no penalty.\\n\\n### `frequencyPenalty`\\n\\nThe frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, `0` means no penalty.\\n\\n### `stopSequences`\\n\\nThe stop sequences to use for stopping the text generation.\\n\\nIf set, the model will stop generating text when one of the stop sequences is generated.\\nProviders may have limits on the number of stop sequences.\\n\\n### `seed`\\n\\nIt is the seed (integer) to use for random sampling.\\nIf set and supported by the model, calls will generate deterministic results.\\n\\n### `maxRetries`\\n\\nMaximum number of retries. Set to 0 to disable retries. Default: `2`.\\n\\n### `abortSignal`\\n\\nAn optional abort signal that can be used to cancel the call.\\n\\nThe abort signal can e.g. be forwarded from a user interface to cancel the call,\\nor to define a timeout.\\n\\n#### Example: Timeout\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  abortSignal: AbortSignal.timeout(5000), // 5 seconds\\n});\\n```\\n\\n### `headers`\\n\\nAdditional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\n\\nYou can use the request headers to provide additional information to the provider,\\ndepending on what the provider supports. For example, some observability providers support\\nheaders such as `Prompt-Id`.\\n\\n```ts\\nimport { generateText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  headers: {\\n    \\'Prompt-Id\\': \\'my-prompt-id\\',\\n  },\\n});\\n```\\n\\n<Note>\\n  The `headers` setting is for request-specific headers. You can also set\\n  `headers` in the provider configuration. These headers will be sent with every\\n  request made by the provider.\\n</Note>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/30-embeddings.mdx'), name='30-embeddings.mdx', displayName='30-embeddings.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Embeddings\\ndescription: Learn how to embed values with the AI SDK.\\n---\\n\\n# Embeddings\\n\\nEmbeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space.\\nIn this space, similar words are close to each other, and the distance between words can be used to measure their similarity.\\n\\n## Embedding a Single Value\\n\\nThe AI SDK provides the [`embed`](/docs/reference/ai-sdk-core/embed) function to embed single values, which is useful for tasks such as finding similar words\\nor phrases or clustering text.\\nYou can use it with embeddings models, e.g. `openai.embeddingModel(\\'text-embedding-3-large\\')` or `mistral.embeddingModel(\\'mistral-embed\\')`.\\n\\n```tsx\\nimport { embed } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// \\'embedding\\' is a single embedding object (number[])\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n## Embedding Many Values\\n\\nWhen loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG),\\nit is often useful to embed many values at once (batch embedding).\\n\\nThe AI SDK provides the [`embedMany`](/docs/reference/ai-sdk-core/embed-many) function for this purpose.\\nSimilar to `embed`, you can use it with embeddings models,\\ne.g. `openai.embeddingModel(\\'text-embedding-3-large\\')` or `mistral.embeddingModel(\\'mistral-embed\\')`.\\n\\n```tsx\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embedMany } from \\'ai\\';\\n\\n// \\'embeddings\\' is an array of embedding objects (number[][]).\\n// It is sorted in the same order as the input values.\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n});\\n```\\n\\n## Embedding Similarity\\n\\nAfter embedding values, you can calculate the similarity between them using the [`cosineSimilarity`](/docs/reference/ai-sdk-core/cosine-similarity) function.\\nThis is useful to e.g. find similar words or phrases in a dataset.\\nYou can also rank and filter related items based on their similarity.\\n\\n```ts highlight={\"2,10\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { cosineSimilarity, embedMany } from \\'ai\\';\\n\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n});\\n\\nconsole.log(\\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\\n);\\n```\\n\\n## Token Usage\\n\\nMany providers charge based on the number of tokens used to generate embeddings.\\nBoth `embed` and `embedMany` provide token usage information in the `usage` property of the result object:\\n\\n```ts highlight={\"4,9\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding, usage } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n\\nconsole.log(usage); // { tokens: 10 }\\n```\\n\\n## Settings\\n\\n### Provider Options\\n\\nEmbedding model settings can be configured using `providerOptions` for provider-specific parameters:\\n\\n```ts highlight={\"5-9\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  providerOptions: {\\n    openai: {\\n      dimensions: 512, // Reduce embedding dimensions\\n    },\\n  },\\n});\\n```\\n\\n### Parallel Requests\\n\\nThe `embedMany` function now supports parallel processing with configurable `maxParallelCalls` to optimize performance:\\n\\n```ts highlight={\"4\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embedMany } from \\'ai\\';\\n\\nconst { embeddings, usage } = await embedMany({\\n  maxParallelCalls: 2, // Limit parallel requests\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n});\\n```\\n\\n### Retries\\n\\nBoth `embed` and `embedMany` accept an optional `maxRetries` parameter of type `number`\\nthat you can use to set the maximum number of retries for the embedding process.\\nIt defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  maxRetries: 0, // Disable retries\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\nBoth `embed` and `embedMany` accept an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the embedding process or set a timeout.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\nBoth `embed` and `embedMany` accept an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the embedding request.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n## Response Information\\n\\nBoth `embed` and `embedMany` return response information that includes the raw provider response:\\n\\n```ts highlight={\"4,9\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding, response } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n\\nconsole.log(response); // Raw provider response\\n```\\n\\n## Embedding Middleware\\n\\nYou can enhance embedding models, e.g. to set default values, using\\n`wrapEmbeddingModel` and `EmbeddingModelV3Middleware`.\\n\\nHere is an example that uses the built-in `defaultEmbeddingSettingsMiddleware`:\\n\\n```ts\\nimport {\\n  customProvider,\\n  defaultEmbeddingSettingsMiddleware,\\n  embed,\\n  wrapEmbeddingModel,\\n  gateway,\\n} from \\'ai\\';\\n\\nconst embeddingModelWithDefaults = wrapEmbeddingModel({\\n  model: gateway.embeddingModel(\\'google/gemini-embedding-001\\'),\\n  middleware: defaultEmbeddingSettingsMiddleware({\\n    settings: {\\n      providerOptions: {\\n        google: {\\n          outputDimensionality: 256,\\n          taskType: \\'CLASSIFICATION\\',\\n        },\\n      },\\n    },\\n  }),\\n});\\n```\\n\\n## Embedding Providers & Models\\n\\nSeveral providers offer embedding models:\\n\\n| Provider                                                                                  | Model                           | Embedding Dimensions |\\n| ----------------------------------------------------------------------------------------- | ------------------------------- | -------------------- |\\n| [OpenAI](/providers/ai-sdk-providers/openai#embedding-models)                             | `text-embedding-3-large`        | 3072                 |\\n| [OpenAI](/providers/ai-sdk-providers/openai#embedding-models)                             | `text-embedding-3-small`        | 1536                 |\\n| [OpenAI](/providers/ai-sdk-providers/openai#embedding-models)                             | `text-embedding-ada-002`        | 1536                 |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `gemini-embedding-001`          | 3072                 |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `text-embedding-004`            | 768                  |\\n| [Mistral](/providers/ai-sdk-providers/mistral#embedding-models)                           | `mistral-embed`                 | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-v3.0`            | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-multilingual-v3.0`       | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-light-v3.0`      | 384                  |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-multilingual-light-v3.0` | 384                  |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-v2.0`            | 4096                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-light-v2.0`      | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-multilingual-v2.0`       | 768                  |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models)             | `amazon.titan-embed-text-v1`    | 1536                 |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models)             | `amazon.titan-embed-text-v2:0`  | 1024                 |\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/31-reranking.mdx'), name='31-reranking.mdx', displayName='31-reranking.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Reranking\\ndescription: Learn how to rerank documents with the AI SDK.\\n---\\n\\n# Reranking\\n\\nReranking is a technique used to improve search relevance by reordering a set of documents based on their relevance to a query.\\nUnlike embedding-based similarity search, reranking models are specifically trained to understand the relationship between queries and documents,\\noften producing more accurate relevance scores.\\n\\n## Reranking Documents\\n\\nThe AI SDK provides the [`rerank`](/docs/reference/ai-sdk-core/rerank) function to rerank documents based on their relevance to a query.\\nYou can use it with reranking models, e.g. `cohere.reranking(\\'rerank-v3.5\\')` or `bedrock.reranking(\\'cohere.rerank-v3-5:0\\')`.\\n\\n```tsx\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  \\'sunny day at the beach\\',\\n  \\'rainy afternoon in the city\\',\\n  \\'snowy night in the mountains\\',\\n];\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'talk about rain\\',\\n  topN: 2, // Return top 2 most relevant documents\\n});\\n\\nconsole.log(ranking);\\n// [\\n//   { originalIndex: 1, score: 0.9, document: \\'rainy afternoon in the city\\' },\\n//   { originalIndex: 0, score: 0.3, document: \\'sunny day at the beach\\' }\\n// ]\\n```\\n\\n## Working with Object Documents\\n\\nReranking also supports structured documents (JSON objects), making it ideal for searching through databases, emails, or other structured content:\\n\\n```tsx\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  {\\n    from: \\'Paul Doe\\',\\n    subject: \\'Follow-up\\',\\n    text: \\'We are happy to give you a discount of 20% on your next order.\\',\\n  },\\n  {\\n    from: \\'John McGill\\',\\n    subject: \\'Missing Info\\',\\n    text: \\'Sorry, but here is the pricing information from Oracle: $5000/month\\',\\n  },\\n];\\n\\nconst { ranking, rerankedDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'Which pricing did we get from Oracle?\\',\\n  topN: 1,\\n});\\n\\nconsole.log(rerankedDocuments[0]);\\n// { from: \\'John McGill\\', subject: \\'Missing Info\\', text: \\'...\\' }\\n```\\n\\n## Understanding the Results\\n\\nThe `rerank` function returns a comprehensive result object:\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking, rerankedDocuments, originalDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n});\\n\\n// ranking: sorted array of { originalIndex, score, document }\\n// rerankedDocuments: documents sorted by relevance (convenience property)\\n// originalDocuments: original documents array\\n```\\n\\nEach item in the `ranking` array contains:\\n\\n- `originalIndex`: Position in the original documents array\\n- `score`: Relevance score (typically 0-1, where higher is more relevant)\\n- `document`: The original document\\n\\n## Settings\\n\\n### Top-N Results\\n\\nUse `topN` to limit the number of results returned. This is useful for retrieving only the most relevant documents:\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'doc1\\', \\'doc2\\', \\'doc3\\', \\'doc4\\', \\'doc5\\'],\\n  query: \\'relevant information\\',\\n  topN: 3, // Return only top 3 most relevant documents\\n});\\n```\\n\\n### Provider Options\\n\\nReranking model settings can be configured using `providerOptions` for provider-specific parameters:\\n\\n```ts highlight={\"8-12\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  providerOptions: {\\n    cohere: {\\n      maxTokensPerDoc: 1000, // Limit tokens per document\\n    },\\n  },\\n});\\n```\\n\\n### Retries\\n\\nThe `rerank` function accepts an optional `maxRetries` parameter of type `number`\\nthat you can use to set the maximum number of retries for the reranking process.\\nIt defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  maxRetries: 0, // Disable retries\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\nThe `rerank` function accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the reranking process or set a timeout.\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  abortSignal: AbortSignal.timeout(5000), // Abort after 5 seconds\\n});\\n```\\n\\n### Custom Headers\\n\\nThe `rerank` function accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the reranking request.\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n## Response Information\\n\\nThe `rerank` function returns response information that includes the raw provider response:\\n\\n```ts highlight={\"4,10\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking, response } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n});\\n\\nconsole.log(response); // { id, timestamp, modelId, headers, body }\\n```\\n\\n## Reranking Providers & Models\\n\\nSeveral providers offer reranking models:\\n\\n| Provider                                                                      | Model                                 |\\n| ----------------------------------------------------------------------------- | ------------------------------------- |\\n| [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)                 | `rerank-v3.5`                         |\\n| [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)                 | `rerank-english-v3.0`                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)                 | `rerank-multilingual-v3.0`            |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `amazon.rerank-v1:0`                  |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `cohere.rerank-v3-5:0`                |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models)        | `Salesforce/Llama-Rank-v1`            |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models)        | `mixedbread-ai/Mxbai-Rerank-Large-V2` |\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/35-image-generation.mdx'), name='35-image-generation.mdx', displayName='35-image-generation.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Image Generation\\ndescription: Learn how to generate images with the AI SDK.\\n---\\n\\n# Image Generation\\n\\n<Note type=\"warning\">Image generation is an experimental feature.</Note>\\n\\nThe AI SDK provides the [`generateImage`](/docs/reference/ai-sdk-core/generate-image)\\nfunction to generate images based on a given prompt using an image model.\\n\\n```tsx\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n});\\n```\\n\\nYou can access the image data using the `base64` or `uint8Array` properties:\\n\\n```tsx\\nconst base64 = image.base64; // base64 image data\\nconst uint8Array = image.uint8Array; // Uint8Array image data\\n```\\n\\n## Settings\\n\\n### Size and Aspect Ratio\\n\\nDepending on the model, you can either specify the size or the aspect ratio.\\n\\n##### Size\\n\\nThe size is specified as a string in the format `{width}x{height}`.\\nModels only support a few sizes, and the supported sizes are different for each model and provider.\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  size: \\'1024x1024\\',\\n});\\n```\\n\\n##### Aspect Ratio\\n\\nThe aspect ratio is specified as a string in the format `{width}:{height}`.\\nModels only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { vertex } from \\'@ai-sdk/google-vertex\\';\\n\\nconst { image } = await generateImage({\\n  model: vertex.image(\\'imagen-3.0-generate-002\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  aspectRatio: \\'16:9\\',\\n});\\n```\\n\\n### Generating Multiple Images\\n\\n`generateImage` also supports generating multiple images at once:\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { images } = await generateImage({\\n  model: openai.image(\\'dall-e-2\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  n: 4, // number of images to generate\\n});\\n```\\n\\n<Note>\\n  `generateImage` will automatically call the model as often as needed (in\\n  parallel) to generate the requested number of images.\\n</Note>\\n\\nEach image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the `n` parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).\\n\\nIf needed, you can override this behavior using the `maxImagesPerCall` setting when generating your image. This is particularly useful when working with new or custom models where the default batch size might not be optimal:\\n\\n```tsx\\nconst { images } = await generateImage({\\n  model: openai.image(\\'dall-e-2\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  maxImagesPerCall: 5, // Override the default batch size\\n  n: 10, // Will make 2 calls of 5 images each\\n});\\n```\\n\\n### Providing a Seed\\n\\nYou can provide a seed to the `generateImage` function to control the output of the image generation process.\\nIf supported by the model, the same seed will always produce the same image.\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  seed: 1234567890,\\n});\\n```\\n\\n### Provider-specific Settings\\n\\nImage models often have provider- or even model-specific settings.\\nYou can pass such settings to the `generateImage` function\\nusing the `providerOptions` parameter. The options for the provider\\n(`openai` in the example below) become request body properties.\\n\\n```tsx highlight={\"9\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  size: \\'1024x1024\\',\\n  providerOptions: {\\n    openai: { style: \\'vivid\\', quality: \\'hd\\' },\\n  },\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\n`generateImage` accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the image generation process or set a timeout.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\n`generateImage` accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the image generation request.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n### Warnings\\n\\nIf the model returns warnings, e.g. for unsupported parameters, they will be available in the `warnings` property of the response.\\n\\n```tsx\\nconst { image, warnings } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n});\\n```\\n\\n### Additional provider-specific meta data\\n\\nSome providers expose additional meta data for the result overall or per image.\\n\\n```tsx\\nconst prompt = \\'Santa Claus driving a Cadillac\\';\\n\\nconst { image, providerMetadata } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt,\\n});\\n\\nconst revisedPrompt = providerMetadata.openai.images[0]?.revisedPrompt;\\n\\nconsole.log({\\n  prompt,\\n  revisedPrompt,\\n});\\n```\\n\\nThe outer key of the returned `providerMetadata` is the provider name. The inner values are the metadata. An `images` key is always present in the metadata and is an array with the same length as the top level `images` key.\\n\\n### Error Handling\\n\\nWhen `generateImage` cannot generate a valid image, it throws a [`AI_NoImageGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-image-generated-error).\\n\\nThis error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:\\n\\n- The model failed to generate a response\\n- The model generated a response that could not be parsed\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `responses`: Metadata about the image model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling\\n\\n```ts\\nimport { generateImage, NoImageGeneratedError } from \\'ai\\';\\n\\ntry {\\n  await generateImage({ model, prompt });\\n} catch (error) {\\n  if (NoImageGeneratedError.isInstance(error)) {\\n    console.log(\\'NoImageGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Responses:\\', error.responses);\\n  }\\n}\\n```\\n\\n## Generating Images with Language Models\\n\\nSome language models such as Google `gemini-2.5-flash-image-preview` support multi-modal outputs including images.\\nWith such models, you can access the generated images using the `files` property of the response.\\n\\n```ts\\nimport { google } from \\'@ai-sdk/google\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: google(\\'gemini-2.5-flash-image-preview\\'),\\n  prompt: \\'Generate an image of a comic cat\\',\\n});\\n\\nfor (const file of result.files) {\\n  if (file.mediaType.startsWith(\\'image/\\')) {\\n    // The file object provides multiple data formats:\\n    // Access images as base64 string, Uint8Array binary data, or check type\\n    // - file.base64: string (data URL format)\\n    // - file.uint8Array: Uint8Array (binary data)\\n    // - file.mediaType: string (e.g. \"image/png\")\\n  }\\n}\\n```\\n\\n## Image Models\\n\\n| Provider                                                                        | Model                                                        | Support sizes (`width x height`) or aspect ratios (`width : height`)                                                                                                |\\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [xAI Grok](/providers/ai-sdk-providers/xai#image-models)                        | `grok-2-image`                                               | 1024x768 (default)                                                                                                                                                  |\\n| [OpenAI](/providers/ai-sdk-providers/openai#image-models)                       | `gpt-image-1`                                                | 1024x1024, 1536x1024, 1024x1536                                                                                                                                     |\\n| [OpenAI](/providers/ai-sdk-providers/openai#image-models)                       | `dall-e-3`                                                   | 1024x1024, 1792x1024, 1024x1792                                                                                                                                     |\\n| [OpenAI](/providers/ai-sdk-providers/openai#image-models)                       | `dall-e-2`                                                   | 256x256, 512x512, 1024x1024                                                                                                                                         |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#image-models)       | `amazon.nova-canvas-v1:0`                                    | 320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels                                                                                                             |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/flux/dev`                                            | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/flux-lora`                                           | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/fast-sdxl`                                           | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/flux-pro/v1.1-ultra`                                 | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/ideogram/v2`                                         | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/recraft-v3`                                          | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/stable-diffusion-3.5-large`                          | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/hyper-sdxl`                                          | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `stabilityai/sd3.5`                                          | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21                                                                                                                      |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-1.1-pro`                             | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-1-schnell`                           | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-1-dev`                               | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-pro`                                 | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `stabilityai/sd3.5-medium`                                   | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21                                                                                                                      |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `stabilityai/sdxl-turbo`                                     | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21                                                                                                                      |\\n| [Replicate](/providers/ai-sdk-providers/replicate)                              | `black-forest-labs/flux-schnell`                             | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9                                                                                                                     |\\n| [Replicate](/providers/ai-sdk-providers/replicate)                              | `recraft-ai/recraft-v3`                                      | 1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024 |\\n| [Google](/providers/ai-sdk-providers/google#image-models)                       | `imagen-3.0-generate-002`                                    | 1:1, 3:4, 4:3, 9:16, 16:9                                                                                                                                           |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models)         | `imagen-3.0-generate-002`                                    | 1:1, 3:4, 4:3, 9:16, 16:9                                                                                                                                           |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models)         | `imagen-3.0-fast-generate-001`                               | 1:1, 3:4, 4:3, 9:16, 16:9                                                                                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/flux-1-dev-fp8`                   | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9                                                                                                                     |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/flux-1-schnell-fp8`               | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9                                                                                                                     |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/playground-v2-5-1024px-aesthetic` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/japanese-stable-diffusion-xl`     | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/playground-v2-1024px-aesthetic`   | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/SSD-1B`                           | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/stable-diffusion-xl-1024-v1-0`    | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Luma](/providers/ai-sdk-providers/luma#image-models)                           | `photon-1`                                                   | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Luma](/providers/ai-sdk-providers/luma#image-models)                           | `photon-flash-1`                                             | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `stabilityai/stable-diffusion-xl-base-1.0`                   | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-dev`                               | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-dev-lora`                          | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-schnell`                           | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-canny`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-depth`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-redux`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1.1-pro`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-pro`                               | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-schnell-Free`                      | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-pro`                                           | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-max`                                           | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1-ultra`                                         | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1`                                               | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.0-fill`                                          | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n\\nAbove are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/36-transcription.mdx'), name='36-transcription.mdx', displayName='36-transcription.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Transcription\\ndescription: Learn how to transcribe audio with the AI SDK.\\n---\\n\\n# Transcription\\n\\n<Note type=\"warning\">Transcription is an experimental feature.</Note>\\n\\nThe AI SDK provides the [`transcribe`](/docs/reference/ai-sdk-core/transcribe)\\nfunction to transcribe audio using a transcription model.\\n\\n```ts\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n});\\n```\\n\\nThe `audio` property can be a `Uint8Array`, `ArrayBuffer`, `Buffer`, `string` (base64 encoded audio data), or a `URL`.\\n\\nTo access the generated transcript:\\n\\n```ts\\nconst text = transcript.text; // transcript text e.g. \"Hello, world!\"\\nconst segments = transcript.segments; // array of segments with start and end times, if available\\nconst language = transcript.language; // language of the transcript e.g. \"en\", if available\\nconst durationInSeconds = transcript.durationInSeconds; // duration of the transcript in seconds, if available\\n```\\n\\n## Settings\\n\\n### Provider-Specific settings\\n\\nTranscription models often have provider or model-specific settings which you can set using the `providerOptions` parameter.\\n\\n```ts highlight=\"8-12\"\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n  providerOptions: {\\n    openai: {\\n      timestampGranularities: [\\'word\\'],\\n    },\\n  },\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\n`transcribe` accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the transcription process or set a timeout.\\n\\n```ts highlight=\"8\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\n`transcribe` accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the transcription request.\\n\\n```ts highlight=\"8\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n### Warnings\\n\\nWarnings (e.g. unsupported parameters) are available on the `warnings` property.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n});\\n\\nconst warnings = transcript.warnings;\\n```\\n\\n### Error Handling\\n\\nWhen `transcribe` cannot generate a valid transcript, it throws a [`AI_NoTranscriptGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error).\\n\\nThis error can arise for any the following reasons:\\n\\n- The model failed to generate a response\\n- The model generated a response that could not be parsed\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `responses`: Metadata about the transcription model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling.\\n\\n```ts\\nimport {\\n  experimental_transcribe as transcribe,\\n  NoTranscriptGeneratedError,\\n} from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\ntry {\\n  await transcribe({\\n    model: openai.transcription(\\'whisper-1\\'),\\n    audio: await readFile(\\'audio.mp3\\'),\\n  });\\n} catch (error) {\\n  if (NoTranscriptGeneratedError.isInstance(error)) {\\n    console.log(\\'NoTranscriptGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Responses:\\', error.responses);\\n  }\\n}\\n```\\n\\n## Transcription Models\\n\\n| Provider                                                                  | Model                        |\\n| ------------------------------------------------------------------------- | ---------------------------- |\\n| [OpenAI](/providers/ai-sdk-providers/openai#transcription-models)         | `whisper-1`                  |\\n| [OpenAI](/providers/ai-sdk-providers/openai#transcription-models)         | `gpt-4o-transcribe`          |\\n| [OpenAI](/providers/ai-sdk-providers/openai#transcription-models)         | `gpt-4o-mini-transcribe`     |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe_v1`                  |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe_v1_experimental`     |\\n| [Groq](/providers/ai-sdk-providers/groq#transcription-models)             | `whisper-large-v3-turbo`     |\\n| [Groq](/providers/ai-sdk-providers/groq#transcription-models)             | `distil-whisper-large-v3-en` |\\n| [Groq](/providers/ai-sdk-providers/groq#transcription-models)             | `whisper-large-v3`           |\\n| [Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)    | `whisper-1`                  |\\n| [Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)    | `gpt-4o-transcribe`          |\\n| [Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)    | `gpt-4o-mini-transcribe`     |\\n| [Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)          | `machine`                    |\\n| [Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)          | `low_cost`                   |\\n| [Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)          | `fusion`                     |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `base` (+ variants)          |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `enhanced` (+ variants)      |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `nova` (+ variants)          |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `nova-2` (+ variants)        |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `nova-3` (+ variants)        |\\n| [Gladia](/providers/ai-sdk-providers/gladia#transcription-models)         | `default`                    |\\n| [AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models) | `best`                       |\\n| [AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models) | `nano`                       |\\n| [Fal](/providers/ai-sdk-providers/fal#transcription-models)               | `whisper`                    |\\n| [Fal](/providers/ai-sdk-providers/fal#transcription-models)               | `wizper`                     |\\n\\nAbove are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/37-speech.mdx'), name='37-speech.mdx', displayName='37-speech.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Speech\\ndescription: Learn how to generate speech from text with the AI SDK.\\n---\\n\\n# Speech\\n\\n<Note type=\"warning\">Speech is an experimental feature.</Note>\\n\\nThe AI SDK provides the [`generateSpeech`](/docs/reference/ai-sdk-core/generate-speech)\\nfunction to generate speech from text using a speech model.\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  voice: \\'alloy\\',\\n});\\n```\\n\\n### Language Setting\\n\\nYou can specify the language for speech generation (provider support varies):\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { lmnt } from \\'@ai-sdk/lmnt\\';\\n\\nconst audio = await generateSpeech({\\n  model: lmnt.speech(\\'aurora\\'),\\n  text: \\'Hola, mundo!\\',\\n  language: \\'es\\', // Spanish\\n});\\n```\\n\\nTo access the generated audio:\\n\\n```ts\\nconst audio = audio.audioData; // audio data e.g. Uint8Array\\n```\\n\\n## Settings\\n\\n### Provider-Specific settings\\n\\nYou can set model-specific settings with the `providerOptions` parameter.\\n\\n```ts highlight=\"7-11\"\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  providerOptions: {\\n    openai: {\\n      // ...\\n    },\\n  },\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\n`generateSpeech` accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the speech generation process or set a timeout.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\n`generateSpeech` accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the speech generation request.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n### Warnings\\n\\nWarnings (e.g. unsupported parameters) are available on the `warnings` property.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n});\\n\\nconst warnings = audio.warnings;\\n```\\n\\n### Error Handling\\n\\nWhen `generateSpeech` cannot generate a valid audio, it throws a [`AI_NoSpeechGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-speech-generated-error).\\n\\nThis error can arise for any the following reasons:\\n\\n- The model failed to generate a response\\n- The model generated a response that could not be parsed\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `responses`: Metadata about the speech model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling.\\n\\n```ts\\nimport {\\n  experimental_generateSpeech as generateSpeech,\\n  NoSpeechGeneratedError,\\n} from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\ntry {\\n  await generateSpeech({\\n    model: openai.speech(\\'tts-1\\'),\\n    text: \\'Hello, world!\\',\\n  });\\n} catch (error) {\\n  if (NoSpeechGeneratedError.isInstance(error)) {\\n    console.log(\\'AI_NoSpeechGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Responses:\\', error.responses);\\n  }\\n}\\n```\\n\\n## Speech Models\\n\\n| Provider                                                           | Model                    |\\n| ------------------------------------------------------------------ | ------------------------ |\\n| [OpenAI](/providers/ai-sdk-providers/openai#speech-models)         | `tts-1`                  |\\n| [OpenAI](/providers/ai-sdk-providers/openai#speech-models)         | `tts-1-hd`               |\\n| [OpenAI](/providers/ai-sdk-providers/openai#speech-models)         | `gpt-4o-mini-tts`        |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_v3`              |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_multilingual_v2` |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_flash_v2_5`      |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_flash_v2`        |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_turbo_v2_5`      |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_turbo_v2`        |\\n| [LMNT](/providers/ai-sdk-providers/lmnt#speech-models)             | `aurora`                 |\\n| [LMNT](/providers/ai-sdk-providers/lmnt#speech-models)             | `blizzard`               |\\n| [Hume](/providers/ai-sdk-providers/hume#speech-models)             | `default`                |\\n\\nAbove are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/40-middleware.mdx'), name='40-middleware.mdx', displayName='40-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Language Model Middleware\\ndescription: Learn how to use middleware to enhance the behavior of language models\\n---\\n\\n# Language Model Middleware\\n\\nLanguage model middleware is a way to enhance the behavior of language models\\nby intercepting and modifying the calls to the language model.\\n\\nIt can be used to add features like guardrails, RAG, caching, and logging\\nin a language model agnostic way. Such middleware can be developed and\\ndistributed independently from the language models that they are applied to.\\n\\n## Using Language Model Middleware\\n\\nYou can use language model middleware with the `wrapLanguageModel` function.\\nIt takes a language model and a language model middleware and returns a new\\nlanguage model that incorporates the middleware.\\n\\n```ts\\nimport { wrapLanguageModel } from \\'ai\\';\\n\\nconst wrappedLanguageModel = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: yourLanguageModelMiddleware,\\n});\\n```\\n\\nThe wrapped language model can be used just like any other language model, e.g. in `streamText`:\\n\\n```ts highlight=\"2\"\\nconst result = streamText({\\n  model: wrappedLanguageModel,\\n  prompt: \\'What cities are in the United States?\\',\\n});\\n```\\n\\n## Multiple middlewares\\n\\nYou can provide multiple middlewares to the `wrapLanguageModel` function.\\nThe middlewares will be applied in the order they are provided.\\n\\n```ts\\nconst wrappedLanguageModel = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: [firstMiddleware, secondMiddleware],\\n});\\n\\n// applied as: firstMiddleware(secondMiddleware(yourModel))\\n```\\n\\n## Built-in Middleware\\n\\nThe AI SDK comes with several built-in middlewares that you can use to configure language models:\\n\\n- `extractReasoningMiddleware`: Extracts reasoning information from the generated text and exposes it as a `reasoning` property on the result.\\n- `simulateStreamingMiddleware`: Simulates streaming behavior with responses from non-streaming language models.\\n- `defaultSettingsMiddleware`: Applies default settings to a language model.\\n\\n### Extract Reasoning\\n\\nSome providers and models expose reasoning information in the generated text using special tags,\\ne.g. &lt;think&gt; and &lt;/think&gt;.\\n\\nThe `extractReasoningMiddleware` function can be used to extract this reasoning information and expose it as a `reasoning` property on the result.\\n\\n```ts\\nimport { wrapLanguageModel, extractReasoningMiddleware } from \\'ai\\';\\n\\nconst model = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: extractReasoningMiddleware({ tagName: \\'think\\' }),\\n});\\n```\\n\\nYou can then use that enhanced model in functions like `generateText` and `streamText`.\\n\\nThe `extractReasoningMiddleware` function also includes a `startWithReasoning` option.\\nWhen set to `true`, the reasoning tag will be prepended to the generated text.\\nThis is useful for models that do not include the reasoning tag at the beginning of the response.\\nFor more details, see the [DeepSeek R1 guide](/docs/guides/r1#deepseek-r1-middleware).\\n\\n### Simulate Streaming\\n\\nThe `simulateStreamingMiddleware` function can be used to simulate streaming behavior with responses from non-streaming language models.\\nThis is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\\n\\n```ts\\nimport { wrapLanguageModel, simulateStreamingMiddleware } from \\'ai\\';\\n\\nconst model = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: simulateStreamingMiddleware(),\\n});\\n```\\n\\n### Default Settings\\n\\nThe `defaultSettingsMiddleware` function can be used to apply default settings to a language model.\\n\\n```ts\\nimport { wrapLanguageModel, defaultSettingsMiddleware } from \\'ai\\';\\n\\nconst model = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: defaultSettingsMiddleware({\\n    settings: {\\n      temperature: 0.5,\\n      maxOutputTokens: 800,\\n      providerOptions: { openai: { store: false } },\\n    },\\n  }),\\n});\\n```\\n\\n## Community Middleware\\n\\nThe AI SDK provides a Language Model Middleware specification. Community members can develop middleware that adheres to this specification, making it compatible with the AI SDK ecosystem.\\n\\nHere are some community middlewares that you can explore:\\n\\n### Custom tool call parser\\n\\nThe [Custom tool call parser](https://github.com/minpeter/ai-sdk-tool-call-middleware) middleware extends tool call capabilities to models that don\\'t natively support the OpenAI-style `tools` parameter. This includes many self-hosted and third-party models that lack native function calling features.\\n\\n<Note>\\n  Using this middleware on models that support native function calls may result\\n  in unintended performance degradation, so check whether your model supports\\n  native function calls before deciding to use it.\\n</Note>\\n\\nThis middleware enables function calling capabilities by converting function schemas into prompt instructions and parsing the model\\'s responses into structured function calls. It works by transforming the JSON function definitions into natural language instructions the model can understand, then analyzing the generated text to extract function call attempts. This approach allows developers to use the same function calling API across different model providers, even with models that don\\'t natively support the OpenAI-style function calling format, providing a consistent function calling experience regardless of the underlying model implementation.\\n\\nThe `@ai-sdk-tool/parser` package offers three middleware variants:\\n\\n- `createToolMiddleware`: A flexible function for creating custom tool call middleware tailored to specific models\\n- `hermesToolMiddleware`: Ready-to-use middleware for Hermes & Qwen format function calls\\n- `gemmaToolMiddleware`: Pre-configured middleware for Gemma 3 model series function call format\\n\\nHere\\'s how you can enable function calls with Gemma models that don\\'t support them natively:\\n\\n```ts\\nimport { wrapLanguageModel } from \\'ai\\';\\nimport { gemmaToolMiddleware } from \\'@ai-sdk-tool/parser\\';\\n\\nconst model = wrapLanguageModel({\\n  model: openrouter(\\'google/gemma-3-27b-it\\'),\\n  middleware: gemmaToolMiddleware,\\n});\\n```\\n\\nFind more examples at this [link](https://github.com/minpeter/ai-sdk-tool-call-middleware/tree/main/examples/core/src).\\n\\n## Implementing Language Model Middleware\\n\\n<Note>\\n  Implementing language model middleware is advanced functionality and requires\\n  a solid understanding of the [language model\\n  specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\\n</Note>\\n\\nYou can implement any of the following three function to modify the behavior of the language model:\\n\\n1. `transformParams`: Transforms the parameters before they are passed to the language model, for both `doGenerate` and `doStream`.\\n2. `wrapGenerate`: Wraps the `doGenerate` method of the [language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\\n   You can modify the parameters, call the language model, and modify the result.\\n3. `wrapStream`: Wraps the `doStream` method of the [language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\\n   You can modify the parameters, call the language model, and modify the result.\\n\\nHere are some examples of how to implement language model middleware:\\n\\n## Examples\\n\\n<Note>\\n  These examples are not meant to be used in production. They are just to show\\n  how you can use middleware to enhance the behavior of language models.\\n</Note>\\n\\n### Logging\\n\\nThis example shows how to log the parameters and generated text of a language model call.\\n\\n```ts\\nimport type {\\n  LanguageModelV3Middleware,\\n  LanguageModelV3StreamPart,\\n} from \\'@ai-sdk/provider\\';\\n\\nexport const yourLogMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    console.log(\\'doGenerate called\\');\\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\\n\\n    const result = await doGenerate();\\n\\n    console.log(\\'doGenerate finished\\');\\n    console.log(`generated text: ${result.text}`);\\n\\n    return result;\\n  },\\n\\n  wrapStream: async ({ doStream, params }) => {\\n    console.log(\\'doStream called\\');\\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\\n\\n    const { stream, ...rest } = await doStream();\\n\\n    let generatedText = \\'\\';\\n    const textBlocks = new Map<string, string>();\\n\\n    const transformStream = new TransformStream<\\n      LanguageModelV3StreamPart,\\n      LanguageModelV3StreamPart\\n    >({\\n      transform(chunk, controller) {\\n        switch (chunk.type) {\\n          case \\'text-start\\': {\\n            textBlocks.set(chunk.id, \\'\\');\\n            break;\\n          }\\n          case \\'text-delta\\': {\\n            const existing = textBlocks.get(chunk.id) || \\'\\';\\n            textBlocks.set(chunk.id, existing + chunk.delta);\\n            generatedText += chunk.delta;\\n            break;\\n          }\\n          case \\'text-end\\': {\\n            console.log(\\n              `Text block ${chunk.id} completed:`,\\n              textBlocks.get(chunk.id),\\n            );\\n            break;\\n          }\\n        }\\n\\n        controller.enqueue(chunk);\\n      },\\n\\n      flush() {\\n        console.log(\\'doStream finished\\');\\n        console.log(`generated text: ${generatedText}`);\\n      },\\n    });\\n\\n    return {\\n      stream: stream.pipeThrough(transformStream),\\n      ...rest,\\n    };\\n  },\\n};\\n```\\n\\n### Caching\\n\\nThis example shows how to build a simple cache for the generated text of a language model call.\\n\\n```ts\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nconst cache = new Map<string, any>();\\n\\nexport const yourCacheMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    const cacheKey = JSON.stringify(params);\\n\\n    if (cache.has(cacheKey)) {\\n      return cache.get(cacheKey);\\n    }\\n\\n    const result = await doGenerate();\\n\\n    cache.set(cacheKey, result);\\n\\n    return result;\\n  },\\n\\n  // here you would implement the caching logic for streaming\\n};\\n```\\n\\n### Retrieval Augmented Generation (RAG)\\n\\nThis example shows how to use RAG as middleware.\\n\\n<Note>\\n  Helper functions like `getLastUserMessageText` and `findSources` are not part\\n  of the AI SDK. They are just used in this example to illustrate the concept of\\n  RAG.\\n</Note>\\n\\n```ts\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nexport const yourRagMiddleware: LanguageModelV3Middleware = {\\n  transformParams: async ({ params }) => {\\n    const lastUserMessageText = getLastUserMessageText({\\n      prompt: params.prompt,\\n    });\\n\\n    if (lastUserMessageText == null) {\\n      return params; // do not use RAG (send unmodified parameters)\\n    }\\n\\n    const instruction =\\n      \\'Use the following information to answer the question:\\\\n\\' +\\n      findSources({ text: lastUserMessageText })\\n        .map(chunk => JSON.stringify(chunk))\\n        .join(\\'\\\\n\\');\\n\\n    return addToLastUserMessage({ params, text: instruction });\\n  },\\n};\\n```\\n\\n### Guardrails\\n\\nGuard rails are a way to ensure that the generated text of a language model call\\nis safe and appropriate. This example shows how to use guardrails as middleware.\\n\\n```ts\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nexport const yourGuardrailMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate }) => {\\n    const { text, ...rest } = await doGenerate();\\n\\n    // filtering approach, e.g. for PII or other sensitive information:\\n    const cleanedText = text?.replace(/badword/g, \\'<REDACTED>\\');\\n\\n    return { text: cleanedText, ...rest };\\n  },\\n\\n  // here you would implement the guardrail logic for streaming\\n  // Note: streaming guardrails are difficult to implement, because\\n  // you do not know the full content of the stream until it\\'s finished.\\n};\\n```\\n\\n## Configuring Per Request Custom Metadata\\n\\nTo send and access custom metadata in Middleware, you can use `providerOptions`. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText, wrapLanguageModel } from \\'ai\\';\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nexport const yourLogMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    console.log(\\'METADATA\\', params?.providerMetadata?.yourLogMiddleware);\\n    const result = await doGenerate();\\n    return result;\\n  },\\n};\\n\\nconst { text } = await generateText({\\n  model: wrapLanguageModel({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    middleware: yourLogMiddleware,\\n  }),\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  providerOptions: {\\n    yourLogMiddleware: {\\n      hello: \\'world\\',\\n    },\\n  },\\n});\\n\\nconsole.log(text);\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/45-provider-management.mdx'), name='45-provider-management.mdx', displayName='45-provider-management.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Provider & Model Management\\ndescription: Learn how to work with multiple providers and models\\n---\\n\\n# Provider & Model Management\\n\\nWhen you work with multiple providers and models, it is often desirable to manage them in a central place\\nand access the models through simple string ids.\\n\\nThe AI SDK offers [custom providers](/docs/reference/ai-sdk-core/custom-provider) and\\na [provider registry](/docs/reference/ai-sdk-core/provider-registry) for this purpose:\\n\\n- With **custom providers**, you can pre-configure model settings, provide model name aliases,\\n  and limit the available models.\\n- The **provider registry** lets you mix multiple providers and access them through simple string ids.\\n\\nYou can mix and match custom providers, the provider registry, and [middleware](/docs/ai-sdk-core/middleware) in your application.\\n\\n## Custom Providers\\n\\nYou can create a [custom provider](/docs/reference/ai-sdk-core/custom-provider) using `customProvider`.\\n\\n### Example: custom model settings\\n\\nYou might want to override the default model settings for a provider or provide model name aliases\\nwith pre-configured settings.\\n\\n```ts\\nimport {\\n  gateway,\\n  customProvider,\\n  defaultSettingsMiddleware,\\n  wrapLanguageModel,\\n} from \\'ai\\';\\n\\n// custom provider with different provider options:\\nexport const openai = customProvider({\\n  languageModels: {\\n    // replacement model with custom provider options:\\n    \\'gpt-5.1\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n    // alias model with custom provider options:\\n    \\'gpt-5.1-high-reasoning\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n  },\\n  fallbackProvider: gateway,\\n});\\n```\\n\\n### Example: model name alias\\n\\nYou can also provide model name aliases, so you can update the model version in one place in the future:\\n\\n```ts\\nimport { customProvider, gateway } from \\'ai\\';\\n\\n// custom provider with alias names:\\nexport const anthropic = customProvider({\\n  languageModels: {\\n    opus: gateway(\\'anthropic/claude-opus-4.1\\'),\\n    sonnet: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    haiku: gateway(\\'anthropic/claude-haiku-4.5\\'),\\n  },\\n  fallbackProvider: gateway,\\n});\\n```\\n\\n### Example: limit available models\\n\\nYou can limit the available models in the system, even if you have multiple providers.\\n\\n```ts\\nimport {\\n  customProvider,\\n  defaultSettingsMiddleware,\\n  wrapLanguageModel,\\n  gateway,\\n} from \\'ai\\';\\n\\nexport const myProvider = customProvider({\\n  languageModels: {\\n    \\'text-medium\\': gateway(\\'anthropic/claude-3-5-sonnet-20240620\\'),\\n    \\'text-small\\': gateway(\\'openai/gpt-5-mini\\'),\\n    \\'reasoning-medium\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n    \\'reasoning-fast\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'low\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n  },\\n  embeddingModels: {\\n    embedding: gateway.embeddingModel(\\'openai/text-embedding-3-small\\'),\\n  },\\n  // no fallback provider\\n});\\n```\\n\\n## Provider Registry\\n\\nYou can create a [provider registry](/docs/reference/ai-sdk-core/provider-registry) with multiple providers and models using `createProviderRegistry`.\\n\\n### Setup\\n\\n```ts filename={\"registry.ts\"}\\nimport { anthropic } from \\'@ai-sdk/anthropic\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createProviderRegistry, gateway } from \\'ai\\';\\n\\nexport const registry = createProviderRegistry({\\n  // register provider with prefix and default setup using gateway:\\n  gateway,\\n\\n  // register provider with prefix and direct provider import:\\n  anthropic,\\n  openai,\\n});\\n```\\n\\n### Setup with Custom Separator\\n\\nBy default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator:\\n\\n```ts filename={\"registry.ts\"}\\nimport { anthropic } from \\'@ai-sdk/anthropic\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createProviderRegistry, gateway } from \\'ai\\';\\n\\nexport const customSeparatorRegistry = createProviderRegistry(\\n  {\\n    gateway,\\n    anthropic,\\n    openai,\\n  },\\n  { separator: \\' > \\' },\\n);\\n```\\n\\n### Example: Use language models\\n\\nYou can access language models by using the `languageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateText } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { text } = await generateText({\\n  model: registry.languageModel(\\'openai:gpt-5.1\\'), // default separator\\n  // or with custom separator:\\n  // model: customSeparatorRegistry.languageModel(\\'openai > gpt-5.1\\'),\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\n### Example: Use text embedding models\\n\\nYou can access text embedding models by using the `.embeddingModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { embed } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { embedding } = await embed({\\n  model: registry.embeddingModel(\\'openai:text-embedding-3-small\\'),\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n### Example: Use image models\\n\\nYou can access image models by using the `imageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateImage } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { image } = await generateImage({\\n  model: registry.imageModel(\\'openai:dall-e-3\\'),\\n  prompt: \\'A beautiful sunset over a calm ocean\\',\\n});\\n```\\n\\n## Combining Custom Providers, Provider Registry, and Middleware\\n\\nThe central idea of provider management is to set up a file that contains all the providers and models you want to use.\\nYou may want to pre-configure model settings, provide model name aliases, limit the available models, and more.\\n\\nHere is an example that implements the following concepts:\\n\\n- pass through gateway with a namespace prefix (here: `gateway > *`)\\n- pass through a full provider with a namespace prefix (here: `xai > *`)\\n- setup an OpenAI-compatible provider with custom api key and base URL (here: `custom > *`)\\n- setup model name aliases (here: `anthropic > fast`, `anthropic > writing`, `anthropic > reasoning`)\\n- pre-configure model settings (here: `anthropic > reasoning`)\\n- validate the provider-specific options (here: `AnthropicProviderOptions`)\\n- use a fallback provider (here: `anthropic > *`)\\n- limit a provider to certain models without a fallback (here: `groq > gemma2-9b-it`, `groq > qwen-qwq-32b`)\\n- define a custom separator for the provider registry (here: `>`)\\n\\n```ts\\nimport { anthropic, AnthropicProviderOptions } from \\'@ai-sdk/anthropic\\';\\nimport { createOpenAICompatible } from \\'@ai-sdk/openai-compatible\\';\\nimport { xai } from \\'@ai-sdk/xai\\';\\nimport { groq } from \\'@ai-sdk/groq\\';\\nimport {\\n  createProviderRegistry,\\n  customProvider,\\n  defaultSettingsMiddleware,\\n  gateway,\\n  wrapLanguageModel,\\n} from \\'ai\\';\\n\\nexport const registry = createProviderRegistry(\\n  {\\n    // pass through gateway with a namespace prefix\\n    gateway,\\n\\n    // pass through full providers with namespace prefixes\\n    xai,\\n\\n    // access an OpenAI-compatible provider with custom setup\\n    custom: createOpenAICompatible({\\n      name: \\'provider-name\\',\\n      apiKey: process.env.CUSTOM_API_KEY,\\n      baseURL: \\'https://api.custom.com/v1\\',\\n    }),\\n\\n    // setup model name aliases\\n    anthropic: customProvider({\\n      languageModels: {\\n        fast: anthropic(\\'claude-haiku-4-5\\'),\\n\\n        // simple model\\n        writing: anthropic(\\'claude-sonnet-4-5\\'),\\n\\n        // extended reasoning model configuration:\\n        reasoning: wrapLanguageModel({\\n          model: anthropic(\\'claude-sonnet-4-5\\'),\\n          middleware: defaultSettingsMiddleware({\\n            settings: {\\n              maxOutputTokens: 100000, // example default setting\\n              providerOptions: {\\n                anthropic: {\\n                  thinking: {\\n                    type: \\'enabled\\',\\n                    budgetTokens: 32000,\\n                  },\\n                } satisfies AnthropicProviderOptions,\\n              },\\n            },\\n          }),\\n        }),\\n      },\\n      fallbackProvider: anthropic,\\n    }),\\n\\n    // limit a provider to certain models without a fallback\\n    groq: customProvider({\\n      languageModels: {\\n        \\'gemma2-9b-it\\': groq(\\'gemma2-9b-it\\'),\\n        \\'qwen-qwq-32b\\': groq(\\'qwen-qwq-32b\\'),\\n      },\\n    }),\\n  },\\n  { separator: \\' > \\' },\\n);\\n\\n// usage:\\nconst model = registry.languageModel(\\'anthropic > reasoning\\');\\n```\\n\\n## Global Provider Configuration\\n\\nThe AI SDK 5 includes a global provider feature that allows you to specify a model using just a plain model ID string:\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\n\\nconst result = await streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\', // Uses the global provider (defaults to gateway)\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\nBy default, the global provider is set to the Vercel AI Gateway.\\n\\n### Customizing the Global Provider\\n\\nYou can set your own preferred global provider:\\n\\n```ts filename=\"setup.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// Initialize once during startup:\\nglobalThis.AI_SDK_DEFAULT_PROVIDER = openai;\\n```\\n\\n```ts filename=\"app.ts\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = await streamText({\\n  model: \\'gpt-5.1\\', // Uses OpenAI provider without prefix\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\nThis simplifies provider usage and makes it easier to switch between providers without changing your model references throughout your codebase.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/50-error-handling.mdx'), name='50-error-handling.mdx', displayName='50-error-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Error Handling\\ndescription: Learn how to handle errors in the AI SDK Core\\n---\\n\\n# Error Handling\\n\\n## Handling regular errors\\n\\nRegular errors are thrown and can be handled using the `try/catch` block.\\n\\n```ts highlight=\"3,8-10\"\\nimport { generateText } from \\'ai\\';\\n\\ntry {\\n  const { text } = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  });\\n} catch (error) {\\n  // handle error\\n}\\n```\\n\\nSee [Error Types](/docs/reference/ai-sdk-errors) for more information on the different types of errors that may be thrown.\\n\\n## Handling streaming errors (simple streams)\\n\\nWhen errors occur during streams that do not support error chunks,\\nthe error is thrown as a regular error.\\nYou can handle these errors using the `try/catch` block.\\n\\n```ts highlight=\"3,12-14\"\\nimport { streamText } from \\'ai\\';\\n\\ntry {\\n  const { textStream } = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  });\\n\\n  for await (const textPart of textStream) {\\n    process.stdout.write(textPart);\\n  }\\n} catch (error) {\\n  // handle error\\n}\\n```\\n\\n## Handling streaming errors (streaming with `error` support)\\n\\nFull streams support error parts.\\nYou can handle those parts similar to other parts.\\nIt is recommended to also add a try-catch block for errors that\\nhappen outside of the streaming.\\n\\n```ts highlight=\"13-21\"\\nimport { streamText } from \\'ai\\';\\n\\ntry {\\n  const { fullStream } = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  });\\n\\n  for await (const part of fullStream) {\\n    switch (part.type) {\\n      // ... handle other part types\\n\\n      case \\'error\\': {\\n        const error = part.error;\\n        // handle error\\n        break;\\n      }\\n\\n      case \\'abort\\': {\\n        // handle stream abort\\n        break;\\n      }\\n\\n      case \\'tool-error\\': {\\n        const error = part.error;\\n        // handle error\\n        break;\\n      }\\n    }\\n  }\\n} catch (error) {\\n  // handle error\\n}\\n```\\n\\n## Handling stream aborts\\n\\nWhen streams are aborted (e.g., via chat stop button), you may want to perform cleanup operations like updating stored messages in your UI. Use the `onAbort` callback to handle these cases.\\n\\nThe `onAbort` callback is called when a stream is aborted via `AbortSignal`, but `onFinish` is not called. This ensures you can still update your UI state appropriately.\\n\\n```ts highlight=\"5-9\"\\nimport { streamText } from \\'ai\\';\\n\\nconst { textStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  onAbort: ({ steps }) => {\\n    // Update stored messages or perform cleanup\\n    console.log(\\'Stream aborted after\\', steps.length, \\'steps\\');\\n  },\\n  onFinish: ({ steps, totalUsage }) => {\\n    // This is called on normal completion\\n    console.log(\\'Stream completed normally\\');\\n  },\\n});\\n\\nfor await (const textPart of textStream) {\\n  process.stdout.write(textPart);\\n}\\n```\\n\\nThe `onAbort` callback receives:\\n\\n- `steps`: An array of all completed steps before the abort\\n\\nYou can also handle abort events directly in the stream:\\n\\n```ts highlight=\"10-13\"\\nimport { streamText } from \\'ai\\';\\n\\nconst { fullStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n});\\n\\nfor await (const chunk of fullStream) {\\n  switch (chunk.type) {\\n    case \\'abort\\': {\\n      // Handle abort directly in stream\\n      console.log(\\'Stream was aborted\\');\\n      break;\\n    }\\n    // ... handle other part types\\n  }\\n}\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/55-testing.mdx'), name='55-testing.mdx', displayName='55-testing.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Testing\\ndescription: Learn how to use AI SDK Core mock providers for testing.\\n---\\n\\n# Testing\\n\\nTesting language models can be challenging, because they are non-deterministic\\nand calling them is slow and expensive.\\n\\nTo enable you to unit test your code that uses the AI SDK, the AI SDK Core\\nincludes mock providers and test helpers. You can import the following helpers from `ai/test`:\\n\\n- `MockEmbeddingModelV3`: A mock embedding model using the [embedding model v3 specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/embedding-model/v3/embedding-model-v3.ts).\\n- `MockLanguageModelV3`: A mock language model using the [language model v3 specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v3/language-model-v3.ts).\\n- `mockId`: Provides an incrementing integer ID.\\n- `mockValues`: Iterates over an array of values with each call. Returns the last value when the array is exhausted.\\n- [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream): Simulates a readable stream with delays.\\n\\nWith mock providers and test helpers, you can control the output of the AI SDK\\nand test your code in a repeatable and deterministic way without actually calling\\na language model provider.\\n\\n## Examples\\n\\nYou can use the test helpers with the AI Core functions in your unit tests:\\n\\n### generateText\\n\\n```ts\\nimport { generateText } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\n\\nconst result = await generateText({\\n  model: new MockLanguageModelV3({\\n    doGenerate: async () => ({\\n      finishReason: \\'stop\\',\\n      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\\n      content: [{ type: \\'text\\', text: `Hello, world!` }],\\n      warnings: [],\\n    }),\\n  }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### streamText\\n\\n```ts\\nimport { streamText, simulateReadableStream } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\n\\nconst result = streamText({\\n  model: new MockLanguageModelV3({\\n    doStream: async () => ({\\n      stream: simulateReadableStream({\\n        chunks: [\\n          { type: \\'text-start\\', id: \\'text-1\\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'Hello\\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\', \\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'world!\\' },\\n          { type: \\'text-end\\', id: \\'text-1\\' },\\n          {\\n            type: \\'finish\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\\n          },\\n        ],\\n      }),\\n    }),\\n  }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### generateObject\\n\\n```ts\\nimport { generateObject } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = await generateObject({\\n  model: new MockLanguageModelV3({\\n    doGenerate: async () => ({\\n      finishReason: \\'stop\\',\\n      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\\n      content: [{ type: \\'text\\', text: `{\"content\":\"Hello, world!\"}` }],\\n      warnings: [],\\n    }),\\n  }),\\n  schema: z.object({ content: z.string() }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### streamObject\\n\\n```ts\\nimport { streamObject, simulateReadableStream } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = streamObject({\\n  model: new MockLanguageModelV3({\\n    doStream: async () => ({\\n      stream: simulateReadableStream({\\n        chunks: [\\n          { type: \\'text-start\\', id: \\'text-1\\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'{ \\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'\"content\": \\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: `\"Hello, ` },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: `world` },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: `!\"` },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\' }\\' },\\n          { type: \\'text-end\\', id: \\'text-1\\' },\\n          {\\n            type: \\'finish\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\\n          },\\n        ],\\n      }),\\n    }),\\n  }),\\n  schema: z.object({ content: z.string() }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### Simulate UI Message Stream Responses\\n\\nYou can also simulate [UI Message Stream](/docs/ai-sdk-ui/stream-protocol#ui-message-stream) responses for testing,\\ndebugging, or demonstration purposes.\\n\\nHere is a Next example:\\n\\n```ts filename=\"route.ts\"\\nimport { simulateReadableStream } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  return new Response(\\n    simulateReadableStream({\\n      initialDelayInMs: 1000, // Delay before the first chunk\\n      chunkDelayInMs: 300, // Delay between chunks\\n      chunks: [\\n        `data: {\"type\":\"start\",\"messageId\":\"msg-123\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-start\",\"id\":\"text-1\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\"This\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" is an\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" example.\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-end\",\"id\":\"text-1\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"finish\"}\\\\n\\\\n`,\\n        `data: [DONE]\\\\n\\\\n`,\\n      ],\\n    }).pipeThrough(new TextEncoderStream()),\\n    {\\n      status: 200,\\n      headers: {\\n        \\'Content-Type\\': \\'text/event-stream\\',\\n        \\'Cache-Control\\': \\'no-cache\\',\\n        Connection: \\'keep-alive\\',\\n        \\'x-vercel-ai-ui-message-stream\\': \\'v1\\',\\n      },\\n    },\\n  );\\n}\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/60-telemetry.mdx'), name='60-telemetry.mdx', displayName='60-telemetry.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Telemetry\\ndescription: Using OpenTelemetry with AI SDK Core\\n---\\n\\n# Telemetry\\n\\n<Note type=\"warning\">\\n  AI SDK Telemetry is experimental and may change in the future.\\n</Note>\\n\\nThe AI SDK uses [OpenTelemetry](https://opentelemetry.io/) to collect telemetry data.\\nOpenTelemetry is an open-source observability framework designed to provide\\nstandardized instrumentation for collecting telemetry data.\\n\\nCheck out the [AI SDK Observability Integrations](/providers/observability)\\nto see providers that offer monitoring and tracing for AI SDK applications.\\n\\n## Enabling telemetry\\n\\nFor Next.js applications, please follow the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry) to enable telemetry first.\\n\\nYou can then use the `experimental_telemetry` option to enable telemetry on specific function calls while the feature is experimental:\\n\\n```ts highlight=\"4\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a short story about a cat.\\',\\n  experimental_telemetry: { isEnabled: true },\\n});\\n```\\n\\nWhen telemetry is enabled, you can also control if you want to record the input values and the output values for the function.\\nBy default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.\\n\\nDisabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons.\\nYou might for example want to disable recording inputs if they contain sensitive information.\\n\\n## Telemetry Metadata\\n\\nYou can provide a `functionId` to identify the function that the telemetry data is for,\\nand `metadata` to include additional information in the telemetry data.\\n\\n```ts highlight=\"6-10\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a short story about a cat.\\',\\n  experimental_telemetry: {\\n    isEnabled: true,\\n    functionId: \\'my-awesome-function\\',\\n    metadata: {\\n      something: \\'custom\\',\\n      someOtherThing: \\'other-value\\',\\n    },\\n  },\\n});\\n```\\n\\n## Custom Tracer\\n\\nYou may provide a `tracer` which must return an OpenTelemetry `Tracer`. This is useful in situations where\\nyou want your traces to use a `TracerProvider` other than the one provided by the `@opentelemetry/api` singleton.\\n\\n```ts highlight=\"7\"\\nconst tracerProvider = new NodeTracerProvider();\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a short story about a cat.\\',\\n  experimental_telemetry: {\\n    isEnabled: true,\\n    tracer: tracerProvider.getTracer(\\'ai\\'),\\n  },\\n});\\n```\\n\\n## Collected Data\\n\\n### generateText function\\n\\n`generateText` records 3 types of spans:\\n\\n- `ai.generateText` (span): the full length of the generateText call. It contains 1 or more `ai.generateText.doGenerate` spans.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateText` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateText\"`\\n  - `ai.prompt`: the prompt that was used when calling `generateText`\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n  - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set\\n\\n- `ai.generateText.doGenerate` (span): a provider doGenerate call. It can contain `ai.toolCall` spans.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateText.doGenerate` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateText.doGenerate\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.\\n    Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).\\n    Provider-defined-client tools have a `name`, `id`, and `input` (Record).\\n  - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property\\n    (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.\\n\\n### streamText function\\n\\n`streamText` records 3 types of spans and 2 types of events:\\n\\n- `ai.streamText` (span): the full length of the streamText call. It contains a `ai.streamText.doStream` span.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamText` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamText\"`\\n  - `ai.prompt`: the prompt that was used when calling `streamText`\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n  - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set\\n\\n- `ai.streamText.doStream` (span): a provider doStream call.\\n  This span contains an `ai.stream.firstChunk` event and `ai.toolCall` spans.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamText.doStream` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamText.doStream\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.\\n    Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).\\n    Provider-defined-client tools have a `name`, `id`, and `input` (Record).\\n  - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property\\n    (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk in milliseconds\\n  - `ai.response.msToFinish`: the time it took to receive the finish part of the LLM stream in milliseconds\\n  - `ai.response.avgCompletionTokensPerSecond`: the average number of completion tokens per second\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.\\n\\n- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.\\n\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\\n\\n- `ai.stream.finish` (event): an event that is emitted when the finish part of the LLM stream is received.\\n\\nIt also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.\\n\\n### generateObject function\\n\\n`generateObject` records 2 types of spans:\\n\\n- `ai.generateObject` (span): the full length of the generateObject call. It contains 1 or more `ai.generateObject.doGenerate` spans.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateObject` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateObject\"`\\n  - `ai.prompt`: the prompt that was used when calling `generateObject`\\n  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `generateObject` function\\n  - `ai.schema.name`: the name of the schema that was passed into the `generateObject` function\\n  - `ai.schema.description`: the description of the schema that was passed into the `generateObject` function\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`\\n\\n- `ai.generateObject.doGenerate` (span): a provider doGenerate call.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateObject.doGenerate` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateObject.doGenerate\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n### streamObject function\\n\\n`streamObject` records 2 types of spans and 1 type of event:\\n\\n- `ai.streamObject` (span): the full length of the streamObject call. It contains 1 or more `ai.streamObject.doStream` spans.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamObject` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamObject\"`\\n  - `ai.prompt`: the prompt that was used when calling `streamObject`\\n  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `streamObject` function\\n  - `ai.schema.name`: the name of the schema that was passed into the `streamObject` function\\n  - `ai.schema.description`: the description of the schema that was passed into the `streamObject` function\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`\\n\\n- `ai.streamObject.doStream` (span): a provider doStream call.\\n  This span contains an `ai.stream.firstChunk` event.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamObject.doStream` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamObject.doStream\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\\n\\n### embed function\\n\\n`embed` records 2 types of spans:\\n\\n- `ai.embed` (span): the full length of the embed call. It contains 1 `ai.embed.doEmbed` spans.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embed` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embed\"`\\n  - `ai.value`: the value that was passed into the `embed` function\\n  - `ai.embedding`: a JSON-stringified embedding\\n\\n- `ai.embed.doEmbed` (span): a provider doEmbed call.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embed.doEmbed` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embed.doEmbed\"`\\n  - `ai.values`: the values that were passed into the provider (array)\\n  - `ai.embeddings`: an array of JSON-stringified embeddings\\n\\n### embedMany function\\n\\n`embedMany` records 2 types of spans:\\n\\n- `ai.embedMany` (span): the full length of the embedMany call. It contains 1 or more `ai.embedMany.doEmbed` spans.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embedMany` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embedMany\"`\\n  - `ai.values`: the values that were passed into the `embedMany` function\\n  - `ai.embeddings`: an array of JSON-stringified embedding\\n\\n- `ai.embedMany.doEmbed` (span): a provider doEmbed call.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embedMany.doEmbed` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embedMany.doEmbed\"`\\n  - `ai.values`: the values that were sent to the provider\\n  - `ai.embeddings`: an array of JSON-stringified embeddings for each value\\n\\n## Span Details\\n\\n### Basic LLM span information\\n\\nMany spans that use LLMs (`ai.generateText`, `ai.generateText.doGenerate`, `ai.streamText`, `ai.streamText.doStream`,\\n`ai.generateObject`, `ai.generateObject.doGenerate`, `ai.streamObject`, `ai.streamObject.doStream`) contain the following attributes:\\n\\n- `resource.name`: the functionId that was set through `telemetry.functionId`\\n- `ai.model.id`: the id of the model\\n- `ai.model.provider`: the provider of the model\\n- `ai.request.headers.*`: the request headers that were passed in through `headers`\\n- `ai.response.providerMetadata`: provider specific metadata returned with the generation response\\n- `ai.settings.maxRetries`: the maximum number of retries that were set\\n- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`\\n- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`\\n- `ai.usage.completionTokens`: the number of completion tokens that were used\\n- `ai.usage.promptTokens`: the number of prompt tokens that were used\\n\\n### Call LLM span information\\n\\nSpans that correspond to individual LLM calls (`ai.generateText.doGenerate`, `ai.streamText.doStream`, `ai.generateObject.doGenerate`, `ai.streamObject.doStream`) contain\\n[basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n- `ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\\n- `ai.response.id`: the id of the response. Uses the ID from the provider when available.\\n- `ai.response.timestamp`: the timestamp of the response. Uses the timestamp from the provider when available.\\n- [Semantic Conventions for GenAI operations](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/)\\n  - `gen_ai.system`: the provider that was used\\n  - `gen_ai.request.model`: the model that was requested\\n  - `gen_ai.request.temperature`: the temperature that was set\\n  - `gen_ai.request.max_tokens`: the maximum number of tokens that were set\\n  - `gen_ai.request.frequency_penalty`: the frequency penalty that was set\\n  - `gen_ai.request.presence_penalty`: the presence penalty that was set\\n  - `gen_ai.request.top_k`: the topK parameter value that was set\\n  - `gen_ai.request.top_p`: the topP parameter value that was set\\n  - `gen_ai.request.stop_sequences`: the stop sequences\\n  - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider\\n  - `gen_ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\\n  - `gen_ai.response.id`: the id of the response. Uses the ID from the provider when available.\\n  - `gen_ai.usage.input_tokens`: the number of prompt tokens that were used\\n  - `gen_ai.usage.output_tokens`: the number of completion tokens that were used\\n\\n### Basic embedding span information\\n\\nMany spans that use embedding models (`ai.embed`, `ai.embed.doEmbed`, `ai.embedMany`, `ai.embedMany.doEmbed`) contain the following attributes:\\n\\n- `ai.model.id`: the id of the model\\n- `ai.model.provider`: the provider of the model\\n- `ai.request.headers.*`: the request headers that were passed in through `headers`\\n- `ai.settings.maxRetries`: the maximum number of retries that were set\\n- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`\\n- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`\\n- `ai.usage.tokens`: the number of tokens that were used\\n- `resource.name`: the functionId that was set through `telemetry.functionId`\\n\\n### Tool call spans\\n\\nTool call spans (`ai.toolCall`) contain the following attributes:\\n\\n- `operation.name`: `\"ai.toolCall\"`\\n- `ai.operationId`: `\"ai.toolCall\"`\\n- `ai.toolCall.name`: the name of the tool\\n- `ai.toolCall.id`: the id of the tool call\\n- `ai.toolCall.args`: the input parameters of the tool call\\n- `ai.toolCall.result`: the output result of the tool call. Only available if the tool call is successful and the result is serializable.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK Core\\ndescription: Learn about AI SDK Core.\\n---\\n\\n# AI SDK Core\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description:\\n        'Learn about AI SDK Core and how to work with Large Language Models (LLMs).',\\n      href: '/docs/ai-sdk-core/overview',\\n    },\\n    {\\n      title: 'Generating Text',\\n      description: 'Learn how to generate text.',\\n      href: '/docs/ai-sdk-core/generating-text',\\n    },\\n    {\\n      title: 'Generating Structured Data',\\n      description: 'Learn how to generate structured data.',\\n      href: '/docs/ai-sdk-core/generating-structured-data',\\n    },\\n    {\\n      title: 'Tool Calling',\\n      description: 'Learn how to do tool calling with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/tools-and-tool-calling',\\n    },\\n    {\\n      title: 'Prompt Engineering',\\n      description: 'Learn how to write prompts with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/prompt-engineering',\\n    },\\n    {\\n      title: 'Settings',\\n      description:\\n        'Learn how to set up settings for language models generations.',\\n      href: '/docs/ai-sdk-core/settings',\\n    },\\n    {\\n      title: 'Embeddings',\\n      description: 'Learn how to use embeddings with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/embeddings',\\n    },\\n    {\\n      title: 'Image Generation',\\n      description: 'Learn how to generate images with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/image-generation',\\n    },\\n    {\\n      title: 'Transcription',\\n      description: 'Learn how to transcribe audio with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/transcription',\\n    },\\n    {\\n      title: 'Speech',\\n      description: 'Learn how to generate speech with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/speech',\\n    },\\n    {\\n      title: 'Provider Management',\\n      description: 'Learn how to work with multiple providers.',\\n      href: '/docs/ai-sdk-core/provider-management',\\n    },\\n    {\\n      title: 'Middleware',\\n      description: 'Learn how to use middleware with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/middleware',\\n    },\\n    {\\n      title: 'Error Handling',\\n      description: 'Learn how to handle errors with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/error-handling',\\n    },\\n    {\\n      title: 'Testing',\\n      description: 'Learn how to test with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/testing',\\n    },\\n    {\\n      title: 'Telemetry',\\n      description: 'Learn how to use telemetry with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/telemetry',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('04-ai-sdk-ui'), name='04-ai-sdk-ui', displayName='04-ai-sdk-ui', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('04-ai-sdk-ui/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of AI SDK UI.\\n---\\n\\n# AI SDK UI\\n\\nAI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a **framework-agnostic toolkit**, streamlining the integration of advanced AI functionalities into your applications.\\n\\nAI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With three main hooks — **`useChat`**, **`useCompletion`**, and **`useObject`** — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.\\n\\n- **[`useChat`](/docs/ai-sdk-ui/chatbot)** offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.\\n- **[`useCompletion`](/docs/ai-sdk-ui/completion)** enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.\\n- **[`useObject`](/docs/ai-sdk-ui/object-generation)** is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.\\n\\nThese hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.\\n\\n## UI Framework Support\\n\\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/), and [Angular](https://angular.dev/).\\nHere is a comparison of the supported functions across these frameworks:\\n\\n| Function                                                  | React               | Svelte                               | Vue.js              | Angular                              |\\n| --------------------------------------------------------- | ------------------- | ------------------------------------ | ------------------- | ------------------------------------ |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)             | <Check size={18} /> | <Check size={18} /> Chat             | <Check size={18} /> | <Check size={18} /> Chat             |\\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion) | <Check size={18} /> | <Check size={18} /> Completion       | <Check size={18} /> | <Check size={18} /> Completion       |\\n| [useObject](/docs/reference/ai-sdk-ui/use-object)         | <Check size={18} /> | <Check size={18} /> StructuredObject | <Cross size={18} /> | <Check size={18} /> StructuredObject |\\n\\n<Note>\\n  [Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\\n  welcome to implement missing features for non-React frameworks.\\n</Note>\\n\\n## Framework Examples\\n\\nExplore these example implementations for different frameworks:\\n\\n- [**Next.js**](https://github.com/vercel/ai/tree/main/examples/next-openai)\\n- [**Nuxt**](https://github.com/vercel/ai/tree/main/examples/nuxt-openai)\\n- [**SvelteKit**](https://github.com/vercel/ai/tree/main/examples/sveltekit-openai)\\n- [**Angular**](https://github.com/vercel/ai/tree/main/examples/angular)\\n\\n## API Reference\\n\\nPlease check out the [AI SDK UI API Reference](/docs/reference/ai-sdk-ui) for more details on each function.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/02-chatbot.mdx'), name='02-chatbot.mdx', displayName='02-chatbot.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot\\ndescription: Learn how to use the useChat hook.\\n---\\n\\n# Chatbot\\n\\nThe `useChat` hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.\\n\\nTo summarize, the `useChat` hook provides the following features:\\n\\n- **Message Streaming**: All the messages from the AI provider are streamed to the chat UI in real-time.\\n- **Managed States**: The hook manages the states for input, messages, status, error and more for you.\\n- **Seamless Integration**: Easily integrate your chat AI into any design or layout with minimal effort.\\n\\nIn this guide, you will learn how to use the `useChat` hook to create a chatbot application with real-time message streaming.\\nCheck out our [chatbot with tools guide](/docs/ai-sdk-ui/chatbot-with-tool-calling) to learn how to use tools in your chatbot.\\nLet\\'s start with the following example first.\\n\\n## Example\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n          placeholder=\"Say something...\"\\n        />\\n        <button type=\"submit\" disabled={status !== \\'ready\\'}>\\n          Submit\\n        </button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/chat/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'You are a helpful assistant.\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n<Note>\\n  The UI messages have a new `parts` property that contains the message parts.\\n  We recommend rendering the messages using the `parts` property instead of the\\n  `content` property. The parts property supports different message types,\\n  including text, tool invocation, and tool result, and allows for more flexible\\n  and complex chat UIs.\\n</Note>\\n\\nIn the `Page` component, the `useChat` hook will request to your AI provider endpoint whenever the user sends a message using `sendMessage`.\\nThe messages are then streamed back in real-time and displayed in the chat UI.\\n\\nThis enables a seamless chat experience where the user can see the AI response as soon as it is available,\\nwithout having to wait for the entire response to be received.\\n\\n## Customized UI\\n\\n`useChat` also provides ways to manage the chat message states via code, show status, and update messages without being triggered by user interactions.\\n\\n### Status\\n\\nThe `useChat` hook returns a `status`. It has the following possible values:\\n\\n- `submitted`: The message has been sent to the API and we\\'re awaiting the start of the response stream.\\n- `streaming`: The response is actively streaming in from the API, receiving chunks of data.\\n- `ready`: The full response has been received and processed; a new user message can be submitted.\\n- `error`: An error occurred during the API request, preventing successful completion.\\n\\nYou can use `status` for e.g. the following purposes:\\n\\n- To show a loading spinner while the chatbot is processing the user\\'s message.\\n- To show a \"Stop\" button to abort the current message.\\n- To disable the submit button.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"6,22-29,36\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status, stop } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      {(status === \\'submitted\\' || status === \\'streaming\\') && (\\n        <div>\\n          {status === \\'submitted\\' && <Spinner />}\\n          <button type=\"button\" onClick={() => stop()}>\\n            Stop\\n          </button>\\n        </div>\\n      )}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n          placeholder=\"Say something...\"\\n        />\\n        <button type=\"submit\" disabled={status !== \\'ready\\'}>\\n          Submit\\n        </button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n### Error State\\n\\nSimilarly, the `error` state reflects the error object thrown during the fetch request.\\nIt can be used to display an error message, disable the submit button, or show a retry button:\\n\\n<Note>\\n  We recommend showing a generic error message to the user, such as \"Something\\n  went wrong.\" This is a good practice to avoid leaking information from the\\n  server.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"6,20-27,33\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, error, reload } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      {error && (\\n        <>\\n          <div>An error occurred.</div>\\n          <button type=\"button\" onClick={() => reload()}>\\n            Retry\\n          </button>\\n        </>\\n      )}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={error != null}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nPlease also see the [error handling](/docs/ai-sdk-ui/error-handling) guide for more information.\\n\\n### Modify messages\\n\\nSometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.\\n\\nThe `setMessages` function can help you achieve these tasks:\\n\\n```tsx\\nconst { messages, setMessages } = useChat()\\n\\nconst handleDelete = (id) => {\\n  setMessages(messages.filter(message => message.id !== id))\\n}\\n\\nreturn <>\\n  {messages.map(message => (\\n    <div key={message.id}>\\n      {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n      {message.parts.map((part, index) => (\\n        part.type === \\'text\\' ? (\\n          <span key={index}>{part.text}</span>\\n        ) : null\\n      ))}\\n      <button onClick={() => handleDelete(message.id)}>Delete</button>\\n    </div>\\n  ))}\\n  ...\\n```\\n\\nYou can think of `messages` and `setMessages` as a pair of `state` and `setState` in React.\\n\\n### Cancellation and regeneration\\n\\nIt\\'s also a common use case to abort the response message while it\\'s still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useChat` hook.\\n\\n```tsx\\nconst { stop, status } = useChat()\\n\\nreturn <>\\n  <button onClick={stop} disabled={!(status === \\'streaming\\' || status === \\'submitted\\')}>Stop</button>\\n  ...\\n```\\n\\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.\\n\\nSimilarly, you can also request the AI provider to reprocess the last message by calling the `regenerate` function returned by the `useChat` hook:\\n\\n```tsx\\nconst { regenerate, status } = useChat();\\n\\nreturn (\\n  <>\\n    <button\\n      onClick={regenerate}\\n      disabled={!(status === \\'ready\\' || status === \\'error\\')}\\n    >\\n      Regenerate\\n    </button>\\n    ...\\n  </>\\n);\\n```\\n\\nWhen the user clicks the \"Regenerate\" button, the AI provider will regenerate the last message and replace the current one correspondingly.\\n\\n### Throttling UI Updates\\n\\n<Note>This feature is currently only available for React.</Note>\\n\\nBy default, the `useChat` hook will trigger a render every time a new chunk is received.\\nYou can throttle the UI updates with the `experimental_throttle` option.\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { messages, ... } = useChat({\\n  // Throttle the messages and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n\\n## Event Callbacks\\n\\n`useChat` provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:\\n\\n- `onFinish`: Called when the assistant response is completed. The event includes the response message, all messages, and flags for abort, disconnect, and errors.\\n- `onError`: Called when an error occurs during the fetch request.\\n- `onData`: Called whenever a data part is received.\\n\\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\\n\\n```tsx\\nimport { UIMessage } from \\'ai\\';\\n\\nconst {\\n  /* ... */\\n} = useChat({\\n  onFinish: ({ message, messages, isAbort, isDisconnect, isError }) => {\\n    // use information to e.g. update other UI states\\n  },\\n  onError: error => {\\n    console.error(\\'An error occurred:\\', error);\\n  },\\n  onData: data => {\\n    console.log(\\'Received data part from server:\\', data);\\n  },\\n});\\n```\\n\\nIt\\'s worth noting that you can abort the processing by throwing an error in the `onData` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\\n\\n## Request Configuration\\n\\n### Custom headers, body, and credentials\\n\\nBy default, the `useChat` hook sends a HTTP POST request to the `/api/chat` endpoint with the message list as the request body. You can customize the request in two ways:\\n\\n#### Hook-Level Configuration (Applied to all requests)\\n\\nYou can configure transport-level options that will be applied to all requests made by the hook:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: \\'/api/custom-chat\\',\\n    headers: {\\n      Authorization: \\'your_token\\',\\n    },\\n    body: {\\n      user_id: \\'123\\',\\n    },\\n    credentials: \\'same-origin\\',\\n  }),\\n});\\n```\\n\\n#### Dynamic Hook-Level Configuration\\n\\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: \\'/api/custom-chat\\',\\n    headers: () => ({\\n      Authorization: `Bearer ${getAuthToken()}`,\\n      \\'X-User-ID\\': getCurrentUserId(),\\n    }),\\n    body: () => ({\\n      sessionId: getCurrentSessionId(),\\n      preferences: getUserPreferences(),\\n    }),\\n    credentials: () => \\'include\\',\\n  }),\\n});\\n```\\n\\n<Note>\\n  For component state that changes over time, use `useRef` to store the current\\n  value and reference `ref.current` in your configuration function, or prefer\\n  request-level options (see next section) for better reliability.\\n</Note>\\n\\n#### Request-Level Configuration (Recommended)\\n\\n<Note>\\n  **Recommended**: Use request-level options for better flexibility and control.\\n  Request-level options take precedence over hook-level options and allow you to\\n  customize each request individually.\\n</Note>\\n\\n```tsx\\n// Pass options as the second parameter to sendMessage\\nsendMessage(\\n  { text: input },\\n  {\\n    headers: {\\n      Authorization: \\'Bearer token123\\',\\n      \\'X-Custom-Header\\': \\'custom-value\\',\\n    },\\n    body: {\\n      temperature: 0.7,\\n      max_tokens: 100,\\n      user_id: \\'123\\',\\n    },\\n    metadata: {\\n      userId: \\'user123\\',\\n      sessionId: \\'session456\\',\\n    },\\n  },\\n);\\n```\\n\\nThe request-level options are merged with hook-level options, with request-level options taking precedence. On your server side, you can handle the request with this additional information.\\n\\n### Setting custom body fields per request\\n\\nYou can configure custom `body` fields on a per-request basis using the second parameter of the `sendMessage` function.\\nThis is useful if you want to pass in additional information to your backend that is not part of the message list.\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"20-25\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage } = useChat();\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            sendMessage(\\n              { text: input },\\n              {\\n                body: {\\n                  customKey: \\'customValue\\',\\n                },\\n              },\\n            );\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nYou can retrieve these custom fields on your server side by destructuring the request body:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"3,4\"\\nexport async function POST(req: Request) {\\n  // Extract additional information (\"customKey\") from the body of the request:\\n  const { messages, customKey }: { messages: UIMessage[]; customKey: string } =\\n    await req.json();\\n  //...\\n}\\n```\\n\\n## Message Metadata\\n\\nYou can attach custom metadata to messages for tracking information like timestamps, model details, and token usage.\\n\\n```ts\\n// Server: Send metadata about the message\\nreturn result.toUIMessageStreamResponse({\\n  messageMetadata: ({ part }) => {\\n    if (part.type === \\'start\\') {\\n      return {\\n        createdAt: Date.now(),\\n        model: \\'gpt-5.1\\',\\n      };\\n    }\\n\\n    if (part.type === \\'finish\\') {\\n      return {\\n        totalTokens: part.totalUsage.totalTokens,\\n      };\\n    }\\n  },\\n});\\n```\\n\\n```tsx\\n// Client: Access metadata via message.metadata\\n{\\n  messages.map(message => (\\n    <div key={message.id}>\\n      {message.role}:{\\' \\'}\\n      {message.metadata?.createdAt &&\\n        new Date(message.metadata.createdAt).toLocaleTimeString()}\\n      {/* Render message content */}\\n      {message.parts.map((part, index) =>\\n        part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n      )}\\n      {/* Show token count if available */}\\n      {message.metadata?.totalTokens && (\\n        <span>{message.metadata.totalTokens} tokens</span>\\n      )}\\n    </div>\\n  ));\\n}\\n```\\n\\nFor complete examples with type safety and advanced use cases, see the [Message Metadata documentation](/docs/ai-sdk-ui/message-metadata).\\n\\n## Transport Configuration\\n\\nYou can configure custom transport behavior using the `transport` option to customize how messages are sent to your API:\\n\\n```tsx filename=\"app/page.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage } = useChat({\\n    id: \\'my-chat\\',\\n    transport: new DefaultChatTransport({\\n      prepareSendMessagesRequest: ({ id, messages }) => {\\n        return {\\n          body: {\\n            id,\\n            message: messages[messages.length - 1],\\n          },\\n        };\\n      },\\n    }),\\n  });\\n\\n  // ... rest of your component\\n}\\n```\\n\\nThe corresponding API route receives the custom request format:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nexport async function POST(req: Request) {\\n  const { id, message } = await req.json();\\n\\n  // Load existing messages and add the new one\\n  const messages = await loadMessages(id);\\n  messages.push(message);\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Advanced: Trigger-based routing\\n\\nFor more complex scenarios like message regeneration, you can use trigger-based routing:\\n\\n```tsx filename=\"app/page.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, regenerate } = useChat({\\n    id: \\'my-chat\\',\\n    transport: new DefaultChatTransport({\\n      prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\\n        if (trigger === \\'submit-user-message\\') {\\n          return {\\n            body: {\\n              trigger: \\'submit-user-message\\',\\n              id,\\n              message: messages[messages.length - 1],\\n              messageId,\\n            },\\n          };\\n        } else if (trigger === \\'regenerate-assistant-message\\') {\\n          return {\\n            body: {\\n              trigger: \\'regenerate-assistant-message\\',\\n              id,\\n              messageId,\\n            },\\n          };\\n        }\\n        throw new Error(`Unsupported trigger: ${trigger}`);\\n      },\\n    }),\\n  });\\n\\n  // ... rest of your component\\n}\\n```\\n\\nThe corresponding API route would handle different triggers:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nexport async function POST(req: Request) {\\n  const { trigger, id, message, messageId } = await req.json();\\n\\n  const chat = await readChat(id);\\n  let messages = chat.messages;\\n\\n  if (trigger === \\'submit-user-message\\') {\\n    // Handle new user message\\n    messages = [...messages, message];\\n  } else if (trigger === \\'regenerate-assistant-message\\') {\\n    // Handle message regeneration - remove messages after messageId\\n    const messageIndex = messages.findIndex(m => m.id === messageId);\\n    if (messageIndex !== -1) {\\n      messages = messages.slice(0, messageIndex);\\n    }\\n  }\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nTo learn more about building custom transports, refer to the [Transport API documentation](/docs/ai-sdk-ui/transport).\\n\\n## Controlling the response stream\\n\\nWith `streamText`, you can control how error messages and usage information are sent back to the client.\\n\\n### Error Messages\\n\\nBy default, the error message is masked for security reasons.\\nThe default error message is \"An error occurred.\"\\nYou can forward error messages or send your own error message by providing a `getErrorMessage` function:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"13-27\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onError: error => {\\n      if (error == null) {\\n        return \\'unknown error\\';\\n      }\\n\\n      if (typeof error === \\'string\\') {\\n        return error;\\n      }\\n\\n      if (error instanceof Error) {\\n        return error.message;\\n      }\\n\\n      return JSON.stringify(error);\\n    },\\n  });\\n}\\n```\\n\\n### Usage Information\\n\\nTrack token consumption and resource usage with [message metadata](/docs/ai-sdk-ui/message-metadata):\\n\\n1. Define a custom metadata type with usage fields (optional, for type safety)\\n2. Attach usage data using `messageMetadata` in your response\\n3. Display usage metrics in your UI components\\n\\nUsage data is attached as metadata to messages and becomes available once the model completes its response generation.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport {\\n  convertToModelMessages,\\n  streamText,\\n  UIMessage,\\n  type LanguageModelUsage,\\n} from \\'ai\\';\\n\\n// Create a new metadata type (optional for type-safety)\\ntype MyMetadata = {\\n  totalUsage: LanguageModelUsage;\\n};\\n\\n// Create a new custom message type with your own metadata\\nexport type MyUIMessage = UIMessage<MyMetadata>;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    messageMetadata: ({ part }) => {\\n      // Send total usage when generation is finished\\n      if (part.type === \\'finish\\') {\\n        return { totalUsage: part.totalUsage };\\n      }\\n    },\\n  });\\n}\\n```\\n\\nThen, on the client, you can access the message-level metadata.\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyUIMessage } from \\'./api/chat/route\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  // Use custom message type defined on the server (optional for type-safety)\\n  const { messages } = useChat<MyUIMessage>({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(m => (\\n        <div key={m.id} className=\"whitespace-pre-wrap\">\\n          {m.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {m.parts.map(part => {\\n            if (part.type === \\'text\\') {\\n              return part.text;\\n            }\\n          })}\\n          {/* Render usage via metadata */}\\n          {m.metadata?.totalUsage && (\\n            <div>Total usage: {m.metadata?.totalUsage.totalTokens} tokens</div>\\n          )}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\nYou can also access your metadata from the `onFinish` callback of `useChat`:\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyUIMessage } from \\'./api/chat/route\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  // Use custom message type defined on the server (optional for type-safety)\\n  const { messages } = useChat<MyUIMessage>({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n    onFinish: ({ message }) => {\\n      // Access message metadata via onFinish callback\\n      console.log(message.metadata?.totalUsage);\\n    },\\n  });\\n}\\n```\\n\\n### Text Streams\\n\\n`useChat` can handle plain text streams by setting the `streamProtocol` option to `text`:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"7\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { TextStreamChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  const { messages } = useChat({\\n    transport: new TextStreamChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return <>...</>;\\n}\\n```\\n\\nThis configuration also works with other backend servers that stream plain text.\\nCheck out the [stream protocol guide](/docs/ai-sdk-ui/stream-protocol) for more information.\\n\\n<Note>\\n  When using `TextStreamChatTransport`, tool calls, usage information and finish\\n  reasons are not available.\\n</Note>\\n\\n## Reasoning\\n\\nSome models such as as DeepSeek `deepseek-r1`\\nand Anthropic `claude-3-7-sonnet-20250219` support reasoning tokens.\\nThese tokens are typically sent before the message content.\\nYou can forward them to the client with the `sendReasoning` option:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"13\"\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'deepseek/deepseek-r1\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    sendReasoning: true,\\n  });\\n}\\n```\\n\\nOn the client side, you can access the reasoning parts of the message object.\\n\\nReasoning parts have a `text` property that contains the reasoning content.\\n\\n```tsx filename=\"app/page.tsx\"\\nmessages.map(message => (\\n  <div key={message.id}>\\n    {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n    {message.parts.map((part, index) => {\\n      // text parts:\\n      if (part.type === \\'text\\') {\\n        return <div key={index}>{part.text}</div>;\\n      }\\n\\n      // reasoning parts:\\n      if (part.type === \\'reasoning\\') {\\n        return <pre key={index}>{part.text}</pre>;\\n      }\\n    })}\\n  </div>\\n));\\n```\\n\\n## Sources\\n\\nSome providers such as [Perplexity](/providers/ai-sdk-providers/perplexity#sources) and\\n[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.\\n\\nCurrently sources are limited to web pages that ground the response.\\nYou can forward them to the client with the `sendSources` option:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"13\"\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'perplexity/sonar-pro\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    sendSources: true,\\n  });\\n}\\n```\\n\\nOn the client side, you can access source parts of the message object.\\nThere are two types of sources: `source-url` for web pages and `source-document` for documents.\\nHere is an example that renders both types of sources:\\n\\n```tsx filename=\"app/page.tsx\"\\nmessages.map(message => (\\n  <div key={message.id}>\\n    {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n\\n    {/* Render URL sources */}\\n    {message.parts\\n      .filter(part => part.type === \\'source-url\\')\\n      .map(part => (\\n        <span key={`source-${part.id}`}>\\n          [\\n          <a href={part.url} target=\"_blank\">\\n            {part.title ?? new URL(part.url).hostname}\\n          </a>\\n          ]\\n        </span>\\n      ))}\\n\\n    {/* Render document sources */}\\n    {message.parts\\n      .filter(part => part.type === \\'source-document\\')\\n      .map(part => (\\n        <span key={`source-${part.id}`}>\\n          [<span>{part.title ?? `Document ${part.id}`}</span>]\\n        </span>\\n      ))}\\n  </div>\\n));\\n```\\n\\n## Image Generation\\n\\nSome models such as Google `gemini-2.5-flash-image-preview` support image generation.\\nWhen images are generated, they are exposed as files to the client.\\nOn the client side, you can access file parts of the message object\\nand render them as images.\\n\\n```tsx filename=\"app/page.tsx\"\\nmessages.map(message => (\\n  <div key={message.id}>\\n    {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n    {message.parts.map((part, index) => {\\n      if (part.type === \\'text\\') {\\n        return <div key={index}>{part.text}</div>;\\n      } else if (part.type === \\'file\\' && part.mediaType.startsWith(\\'image/\\')) {\\n        return <img key={index} src={part.url} alt=\"Generated image\" />;\\n      }\\n    })}\\n  </div>\\n));\\n```\\n\\n## Attachments\\n\\nThe `useChat` hook supports sending file attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.\\n\\nThere are two ways to send files with a message: using a `FileList` object from file inputs or using an array of file objects.\\n\\n### FileList\\n\\nBy using `FileList`, you can send multiple files as attachments along with a message using the file input element. The `useChat` hook will automatically convert them into data URLs and send them to the AI provider.\\n\\n<Note>\\n  Currently, only `image/*` and `text/*` content types get automatically\\n  converted into [multi-modal content\\n  parts](/docs/foundations/prompts#multi-modal-messages). You will need to\\n  handle other content types manually.\\n</Note>\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useRef, useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status } = useChat();\\n\\n  const [input, setInput] = useState(\\'\\');\\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\\n  const fileInputRef = useRef<HTMLInputElement>(null);\\n\\n  return (\\n    <div>\\n      <div>\\n        {messages.map(message => (\\n          <div key={message.id}>\\n            <div>{`${message.role}: `}</div>\\n\\n            <div>\\n              {message.parts.map((part, index) => {\\n                if (part.type === \\'text\\') {\\n                  return <span key={index}>{part.text}</span>;\\n                }\\n\\n                if (\\n                  part.type === \\'file\\' &&\\n                  part.mediaType?.startsWith(\\'image/\\')\\n                ) {\\n                  return <img key={index} src={part.url} alt={part.filename} />;\\n                }\\n\\n                return null;\\n              })}\\n            </div>\\n          </div>\\n        ))}\\n      </div>\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({\\n              text: input,\\n              files,\\n            });\\n            setInput(\\'\\');\\n            setFiles(undefined);\\n\\n            if (fileInputRef.current) {\\n              fileInputRef.current.value = \\'\\';\\n            }\\n          }\\n        }}\\n      >\\n        <input\\n          type=\"file\"\\n          onChange={event => {\\n            if (event.target.files) {\\n              setFiles(event.target.files);\\n            }\\n          }}\\n          multiple\\n          ref={fileInputRef}\\n        />\\n        <input\\n          value={input}\\n          placeholder=\"Send message...\"\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### File Objects\\n\\nYou can also send files as objects along with a message. This can be useful for sending pre-uploaded files or data URLs.\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport { FileUIPart } from \\'ai\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status } = useChat();\\n\\n  const [input, setInput] = useState(\\'\\');\\n  const [files] = useState<FileUIPart[]>([\\n    {\\n      type: \\'file\\',\\n      filename: \\'earth.png\\',\\n      mediaType: \\'image/png\\',\\n      url: \\'https://example.com/earth.png\\',\\n    },\\n    {\\n      type: \\'file\\',\\n      filename: \\'moon.png\\',\\n      mediaType: \\'image/png\\',\\n      url: \\'data:image/png;base64,iVBORw0KGgo...\\',\\n    },\\n  ]);\\n\\n  return (\\n    <div>\\n      <div>\\n        {messages.map(message => (\\n          <div key={message.id}>\\n            <div>{`${message.role}: `}</div>\\n\\n            <div>\\n              {message.parts.map((part, index) => {\\n                if (part.type === \\'text\\') {\\n                  return <span key={index}>{part.text}</span>;\\n                }\\n\\n                if (\\n                  part.type === \\'file\\' &&\\n                  part.mediaType?.startsWith(\\'image/\\')\\n                ) {\\n                  return <img key={index} src={part.url} alt={part.filename} />;\\n                }\\n\\n                return null;\\n              })}\\n            </div>\\n          </div>\\n        ))}\\n      </div>\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({\\n              text: input,\\n              files,\\n            });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          placeholder=\"Send message...\"\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n## Type Inference for Tools\\n\\nWhen working with tools in TypeScript, AI SDK UI provides type inference helpers to ensure type safety for your tool inputs and outputs.\\n\\n### InferUITool\\n\\nThe `InferUITool` type helper infers the input and output types of a single tool for use in UI messages:\\n\\n```tsx\\nimport { InferUITool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst weatherTool = {\\n  description: \\'Get the current weather\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The city and state\\'),\\n  }),\\n  execute: async ({ location }) => {\\n    return `The weather in ${location} is sunny.`;\\n  },\\n};\\n\\n// Infer the types from the tool\\ntype WeatherUITool = InferUITool<typeof weatherTool>;\\n// This creates a type with:\\n// {\\n//   input: { location: string };\\n//   output: string;\\n// }\\n```\\n\\n### InferUITools\\n\\nThe `InferUITools` type helper infers the input and output types of a `ToolSet`:\\n\\n```tsx\\nimport { InferUITools, ToolSet } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst tools = {\\n  weather: {\\n    description: \\'Get the current weather\\',\\n    inputSchema: z.object({\\n      location: z.string().describe(\\'The city and state\\'),\\n    }),\\n    execute: async ({ location }) => {\\n      return `The weather in ${location} is sunny.`;\\n    },\\n  },\\n  calculator: {\\n    description: \\'Perform basic arithmetic\\',\\n    inputSchema: z.object({\\n      operation: z.enum([\\'add\\', \\'subtract\\', \\'multiply\\', \\'divide\\']),\\n      a: z.number(),\\n      b: z.number(),\\n    }),\\n    execute: async ({ operation, a, b }) => {\\n      switch (operation) {\\n        case \\'add\\':\\n          return a + b;\\n        case \\'subtract\\':\\n          return a - b;\\n        case \\'multiply\\':\\n          return a * b;\\n        case \\'divide\\':\\n          return a / b;\\n      }\\n    },\\n  },\\n} satisfies ToolSet;\\n\\n// Infer the types from the tool set\\ntype MyUITools = InferUITools<typeof tools>;\\n// This creates a type with:\\n// {\\n//   weather: { input: { location: string }; output: string };\\n//   calculator: { input: { operation: \\'add\\' | \\'subtract\\' | \\'multiply\\' | \\'divide\\'; a: number; b: number }; output: number };\\n// }\\n```\\n\\n### Using Inferred Types\\n\\nYou can use these inferred types to create a custom UIMessage type and pass it to various AI SDK UI functions:\\n\\n```tsx\\nimport { InferUITools, UIMessage, UIDataTypes } from \\'ai\\';\\n\\ntype MyUITools = InferUITools<typeof tools>;\\ntype MyUIMessage = UIMessage<never, UIDataTypes, MyUITools>;\\n```\\n\\nPass the custom type to `useChat` or `createUIMessageStream`:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { createUIMessageStream } from \\'ai\\';\\nimport type { MyUIMessage } from \\'./types\\';\\n\\n// With useChat\\nconst { messages } = useChat<MyUIMessage>();\\n\\n// With createUIMessageStream\\nconst stream = createUIMessageStream<MyUIMessage>(/* ... */);\\n```\\n\\nThis provides full type safety for tool inputs and outputs on the client and server.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/03-chatbot-message-persistence.mdx'), name='03-chatbot-message-persistence.mdx', displayName='03-chatbot-message-persistence.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot Message Persistence\\ndescription: Learn how to store and load chat messages in a chatbot.\\n---\\n\\n# Chatbot Message Persistence\\n\\nBeing able to store and load chat messages is crucial for most AI chatbots.\\nIn this guide, we\\'ll show how to implement message persistence with `useChat` and `streamText`.\\n\\n<Note>\\n  This guide does not cover authorization, error handling, or other real-world\\n  considerations. It is intended to be a simple example of how to implement\\n  message persistence.\\n</Note>\\n\\n## Starting a new chat\\n\\nWhen the user navigates to the chat page without providing a chat ID,\\nwe need to create a new chat and redirect to the chat page with the new chat ID.\\n\\n```tsx filename=\"app/chat/page.tsx\"\\nimport { redirect } from \\'next/navigation\\';\\nimport { createChat } from \\'@util/chat-store\\';\\n\\nexport default async function Page() {\\n  const id = await createChat(); // create a new chat\\n  redirect(`/chat/${id}`); // redirect to chat page, see below\\n}\\n```\\n\\nOur example chat store implementation uses files to store the chat messages.\\nIn a real-world application, you would use a database or a cloud storage service,\\nand get the chat ID from the database.\\nThat being said, the function interfaces are designed to be easily replaced with other implementations.\\n\\n```tsx filename=\"util/chat-store.ts\"\\nimport { generateId } from \\'ai\\';\\nimport { existsSync, mkdirSync } from \\'fs\\';\\nimport { writeFile } from \\'fs/promises\\';\\nimport path from \\'path\\';\\n\\nexport async function createChat(): Promise<string> {\\n  const id = generateId(); // generate a unique chat ID\\n  await writeFile(getChatFile(id), \\'[]\\'); // create an empty chat file\\n  return id;\\n}\\n\\nfunction getChatFile(id: string): string {\\n  const chatDir = path.join(process.cwd(), \\'.chats\\');\\n  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });\\n  return path.join(chatDir, `${id}.json`);\\n}\\n```\\n\\n## Loading an existing chat\\n\\nWhen the user navigates to the chat page with a chat ID, we need to load the chat messages from storage.\\n\\nThe `loadChat` function in our file-based chat store is implemented as follows:\\n\\n```tsx filename=\"util/chat-store.ts\"\\nimport { UIMessage } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nexport async function loadChat(id: string): Promise<UIMessage[]> {\\n  return JSON.parse(await readFile(getChatFile(id), \\'utf8\\'));\\n}\\n\\n// ... rest of the file\\n```\\n\\n## Validating messages on the server\\n\\nWhen processing messages on the server that contain tool calls, custom metadata, or data parts, you should validate them using `validateUIMessages` before sending them to the model.\\n\\n### Validation with tools\\n\\nWhen your messages include tool calls, validate them against your tool definitions:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"7-25,32-37\"\\nimport {\\n  convertToModelMessages,\\n  streamText,\\n  UIMessage,\\n  validateUIMessages,\\n  tool,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\nimport { loadChat, saveChat } from \\'@util/chat-store\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { dataPartsSchema, metadataSchema } from \\'@util/schemas\\';\\n\\n// Define your tools\\nconst tools = {\\n  weather: tool({\\n    description: \\'Get weather information\\',\\n    parameters: z.object({\\n      location: z.string(),\\n      units: z.enum([\\'celsius\\', \\'fahrenheit\\']),\\n    }),\\n    execute: async ({ location, units }) => {\\n      /* tool implementation */\\n    },\\n  }),\\n  // other tools\\n};\\n\\nexport async function POST(req: Request) {\\n  const { message, id } = await req.json();\\n\\n  // Load previous messages from database\\n  const previousMessages = await loadChat(id);\\n\\n  // Append new message to previousMessages messages\\n  const messages = [...previousMessages, message];\\n\\n  // Validate loaded messages against\\n  // tools, data parts schema, and metadata schema\\n  const validatedMessages = await validateUIMessages({\\n    messages,\\n    tools, // Ensures tool calls in messages match current schemas\\n    dataPartsSchema,\\n    metadataSchema,\\n  });\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(validatedMessages),\\n    tools,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId: id, messages });\\n    },\\n  });\\n}\\n```\\n\\n### Handling validation errors\\n\\nHandle validation errors gracefully when messages from the database don\\'t match current schemas:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"3,10-24\"\\nimport {\\n  convertToModelMessages,\\n  streamText,\\n  validateUIMessages,\\n  TypeValidationError,\\n} from \\'ai\\';\\nimport { type MyUIMessage } from \\'@/types\\';\\n\\nexport async function POST(req: Request) {\\n  const { message, id } = await req.json();\\n\\n  // Load and validate messages from database\\n  let validatedMessages: MyUIMessage[];\\n\\n  try {\\n    const previousMessages = await loadMessagesFromDB(id);\\n    validatedMessages = await validateUIMessages({\\n      // append the new message to the previous messages:\\n      messages: [...previousMessages, message],\\n      tools,\\n      metadataSchema,\\n    });\\n  } catch (error) {\\n    if (error instanceof TypeValidationError) {\\n      // Log validation error for monitoring\\n      console.error(\\'Database messages validation failed:\\', error);\\n      // Could implement message migration or filtering here\\n      // For now, start with empty history\\n      validatedMessages = [];\\n    } else {\\n      throw error;\\n    }\\n  }\\n\\n  // Continue with validated messages...\\n}\\n```\\n\\n## Displaying the chat\\n\\nOnce messages are loaded from storage, you can display them in your chat UI. Here\\'s how to set up the page component and the chat display:\\n\\n```tsx filename=\"app/chat/[id]/page.tsx\"\\nimport { loadChat } from \\'@util/chat-store\\';\\nimport Chat from \\'@ui/chat\\';\\n\\nexport default async function Page(props: { params: Promise<{ id: string }> }) {\\n  const { id } = await props.params;\\n  const messages = await loadChat(id);\\n  return <Chat id={id} initialMessages={messages} />;\\n}\\n```\\n\\nThe chat component uses the `useChat` hook to manage the conversation:\\n\\n```tsx filename=\"ui/chat.tsx\" highlight=\"10-16\"\\n\\'use client\\';\\n\\nimport { UIMessage, useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat({\\n  id,\\n  initialMessages,\\n}: { id?: string | undefined; initialMessages?: UIMessage[] } = {}) {\\n  const [input, setInput] = useState(\\'\\');\\n  const { sendMessage, messages } = useChat({\\n    id, // use the provided chat ID\\n    messages: initialMessages, // load initial messages\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    if (input.trim()) {\\n      sendMessage({ text: input });\\n      setInput(\\'\\');\\n    }\\n  };\\n\\n  // simplified rendering code, extend as needed:\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {m.parts\\n            .map(part => (part.type === \\'text\\' ? part.text : \\'\\'))\\n            .join(\\'\\')}\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Type a message...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n## Storing messages\\n\\n`useChat` sends the chat id and the messages to the backend.\\n\\n<Note>\\n  The `useChat` message format is different from the `ModelMessage` format. The\\n  `useChat` message format is designed for frontend display, and contains\\n  additional fields such as `id` and `createdAt`. We recommend storing the\\n  messages in the `useChat` message format.\\n\\nWhen loading messages from storage that contain tools, metadata, or custom data\\nparts, validate them using `validateUIMessages` before processing (see the\\n[validation section](#validating-messages-from-database) above).\\n\\n</Note>\\n\\nStoring messages is done in the `onFinish` callback of the `toUIMessageStreamResponse` function.\\n`onFinish` receives the complete messages including the new AI response as `UIMessage[]`.\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"6,11-17\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { saveChat } from \\'@util/chat-store\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\\n    await req.json();\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId, messages });\\n    },\\n  });\\n}\\n```\\n\\nThe actual storage of the messages is done in the `saveChat` function, which in\\nour file-based chat store is implemented as follows:\\n\\n```tsx filename=\"util/chat-store.ts\"\\nimport { UIMessage } from \\'ai\\';\\nimport { writeFile } from \\'fs/promises\\';\\n\\nexport async function saveChat({\\n  chatId,\\n  messages,\\n}: {\\n  chatId: string;\\n  messages: UIMessage[];\\n}): Promise<void> {\\n  const content = JSON.stringify(messages, null, 2);\\n  await writeFile(getChatFile(chatId), content);\\n}\\n\\n// ... rest of the file\\n```\\n\\n## Message IDs\\n\\nIn addition to a chat ID, each message has an ID.\\nYou can use this message ID to e.g. manipulate individual messages.\\n\\n### Client-side vs Server-side ID Generation\\n\\nBy default, message IDs are generated client-side:\\n\\n- User message IDs are generated by the `useChat` hook on the client\\n- AI response message IDs are generated by `streamText` on the server\\n\\nFor applications without persistence, client-side ID generation works perfectly.\\nHowever, **for persistence, you need server-side generated IDs** to ensure consistency across sessions and prevent ID conflicts when messages are stored and retrieved.\\n\\n### Setting Up Server-side ID Generation\\n\\nWhen implementing persistence, you have two options for generating server-side IDs:\\n\\n1. **Using `generateMessageId` in `toUIMessageStreamResponse`**\\n2. **Setting IDs in your start message part with `createUIMessageStream`**\\n\\n#### Option 1: Using `generateMessageId` in `toUIMessageStreamResponse`\\n\\nYou can control the ID format by providing ID generators using [`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator):\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"7-11\"\\nimport { createIdGenerator, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  // ...\\n  const result = streamText({\\n    // ...\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    // Generate consistent server-side IDs for persistence:\\n    generateMessageId: createIdGenerator({\\n      prefix: \\'msg\\',\\n      size: 16,\\n    }),\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId, messages });\\n    },\\n  });\\n}\\n```\\n\\n#### Option 2: Setting IDs with `createUIMessageStream`\\n\\nAlternatively, you can use `createUIMessageStream` to control the message ID by writing a start message part:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"8-18\"\\nimport {\\n  generateId,\\n  streamText,\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n} from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages, chatId } = await req.json();\\n\\n  const stream = createUIMessageStream({\\n    execute: ({ writer }) => {\\n      // Write start message part with custom ID\\n      writer.write({\\n        type: \\'start\\',\\n        messageId: generateId(), // Generate server-side ID for persistence\\n      });\\n\\n      const result = streamText({\\n        model: \\'openai/gpt-5-mini\\',\\n        messages: convertToModelMessages(messages),\\n      });\\n\\n      writer.merge(result.toUIMessageStream({ sendStart: false })); // omit start message part\\n    },\\n    originalMessages: messages,\\n    onFinish: ({ responseMessage }) => {\\n      // save your chat here\\n    },\\n  });\\n\\n  return createUIMessageStreamResponse({ stream });\\n}\\n```\\n\\n<Note>\\n  For client-side applications that don\\'t require persistence, you can still customize client-side ID generation:\\n\\n```tsx filename=\"ui/chat.tsx\"\\nimport { createIdGenerator } from \\'ai\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nconst { ... } = useChat({\\n  generateId: createIdGenerator({\\n    prefix: \\'msgc\\',\\n    size: 16,\\n  }),\\n  // ...\\n});\\n```\\n\\n</Note>\\n\\n## Sending only the last message\\n\\nOnce you have implemented message persistence, you might want to send only the last message to the server.\\nThis reduces the amount of data sent to the server on each request and can improve performance.\\n\\nTo achieve this, you can provide a `prepareSendMessagesRequest` function to the transport.\\nThis function receives the messages and the chat ID, and returns the request body to be sent to the server.\\n\\n```tsx filename=\"ui/chat.tsx\" highlight=\"7-12\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nconst {\\n  // ...\\n} = useChat({\\n  // ...\\n  transport: new DefaultChatTransport({\\n    api: \\'/api/chat\\',\\n    // only send the last message to the server:\\n    prepareSendMessagesRequest({ messages, id }) {\\n      return { body: { message: messages[messages.length - 1], id } };\\n    },\\n  }),\\n});\\n```\\n\\nOn the server, you can then load the previous messages and append the new message to the previous messages. If your messages contain tools, metadata, or custom data parts, you should validate them:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"2-11,14-18\"\\nimport { convertToModelMessages, UIMessage, validateUIMessages } from \\'ai\\';\\n// import your tools and schemas\\n\\nexport async function POST(req: Request) {\\n  // get the last message from the client:\\n  const { message, id } = await req.json();\\n\\n  // load the previous messages from the server:\\n  const previousMessages = await loadChat(id);\\n\\n  // validate messages if they contain tools, metadata, or data parts:\\n  const validatedMessages = await validateUIMessages({\\n    // append the new message to the previous messages:\\n    messages: [...previousMessages, message],\\n    tools, // if using tools\\n    metadataSchema, // if using custom metadata\\n    dataSchemas, // if using custom data parts\\n  });\\n\\n  const result = streamText({\\n    // ...\\n    messages: convertToModelMessages(validatedMessages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: validatedMessages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId: id, messages });\\n    },\\n  });\\n}\\n```\\n\\n## Handling client disconnects\\n\\nBy default, the AI SDK `streamText` function uses backpressure to the language model provider to prevent\\nthe consumption of tokens that are not yet requested.\\n\\nHowever, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue,\\nthe stream from the LLM will be aborted and the conversation may end up in a broken state.\\n\\nAssuming that you have a [storage solution](#storing-messages) in place, you can use the `consumeStream` method to consume the stream on the backend,\\nand then save the result as usual.\\n`consumeStream` effectively removes the backpressure,\\nmeaning that the result is stored even when the client has already disconnected.\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"19-21\"\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\nimport { saveChat } from \\'@util/chat-store\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\\n    await req.json();\\n\\n  const result = streamText({\\n    model,\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  // consume the stream to ensure it runs to completion & triggers onFinish\\n  // even when the client response is aborted:\\n  result.consumeStream(); // no await\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId, messages });\\n    },\\n  });\\n}\\n```\\n\\nWhen the client reloads the page after a disconnect, the chat will be restored from the storage solution.\\n\\n<Note>\\n  In production applications, you would also track the state of the request (in\\n  progress, complete) in your stored messages and use it on the client to cover\\n  the case where the client reloads the page after a disconnection, but the\\n  streaming is not yet complete.\\n</Note>\\n\\nFor more robust handling of disconnects, you may want to add resumability on disconnects. Check out the [Chatbot Resume Streams](/docs/ai-sdk-ui/chatbot-resume-streams) documentation to learn more.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/03-chatbot-resume-streams.mdx'), name='03-chatbot-resume-streams.mdx', displayName='03-chatbot-resume-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot Resume Streams\\ndescription: Learn how to resume chatbot streams after client disconnects.\\n---\\n\\n# Chatbot Resume Streams\\n\\n`useChat` supports resuming ongoing streams after page reloads. Use this feature to build applications with long-running generations.\\n\\n<Note type=\"warning\">\\n  Stream resumption is not compatible with abort functionality. Closing a tab or\\n  refreshing the page triggers an abort signal that will break the resumption\\n  mechanism. Do not use `resume: true` if you need abort functionality in your\\n  application. See\\n  [troubleshooting](/docs/troubleshooting/abort-breaks-resumable-streams) for\\n  more details.\\n</Note>\\n\\n## How stream resumption works\\n\\nStream resumption requires persistence for messages and active streams in your application. The AI SDK provides tools to connect to storage, but you need to set up the storage yourself.\\n\\n**The AI SDK provides:**\\n\\n- A `resume` option in `useChat` that automatically reconnects to active streams\\n- Access to the outgoing stream through the `consumeSseStream` callback\\n- Automatic HTTP requests to your resume endpoints\\n\\n**You build:**\\n\\n- Storage to track which stream belongs to each chat\\n- Redis to store the UIMessage stream\\n- Two API endpoints: POST to create streams, GET to resume them\\n- Integration with [`resumable-stream`](https://www.npmjs.com/package/resumable-stream) to manage Redis storage\\n\\n## Prerequisites\\n\\nTo implement resumable streams in your chat application, you need:\\n\\n1. **The `resumable-stream` package** - Handles the publisher/subscriber mechanism for streams\\n2. **A Redis instance** - Stores stream data (e.g. [Redis through Vercel](https://vercel.com/marketplace/redis))\\n3. **A persistence layer** - Tracks which stream ID is active for each chat (e.g. database)\\n\\n## Implementation\\n\\n### 1. Client-side: Enable stream resumption\\n\\nUse the `resume` option in the `useChat` hook to enable stream resumption. When `resume` is true, the hook automatically attempts to reconnect to any active stream for the chat on mount:\\n\\n```tsx filename=\"app/chat/[chatId]/chat.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport, type UIMessage } from \\'ai\\';\\n\\nexport function Chat({\\n  chatData,\\n  resume = false,\\n}: {\\n  chatData: { id: string; messages: UIMessage[] };\\n  resume?: boolean;\\n}) {\\n  const { messages, sendMessage, status } = useChat({\\n    id: chatData.id,\\n    messages: chatData.messages,\\n    resume, // Enable automatic stream resumption\\n    transport: new DefaultChatTransport({\\n      // You must send the id of the chat\\n      prepareSendMessagesRequest: ({ id, messages }) => {\\n        return {\\n          body: {\\n            id,\\n            message: messages[messages.length - 1],\\n          },\\n        };\\n      },\\n    }),\\n  });\\n\\n  return <div>{/* Your chat UI */}</div>;\\n}\\n```\\n\\n<Note>\\n  You must send the chat ID with each request (see\\n  `prepareSendMessagesRequest`).\\n</Note>\\n\\nWhen you enable `resume`, the `useChat` hook makes a `GET` request to `/api/chat/[id]/stream` on mount to check for and resume any active streams.\\n\\nLet\\'s start by creating the POST handler to create the resumable stream.\\n\\n### 2. Create the POST handler\\n\\nThe POST handler creates resumable streams using the `consumeSseStream` callback:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readChat, saveChat } from \\'@util/chat-store\\';\\nimport {\\n  convertToModelMessages,\\n  generateId,\\n  streamText,\\n  type UIMessage,\\n} from \\'ai\\';\\nimport { after } from \\'next/server\\';\\nimport { createResumableStreamContext } from \\'resumable-stream\\';\\n\\nexport async function POST(req: Request) {\\n  const {\\n    message,\\n    id,\\n  }: {\\n    message: UIMessage | undefined;\\n    id: string;\\n  } = await req.json();\\n\\n  const chat = await readChat(id);\\n  let messages = chat.messages;\\n\\n  messages = [...messages, message!];\\n\\n  // Clear any previous active stream and save the user message\\n  saveChat({ id, messages, activeStreamId: null });\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    generateMessageId: generateId,\\n    onFinish: ({ messages }) => {\\n      // Clear the active stream when finished\\n      saveChat({ id, messages, activeStreamId: null });\\n    },\\n    async consumeSseStream({ stream }) {\\n      const streamId = generateId();\\n\\n      // Create a resumable stream from the SSE stream\\n      const streamContext = createResumableStreamContext({ waitUntil: after });\\n      await streamContext.createNewResumableStream(streamId, () => stream);\\n\\n      // Update the chat with the active stream ID\\n      saveChat({ id, activeStreamId: streamId });\\n    },\\n  });\\n}\\n```\\n\\n### 3. Implement the GET handler\\n\\nCreate a GET handler at `/api/chat/[id]/stream` that:\\n\\n1. Reads the chat ID from the route params\\n2. Loads the chat data to check for an active stream\\n3. Returns 204 (No Content) if no stream is active\\n4. Resumes the existing stream if one is found\\n\\n```ts filename=\"app/api/chat/[id]/stream/route.ts\"\\nimport { readChat } from \\'@util/chat-store\\';\\nimport { UI_MESSAGE_STREAM_HEADERS } from \\'ai\\';\\nimport { after } from \\'next/server\\';\\nimport { createResumableStreamContext } from \\'resumable-stream\\';\\n\\nexport async function GET(\\n  _: Request,\\n  { params }: { params: Promise<{ id: string }> },\\n) {\\n  const { id } = await params;\\n\\n  const chat = await readChat(id);\\n\\n  if (chat.activeStreamId == null) {\\n    // no content response when there is no active stream\\n    return new Response(null, { status: 204 });\\n  }\\n\\n  const streamContext = createResumableStreamContext({\\n    waitUntil: after,\\n  });\\n\\n  return new Response(\\n    await streamContext.resumeExistingStream(chat.activeStreamId),\\n    { headers: UI_MESSAGE_STREAM_HEADERS },\\n  );\\n}\\n```\\n\\n<Note>\\n  The `after` function from Next.js allows work to continue after the response\\n  has been sent. This ensures that the resumable stream persists in Redis even\\n  after the initial response is returned to the client, enabling reconnection\\n  later.\\n</Note>\\n\\n## How it works\\n\\n### Request lifecycle\\n\\n![Diagram showing the architecture and lifecycle of resumable stream requests](https://e742qlubrjnjqpp0.public.blob.vercel-storage.com/resume-stream-diagram.png)\\n\\nThe diagram above shows the complete lifecycle of a resumable stream:\\n\\n1. **Stream creation**: When you send a new message, the POST handler uses `streamText` to generate the response. The `consumeSseStream` callback creates a resumable stream with a unique ID and stores it in Redis through the `resumable-stream` package\\n2. **Stream tracking**: Your persistence layer saves the `activeStreamId` in the chat data\\n3. **Client reconnection**: When the client reconnects (page reload), the `resume` option triggers a GET request to `/api/chat/[id]/stream`\\n4. **Stream recovery**: The GET handler checks for an `activeStreamId` and uses `resumeExistingStream` to reconnect. If no active stream exists, it returns a 204 (No Content) response\\n5. **Completion cleanup**: When the stream finishes, the `onFinish` callback clears the `activeStreamId` by setting it to `null`\\n\\n## Customize the resume endpoint\\n\\nBy default, the `useChat` hook makes a GET request to `/api/chat/[id]/stream` when resuming. Customize this endpoint, credentials, and headers, using the `prepareReconnectToStreamRequest` option in `DefaultChatTransport`:\\n\\n```tsx filename=\"app/chat/[chatId]/chat.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport function Chat({ chatData, resume }) {\\n  const { messages, sendMessage } = useChat({\\n    id: chatData.id,\\n    messages: chatData.messages,\\n    resume,\\n    transport: new DefaultChatTransport({\\n      // Customize reconnect settings (optional)\\n      prepareReconnectToStreamRequest: ({ id }) => {\\n        return {\\n          api: `/api/chat/${id}/stream`, // Default pattern\\n          // Or use a different pattern:\\n          // api: `/api/streams/${id}/resume`,\\n          // api: `/api/resume-chat?id=${id}`,\\n          credentials: \\'include\\', // Include cookies/auth\\n          headers: {\\n            Authorization: \\'Bearer token\\',\\n            \\'X-Custom-Header\\': \\'value\\',\\n          },\\n        };\\n      },\\n    }),\\n  });\\n\\n  return <div>{/* Your chat UI */}</div>;\\n}\\n```\\n\\nThis lets you:\\n\\n- Match your existing API route structure\\n- Add query parameters or custom paths\\n- Integrate with different backend architectures\\n\\n## Important considerations\\n\\n- **Incompatibility with abort**: Stream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use `resume: true` if you need abort functionality in your application\\n- **Stream expiration**: Streams in Redis expire after a set time (configurable in the `resumable-stream` package)\\n- **Multiple clients**: Multiple clients can connect to the same stream simultaneously\\n- **Error handling**: When no active stream exists, the GET handler returns a 204 (No Content) status code\\n- **Security**: Ensure proper authentication and authorization for both creating and resuming streams\\n- **Race conditions**: Clear the `activeStreamId` when starting a new stream to prevent resuming outdated streams\\n\\n<br />\\n<GithubLink link=\"https://github.com/vercel/ai/blob/main/examples/next\" />\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/03-chatbot-tool-usage.mdx'), name='03-chatbot-tool-usage.mdx', displayName='03-chatbot-tool-usage.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot Tool Usage\\ndescription: Learn how to use tools with the useChat hook.\\n---\\n\\n# Chatbot Tool Usage\\n\\nWith [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`streamText`](/docs/reference/ai-sdk-core/stream-text), you can use tools in your chatbot application.\\nThe AI SDK supports three types of tools in this context:\\n\\n1. Automatically executed server-side tools\\n2. Automatically executed client-side tools\\n3. Tools that require user interaction, such as confirmation dialogs\\n\\nThe flow is as follows:\\n\\n1. The user enters a message in the chat UI.\\n1. The message is sent to the API route.\\n1. In your server side route, the language model generates tool calls during the `streamText` call.\\n1. All tool calls are forwarded to the client.\\n1. Server-side tools are executed using their `execute` method and their results are forwarded to the client.\\n1. Client-side tools that should be automatically executed are handled with the `onToolCall` callback.\\n   You must call `addToolOutput` to provide the tool result.\\n1. Client-side tool that require user interactions can be displayed in the UI.\\n   The tool calls and results are available as tool invocation parts in the `parts` property of the last assistant message.\\n1. When the user interaction is done, `addToolOutput` can be used to add the tool result to the chat.\\n1. The chat can be configured to automatically submit when all tool results are available using `sendAutomaticallyWhen`.\\n   This triggers another iteration of this flow.\\n\\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts.\\nA tool part is at first a tool call, and then it becomes a tool result when the tool is executed.\\nThe tool result contains all information about the tool call as well as the result of the tool execution.\\n\\n<Note>\\n  Tool result submission can be configured using the `sendAutomaticallyWhen`\\n  option. You can use the `lastAssistantMessageIsCompleteWithToolCalls` helper\\n  to automatically submit when all tool results are available. This simplifies\\n  the client-side code while still allowing full control when needed.\\n</Note>\\n\\n## Example\\n\\nIn this example, we\\'ll use three tools:\\n\\n- `getWeatherInformation`: An automatically executed server-side tool that returns the weather in a given city.\\n- `askForConfirmation`: A user-interaction client-side tool that asks the user for confirmation.\\n- `getLocation`: An automatically executed client-side tool that returns a random city.\\n\\n### API route\\n\\n```tsx filename=\\'app/api/chat/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      // server-side tool with execute function:\\n      getWeatherInformation: {\\n        description: \\'show the weather in a given city to the user\\',\\n        inputSchema: z.object({ city: z.string() }),\\n        execute: async ({}: { city: string }) => {\\n          const weatherOptions = [\\'sunny\\', \\'cloudy\\', \\'rainy\\', \\'snowy\\', \\'windy\\'];\\n          return weatherOptions[\\n            Math.floor(Math.random() * weatherOptions.length)\\n          ];\\n        },\\n      },\\n      // client-side tool that starts user interaction:\\n      askForConfirmation: {\\n        description: \\'Ask the user for confirmation.\\',\\n        inputSchema: z.object({\\n          message: z.string().describe(\\'The message to ask for confirmation.\\'),\\n        }),\\n      },\\n      // client-side tool that is automatically executed on the client:\\n      getLocation: {\\n        description:\\n          \\'Get the user location. Always ask for confirmation before using this tool.\\',\\n        inputSchema: z.object({}),\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Client-side page\\n\\nThe client-side page uses the `useChat` hook to create a chatbot application with real-time message streaming.\\nTool calls are displayed in the chat UI as typed tool parts.\\nPlease make sure to render the messages using the `parts` property of the message.\\n\\nThere are three things worth mentioning:\\n\\n1. The [`onToolCall`](/docs/reference/ai-sdk-ui/use-chat#on-tool-call) callback is used to handle client-side tools that should be automatically executed.\\n   In this example, the `getLocation` tool is a client-side tool that returns a random city.\\n   You call `addToolOutput` to provide the result (without `await` to avoid potential deadlocks).\\n\\n   <Note>\\n     Always check `if (toolCall.dynamic)` first in your `onToolCall` handler.\\n     Without this check, TypeScript will throw an error like: `Type \\'string\\' is\\n     not assignable to type \\'\"toolName1\" | \"toolName2\"\\'` when you try to use\\n     `toolCall.toolName` in `addToolOutput`.\\n   </Note>\\n\\n2. The [`sendAutomaticallyWhen`](/docs/reference/ai-sdk-ui/use-chat#send-automatically-when) option with `lastAssistantMessageIsCompleteWithToolCalls` helper automatically submits when all tool results are available.\\n\\n3. The `parts` array of assistant messages contains tool parts with typed names like `tool-askForConfirmation`.\\n   The client-side tool `askForConfirmation` is displayed in the UI.\\n   It asks the user for confirmation and displays the result once the user confirms or denies the execution.\\n   The result is added to the chat using `addToolOutput` with the `tool` parameter for type safety.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"2,6,10,14-20\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport {\\n  DefaultChatTransport,\\n  lastAssistantMessageIsCompleteWithToolCalls,\\n} from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, addToolOutput } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n\\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n\\n    // run client-side tools that are automatically executed:\\n    async onToolCall({ toolCall }) {\\n      // Check if it\\'s a dynamic tool first for proper type narrowing\\n      if (toolCall.dynamic) {\\n        return;\\n      }\\n\\n      if (toolCall.toolName === \\'getLocation\\') {\\n        const cities = [\\'New York\\', \\'Los Angeles\\', \\'Chicago\\', \\'San Francisco\\'];\\n\\n        // No await - avoids potential deadlocks\\n        addToolOutput({\\n          tool: \\'getLocation\\',\\n          toolCallId: toolCall.toolCallId,\\n          output: cities[Math.floor(Math.random() * cities.length)],\\n        });\\n      }\\n    },\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <>\\n      {messages?.map(message => (\\n        <div key={message.id}>\\n          <strong>{`${message.role}: `}</strong>\\n          {message.parts.map(part => {\\n            switch (part.type) {\\n              // render text parts as simple text:\\n              case \\'text\\':\\n                return part.text;\\n\\n              // for tool parts, use the typed tool part names:\\n              case \\'tool-askForConfirmation\\': {\\n                const callId = part.toolCallId;\\n\\n                switch (part.state) {\\n                  case \\'input-streaming\\':\\n                    return (\\n                      <div key={callId}>Loading confirmation request...</div>\\n                    );\\n                  case \\'input-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        {part.input.message}\\n                        <div>\\n                          <button\\n                            onClick={() =>\\n                              addToolOutput({\\n                                tool: \\'askForConfirmation\\',\\n                                toolCallId: callId,\\n                                output: \\'Yes, confirmed.\\',\\n                              })\\n                            }\\n                          >\\n                            Yes\\n                          </button>\\n                          <button\\n                            onClick={() =>\\n                              addToolOutput({\\n                                tool: \\'askForConfirmation\\',\\n                                toolCallId: callId,\\n                                output: \\'No, denied\\',\\n                              })\\n                            }\\n                          >\\n                            No\\n                          </button>\\n                        </div>\\n                      </div>\\n                    );\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        Location access allowed: {part.output}\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={callId}>Error: {part.errorText}</div>;\\n                }\\n                break;\\n              }\\n\\n              case \\'tool-getLocation\\': {\\n                const callId = part.toolCallId;\\n\\n                switch (part.state) {\\n                  case \\'input-streaming\\':\\n                    return (\\n                      <div key={callId}>Preparing location request...</div>\\n                    );\\n                  case \\'input-available\\':\\n                    return <div key={callId}>Getting location...</div>;\\n                  case \\'output-available\\':\\n                    return <div key={callId}>Location: {part.output}</div>;\\n                  case \\'output-error\\':\\n                    return (\\n                      <div key={callId}>\\n                        Error getting location: {part.errorText}\\n                      </div>\\n                    );\\n                }\\n                break;\\n              }\\n\\n              case \\'tool-getWeatherInformation\\': {\\n                const callId = part.toolCallId;\\n\\n                switch (part.state) {\\n                  // example of pre-rendering streaming tool inputs:\\n                  case \\'input-streaming\\':\\n                    return (\\n                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\\n                    );\\n                  case \\'input-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        Getting weather information for {part.input.city}...\\n                      </div>\\n                    );\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        Weather in {part.input.city}: {part.output}\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return (\\n                      <div key={callId}>\\n                        Error getting weather for {part.input.city}:{\\' \\'}\\n                        {part.errorText}\\n                      </div>\\n                    );\\n                }\\n                break;\\n              }\\n            }\\n          })}\\n          <br />\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n### Error handling\\n\\nSometimes an error may occur during client-side tool execution. Use the `addToolOutput` method with a `state` of `output-error` and `errorText` value instead of `output` record the error.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"19,36-41\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport {\\n  DefaultChatTransport,\\n  lastAssistantMessageIsCompleteWithToolCalls,\\n} from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, addToolOutput } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n\\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n\\n    // run client-side tools that are automatically executed:\\n    async onToolCall({ toolCall }) {\\n      // Check if it\\'s a dynamic tool first for proper type narrowing\\n      if (toolCall.dynamic) {\\n        return;\\n      }\\n\\n      if (toolCall.toolName === \\'getWeatherInformation\\') {\\n        try {\\n          const weather = await getWeatherInformation(toolCall.input);\\n\\n          // No await - avoids potential deadlocks\\n          addToolOutput({\\n            tool: \\'getWeatherInformation\\',\\n            toolCallId: toolCall.toolCallId,\\n            output: weather,\\n          });\\n        } catch (err) {\\n          addToolOutput({\\n            tool: \\'getWeatherInformation\\',\\n            toolCallId: toolCall.toolCallId,\\n            state: \\'output-error\\',\\n            errorText: \\'Unable to get the weather information\\',\\n          });\\n        }\\n      }\\n    },\\n  });\\n}\\n```\\n\\n## Dynamic Tools\\n\\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic `dynamic-tool` type instead of specific tool types:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n{\\n  message.parts.map((part, index) => {\\n    switch (part.type) {\\n      // Static tools with specific (`tool-${toolName}`) types\\n      case \\'tool-getWeatherInformation\\':\\n        return <WeatherDisplay part={part} />;\\n\\n      // Dynamic tools use generic `dynamic-tool` type\\n      case \\'dynamic-tool\\':\\n        return (\\n          <div key={index}>\\n            <h4>Tool: {part.toolName}</h4>\\n            {part.state === \\'input-streaming\\' && (\\n              <pre>{JSON.stringify(part.input, null, 2)}</pre>\\n            )}\\n            {part.state === \\'output-available\\' && (\\n              <pre>{JSON.stringify(part.output, null, 2)}</pre>\\n            )}\\n            {part.state === \\'output-error\\' && (\\n              <div>Error: {part.errorText}</div>\\n            )}\\n          </div>\\n        );\\n    }\\n  });\\n}\\n```\\n\\nDynamic tools are useful when integrating with:\\n\\n- MCP (Model Context Protocol) tools without schemas\\n- User-defined functions loaded at runtime\\n- External tool providers\\n\\n## Tool call streaming\\n\\nTool call streaming is **enabled by default** in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\\n\\n```tsx filename=\\'app/api/chat/route.ts\\'\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    // toolCallStreaming is enabled by default in v5\\n    // ...\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream.\\nThey are available through the `useChat` hook.\\nThe typed tool parts of assistant messages will also contain partial tool calls.\\nYou can use the `state` property of the tool part to render the correct UI.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"9,10\"\\nexport default function Chat() {\\n  // ...\\n  return (\\n    <>\\n      {messages?.map(message => (\\n        <div key={message.id}>\\n          {message.parts.map(part => {\\n            switch (part.type) {\\n              case \\'tool-askForConfirmation\\':\\n              case \\'tool-getLocation\\':\\n              case \\'tool-getWeatherInformation\\':\\n                switch (part.state) {\\n                  case \\'input-streaming\\':\\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\\n                  case \\'input-available\\':\\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\\n                  case \\'output-available\\':\\n                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\\n                  case \\'output-error\\':\\n                    return <div>Error: {part.errorText}</div>;\\n                }\\n            }\\n          })}\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n## Step start parts\\n\\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages.\\nIf you want to display boundaries between tool calls, you can use the `step-start` parts as follows:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n// ...\\n// where you render the message parts:\\nmessage.parts.map((part, index) => {\\n  switch (part.type) {\\n    case \\'step-start\\':\\n      // show step boundaries as horizontal lines:\\n      return index > 0 ? (\\n        <div key={index} className=\"text-gray-500\">\\n          <hr className=\"my-2 border-gray-300\" />\\n        </div>\\n      ) : null;\\n    case \\'text\\':\\n    // ...\\n    case \\'tool-askForConfirmation\\':\\n    case \\'tool-getLocation\\':\\n    case \\'tool-getWeatherInformation\\':\\n    // ...\\n  }\\n});\\n// ...\\n```\\n\\n## Server-side Multi-Step Calls\\n\\nYou can also use multi-step calls on the server-side with `streamText`.\\nThis works when all invoked tools have an `execute` function on the server side.\\n\\n```tsx filename=\\'app/api/chat/route.ts\\' highlight=\"15-21,24\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage, stepCountIs } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      getWeatherInformation: {\\n        description: \\'show the weather in a given city to the user\\',\\n        inputSchema: z.object({ city: z.string() }),\\n        // tool has execute function:\\n        execute: async ({}: { city: string }) => {\\n          const weatherOptions = [\\'sunny\\', \\'cloudy\\', \\'rainy\\', \\'snowy\\', \\'windy\\'];\\n          return weatherOptions[\\n            Math.floor(Math.random() * weatherOptions.length)\\n          ];\\n        },\\n      },\\n    },\\n    stopWhen: stepCountIs(5),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Errors\\n\\nLanguage models can make errors when calling tools.\\nBy default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\\n\\nTo surface the errors, you can use the `onError` function when calling `toUIMessageResponse`.\\n\\n```tsx\\nexport function errorHandler(error: unknown) {\\n  if (error == null) {\\n    return \\'unknown error\\';\\n  }\\n\\n  if (typeof error === \\'string\\') {\\n    return error;\\n  }\\n\\n  if (error instanceof Error) {\\n    return error.message;\\n  }\\n\\n  return JSON.stringify(error);\\n}\\n```\\n\\n```tsx\\nconst result = streamText({\\n  // ...\\n});\\n\\nreturn result.toUIMessageStreamResponse({\\n  onError: errorHandler,\\n});\\n```\\n\\nIn case you are using `createUIMessageResponse`, you can use the `onError` function when calling `toUIMessageResponse`:\\n\\n```tsx\\nconst response = createUIMessageResponse({\\n  // ...\\n  async execute(dataStream) {\\n    // ...\\n  },\\n  onError: error => `Custom error: ${error.message}`,\\n});\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/04-generative-user-interfaces.mdx'), name='04-generative-user-interfaces.mdx', displayName='04-generative-user-interfaces.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Generative User Interfaces\\ndescription: Learn how to build Generative UI with AI SDK UI.\\n---\\n\\n# Generative User Interfaces\\n\\nGenerative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and \"generate UI\". This creates a more engaging and AI-native experience for users.\\n\\n<WeatherSearch />\\n\\nAt the core of generative UI are [ tools ](/docs/ai-sdk-core/tools-and-tool-calling), which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.\\n\\nGenerative UI is the process of connecting the results of a tool call to a React component. Here\\'s how it works:\\n\\n1. You provide the model with a prompt or conversation history, along with a set of tools.\\n2. Based on the context, the model may decide to call a tool.\\n3. If a tool is called, it will execute and return data.\\n4. This data can then be passed to a React component for rendering.\\n\\nBy passing the tool results to React components, you can create a generative UI experience that\\'s more engaging and adaptive to your needs.\\n\\n## Build a Generative UI Chat Interface\\n\\nLet\\'s create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.\\n\\n### Basic Chat Implementation\\n\\nStart with a basic chat implementation using the `useChat` hook:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}</div>\\n          <div>\\n            {message.parts.map((part, index) => {\\n              if (part.type === \\'text\\') {\\n                return <span key={index}>{part.text}</span>;\\n              }\\n              return null;\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Type a message...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nTo handle the chat requests and model responses, set up an API route:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from \\'ai\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'You are a friendly assistant!\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nThis API route uses the `streamText` function to process chat messages and stream the model\\'s responses back to the client.\\n\\n### Create a Tool\\n\\nBefore enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.\\n\\nCreate a new file called `ai/tools.ts` with the following content:\\n\\n```ts filename=\"ai/tools.ts\"\\nimport { tool as createTool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const weatherTool = createTool({\\n  description: \\'Display the weather for a location\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The location to get the weather for\\'),\\n  }),\\n  execute: async function ({ location }) {\\n    await new Promise(resolve => setTimeout(resolve, 2000));\\n    return { weather: \\'Sunny\\', temperature: 75, location };\\n  },\\n});\\n\\nexport const tools = {\\n  displayWeather: weatherTool,\\n};\\n```\\n\\nIn this file, you\\'ve created a tool called `weatherTool`. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.\\n\\n### Update the API Route\\n\\nUpdate the API route to include the tool you\\'ve defined:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"3,8,14\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from \\'ai\\';\\nimport { tools } from \\'@/ai/tools\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'You are a friendly assistant!\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nNow that you\\'ve defined the tool and added it to your `streamText` call, let\\'s build a React component to display the weather information it returns.\\n\\n### Create UI Components\\n\\nCreate a new file called `components/weather.tsx`:\\n\\n```tsx filename=\"components/weather.tsx\"\\ntype WeatherProps = {\\n  temperature: number;\\n  weather: string;\\n  location: string;\\n};\\n\\nexport const Weather = ({ temperature, weather, location }: WeatherProps) => {\\n  return (\\n    <div>\\n      <h2>Current Weather for {location}</h2>\\n      <p>Condition: {weather}</p>\\n      <p>Temperature: {temperature}°C</p>\\n    </div>\\n  );\\n};\\n```\\n\\nThis component will display the weather information for a given location. It takes three props: `temperature`, `weather`, and `location` (exactly what the `weatherTool` returns).\\n\\n### Render the Weather Component\\n\\nNow that you have your tool and corresponding React component, let\\'s integrate them into your chat interface. You\\'ll render the Weather component when the model calls the weather tool.\\n\\nTo check if the model has called a tool, you can check the `parts` array of the UIMessage object for tool-specific parts. In AI SDK 5.0, tool parts use typed naming: `tool-${toolName}` instead of generic types.\\n\\nUpdate your `page.tsx` file:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"4,9,14-15,19-46\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport { Weather } from \\'@/components/weather\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}</div>\\n          <div>\\n            {message.parts.map((part, index) => {\\n              if (part.type === \\'text\\') {\\n                return <span key={index}>{part.text}</span>;\\n              }\\n\\n              if (part.type === \\'tool-displayWeather\\') {\\n                switch (part.state) {\\n                  case \\'input-available\\':\\n                    return <div key={index}>Loading weather...</div>;\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={index}>\\n                        <Weather {...part.output} />\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={index}>Error: {part.errorText}</div>;\\n                  default:\\n                    return null;\\n                }\\n              }\\n\\n              return null;\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Type a message...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nIn this updated code snippet, you:\\n\\n1. Use manual input state management with `useState` instead of the built-in `input` and `handleInputChange`.\\n2. Use `sendMessage` instead of `handleSubmit` to send messages.\\n3. Check the `parts` array of each message for different content types.\\n4. Handle tool parts with type `tool-displayWeather` and their different states (`input-available`, `output-available`, `output-error`).\\n\\nThis approach allows you to dynamically render UI components based on the model\\'s responses, creating a more interactive and context-aware chat experience.\\n\\n## Expanding Your Generative UI Application\\n\\nYou can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here\\'s how you can expand your application:\\n\\n### Adding More Tools\\n\\nTo add more tools, simply define them in your `ai/tools.ts` file:\\n\\n```ts\\n// Add a new stock tool\\nexport const stockTool = createTool({\\n  description: \\'Get price for a stock\\',\\n  inputSchema: z.object({\\n    symbol: z.string().describe(\\'The stock symbol to get the price for\\'),\\n  }),\\n  execute: async function ({ symbol }) {\\n    // Simulated API call\\n    await new Promise(resolve => setTimeout(resolve, 2000));\\n    return { symbol, price: 100 };\\n  },\\n});\\n\\n// Update the tools object\\nexport const tools = {\\n  displayWeather: weatherTool,\\n  getStockPrice: stockTool,\\n};\\n```\\n\\nNow, create a new file called `components/stock.tsx`:\\n\\n```tsx\\ntype StockProps = {\\n  price: number;\\n  symbol: string;\\n};\\n\\nexport const Stock = ({ price, symbol }: StockProps) => {\\n  return (\\n    <div>\\n      <h2>Stock Information</h2>\\n      <p>Symbol: {symbol}</p>\\n      <p>Price: ${price}</p>\\n    </div>\\n  );\\n};\\n```\\n\\nFinally, update your `page.tsx` file to include the new Stock component:\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport { Weather } from \\'@/components/weather\\';\\nimport { Stock } from \\'@/components/stock\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>\\n            {message.parts.map((part, index) => {\\n              if (part.type === \\'text\\') {\\n                return <span key={index}>{part.text}</span>;\\n              }\\n\\n              if (part.type === \\'tool-displayWeather\\') {\\n                switch (part.state) {\\n                  case \\'input-available\\':\\n                    return <div key={index}>Loading weather...</div>;\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={index}>\\n                        <Weather {...part.output} />\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={index}>Error: {part.errorText}</div>;\\n                  default:\\n                    return null;\\n                }\\n              }\\n\\n              if (part.type === \\'tool-getStockPrice\\') {\\n                switch (part.state) {\\n                  case \\'input-available\\':\\n                    return <div key={index}>Loading stock price...</div>;\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={index}>\\n                        <Stock {...part.output} />\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={index}>Error: {part.errorText}</div>;\\n                  default:\\n                    return null;\\n                }\\n              }\\n\\n              return null;\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nBy following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/05-completion.mdx'), name='05-completion.mdx', displayName='05-completion.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Completion\\ndescription: Learn how to use the useCompletion hook.\\n---\\n\\n# Completion\\n\\nThe `useCompletion` hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\\n\\n<Note>\\n  The `useCompletion` hook is now part of the `@ai-sdk/react` package.\\n</Note>\\n\\nIn this guide, you will learn how to use the `useCompletion` hook in your application to generate text completions and stream them in real-time to your users.\\n\\n## Example\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useCompletion } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\\n    api: \\'/api/completion\\',\\n  });\\n\\n  return (\\n    <form onSubmit={handleSubmit}>\\n      <input\\n        name=\"prompt\"\\n        value={input}\\n        onChange={handleInputChange}\\n        id=\"input\"\\n      />\\n      <button type=\"submit\">Submit</button>\\n      <div>{completion}</div>\\n    </form>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/completion/route.ts\\'\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { prompt }: { prompt: string } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn the `Page` component, the `useCompletion` hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.\\n\\nThis enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\\n\\n## Customized UI\\n\\n`useCompletion` also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.\\n\\n### Loading and error states\\n\\nTo show a loading spinner while the chatbot is processing the user\\'s message, you can use the `isLoading` state returned by the `useCompletion` hook:\\n\\n```tsx\\nconst { isLoading, ... } = useCompletion()\\n\\nreturn(\\n  <>\\n    {isLoading ? <Spinner /> : null}\\n  </>\\n)\\n```\\n\\nSimilarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:\\n\\n```tsx\\nconst { error, ... } = useCompletion()\\n\\nuseEffect(() => {\\n  if (error) {\\n    toast.error(error.message)\\n  }\\n}, [error])\\n\\n// Or display the error message in the UI:\\nreturn (\\n  <>\\n    {error ? <div>{error.message}</div> : null}\\n  </>\\n)\\n```\\n\\n### Controlled input\\n\\nIn the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.\\n\\nThe following example demonstrates how to use more granular APIs like `setInput` with your custom input and submit button components:\\n\\n```tsx\\nconst { input, setInput } = useCompletion();\\n\\nreturn (\\n  <>\\n    <MyCustomInput value={input} onChange={value => setInput(value)} />\\n  </>\\n);\\n```\\n\\n### Cancelation\\n\\nIt\\'s also a common use case to abort the response message while it\\'s still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useCompletion` hook.\\n\\n```tsx\\nconst { stop, isLoading, ... } = useCompletion()\\n\\nreturn (\\n  <>\\n    <button onClick={stop} disabled={!isLoading}>Stop</button>\\n  </>\\n)\\n```\\n\\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.\\n\\n### Throttling UI Updates\\n\\n<Note>This feature is currently only available for React.</Note>\\n\\nBy default, the `useCompletion` hook will trigger a render every time a new chunk is received.\\nYou can throttle the UI updates with the `experimental_throttle` option.\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { completion, ... } = useCompletion({\\n  // Throttle the completion and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n\\n## Event Callbacks\\n\\n`useCompletion` also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\\n\\n```tsx\\nconst { ... } = useCompletion({\\n  onResponse: (response: Response) => {\\n    console.log(\\'Received response from server:\\', response)\\n  },\\n  onFinish: (prompt: string, completion: string) => {\\n    console.log(\\'Finished streaming completion:\\', completion)\\n  },\\n  onError: (error: Error) => {\\n    console.error(\\'An error occurred:\\', error)\\n  },\\n})\\n```\\n\\nIt\\'s worth noting that you can abort the processing by throwing an error in the `onResponse` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\\n\\n## Configure Request Options\\n\\nBy default, the `useCompletion` hook sends a HTTP POST request to the `/api/completion` endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the `useCompletion` hook:\\n\\n```tsx\\nconst { messages, input, handleInputChange, handleSubmit } = useCompletion({\\n  api: \\'/api/custom-completion\\',\\n  headers: {\\n    Authorization: \\'your_token\\',\\n  },\\n  body: {\\n    user_id: \\'123\\',\\n  },\\n  credentials: \\'same-origin\\',\\n});\\n```\\n\\nIn this example, the `useCompletion` hook sends a POST request to the `/api/completion` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/08-object-generation.mdx'), name='08-object-generation.mdx', displayName='08-object-generation.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Object Generation\\ndescription: Learn how to use the useObject hook.\\n---\\n\\n# Object Generation\\n\\n<Note>\\n  `useObject` is an experimental feature and only available in React, Svelte,\\n  and Vue.\\n</Note>\\n\\nThe [`useObject`](/docs/reference/ai-sdk-ui/use-object) hook allows you to create interfaces that represent a structured JSON object that is being streamed.\\n\\nIn this guide, you will learn how to use the `useObject` hook in your application to generate UIs for structured data on the fly.\\n\\n## Example\\n\\nThe example shows a small notifications demo app that generates fake notifications in real-time.\\n\\n### Schema\\n\\nIt is helpful to set up the schema in a separate file that is imported on both the client and server.\\n\\n```ts filename=\\'app/api/notifications/schema.ts\\'\\nimport { z } from \\'zod\\';\\n\\n// define a schema for the notifications\\nexport const notificationSchema = z.object({\\n  notifications: z.array(\\n    z.object({\\n      name: z.string().describe(\\'Name of a fictional person.\\'),\\n      message: z.string().describe(\\'Message. Do not use emojis or links.\\'),\\n    }),\\n  ),\\n});\\n```\\n\\n### Client\\n\\nThe client uses [`useObject`](/docs/reference/ai-sdk-ui/use-object) to stream the object generation process.\\n\\nThe results are partial and are displayed as they are received.\\nPlease note the code for handling `undefined` values in the JSX.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\nimport { notificationSchema } from \\'./api/notifications/schema\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n### Server\\n\\nOn the server, we use [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to stream the object generation process.\\n\\n```typescript filename=\\'app/api/notifications/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\nimport { notificationSchema } from \\'./schema\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const context = await req.json();\\n\\n  const result = streamObject({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    schema: notificationSchema,\\n    prompt:\\n      `Generate 3 notifications for a messages app in this context:` + context,\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## Enum Output Mode\\n\\nWhen you need to classify or categorize input into predefined options, you can use the `enum` output mode with `useObject`. This requires a specific schema structure where the object has `enum` as a key with `z.enum` containing your possible values.\\n\\n### Example: Text Classification\\n\\nThis example shows how to build a simple text classifier that categorizes statements as true or false.\\n\\n#### Client\\n\\nWhen using `useObject` with enum output mode, your schema must be an object with `enum` as the key:\\n\\n```tsx filename=\\'app/classify/page.tsx\\'\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\nimport { z } from \\'zod\\';\\n\\nexport default function ClassifyPage() {\\n  const { object, submit, isLoading } = useObject({\\n    api: \\'/api/classify\\',\\n    schema: z.object({ enum: z.enum([\\'true\\', \\'false\\']) }),\\n  });\\n\\n  return (\\n    <>\\n      <button onClick={() => submit(\\'The earth is flat\\')} disabled={isLoading}>\\n        Classify statement\\n      </button>\\n\\n      {object && <div>Classification: {object.enum}</div>}\\n    </>\\n  );\\n}\\n```\\n\\n#### Server\\n\\nOn the server, use `streamObject` with `output: \\'enum\\'` to stream the classification result:\\n\\n```typescript filename=\\'app/api/classify/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const context = await req.json();\\n\\n  const result = streamObject({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    output: \\'enum\\',\\n    enum: [\\'true\\', \\'false\\'],\\n    prompt: `Classify this statement as true or false: ${context}`,\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## Customized UI\\n\\n`useObject` also provides ways to show loading and error states:\\n\\n### Loading State\\n\\nThe `isLoading` state returned by the `useObject` hook can be used for several\\npurposes:\\n\\n- To show a loading spinner while the object is generated.\\n- To disable the submit button.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"6,13-20,24\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { isLoading, object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      {isLoading && <Spinner />}\\n\\n      <button\\n        onClick={() => submit(\\'Messages during finals week.\\')}\\n        disabled={isLoading}\\n      >\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n### Stop Handler\\n\\nThe `stop` function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"6,14-16\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { isLoading, stop, object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      {isLoading && (\\n        <button type=\"button\" onClick={() => stop()}>\\n          Stop\\n        </button>\\n      )}\\n\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n### Error State\\n\\nSimilarly, the `error` state reflects the error object thrown during the fetch request.\\nIt can be used to display an error message, or to disable the submit button:\\n\\n<Note>\\n  We recommend showing a generic error message to the user, such as \"Something\\n  went wrong.\" This is a good practice to avoid leaking information from the\\n  server.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"6,13\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { error, object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      {error && <div>An error occurred.</div>}\\n\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n## Event Callbacks\\n\\n`useObject` provides optional event callbacks that you can use to handle life-cycle events.\\n\\n- `onFinish`: Called when the object generation is completed.\\n- `onError`: Called when an error occurs during the fetch request.\\n\\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"10-20\"\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\nimport { notificationSchema } from \\'./api/notifications/schema\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n    onFinish({ object, error }) {\\n      // typed object, undefined if schema validation fails:\\n      console.log(\\'Object generation completed:\\', object);\\n\\n      // error, undefined if schema validation succeeds:\\n      console.log(\\'Schema validation error:\\', error);\\n    },\\n    onError(error) {\\n      // error during fetch request:\\n      console.error(\\'An error occurred:\\', error);\\n    },\\n  });\\n\\n  return (\\n    <div>\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n## Configure Request Options\\n\\nYou can configure the API endpoint, optional headers and credentials using the `api`, `headers` and `credentials` settings.\\n\\n```tsx highlight=\"2-5\"\\nconst { submit, object } = useObject({\\n  api: \\'/api/use-object\\',\\n  headers: {\\n    \\'X-Custom-Header\\': \\'CustomValue\\',\\n  },\\n  credentials: \\'include\\',\\n  schema: yourSchema,\\n});\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/20-streaming-data.mdx'), name='20-streaming-data.mdx', displayName='20-streaming-data.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming Custom Data\\ndescription: Learn how to stream custom data from the server to the client.\\n---\\n\\n# Streaming Custom Data\\n\\nIt is often useful to send additional data alongside the model\\'s response.\\nFor example, you may want to send status information, the message ids after storing them,\\nor references to content that the language model is referring to.\\n\\nThe AI SDK provides several helpers that allows you to stream additional data to the client\\nand attach it to the `UIMessage` parts array:\\n\\n- `createUIMessageStream`: creates a data stream\\n- `createUIMessageStreamResponse`: creates a response object that streams data\\n- `pipeUIMessageStreamToResponse`: pipes a data stream to a server response object\\n\\nThe data is streamed as part of the response stream using Server-Sent Events.\\n\\n## Setting Up Type-Safe Data Streaming\\n\\nFirst, define your custom message type with data part schemas for type safety:\\n\\n```tsx filename=\"ai/types.ts\"\\nimport { UIMessage } from \\'ai\\';\\n\\n// Define your custom message type with data part schemas\\nexport type MyUIMessage = UIMessage<\\n  never, // metadata type\\n  {\\n    weather: {\\n      city: string;\\n      weather?: string;\\n      status: \\'loading\\' | \\'success\\';\\n    };\\n    notification: {\\n      message: string;\\n      level: \\'info\\' | \\'warning\\' | \\'error\\';\\n    };\\n  } // data parts type\\n>;\\n```\\n\\n## Streaming Data from the Server\\n\\nIn your server-side route handler, you can create a `UIMessageStream` and then pass it to `createUIMessageStreamResponse`:\\n\\n```tsx filename=\"route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport {\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n  streamText,\\n  convertToModelMessages,\\n} from \\'ai\\';\\nimport type { MyUIMessage } from \\'@/ai/types\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const stream = createUIMessageStream<MyUIMessage>({\\n    execute: ({ writer }) => {\\n      // 1. Send initial status (transient - won\\'t be added to message history)\\n      writer.write({\\n        type: \\'data-notification\\',\\n        data: { message: \\'Processing your request...\\', level: \\'info\\' },\\n        transient: true, // This part won\\'t be added to message history\\n      });\\n\\n      // 2. Send sources (useful for RAG use cases)\\n      writer.write({\\n        type: \\'source\\',\\n        value: {\\n          type: \\'source\\',\\n          sourceType: \\'url\\',\\n          id: \\'source-1\\',\\n          url: \\'https://weather.com\\',\\n          title: \\'Weather Data Source\\',\\n        },\\n      });\\n\\n      // 3. Send data parts with loading state\\n      writer.write({\\n        type: \\'data-weather\\',\\n        id: \\'weather-1\\',\\n        data: { city: \\'San Francisco\\', status: \\'loading\\' },\\n      });\\n\\n      const result = streamText({\\n        model: \\'anthropic/claude-sonnet-4.5\\',\\n        messages: convertToModelMessages(messages),\\n        onFinish() {\\n          // 4. Update the same data part (reconciliation)\\n          writer.write({\\n            type: \\'data-weather\\',\\n            id: \\'weather-1\\', // Same ID = update existing part\\n            data: {\\n              city: \\'San Francisco\\',\\n              weather: \\'sunny\\',\\n              status: \\'success\\',\\n            },\\n          });\\n\\n          // 5. Send completion notification (transient)\\n          writer.write({\\n            type: \\'data-notification\\',\\n            data: { message: \\'Request completed\\', level: \\'info\\' },\\n            transient: true, // Won\\'t be added to message history\\n          });\\n        },\\n      });\\n\\n      writer.merge(result.toUIMessageStream());\\n    },\\n  });\\n\\n  return createUIMessageStreamResponse({ stream });\\n}\\n```\\n\\n<Note>\\n  You can also send stream data from custom backends, e.g. Python / FastAPI,\\n  using the [UI Message Stream\\n  Protocol](/docs/ai-sdk-ui/stream-protocol#ui-message-stream-protocol).\\n</Note>\\n\\n## Types of Streamable Data\\n\\n### Data Parts (Persistent)\\n\\nRegular data parts are added to the message history and appear in `message.parts`:\\n\\n```tsx\\nwriter.write({\\n  type: \\'data-weather\\',\\n  id: \\'weather-1\\', // Optional: enables reconciliation\\n  data: { city: \\'San Francisco\\', status: \\'loading\\' },\\n});\\n```\\n\\n### Sources\\n\\nSources are useful for RAG implementations where you want to show which documents or URLs were referenced:\\n\\n```tsx\\nwriter.write({\\n  type: \\'source\\',\\n  value: {\\n    type: \\'source\\',\\n    sourceType: \\'url\\',\\n    id: \\'source-1\\',\\n    url: \\'https://example.com\\',\\n    title: \\'Example Source\\',\\n  },\\n});\\n```\\n\\n### Transient Data Parts (Ephemeral)\\n\\nTransient parts are sent to the client but not added to the message history. They are only accessible via the `onData` useChat handler:\\n\\n```tsx\\n// server\\nwriter.write({\\n  type: \\'data-notification\\',\\n  data: { message: \\'Processing...\\', level: \\'info\\' },\\n  transient: true, // Won\\'t be added to message history\\n});\\n\\n// client\\nconst [notification, setNotification] = useState();\\n\\nconst { messages } = useChat({\\n  onData: ({ data, type }) => {\\n    if (type === \\'data-notification\\') {\\n      setNotification({ message: data.message, level: data.level });\\n    }\\n  },\\n});\\n```\\n\\n## Data Part Reconciliation\\n\\nWhen you write to a data part with the same ID, the client automatically reconciles and updates that part. This enables powerful dynamic experiences like:\\n\\n- **Collaborative artifacts** - Update code, documents, or designs in real-time\\n- **Progressive data loading** - Show loading states that transform into final results\\n- **Live status updates** - Update progress bars, counters, or status indicators\\n- **Interactive components** - Build UI elements that evolve based on user interaction\\n\\nThe reconciliation happens automatically - simply use the same `id` when writing to the stream.\\n\\n## Processing Data on the Client\\n\\n### Using the onData Callback\\n\\nThe `onData` callback is essential for handling streaming data, especially transient parts:\\n\\n```tsx filename=\"page.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyUIMessage } from \\'@/ai/types\\';\\n\\nconst { messages } = useChat<MyUIMessage>({\\n  api: \\'/api/chat\\',\\n  onData: dataPart => {\\n    // Handle all data parts as they arrive (including transient parts)\\n    console.log(\\'Received data part:\\', dataPart);\\n\\n    // Handle different data part types\\n    if (dataPart.type === \\'data-weather\\') {\\n      console.log(\\'Weather update:\\', dataPart.data);\\n    }\\n\\n    // Handle transient notifications (ONLY available here, not in message.parts)\\n    if (dataPart.type === \\'data-notification\\') {\\n      showToast(dataPart.data.message, dataPart.data.level);\\n    }\\n  },\\n});\\n```\\n\\n**Important:** Transient data parts are **only** available through the `onData` callback. They will not appear in the `message.parts` array since they\\'re not added to message history.\\n\\n### Rendering Persistent Data Parts\\n\\nYou can filter and render data parts from the message parts array:\\n\\n```tsx filename=\"page.tsx\"\\nconst result = (\\n  <>\\n    {messages?.map(message => (\\n      <div key={message.id}>\\n        {/* Render weather data parts */}\\n        {message.parts\\n          .filter(part => part.type === \\'data-weather\\')\\n          .map((part, index) => (\\n            <div key={index} className=\"weather-widget\">\\n              {part.data.status === \\'loading\\' ? (\\n                <>Getting weather for {part.data.city}...</>\\n              ) : (\\n                <>\\n                  Weather in {part.data.city}: {part.data.weather}\\n                </>\\n              )}\\n            </div>\\n          ))}\\n\\n        {/* Render text content */}\\n        {message.parts\\n          .filter(part => part.type === \\'text\\')\\n          .map((part, index) => (\\n            <div key={index}>{part.text}</div>\\n          ))}\\n\\n        {/* Render sources */}\\n        {message.parts\\n          .filter(part => part.type === \\'source\\')\\n          .map((part, index) => (\\n            <div key={index} className=\"source\">\\n              Source: <a href={part.url}>{part.title}</a>\\n            </div>\\n          ))}\\n      </div>\\n    ))}\\n  </>\\n);\\n```\\n\\n### Complete Example\\n\\n```tsx filename=\"page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport type { MyUIMessage } from \\'@/ai/types\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n\\n  const { messages, sendMessage } = useChat<MyUIMessage>({\\n    api: \\'/api/chat\\',\\n    onData: dataPart => {\\n      // Handle transient notifications\\n      if (dataPart.type === \\'data-notification\\') {\\n        console.log(\\'Notification:\\', dataPart.data.message);\\n      }\\n    },\\n  });\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <>\\n      {messages?.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n\\n          {/* Render weather data */}\\n          {message.parts\\n            .filter(part => part.type === \\'data-weather\\')\\n            .map((part, index) => (\\n              <span key={index} className=\"weather-update\">\\n                {part.data.status === \\'loading\\' ? (\\n                  <>Getting weather for {part.data.city}...</>\\n                ) : (\\n                  <>\\n                    Weather in {part.data.city}: {part.data.weather}\\n                  </>\\n                )}\\n              </span>\\n            ))}\\n\\n          {/* Render text content */}\\n          {message.parts\\n            .filter(part => part.type === \\'text\\')\\n            .map((part, index) => (\\n              <div key={index}>{part.text}</div>\\n            ))}\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Ask about the weather...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n## Use Cases\\n\\n- **RAG Applications** - Stream sources and retrieved documents\\n- **Real-time Status** - Show loading states and progress updates\\n- **Collaborative Tools** - Stream live updates to shared artifacts\\n- **Analytics** - Send usage data without cluttering message history\\n- **Notifications** - Display temporary alerts and status messages\\n\\n## Message Metadata vs Data Parts\\n\\nBoth [message metadata](/docs/ai-sdk-ui/message-metadata) and data parts allow you to send additional information alongside messages, but they serve different purposes:\\n\\n### Message Metadata\\n\\nMessage metadata is best for **message-level information** that describes the message as a whole:\\n\\n- Attached at the message level via `message.metadata`\\n- Sent using the `messageMetadata` callback in `toUIMessageStreamResponse`\\n- Ideal for: timestamps, model info, token usage, user context\\n- Type-safe with custom metadata types\\n\\n```ts\\n// Server: Send metadata about the message\\nreturn result.toUIMessageStreamResponse({\\n  messageMetadata: ({ part }) => {\\n    if (part.type === \\'finish\\') {\\n      return {\\n        model: part.response.modelId,\\n        totalTokens: part.totalUsage.totalTokens,\\n        createdAt: Date.now(),\\n      };\\n    }\\n  },\\n});\\n```\\n\\n### Data Parts\\n\\nData parts are best for streaming **dynamic arbitrary data**:\\n\\n- Added to the message parts array via `message.parts`\\n- Streamed using `createUIMessageStream` and `writer.write()`\\n- Can be reconciled/updated using the same ID\\n- Support transient parts that don\\'t persist\\n- Ideal for: dynamic content, loading states, interactive components\\n\\n```ts\\n// Server: Stream data as part of message content\\nwriter.write({\\n  type: \\'data-weather\\',\\n  id: \\'weather-1\\',\\n  data: { city: \\'San Francisco\\', status: \\'loading\\' },\\n});\\n```\\n\\nFor more details on message metadata, see the [Message Metadata documentation](/docs/ai-sdk-ui/message-metadata).\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/21-error-handling.mdx'), name='21-error-handling.mdx', displayName='21-error-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Error Handling\\ndescription: Learn how to handle errors in the AI SDK UI\\n---\\n\\n# Error Handling and warnings\\n\\n## Warnings\\n\\nThe AI SDK shows warnings when something might not work as expected.\\nThese warnings help you fix problems before they cause errors.\\n\\n### When Warnings Appear\\n\\nWarnings are shown in the browser console when:\\n\\n- **Unsupported features**: You use a feature or setting that is not supported by the AI model (e.g., certain options or parameters).\\n- **Compatibility warnings**: A feature is used in a compatibility mode, which might work differently or less optimally than intended.\\n- **Other warnings**: The AI model reports another type of issue, such as general problems or advisory messages.\\n\\n### Warning Messages\\n\\nAll warnings start with \"AI SDK Warning:\" so you can easily find them. For example:\\n\\n```\\nAI SDK Warning: The feature \"temperature\" is not supported by this model\\n```\\n\\n### Turning Off Warnings\\n\\nBy default, warnings are shown in the console. You can control this behavior:\\n\\n#### Turn Off All Warnings\\n\\nSet a global variable to turn off warnings completely:\\n\\n```ts\\nglobalThis.AI_SDK_LOG_WARNINGS = false;\\n```\\n\\n#### Custom Warning Handler\\n\\nYou can also provide your own function to handle warnings.\\nIt receives provider id, model id, and a list of warnings.\\n\\n```ts\\nglobalThis.AI_SDK_LOG_WARNINGS = ({ warnings, provider, model }) => {\\n  // Handle warnings your own way\\n};\\n```\\n\\n## Error Handling\\n\\n### Error Helper Object\\n\\nEach AI SDK UI hook also returns an [error](/docs/reference/ai-sdk-ui/use-chat#error) object that you can use to render the error in your UI.\\nYou can use the error object to show an error message, disable the submit button, or show a retry button.\\n\\n<Note>\\n  We recommend showing a generic error message to the user, such as \"Something\\n  went wrong.\" This is a good practice to avoid leaking information from the\\n  server.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"7,18-25,31\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage, error, regenerate } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts\\n            .filter(part => part.type === \\'text\\')\\n            .map(part => part.text)\\n            .join(\\'\\')}\\n        </div>\\n      ))}\\n\\n      {error && (\\n        <>\\n          <div>An error occurred.</div>\\n          <button type=\"button\" onClick={() => regenerate()}>\\n            Retry\\n          </button>\\n        </>\\n      )}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={error != null}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n#### Alternative: replace last message\\n\\nAlternatively you can write a custom submit handler that replaces the last message when an error is present.\\n\\n```tsx file=\"app/page.tsx\" highlight=\"17-23,35\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { sendMessage, error, messages, setMessages } = useChat();\\n\\n  function customSubmit(event: React.FormEvent<HTMLFormElement>) {\\n    event.preventDefault();\\n\\n    if (error != null) {\\n      setMessages(messages.slice(0, -1)); // remove last message\\n    }\\n\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  }\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts\\n            .filter(part => part.type === \\'text\\')\\n            .map(part => part.text)\\n            .join(\\'\\')}\\n        </div>\\n      ))}\\n\\n      {error && <div>An error occurred.</div>}\\n\\n      <form onSubmit={customSubmit}>\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Error Handling Callback\\n\\nErrors can be processed by passing an [`onError`](/docs/reference/ai-sdk-ui/use-chat#on-error) callback function as an option to the [`useChat`](/docs/reference/ai-sdk-ui/use-chat) or [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) hooks.\\nThe callback function receives an error object as an argument.\\n\\n```tsx file=\"app/page.tsx\" highlight=\"6-9\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const {\\n    /* ... */\\n  } = useChat({\\n    // handle error:\\n    onError: error => {\\n      console.error(error);\\n    },\\n  });\\n}\\n```\\n\\n### Injecting Errors for Testing\\n\\nYou might want to create errors for testing.\\nYou can easily do so by throwing an error in your route handler:\\n\\n```ts file=\"app/api/chat/route.ts\"\\nexport async function POST(req: Request) {\\n  throw new Error(\\'This is a test error\\');\\n}\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/21-transport.mdx'), name='21-transport.mdx', displayName='21-transport.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Transport\\ndescription: Learn how to use custom transports with useChat.\\n---\\n\\n# Transport\\n\\nThe `useChat` transport system provides fine-grained control over how messages are sent to your API endpoints and how responses are processed. This is particularly useful for alternative communication protocols like WebSockets, custom authentication patterns, or specialized backend integrations.\\n\\n## Default Transport\\n\\nBy default, `useChat` uses HTTP POST requests to send messages to `/api/chat`:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\n\\n// Uses default HTTP transport\\nconst { messages, sendMessage } = useChat();\\n```\\n\\nThis is equivalent to:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n  }),\\n});\\n```\\n\\n## Custom Transport Configuration\\n\\nConfigure the default transport with custom options:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/custom-chat',\\n    headers: {\\n      Authorization: 'Bearer your-token',\\n      'X-API-Version': '2024-01',\\n    },\\n    credentials: 'include',\\n  }),\\n});\\n```\\n\\n### Dynamic Configuration\\n\\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: () => ({\\n      Authorization: `Bearer ${getAuthToken()}`,\\n      'X-User-ID': getCurrentUserId(),\\n    }),\\n    body: () => ({\\n      sessionId: getCurrentSessionId(),\\n      preferences: getUserPreferences(),\\n    }),\\n    credentials: () => 'include',\\n  }),\\n});\\n```\\n\\n### Request Transformation\\n\\nTransform requests before sending to your API:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\\n      return {\\n        headers: {\\n          'X-Session-ID': id,\\n        },\\n        body: {\\n          messages: messages.slice(-10), // Only send last 10 messages\\n          trigger,\\n          messageId,\\n        },\\n      };\\n    },\\n  }),\\n});\\n```\\n\\n## Building Custom Transports\\n\\nTo understand how to build your own transport, refer to the source code of the default implementation:\\n\\n- **[DefaultChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/default-chat-transport.ts)** - The complete default HTTP transport implementation\\n- **[HttpChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/http-chat-transport.ts)** - Base HTTP transport with request handling\\n- **[ChatTransport Interface](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/chat-transport.ts)** - The transport interface you need to implement\\n\\nThese implementations show you exactly how to:\\n\\n- Handle the `sendMessages` method\\n- Process UI message streams\\n- Transform requests and responses\\n- Handle errors and connection management\\n\\nThe transport system gives you complete control over how your chat application communicates, enabling integration with any backend protocol or service.\\n\", children=[]), DocItem(origPath=Path('04-ai-sdk-ui/24-reading-ui-message-streams.mdx'), name='24-reading-ui-message-streams.mdx', displayName='24-reading-ui-message-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Reading UIMessage Streams\\ndescription: Learn how to read UIMessage streams.\\n---\\n\\n# Reading UI Message Streams\\n\\n`UIMessage` streams are useful outside of traditional chat use cases. You can consume them for terminal UIs, custom stream processing on the client, or React Server Components (RSC).\\n\\nThe `readUIMessageStream` helper transforms a stream of `UIMessageChunk` objects into an `AsyncIterableStream` of `UIMessage` objects, allowing you to process messages as they're being constructed.\\n\\n## Basic Usage\\n\\n```tsx\\nimport { openai } from '@ai-sdk/openai';\\nimport { readUIMessageStream, streamText } from 'ai';\\n\\nasync function main() {\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    prompt: 'Write a short story about a robot.',\\n  });\\n\\n  for await (const uiMessage of readUIMessageStream({\\n    stream: result.toUIMessageStream(),\\n  })) {\\n    console.log('Current message state:', uiMessage);\\n  }\\n}\\n```\\n\\n## Tool Calls Integration\\n\\nHandle streaming responses that include tool calls:\\n\\n```tsx\\nimport { openai } from '@ai-sdk/openai';\\nimport { readUIMessageStream, streamText, tool } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function handleToolCalls() {\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    tools: {\\n      weather: tool({\\n        description: 'Get the weather in a location',\\n        inputSchema: z.object({\\n          location: z.string().describe('The location to get the weather for'),\\n        }),\\n        execute: ({ location }) => ({\\n          location,\\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n        }),\\n      }),\\n    },\\n    prompt: 'What is the weather in Tokyo?',\\n  });\\n\\n  for await (const uiMessage of readUIMessageStream({\\n    stream: result.toUIMessageStream(),\\n  })) {\\n    // Handle different part types\\n    uiMessage.parts.forEach(part => {\\n      switch (part.type) {\\n        case 'text':\\n          console.log('Text:', part.text);\\n          break;\\n        case 'tool-call':\\n          console.log('Tool called:', part.toolName, 'with args:', part.args);\\n          break;\\n        case 'tool-result':\\n          console.log('Tool result:', part.result);\\n          break;\\n      }\\n    });\\n  }\\n}\\n```\\n\\n## Resuming Conversations\\n\\nResume streaming from a previous message state:\\n\\n```tsx\\nimport { readUIMessageStream, streamText } from 'ai';\\n\\nasync function resumeConversation(lastMessage: UIMessage) {\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages: [\\n      { role: 'user', content: 'Continue our previous conversation.' },\\n    ],\\n  });\\n\\n  // Resume from the last message\\n  for await (const uiMessage of readUIMessageStream({\\n    stream: result.toUIMessageStream(),\\n    message: lastMessage, // Resume from this message\\n  })) {\\n    console.log('Resumed message:', uiMessage);\\n  }\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('04-ai-sdk-ui/25-message-metadata.mdx'), name='25-message-metadata.mdx', displayName='25-message-metadata.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Message Metadata\\ndescription: Learn how to attach and use metadata with messages in AI SDK UI\\n---\\n\\n# Message Metadata\\n\\nMessage metadata allows you to attach custom information to messages at the message level. This is useful for tracking timestamps, model information, token usage, user context, and other message-level data.\\n\\n## Overview\\n\\nMessage metadata differs from [data parts](/docs/ai-sdk-ui/streaming-data) in that it\\'s attached at the message level rather than being part of the message content. While data parts are ideal for dynamic content that forms part of the message, metadata is perfect for information about the message itself.\\n\\n## Getting Started\\n\\nHere\\'s a simple example of using message metadata to track timestamps and model information:\\n\\n### Defining Metadata Types\\n\\nFirst, define your metadata type for type safety:\\n\\n```tsx filename=\"app/types.ts\"\\nimport { UIMessage } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Define your metadata schema\\nexport const messageMetadataSchema = z.object({\\n  createdAt: z.number().optional(),\\n  model: z.string().optional(),\\n  totalTokens: z.number().optional(),\\n});\\n\\nexport type MessageMetadata = z.infer<typeof messageMetadataSchema>;\\n\\n// Create a typed UIMessage\\nexport type MyUIMessage = UIMessage<MessageMetadata>;\\n```\\n\\n### Sending Metadata from the Server\\n\\nUse the `messageMetadata` callback in `toUIMessageStreamResponse` to send metadata at different streaming stages:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"11-20\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\nimport type { MyUIMessage } from \\'@/types\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages, // pass this in for type-safe return objects\\n    messageMetadata: ({ part }) => {\\n      // Send metadata when streaming starts\\n      if (part.type === \\'start\\') {\\n        return {\\n          createdAt: Date.now(),\\n          model: \\'gpt-5.1\\',\\n        };\\n      }\\n\\n      // Send additional metadata when streaming completes\\n      if (part.type === \\'finish\\') {\\n        return {\\n          totalTokens: part.totalUsage.totalTokens,\\n        };\\n      }\\n    },\\n  });\\n}\\n```\\n\\n<Note>\\n  To enable type-safe metadata return object in `messageMetadata`, pass in the\\n  `originalMessages` parameter typed to your UIMessage type.\\n</Note>\\n\\n### Accessing Metadata on the Client\\n\\nAccess metadata through the `message.metadata` property:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"8,18-23\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport type { MyUIMessage } from \\'@/types\\';\\n\\nexport default function Chat() {\\n  const { messages } = useChat<MyUIMessage>({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>\\n            {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n            {message.metadata?.createdAt && (\\n              <span className=\"text-sm text-gray-500\">\\n                {new Date(message.metadata.createdAt).toLocaleTimeString()}\\n              </span>\\n            )}\\n          </div>\\n\\n          {/* Render message content */}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <div key={index}>{part.text}</div> : null,\\n          )}\\n\\n          {/* Display additional metadata */}\\n          {message.metadata?.totalTokens && (\\n            <div className=\"text-xs text-gray-400\">\\n              {message.metadata.totalTokens} tokens\\n            </div>\\n          )}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n<Note>\\n  For streaming arbitrary data that changes during generation, consider using\\n  [data parts](/docs/ai-sdk-ui/streaming-data) instead.\\n</Note>\\n\\n## Common Use Cases\\n\\nMessage metadata is ideal for:\\n\\n- **Timestamps**: When messages were created or completed\\n- **Model Information**: Which AI model was used\\n- **Token Usage**: Track costs and usage limits\\n- **User Context**: User IDs, session information\\n- **Performance Metrics**: Generation time, time to first token\\n- **Quality Indicators**: Finish reason, confidence scores\\n\\n## See Also\\n\\n- [Chatbot Guide](/docs/ai-sdk-ui/chatbot#message-metadata) - Message metadata in the context of building chatbots\\n- [Streaming Data](/docs/ai-sdk-ui/streaming-data#message-metadata-vs-data-parts) - Comparison with data parts\\n- [UIMessage Reference](/docs/reference/ai-sdk-core/ui-message) - Complete UIMessage type reference\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/50-stream-protocol.mdx'), name='50-stream-protocol.mdx', displayName='50-stream-protocol.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stream Protocols\\ndescription: Learn more about the supported stream protocols in the AI SDK.\\n---\\n\\n# Stream Protocols\\n\\nAI SDK UI functions such as `useChat` and `useCompletion` support both text streams and data streams.\\nThe stream protocol defines how the data is streamed to the frontend on top of the HTTP protocol.\\n\\nThis page describes both protocols and how to use them in the backend and frontend.\\n\\nYou can use this information to develop custom backends and frontends for your use case, e.g.,\\nto provide compatible API endpoints that are implemented in a different language such as Python.\\n\\nFor instance, here\\'s an example using [FastAPI](https://github.com/vercel/ai/tree/main/examples/next-fastapi) as a backend.\\n\\n## Text Stream Protocol\\n\\nA text stream contains chunks in plain text, that are streamed to the frontend.\\nEach chunk is then appended together to form a full text response.\\n\\nText streams are supported by `useChat`, `useCompletion`, and `useObject`.\\nWhen you use `useChat` or `useCompletion`, you need to enable text streaming\\nby setting the `streamProtocol` options to `text`.\\n\\nYou can generate text streams with `streamText` in the backend.\\nWhen you call `toTextStreamResponse()` on the result object,\\na streaming HTTP response is returned.\\n\\n<Note>\\n  Text streams only support basic text data. If you need to stream other types\\n  of data such as tool calls, use data streams.\\n</Note>\\n\\n### Text Stream Example\\n\\nHere is a Next.js example that uses the text stream protocol:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { TextStreamChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat({\\n    transport: new TextStreamChatTransport({ api: \\'/api/chat\\' }),\\n  });\\n\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/chat/route.ts\\'\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## Data Stream Protocol\\n\\nA data stream follows a special protocol that the AI SDK provides to send information to the frontend.\\n\\nThe data stream protocol uses Server-Sent Events (SSE) format for improved standardization, keep-alive through ping, reconnect capabilities, and better cache handling.\\n\\n<Note>\\n  When you provide data streams from a custom backend, you need to set the\\n  `x-vercel-ai-ui-message-stream` header to `v1`.\\n</Note>\\n\\nThe following stream parts are currently supported:\\n\\n### Message Start Part\\n\\nIndicates the beginning of a new message with metadata.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"start\",\"messageId\":\"...\"}\\n\\n```\\n\\n### Text Parts\\n\\nText content is streamed using a start/delta/end pattern with unique IDs for each text block.\\n\\n#### Text Start Part\\n\\nIndicates the beginning of a text block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"text-start\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\\n\\n```\\n\\n#### Text Delta Part\\n\\nContains incremental text content for the text block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"text-delta\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\",\"delta\":\"Hello\"}\\n\\n```\\n\\n#### Text End Part\\n\\nIndicates the completion of a text block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"text-end\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\\n\\n```\\n\\n### Reasoning Parts\\n\\nReasoning content is streamed using a start/delta/end pattern with unique IDs for each reasoning block.\\n\\n#### Reasoning Start Part\\n\\nIndicates the beginning of a reasoning block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"reasoning-start\",\"id\":\"reasoning_123\"}\\n\\n```\\n\\n#### Reasoning Delta Part\\n\\nContains incremental reasoning content for the reasoning block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"reasoning-delta\",\"id\":\"reasoning_123\",\"delta\":\"This is some reasoning\"}\\n\\n```\\n\\n#### Reasoning End Part\\n\\nIndicates the completion of a reasoning block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"reasoning-end\",\"id\":\"reasoning_123\"}\\n\\n```\\n\\n### Source Parts\\n\\nSource parts provide references to external content sources.\\n\\n#### Source URL Part\\n\\nReferences to external URLs.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"source-url\",\"sourceId\":\"https://example.com\",\"url\":\"https://example.com\"}\\n\\n```\\n\\n#### Source Document Part\\n\\nReferences to documents or files.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"source-document\",\"sourceId\":\"https://example.com\",\"mediaType\":\"file\",\"title\":\"Title\"}\\n\\n```\\n\\n### File Part\\n\\nThe file parts contain references to files with their media type.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"file\",\"url\":\"https://example.com/file.png\",\"mediaType\":\"image/png\"}\\n\\n```\\n\\n### Data Parts\\n\\nCustom data parts allow streaming of arbitrary structured data with type-specific handling.\\n\\nFormat: Server-Sent Event with JSON object where the type includes a custom suffix\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"data-weather\",\"data\":{\"location\":\"SF\",\"temperature\":100}}\\n\\n```\\n\\nThe `data-*` type pattern allows you to define custom data types that your frontend can handle specifically.\\n\\n### Error Part\\n\\nThe error parts are appended to the message as they are received.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"error\",\"errorText\":\"error message\"}\\n\\n```\\n\\n### Tool Input Start Part\\n\\nIndicates the beginning of tool input streaming.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-input-start\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\"}\\n\\n```\\n\\n### Tool Input Delta Part\\n\\nIncremental chunks of tool input as it\\'s being generated.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-input-delta\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"inputTextDelta\":\"San Francisco\"}\\n\\n```\\n\\n### Tool Input Available Part\\n\\nIndicates that tool input is complete and ready for execution.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-input-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\",\"input\":{\"city\":\"San Francisco\"}}\\n\\n```\\n\\n### Tool Output Available Part\\n\\nContains the result of tool execution.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-output-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"output\":{\"city\":\"San Francisco\",\"weather\":\"sunny\"}}\\n\\n```\\n\\n### Start Step Part\\n\\nA part indicating the start of a step.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"start-step\"}\\n\\n```\\n\\n### Finish Step Part\\n\\nA part indicating that a step (i.e., one LLM API call in the backend) has been completed.\\n\\nThis part is necessary to correctly process multiple stitched assistant calls, e.g. when calling tools in the backend, and using steps in `useChat` at the same time.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"finish-step\"}\\n\\n```\\n\\n### Finish Message Part\\n\\nA part indicating the completion of a message.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"finish\"}\\n\\n```\\n\\n### Stream Termination\\n\\nThe stream ends with a special `[DONE]` marker.\\n\\nFormat: Server-Sent Event with literal `[DONE]`\\n\\nExample:\\n\\n```\\ndata: [DONE]\\n\\n```\\n\\nThe data stream protocol is supported\\nby `useChat` and `useCompletion` on the frontend and used by default.\\n`useCompletion` only supports the `text` and `data` stream parts.\\n\\nOn the backend, you can use `toUIMessageStreamResponse()` from the `streamText` result object to return a streaming HTTP response.\\n\\n### UI Message Stream Example\\n\\nHere is a Next.js example that uses the UI message stream protocol:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/chat/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK UI\\ndescription: Learn about the AI SDK UI.\\n---\\n\\n# AI SDK UI\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description: 'Get an overview about the AI SDK UI.',\\n      href: '/docs/ai-sdk-ui/overview'\\n    },\\n    {\\n      title: 'Chatbot',\\n      description: 'Learn how to integrate an interface for a chatbot.',\\n      href: '/docs/ai-sdk-ui/chatbot'\\n    },\\n    {\\n      title: 'Chatbot Message Persistence',\\n      description: 'Learn how to store and load chat messages in a chatbot.',\\n      href: '/docs/ai-sdk-ui/chatbot-message-persistence'\\n    },\\n    {\\n      title: 'Chatbot Tool Usage',\\n      description:\\n        'Learn how to integrate an interface for a chatbot with tool calling.',\\n      href: '/docs/ai-sdk-ui/chatbot-tool-usage'\\n    },\\n    {\\n      title: 'Completion',\\n      description: 'Learn how to integrate an interface for text completion.',\\n      href: '/docs/ai-sdk-ui/completion'\\n    },\\n    {\\n      title: 'Object Generation',\\n      description: 'Learn how to integrate an interface for object generation.',\\n      href: '/docs/ai-sdk-ui/object-generation'\\n    },\\n    {\\n      title: 'Streaming Data',\\n      description: 'Learn how to stream data.',\\n      href: '/docs/ai-sdk-ui/streaming-data'\\n    },\\n    {\\n      title: 'Reading UI Message Streams',\\n      description: 'Learn how to read UIMessage streams for terminal UIs, custom clients, and server components.',\\n      href: '/docs/ai-sdk-ui/reading-ui-message-streams'\\n    },\\n    {\\n      title: 'Error Handling',\\n      description: 'Learn how to handle errors.',\\n      href: '/docs/ai-sdk-ui/error-handling'\\n    },\\n    {\\n      title: 'Stream Protocol',\\n      description:\\n        'The stream protocol defines how data is sent from the backend to the AI SDK UI frontend.',\\n      href: '/docs/ai-sdk-ui/stream-protocol'\\n    }\\n\\n]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('05-ai-sdk-rsc'), name='05-ai-sdk-rsc', displayName='05-ai-sdk-rsc', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('05-ai-sdk-rsc/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of AI SDK RSC.\\n---\\n\\n# AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n<Note>\\n  The `@ai-sdk/rsc` package is compatible with frameworks that support React\\n  Server Components.\\n</Note>\\n\\n[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components) (RSC) allow you to write UI that can be rendered on the server and streamed to the client. RSCs enable [ Server Actions ](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components), a new way to call server-side code directly from the client just like any other function with end-to-end type-safety. This combination opens the door to a new way of building AI applications, allowing the large language model (LLM) to generate and stream UI directly from the server to the client.\\n\\n## AI SDK RSC Functions\\n\\nAI SDK RSC has various functions designed to help you build AI-native applications with React Server Components. These functions:\\n\\n1. Provide abstractions for building Generative UI applications.\\n   - [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui): calls a model and allows it to respond with React Server Components.\\n   - [`useUIState`](/docs/reference/ai-sdk-rsc/use-ui-state): returns the current UI state and a function to update the UI State (like React\\'s `useState`). UI State is the visual representation of the AI state.\\n   - [`useAIState`](/docs/reference/ai-sdk-rsc/use-ai-state): returns the current AI state and a function to update the AI State (like React\\'s `useState`). The AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.\\n   - [`useActions`](/docs/reference/ai-sdk-rsc/use-actions): provides access to your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.\\n   - [`createAI`](/docs/reference/ai-sdk-rsc/create-ai): creates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.\\n2. Make it simple to work with streamable values between the server and client.\\n   - [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value): creates a stream that sends values from the server to the client. The value can be any serializable data.\\n   - [`readStreamableValue`](/docs/reference/ai-sdk-rsc/read-streamable-value): reads a streamable value from the client that was originally created using `createStreamableValue`.\\n   - [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui): creates a stream that sends UI from the server to the client.\\n   - [`useStreamableValue`](/docs/reference/ai-sdk-rsc/use-streamable-value): accepts a streamable value created using `createStreamableValue` and returns the current value, error, and pending state.\\n\\n## Templates\\n\\nCheck out the following templates to see AI SDK RSC in action.\\n\\n<Templates type=\"generative-ui\" />\\n\\n## API Reference\\n\\nPlease check out the [AI SDK RSC API Reference](/docs/reference/ai-sdk-rsc) for more details on each function.\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/02-streaming-react-components.mdx'), name='02-streaming-react-components.mdx', displayName='02-streaming-react-components.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming React Components\\ndescription: Overview of streaming RSCs\\n---\\n\\nimport { UIPreviewCard, Card } from \\'@/components/home/card\\';\\nimport { EventPlanning } from \\'@/components/home/event-planning\\';\\nimport { Searching } from \\'@/components/home/searching\\';\\nimport { Weather } from \\'@/components/home/weather\\';\\n\\n# Streaming React Components\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nThe RSC API allows you to stream React components from the server to the client with the [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function. This is useful when you want to go beyond raw text and stream components to the client in real-time.\\n\\nSimilar to [ AI SDK Core ](/docs/ai-sdk-core/overview) APIs (like [ `streamText` ](/docs/reference/ai-sdk-core/stream-text) and [ `streamObject` ](/docs/reference/ai-sdk-core/stream-object)), `streamUI` provides a single function to call a model and allow it to respond with React Server Components.\\nIt supports the same model interfaces as AI SDK Core APIs.\\n\\n### Concepts\\n\\nTo give the model the ability to respond to a user\\'s prompt with a React component, you can leverage [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n<Note>\\n  Remember, tools are like programs you can give to the model, and the model can\\n  decide as and when to use based on the context of the conversation.\\n</Note>\\n\\nWith the `streamUI` function, **you provide tools that return React components**. With the ability to stream components, the model is akin to a dynamic router that is able to understand the user\\'s intention and display relevant UI.\\n\\nAt a high level, the `streamUI` works like other AI SDK Core functions: you can provide the model with a prompt or some conversation history and, optionally, some tools. If the model decides, based on the context of the conversation, to call a tool, it will generate a tool call. The `streamUI` function will then run the respective tool, returning a React component. If the model doesn\\'t have a relevant tool to use, it will return a text generation, which will be passed to the `text` function, for you to handle (render and return as a React component).\\n\\n<Note>Remember, the `streamUI` function must return a React component. </Note>\\n\\n```tsx\\nconst result = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  prompt: \\'Get the weather for San Francisco\\',\\n  text: ({ content }) => <div>{content}</div>,\\n  tools: {},\\n});\\n```\\n\\nThis example calls the `streamUI` function using OpenAI\\'s `gpt-4o` model, passes a prompt, specifies how the model\\'s plain text response (`content`) should be rendered, and then provides an empty object for tools. Even though this example does not define any tools, it will stream the model\\'s response as a `div` rather than plain text.\\n\\n### Adding A Tool\\n\\nUsing tools with `streamUI` is similar to how you use tools with `generateText` and `streamText`.\\nA tool is an object that has:\\n\\n- `description`: a string telling the model what the tool does and when to use it\\n- `inputSchema`: a Zod schema describing what the tool needs in order to run\\n- `generate`: an asynchronous function that will be run if the model calls the tool. This must return a React component\\n\\nLet\\'s expand the previous example to add a tool.\\n\\n```tsx highlight=\"6-14\"\\nconst result = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  prompt: \\'Get the weather for San Francisco\\',\\n  text: ({ content }) => <div>{content}</div>,\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      inputSchema: z.object({ location: z.string() }),\\n      generate: async function* ({ location }) {\\n        yield <LoadingComponent />;\\n        const weather = await getWeather(location);\\n        return <WeatherComponent weather={weather} location={location} />;\\n      },\\n    },\\n  },\\n});\\n```\\n\\nThis tool would be run if the user asks for the weather for their location. If the user hasn\\'t specified a location, the model will ask for it before calling the tool. When the model calls the tool, the generate function will initially return a loading component. This component will show until the awaited call to `getWeather` is resolved, at which point, the model will stream the `<WeatherComponent />` to the user.\\n\\n<Note>\\n  Note: This example uses a [ generator function\\n  ](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*)\\n  (`function*`), which allows you to pause its execution and return a value,\\n  then resume from where it left off on the next call. This is useful for\\n  handling data streams, as you can fetch and return data from an asynchronous\\n  source like an API, then resume the function to fetch the next chunk when\\n  needed. By yielding values one at a time, generator functions enable efficient\\n  processing of streaming data without blocking the main thread.\\n</Note>\\n\\n## Using `streamUI` with Next.js\\n\\nLet\\'s see how you can use the example above in a Next.js application.\\n\\nTo use `streamUI` in a Next.js application, you will need two things:\\n\\n1. A Server Action (where you will call `streamUI`)\\n2. A page to call the Server Action and render the resulting components\\n\\n### Step 1: Create a Server Action\\n\\n<Note>\\n  Server Actions are server-side functions that you can call directly from the\\n  frontend. For more info, see [the\\n  documentation](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components).\\n</Note>\\n\\nCreate a Server Action at `app/actions.tsx` and add the following code:\\n\\n```tsx filename=\"app/actions.tsx\"\\n\\'use server\\';\\n\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst LoadingComponent = () => (\\n  <div className=\"animate-pulse p-4\">getting weather...</div>\\n);\\n\\nconst getWeather = async (location: string) => {\\n  await new Promise(resolve => setTimeout(resolve, 2000));\\n  return \\'82°F️ ☀️\\';\\n};\\n\\ninterface WeatherProps {\\n  location: string;\\n  weather: string;\\n}\\n\\nconst WeatherComponent = (props: WeatherProps) => (\\n  <div className=\"border border-neutral-200 p-4 rounded-lg max-w-fit\">\\n    The weather in {props.location} is {props.weather}\\n  </div>\\n);\\n\\nexport async function streamComponent() {\\n  const result = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    prompt: \\'Get the weather for San Francisco\\',\\n    text: ({ content }) => <div>{content}</div>,\\n    tools: {\\n      getWeather: {\\n        description: \\'Get the weather for a location\\',\\n        inputSchema: z.object({\\n          location: z.string(),\\n        }),\\n        generate: async function* ({ location }) {\\n          yield <LoadingComponent />;\\n          const weather = await getWeather(location);\\n          return <WeatherComponent weather={weather} location={location} />;\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.value;\\n}\\n```\\n\\nThe `getWeather` tool should look familiar as it is identical to the example in the previous section. In order for this tool to work:\\n\\n1. First define a `LoadingComponent`, which renders a pulsing `div` that will show some loading text.\\n2. Next, define a `getWeather` function that will timeout for 2 seconds (to simulate fetching the weather externally) before returning the \"weather\" for a `location`. Note: you could run any asynchronous TypeScript code here.\\n3. Finally, define a `WeatherComponent` which takes in `location` and `weather` as props, which are then rendered within a `div`.\\n\\nYour Server Action is an asynchronous function called `streamComponent` that takes no inputs, and returns a `ReactNode`. Within the action, you call the `streamUI` function, specifying the model (`gpt-4o`), the prompt, the component that should be rendered if the model chooses to return text, and finally, your `getWeather` tool. Last but not least, you return the resulting component generated by the model with `result.value`.\\n\\nTo call this Server Action and display the resulting React Component, you will need a page.\\n\\n### Step 2: Create a Page\\n\\nCreate or update your root page (`app/page.tsx`) with the following code:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { Button } from \\'@/components/ui/button\\';\\nimport { streamComponent } from \\'./actions\\';\\n\\nexport default function Page() {\\n  const [component, setComponent] = useState<React.ReactNode>();\\n\\n  return (\\n    <div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          setComponent(await streamComponent());\\n        }}\\n      >\\n        <Button>Stream Component</Button>\\n      </form>\\n      <div>{component}</div>\\n    </div>\\n  );\\n}\\n```\\n\\nThis page is first marked as a client component with the `\"use client\";` directive given it will be using hooks and interactivity. On the page, you render a form. When that form is submitted, you call the `streamComponent` action created in the previous step (just like any other function). The `streamComponent` action returns a `ReactNode` that you can then render on the page using React state (`setComponent`).\\n\\n## Going beyond a single prompt\\n\\nYou can now allow the model to respond to your prompt with a React component. However, this example is limited to a static prompt that is set within your Server Action. You could make this example interactive by turning it into a chatbot.\\n\\nLearn how to stream React components with the Next.js App Router using `streamUI` with this [example](/examples/next-app/interface/route-components).\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/03-generative-ui-state.mdx'), name='03-generative-ui-state.mdx', displayName='03-generative-ui-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Managing Generative UI State\\ndescription: Overview of the AI and UI states\\n---\\n\\n# Managing Generative UI State\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nState is an essential part of any application. State is particularly important in AI applications as it is passed to large language models (LLMs) on each request to ensure they have the necessary context to produce a great generation. Traditional chatbots are text-based and have a structure that mirrors that of any chat application.\\n\\nFor example, in a chatbot, state is an array of `messages` where each `message` has:\\n\\n- `id`: a unique identifier\\n- `role`: who sent the message (user/assistant/system/tool)\\n- `content`: the content of the message\\n\\nThis state can be rendered in the UI and sent to the model without any modifications.\\n\\nWith Generative UI, the model can now return a React component, rather than a plain text message. The client can render that component without issue, but that state can\\'t be sent back to the model because React components aren\\'t serialisable. So, what can you do?\\n\\n**The solution is to split the state in two, where one (AI State) becomes a proxy for the other (UI State)**.\\n\\nOne way to understand this concept is through a Lego analogy. Imagine a 10,000 piece Lego model that, once built, cannot be easily transported because it is fragile. By taking the model apart, it can be easily transported, and then rebuilt following the steps outlined in the instructions pamphlet. In this way, the instructions pamphlet is a proxy to the physical structure. Similarly, AI State provides a serialisable (JSON) representation of your UI that can be passed back and forth to the model.\\n\\n## What is AI and UI State?\\n\\nThe RSC API simplifies how you manage AI State and UI State, providing a robust way to keep them in sync between your database, server and client.\\n\\n### AI State\\n\\nAI State refers to the state of your application in a serialisable format that will be used on the server and can be shared with the language model.\\n\\nFor a chat app, the AI State is the conversation history (messages) between the user and the assistant. Components generated by the model would be represented in a JSON format as a tool alongside any necessary props. AI State can also be used to store other values and meta information such as `createdAt` for each message and `chatId` for each conversation. The LLM reads this history so it can generate the next message. This state serves as the source of truth for the current application state.\\n\\n<Note>\\n  **Note**: AI state can be accessed/modified from both the server and the\\n  client.\\n</Note>\\n\\n### UI State\\n\\nUI State refers to the state of your application that is rendered on the client. It is a fully client-side state (similar to `useState`) that can store anything from Javascript values to React elements. UI state is a list of actual UI elements that are rendered on the client.\\n\\n<Note>**Note**: UI State can only be accessed client-side.</Note>\\n\\n## Using AI / UI State\\n\\n### Creating the AI Context\\n\\nAI SDK RSC simplifies managing AI and UI state across your application by providing several hooks. These hooks are powered by [ React context ](https://react.dev/reference/react/hooks#context-hooks) under the hood.\\n\\nNotably, this means you do not have to pass the message history to the server explicitly for each request. You also can access and update your application state in any child component of the context provider. As you begin building [multistep generative interfaces](/docs/ai-sdk-rsc/multistep-interfaces), this will be particularly helpful.\\n\\nTo use `@ai-sdk/rsc` to manage AI and UI State in your application, you can create a React context using [`createAI`](/docs/reference/ai-sdk-rsc/create-ai):\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n// Define the AI state and UI state types\\nexport type ServerMessage = {\\n  role: \\'user\\' | \\'assistant\\';\\n  content: string;\\n};\\n\\nexport type ClientMessage = {\\n  id: string;\\n  role: \\'user\\' | \\'assistant\\';\\n  display: ReactNode;\\n};\\n\\nexport const sendMessage = async (input: string): Promise<ClientMessage> => {\\n  \"use server\"\\n  ...\\n}\\n```\\n\\n```tsx filename=\\'app/ai.ts\\'\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { ClientMessage, ServerMessage, sendMessage } from \\'./actions\\';\\n\\nexport type AIState = ServerMessage[];\\nexport type UIState = ClientMessage[];\\n\\n// Create the AI provider with the initial states and allowed actions\\nexport const AI = createAI<AIState, UIState>({\\n  initialAIState: [],\\n  initialUIState: [],\\n  actions: {\\n    sendMessage,\\n  },\\n});\\n```\\n\\n<Note>You must pass Server Actions to the `actions` object.</Note>\\n\\nIn this example, you define types for AI State and UI State, respectively.\\n\\nNext, wrap your application with your newly created context. With that, you can get and set AI and UI State across your entire application.\\n\\n```tsx filename=\\'app/layout.tsx\\'\\nimport { type ReactNode } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default function RootLayout({\\n  children,\\n}: Readonly<{ children: ReactNode }>) {\\n  return (\\n    <AI>\\n      <html lang=\"en\">\\n        <body>{children}</body>\\n      </html>\\n    </AI>\\n  );\\n}\\n```\\n\\n## Reading UI State in Client\\n\\nThe UI state can be accessed in Client Components using the [`useUIState`](/docs/reference/ai-sdk-rsc/use-ui-state) hook provided by the RSC API. The hook returns the current UI state and a function to update the UI state like React\\'s `useState`.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [messages, setMessages] = useUIState();\\n\\n  return (\\n    <ul>\\n      {messages.map(message => (\\n        <li key={message.id}>{message.display}</li>\\n      ))}\\n    </ul>\\n  );\\n}\\n```\\n\\n## Reading AI State in Client\\n\\nThe AI state can be accessed in Client Components using the [`useAIState`](/docs/reference/ai-sdk-rsc/use-ai-state) hook provided by the RSC API. The hook returns the current AI state.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useAIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [messages, setMessages] = useAIState();\\n\\n  return (\\n    <ul>\\n      {messages.map(message => (\\n        <li key={message.id}>{message.content}</li>\\n      ))}\\n    </ul>\\n  );\\n}\\n```\\n\\n## Reading AI State on Server\\n\\nThe AI State can be accessed within any Server Action provided to the `createAI` context using the [`getAIState`](/docs/reference/ai-sdk-rsc/get-ai-state) function. It returns the current AI state as a read-only value:\\n\\n```tsx filename=\\'app/actions.ts\\'\\nimport { getAIState } from \\'@ai-sdk/rsc\\';\\n\\nexport async function sendMessage(message: string) {\\n  \\'use server\\';\\n\\n  const history = getAIState();\\n\\n  const response = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: [...history, { role: \\'user\\', content: message }],\\n  });\\n\\n  return response;\\n}\\n```\\n\\n<Note>\\n  Remember, you can only access state within actions that have been passed to\\n  the `createAI` context within the `actions` key.\\n</Note>\\n\\n## Updating AI State on Server\\n\\nThe AI State can also be updated from within your Server Action with the [`getMutableAIState`](/docs/reference/ai-sdk-rsc/get-mutable-ai-state) function. This function is similar to `getAIState`, but it returns the state with methods to read and update it:\\n\\n```tsx filename=\\'app/actions.ts\\'\\nimport { getMutableAIState } from \\'@ai-sdk/rsc\\';\\n\\nexport async function sendMessage(message: string) {\\n  \\'use server\\';\\n\\n  const history = getMutableAIState();\\n\\n  // Update the AI state with the new user message.\\n  history.update([...history.get(), { role: \\'user\\', content: message }]);\\n\\n  const response = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: history.get(),\\n  });\\n\\n  // Update the AI state again with the response from the model.\\n  history.done([...history.get(), { role: \\'assistant\\', content: response }]);\\n\\n  return response;\\n}\\n```\\n\\n<Note>\\n  It is important to update the AI State with new responses using `.update()`\\n  and `.done()` to keep the conversation history in sync.\\n</Note>\\n\\n## Calling Server Actions from the Client\\n\\nTo call the `sendMessage` action from the client, you can use the [`useActions`](/docs/reference/ai-sdk-rsc/use-actions) hook. The hook returns all the available Actions that were provided to `createAI`:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default function Page() {\\n  const { sendMessage } = useActions<typeof AI>();\\n  const [messages, setMessages] = useUIState();\\n\\n  const handleSubmit = async event => {\\n    event.preventDefault();\\n\\n    setMessages([\\n      ...messages,\\n      { id: Date.now(), role: \\'user\\', display: event.target.message.value },\\n    ]);\\n\\n    const response = await sendMessage(event.target.message.value);\\n\\n    setMessages([\\n      ...messages,\\n      { id: Date.now(), role: \\'assistant\\', display: response },\\n    ]);\\n  };\\n\\n  return (\\n    <>\\n      <ul>\\n        {messages.map(message => (\\n          <li key={message.id}>{message.display}</li>\\n        ))}\\n      </ul>\\n      <form onSubmit={handleSubmit}>\\n        <input type=\"text\" name=\"message\" />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\nWhen the user submits a message, the `sendMessage` action is called with the message content. The response from the action is then added to the UI state, updating the displayed messages.\\n\\n<Note>\\n  Important! Don\\'t forget to update the UI State after you call your Server\\n  Action otherwise the streamed component will not show in the UI.\\n</Note>\\n\\nTo learn more, check out this [example](/examples/next-app/state-management/ai-ui-states) on managing AI and UI state using `@ai-sdk/rsc`.\\n\\n---\\n\\nNext, you will learn how you can save and restore state with `@ai-sdk/rsc`.\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/03-saving-and-restoring-states.mdx'), name='03-saving-and-restoring-states.mdx', displayName='03-saving-and-restoring-states.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Saving and Restoring States\\ndescription: Saving and restoring AI and UI states with onGetUIState and onSetAIState\\n---\\n\\n# Saving and Restoring States\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nAI SDK RSC provides convenient methods for saving and restoring AI and UI state. This is useful for saving the state of your application after every model generation, and restoring it when the user revisits the generations.\\n\\n## AI State\\n\\n### Saving AI state\\n\\nThe AI state can be saved using the [`onSetAIState`](/docs/reference/ai-sdk-rsc/create-ai#on-set-ai-state) callback, which gets called whenever the AI state is updated. In the following example, you save the chat history to a database whenever the generation is marked as done.\\n\\n```tsx filename=\\'app/ai.ts\\'\\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\\n  actions: {\\n    continueConversation,\\n  },\\n  onSetAIState: async ({ state, done }) => {\\n    \\'use server\\';\\n\\n    if (done) {\\n      saveChatToDB(state);\\n    }\\n  },\\n});\\n```\\n\\n### Restoring AI state\\n\\nThe AI state can be restored using the [`initialAIState`](/docs/reference/ai-sdk-rsc/create-ai#initial-ai-state) prop passed to the context provider created by the [`createAI`](/docs/reference/ai-sdk-rsc/create-ai) function. In the following example, you restore the chat history from a database when the component is mounted.\\n\\n```tsx file=\\'app/layout.tsx\\'\\nimport { ReactNode } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default async function RootLayout({\\n  children,\\n}: Readonly<{ children: ReactNode }>) {\\n  const chat = await loadChatFromDB();\\n\\n  return (\\n    <html lang=\"en\">\\n      <body>\\n        <AI initialAIState={chat}>{children}</AI>\\n      </body>\\n    </html>\\n  );\\n}\\n```\\n\\n## UI State\\n\\n### Saving UI state\\n\\nThe UI state cannot be saved directly, since the contents aren\\'t yet serializable. Instead, you can use the AI state as proxy to store details about the UI state and use it to restore the UI state when needed.\\n\\n### Restoring UI state\\n\\nThe UI state can be restored using the AI state as a proxy. In the following example, you restore the chat history from the AI state when the component is mounted. You use the [`onGetUIState`](/docs/reference/ai-sdk-rsc/create-ai#on-get-ui-state) callback to listen for SSR events and restore the UI state.\\n\\n```tsx filename=\\'app/ai.ts\\'\\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\\n  actions: {\\n    continueConversation,\\n  },\\n  onGetUIState: async () => {\\n    \\'use server\\';\\n\\n    const historyFromDB: ServerMessage[] = await loadChatFromDB();\\n    const historyFromApp: ServerMessage[] = getAIState();\\n\\n    // If the history from the database is different from the\\n    // history in the app, they\\'re not in sync so return the UIState\\n    // based on the history from the database\\n\\n    if (historyFromDB.length !== historyFromApp.length) {\\n      return historyFromDB.map(({ role, content }) => ({\\n        id: generateId(),\\n        role,\\n        display:\\n          role === \\'function\\' ? (\\n            <Component {...JSON.parse(content)} />\\n          ) : (\\n            content\\n          ),\\n      }));\\n    }\\n  },\\n});\\n```\\n\\nTo learn more, check out this [example](/examples/next-app/state-management/save-and-restore-states) that persists and restores states in your Next.js application.\\n\\n---\\n\\nNext, you will learn how you can use `@ai-sdk/rsc` functions like `useActions` and `useUIState` to create interactive, multistep interfaces.\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/04-multistep-interfaces.mdx'), name='04-multistep-interfaces.mdx', displayName='04-multistep-interfaces.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Multistep Interfaces\\ndescription: Overview of Building Multistep Interfaces with AI SDK RSC\\n---\\n\\n# Designing Multistep Interfaces\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nMultistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.\\n\\nFor example, if you wanted to build a Generative UI chatbot capable of booking flights, it could have three steps:\\n\\n- Search all flights\\n- Pick flight\\n- Check availability\\n\\nTo build this kind of application you will leverage two concepts, **tool composition** and **application context**.\\n\\n**Tool composition** is the process of combining multiple [tools](/docs/ai-sdk-core/tools-and-tool-calling) to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps. In the example above, _\"search all flights\"_, _\"pick flight\"_, and _\"check availability\"_ come together to create a holistic _\"book flight\"_ tool.\\n\\n**Application context** refers to the state of the application at any given point in time. This includes the user\\'s input, the output of the language model, and any other relevant information. In the example above, the flight selected in _\"pick flight\"_ would be used as context necessary to complete the _\"check availability\"_ task.\\n\\n## Overview\\n\\nIn order to build a multistep interface with `@ai-sdk/rsc`, you will need a few things:\\n\\n- A Server Action that calls and returns the result from the `streamUI` function\\n- Tool(s) (sub-tasks necessary to complete your overall task)\\n- React component(s) that should be rendered when the tool is called\\n- A page to render your chatbot\\n\\nThe general flow that you will follow is:\\n\\n- User sends a message (calls your Server Action with `useActions`, passing the message as an input)\\n- Message is appended to the AI State and then passed to the model alongside a number of tools\\n- Model can decide to call a tool, which will render the `<SomeTool />` component\\n- Within that component, you can add interactivity by using `useActions` to call the model with your Server Action and `useUIState` to append the model\\'s response (`<SomeOtherTool />`) to the UI State\\n- And so on...\\n\\n## Implementation\\n\\nThe turn-by-turn implementation is the simplest form of multistep interfaces. In this implementation, the user and the model take turns during the conversation. For every user input, the model generates a response, and the conversation continues in this turn-by-turn fashion.\\n\\nIn the following example, you specify two tools (`searchFlights` and `lookupFlight`) that the model can use to search for flights and lookup details for a specific flight.\\n\\n```tsx filename=\"app/actions.tsx\"\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst searchFlights = async (\\n  source: string,\\n  destination: string,\\n  date: string,\\n) => {\\n  return [\\n    {\\n      id: \\'1\\',\\n      flightNumber: \\'AA123\\',\\n    },\\n    {\\n      id: \\'2\\',\\n      flightNumber: \\'AA456\\',\\n    },\\n  ];\\n};\\n\\nconst lookupFlight = async (flightNumber: string) => {\\n  return {\\n    flightNumber: flightNumber,\\n    departureTime: \\'10:00 AM\\',\\n    arrivalTime: \\'12:00 PM\\',\\n  };\\n};\\n\\nexport async function submitUserMessage(input: string) {\\n  \\'use server\\';\\n\\n  const ui = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    system: \\'you are a flight booking assistant\\',\\n    prompt: input,\\n    text: async ({ content }) => <div>{content}</div>,\\n    tools: {\\n      searchFlights: {\\n        description: \\'search for flights\\',\\n        inputSchema: z.object({\\n          source: z.string().describe(\\'The origin of the flight\\'),\\n          destination: z.string().describe(\\'The destination of the flight\\'),\\n          date: z.string().describe(\\'The date of the flight\\'),\\n        }),\\n        generate: async function* ({ source, destination, date }) {\\n          yield `Searching for flights from ${source} to ${destination} on ${date}...`;\\n          const results = await searchFlights(source, destination, date);\\n\\n          return (\\n            <div>\\n              {results.map(result => (\\n                <div key={result.id}>\\n                  <div>{result.flightNumber}</div>\\n                </div>\\n              ))}\\n            </div>\\n          );\\n        },\\n      },\\n      lookupFlight: {\\n        description: \\'lookup details for a flight\\',\\n        parameters: z.object({\\n          flightNumber: z.string().describe(\\'The flight number\\'),\\n        }),\\n        generate: async function* ({ flightNumber }) {\\n          yield `Looking up details for flight ${flightNumber}...`;\\n          const details = await lookupFlight(flightNumber);\\n\\n          return (\\n            <div>\\n              <div>Flight Number: {details.flightNumber}</div>\\n              <div>Departure Time: {details.departureTime}</div>\\n              <div>Arrival Time: {details.arrivalTime}</div>\\n            </div>\\n          );\\n        },\\n      },\\n    },\\n  });\\n\\n  return ui.value;\\n}\\n```\\n\\nNext, create an AI context that will hold the UI State and AI State.\\n\\n```ts filename=\\'app/ai.ts\\'\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { submitUserMessage } from \\'./actions\\';\\n\\nexport const AI = createAI<any[], React.ReactNode[]>({\\n  initialUIState: [],\\n  initialAIState: [],\\n  actions: {\\n    submitUserMessage,\\n  },\\n});\\n```\\n\\nNext, wrap your application with your newly created context.\\n\\n```tsx filename=\\'app/layout.tsx\\'\\nimport { type ReactNode } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default function RootLayout({\\n  children,\\n}: Readonly<{ children: ReactNode }>) {\\n  return (\\n    <AI>\\n      <html lang=\"en\">\\n        <body>{children}</body>\\n      </html>\\n    </AI>\\n  );\\n}\\n```\\n\\nTo call your Server Action, update your root page with the following:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [conversation, setConversation] = useUIState<typeof AI>();\\n  const { submitUserMessage } = useActions();\\n\\n  const handleSubmit = async (e: React.FormEvent<HTMLFormElement>) => {\\n    e.preventDefault();\\n    setInput(\\'\\');\\n    setConversation(currentConversation => [\\n      ...currentConversation,\\n      <div>{input}</div>,\\n    ]);\\n    const message = await submitUserMessage(input);\\n    setConversation(currentConversation => [...currentConversation, message]);\\n  };\\n\\n  return (\\n    <div>\\n      <div>\\n        {conversation.map((message, i) => (\\n          <div key={i}>{message}</div>\\n        ))}\\n      </div>\\n      <div>\\n        <form onSubmit={handleSubmit}>\\n          <input\\n            type=\"text\"\\n            value={input}\\n            onChange={e => setInput(e.target.value)}\\n          />\\n          <button>Send Message</button>\\n        </form>\\n      </div>\\n    </div>\\n  );\\n}\\n```\\n\\nThis page pulls in the current UI State using the `useUIState` hook, which is then mapped over and rendered in the UI. To access the Server Action, you use the `useActions` hook which will return all actions that were passed to the `actions` key of the `createAI` function in your `actions.tsx` file. Finally, you call the `submitUserMessage` function like any other TypeScript function. This function returns a React component (`message`) that is then rendered in the UI by updating the UI State with `setConversation`.\\n\\nIn this example, to call the next tool, the user must respond with plain text. **Given you are streaming a React component, you can add a button to trigger the next step in the conversation**.\\n\\nTo add user interaction, you will have to convert the component into a client component and use the `useAction` hook to trigger the next step in the conversation.\\n\\n```tsx filename=\"components/flights.tsx\"\\n\\'use client\\';\\n\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\nimport { ReactNode } from \\'react\\';\\n\\ninterface FlightsProps {\\n  flights: { id: string; flightNumber: string }[];\\n}\\n\\nexport const Flights = ({ flights }: FlightsProps) => {\\n  const { submitUserMessage } = useActions();\\n  const [_, setMessages] = useUIState();\\n\\n  return (\\n    <div>\\n      {flights.map(result => (\\n        <div key={result.id}>\\n          <div\\n            onClick={async () => {\\n              const display = await submitUserMessage(\\n                `lookupFlight ${result.flightNumber}`,\\n              );\\n\\n              setMessages((messages: ReactNode[]) => [...messages, display]);\\n            }}\\n          >\\n            {result.flightNumber}\\n          </div>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n};\\n```\\n\\nNow, update your `searchFlights` tool to render the new `<Flights />` component.\\n\\n```tsx filename=\"actions.tsx\"\\n...\\nsearchFlights: {\\n  description: \\'search for flights\\',\\n  parameters: z.object({\\n    source: z.string().describe(\\'The origin of the flight\\'),\\n    destination: z.string().describe(\\'The destination of the flight\\'),\\n    date: z.string().describe(\\'The date of the flight\\'),\\n  }),\\n  generate: async function* ({ source, destination, date }) {\\n    yield `Searching for flights from ${source} to ${destination} on ${date}...`;\\n    const results = await searchFlights(source, destination, date);\\n    return (<Flights flights={results} />);\\n  },\\n}\\n...\\n```\\n\\nIn the above example, the `Flights` component is used to display the search results. When the user clicks on a flight number, the `lookupFlight` tool is called with the flight number as a parameter. The `submitUserMessage` action is then called to trigger the next step in the conversation.\\n\\nLearn more about tool calling in Next.js App Router by checking out examples [here](/examples/next-app/tools).\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/05-streaming-values.mdx'), name='05-streaming-values.mdx', displayName='05-streaming-values.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming Values\\ndescription: Overview of streaming RSCs\\n---\\n\\nimport { UIPreviewCard, Card } from \\'@/components/home/card\\';\\nimport { EventPlanning } from \\'@/components/home/event-planning\\';\\nimport { Searching } from \\'@/components/home/searching\\';\\nimport { Weather } from \\'@/components/home/weather\\';\\n\\n# Streaming Values\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nThe RSC API provides several utility functions to allow you to stream values from the server to the client. This is useful when you need more granular control over what you are streaming and how you are streaming it.\\n\\n<Note>\\n  These utilities can also be paired with [AI SDK Core](/docs/ai-sdk-core)\\n  functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text) and\\n  [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to easily stream\\n  LLM generations from the server to the client.\\n</Note>\\n\\nThere are two functions provided by the RSC API that allow you to create streamable values:\\n\\n- [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) - creates a streamable (serializable) value, with full control over how you create, update, and close the stream.\\n- [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) - creates a streamable React component, with full control over how you create, update, and close the stream.\\n\\n## `createStreamableValue`\\n\\nThe RSC API allows you to stream serializable Javascript values from the server to the client using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value), such as strings, numbers, objects, and arrays.\\n\\nThis is useful when you want to stream:\\n\\n- Text generations from the language model in real-time.\\n- Buffer values of image and audio generations from multi-modal models.\\n- Progress updates from multi-step agent runs.\\n\\n## Creating a Streamable Value\\n\\nYou can import `createStreamableValue` from `@ai-sdk/rsc` and use it to create a streamable value.\\n\\n```tsx file=\\'app/actions.ts\\'\\n\\'use server\\';\\n\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport const runThread = async () => {\\n  const streamableStatus = createStreamableValue(\\'thread.init\\');\\n\\n  setTimeout(() => {\\n    streamableStatus.update(\\'thread.run.create\\');\\n    streamableStatus.update(\\'thread.run.update\\');\\n    streamableStatus.update(\\'thread.run.end\\');\\n    streamableStatus.done(\\'thread.end\\');\\n  }, 1000);\\n\\n  return {\\n    status: streamableStatus.value,\\n  };\\n};\\n```\\n\\n## Reading a Streamable Value\\n\\nYou can read streamable values on the client using `readStreamableValue`. It returns an async iterator that yields the value of the streamable as it is updated:\\n\\n```tsx file=\\'app/page.tsx\\'\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { runThread } from \\'@/actions\\';\\n\\nexport default function Page() {\\n  return (\\n    <button\\n      onClick={async () => {\\n        const { status } = await runThread();\\n\\n        for await (const value of readStreamableValue(status)) {\\n          console.log(value);\\n        }\\n      }}\\n    >\\n      Ask\\n    </button>\\n  );\\n}\\n```\\n\\nLearn how to stream a text generation (with `streamText`) using the Next.js App Router and `createStreamableValue` in this [example](/examples/next-app/basics/streaming-text-generation).\\n\\n## `createStreamableUI`\\n\\n`createStreamableUI` creates a stream that holds a React component. Unlike AI SDK Core APIs, this function does not call a large language model. Instead, it provides a primitive that can be used to have granular control over streaming a React component.\\n\\n## Using `createStreamableUI`\\n\\nLet\\'s look at how you can use the `createStreamableUI` function with a Server Action.\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n\\'use server\\';\\n\\nimport { createStreamableUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function getWeather() {\\n  const weatherUI = createStreamableUI();\\n\\n  weatherUI.update(<div style={{ color: \\'gray\\' }}>Loading...</div>);\\n\\n  setTimeout(() => {\\n    weatherUI.done(<div>It&apos;s a sunny day!</div>);\\n  }, 1000);\\n\\n  return weatherUI.value;\\n}\\n```\\n\\nFirst, you create a streamable UI with an empty state and then update it with a loading message. After 1 second, you mark the stream as done passing in the actual weather information as its final value. The `.value` property contains the actual UI that can be sent to the client.\\n\\n## Reading a Streamable UI\\n\\nOn the client side, you can call the `getWeather` Server Action and render the returned UI like any other React component.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { getWeather } from \\'@/actions\\';\\n\\nexport default function Page() {\\n  const [weather, setWeather] = useState<React.ReactNode | null>(null);\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const weatherUI = await getWeather();\\n          setWeather(weatherUI);\\n        }}\\n      >\\n        What&apos;s the weather?\\n      </button>\\n\\n      {weather}\\n    </div>\\n  );\\n}\\n```\\n\\nWhen the button is clicked, the `getWeather` function is called, and the returned UI is set to the `weather` state and rendered on the page. Users will see the loading message first and then the actual weather information after 1 second.\\n\\nLearn more about handling multiple streams in a single request in the [Multiple Streamables](/docs/advanced/multiple-streamables) guide.\\n\\nLearn more about handling state for more complex use cases with [ AI/UI State ](/docs/ai-sdk-rsc/generative-ui-state).\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/06-loading-state.mdx'), name='06-loading-state.mdx', displayName='06-loading-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Handling Loading State\\ndescription: Overview of handling loading state with AI SDK RSC\\n---\\n\\n# Handling Loading State\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nGiven that responses from language models can often take a while to complete, it\\'s crucial to be able to show loading state to users. This provides visual feedback that the system is working on their request and helps maintain a positive user experience.\\n\\nThere are three approaches you can take to handle loading state with the AI SDK RSC:\\n\\n- Managing loading state similar to how you would in a traditional Next.js application. This involves setting a loading state variable in the client and updating it when the response is received.\\n- Streaming loading state from the server to the client. This approach allows you to track loading state on a more granular level and provide more detailed feedback to the user.\\n- Streaming loading component from the server to the client. This approach allows you to stream a React Server Component to the client while awaiting the model\\'s response.\\n\\n## Handling Loading State on the Client\\n\\n### Client\\n\\nLet\\'s create a simple Next.js page that will call the `generateResponse` function when the form is submitted. The function will take in the user\\'s prompt (`input`) and then generate a response (`response`). To handle the loading state, use the `loading` state variable. When the form is submitted, set `loading` to `true`, and when the response is received, set it back to `false`. While the response is being streamed, the input field will be disabled.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { generateResponse } from \\'./actions\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport default function Home() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [generation, setGeneration] = useState<string>(\\'\\');\\n  const [loading, setLoading] = useState<boolean>(false);\\n\\n  return (\\n    <div>\\n      <div>{generation}</div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          setLoading(true);\\n          const response = await generateResponse(input);\\n\\n          let textContent = \\'\\';\\n\\n          for await (const delta of readStreamableValue(response)) {\\n            textContent = `${textContent}${delta}`;\\n            setGeneration(textContent);\\n          }\\n          setInput(\\'\\');\\n          setLoading(false);\\n        }}\\n      >\\n        <input\\n          type=\"text\"\\n          value={input}\\n          disabled={loading}\\n          className=\"disabled:opacity-50\"\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button>Send Message</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Server\\n\\nNow let\\'s implement the `generateResponse` function. Use the `streamText` function to generate a response to the input.\\n\\n```typescript filename=\\'app/actions.ts\\'\\n\\'use server\\';\\n\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport async function generateResponse(prompt: string) {\\n  const stream = createStreamableValue();\\n\\n  (async () => {\\n    const { textStream } = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      prompt,\\n    });\\n\\n    for await (const text of textStream) {\\n      stream.update(text);\\n    }\\n\\n    stream.done();\\n  })();\\n\\n  return stream.value;\\n}\\n```\\n\\n## Streaming Loading State from the Server\\n\\nIf you are looking to track loading state on a more granular level, you can create a new streamable value to store a custom variable and then read this on the frontend. Let\\'s update the example to create a new streamable value for tracking loading state:\\n\\n### Server\\n\\n```typescript filename=\\'app/actions.ts\\' highlight=\\'9,22,25\\'\\n\\'use server\\';\\n\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport async function generateResponse(prompt: string) {\\n  const stream = createStreamableValue();\\n  const loadingState = createStreamableValue({ loading: true });\\n\\n  (async () => {\\n    const { textStream } = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      prompt,\\n    });\\n\\n    for await (const text of textStream) {\\n      stream.update(text);\\n    }\\n\\n    stream.done();\\n    loadingState.done({ loading: false });\\n  })();\\n\\n  return { response: stream.value, loadingState: loadingState.value };\\n}\\n```\\n\\n### Client\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"22,30-34\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { generateResponse } from \\'./actions\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport default function Home() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [generation, setGeneration] = useState<string>(\\'\\');\\n  const [loading, setLoading] = useState<boolean>(false);\\n\\n  return (\\n    <div>\\n      <div>{generation}</div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          setLoading(true);\\n          const { response, loadingState } = await generateResponse(input);\\n\\n          let textContent = \\'\\';\\n\\n          for await (const responseDelta of readStreamableValue(response)) {\\n            textContent = `${textContent}${responseDelta}`;\\n            setGeneration(textContent);\\n          }\\n          for await (const loadingDelta of readStreamableValue(loadingState)) {\\n            if (loadingDelta) {\\n              setLoading(loadingDelta.loading);\\n            }\\n          }\\n          setInput(\\'\\');\\n          setLoading(false);\\n        }}\\n      >\\n        <input\\n          type=\"text\"\\n          value={input}\\n          disabled={loading}\\n          className=\"disabled:opacity-50\"\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button>Send Message</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis allows you to provide more detailed feedback about the generation process to your users.\\n\\n## Streaming Loading Components with `streamUI`\\n\\nIf you are using the [ `streamUI` ](/docs/reference/ai-sdk-rsc/stream-ui) function, you can stream the loading state to the client in the form of a React component. `streamUI` supports the usage of [ JavaScript generator functions ](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*), which allow you to yield some value (in this case a React component) while some other blocking work completes.\\n\\n## Server\\n\\n```ts\\n\\'use server\\';\\n\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function generateResponse(prompt: string) {\\n  const result = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    prompt,\\n    text: async function* ({ content }) {\\n      yield <div>loading...</div>;\\n      return <div>{content}</div>;\\n    },\\n  });\\n\\n  return result.value;\\n}\\n```\\n\\n<Note>\\n  Remember to update the file from `.ts` to `.tsx` because you are defining a\\n  React component in the `streamUI` function.\\n</Note>\\n\\n## Client\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { generateResponse } from \\'./actions\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport default function Home() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [generation, setGeneration] = useState<React.ReactNode>();\\n\\n  return (\\n    <div>\\n      <div>{generation}</div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          const result = await generateResponse(input);\\n          setGeneration(result);\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button>Send Message</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/08-error-handling.mdx'), name='08-error-handling.mdx', displayName='08-error-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Error Handling\\ndescription: Learn how to handle errors with the AI SDK.\\n---\\n\\n# Error Handling\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nTwo categories of errors can occur when working with the RSC API: errors while streaming user interfaces and errors while streaming other values.\\n\\n## Handling UI Errors\\n\\nTo handle errors while generating UI, the [`streamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) object exposes an `error()` method.\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n\\'use server\\';\\n\\nimport { createStreamableUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function getStreamedUI() {\\n  const ui = createStreamableUI();\\n\\n  (async () => {\\n    ui.update(<div>loading</div>);\\n    const data = await fetchData();\\n    ui.done(<div>{data}</div>);\\n  })().catch(e => {\\n    ui.error(<div>Error: {e.message}</div>);\\n  });\\n\\n  return ui.value;\\n}\\n```\\n\\nWith this method, you can catch any error with the stream, and return relevant UI. On the client, you can also use a [React Error Boundary](https://react.dev/reference/react/Component#catching-rendering-errors-with-an-error-boundary) to wrap the streamed component and catch any additional errors.\\n\\n```tsx filename=\\'app/page.tsx\\'\\nimport { getStreamedUI } from \\'@/actions\\';\\nimport { useState } from \\'react\\';\\nimport { ErrorBoundary } from \\'./ErrorBoundary\\';\\n\\nexport default function Page() {\\n  const [streamedUI, setStreamedUI] = useState(null);\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const newUI = await getStreamedUI();\\n          setStreamedUI(newUI);\\n        }}\\n      >\\n        What does the new UI look like?\\n      </button>\\n      <ErrorBoundary>{streamedUI}</ErrorBoundary>\\n    </div>\\n  );\\n}\\n```\\n\\n## Handling Other Errors\\n\\nTo handle other errors while streaming, you can return an error object that the receiver can use to determine why the failure occurred.\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n\\'use server\\';\\n\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { fetchData, emptyData } from \\'../utils/data\\';\\n\\nexport const getStreamedData = async () => {\\n  const streamableData = createStreamableValue<string>(emptyData);\\n\\n  try {\\n    (() => {\\n      const data1 = await fetchData();\\n      streamableData.update(data1);\\n\\n      const data2 = await fetchData();\\n      streamableData.update(data2);\\n\\n      const data3 = await fetchData();\\n      streamableData.done(data3);\\n    })();\\n\\n    return { data: streamableData.value };\\n  } catch (e) {\\n    return { error: e.message };\\n  }\\n};\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/09-authentication.mdx'), name='09-authentication.mdx', displayName='09-authentication.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Handling Authentication\\ndescription: Learn how to authenticate with the AI SDK.\\n---\\n\\n# Authentication\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nThe RSC API makes extensive use of [`Server Actions`](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations) to power streaming values and UI from the server.\\n\\nServer Actions are exposed as public, unprotected endpoints. As a result, you should treat Server Actions as you would public-facing API endpoints and ensure that the user is authorized to perform the action before returning any data.\\n\\n```tsx filename=\"app/actions.tsx\"\\n\\'use server\\';\\n\\nimport { cookies } from \\'next/headers\\';\\nimport { createStremableUI } from \\'@ai-sdk/rsc\\';\\nimport { validateToken } from \\'../utils/auth\\';\\n\\nexport const getWeather = async () => {\\n  const token = cookies().get(\\'token\\');\\n\\n  if (!token || !validateToken(token)) {\\n    return {\\n      error: \\'This action requires authentication\\',\\n    };\\n  }\\n  const streamableDisplay = createStreamableUI(null);\\n\\n  streamableDisplay.update(<Skeleton />);\\n  streamableDisplay.done(<Weather />);\\n\\n  return {\\n    display: streamableDisplay.value,\\n  };\\n};\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/10-migrating-to-ui.mdx'), name='10-migrating-to-ui.mdx', displayName='10-migrating-to-ui.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Migrating from RSC to UI\\ndescription: Learn how to migrate from AI SDK RSC to AI SDK UI.\\n---\\n\\n# Migrating from RSC to UI\\n\\nThis guide helps you migrate from AI SDK RSC to AI SDK UI.\\n\\n## Background\\n\\nThe AI SDK has two packages that help you build the frontend for your applications – [AI SDK UI](/docs/ai-sdk-ui) and [AI SDK RSC](/docs/ai-sdk-rsc).\\n\\nWe introduced support for using [React Server Components](https://react.dev/reference/rsc/server-components) (RSC) within the AI SDK to simplify building generative user interfaces for frameworks that support RSC.\\n\\nHowever, given we\\'re pushing the boundaries of this technology, AI SDK RSC currently faces significant limitations that make it unsuitable for stable production use.\\n\\n- It is not possible to abort a stream using server actions. This will be improved in future releases of React and Next.js [(1122)](https://github.com/vercel/ai/issues/1122).\\n- When using `createStreamableUI` and `streamUI`, components remount on `.done()`, causing them to flicker [(2939)](https://github.com/vercel/ai/issues/2939).\\n- Many suspense boundaries can lead to crashes [(2843)](https://github.com/vercel/ai/issues/2843).\\n- Using\\xa0`createStreamableUI`\\xa0can lead to quadratic data transfer. You can avoid this using\\xa0createStreamableValue\\xa0instead, and rendering the component client-side.\\n- Closed RSC streams cause update issues [(3007)](https://github.com/vercel/ai/issues/3007).\\n\\nDue to these limitations, AI SDK RSC is marked as experimental, and we do not recommend using it for stable production environments.\\n\\nAs a result, we strongly recommend migrating to AI SDK UI, which has undergone extensive development to provide a more stable and production grade experience.\\n\\nIn building [v0](https://v0.dev), we have invested considerable time exploring how to create the best chat experience on the web. AI SDK UI ships with many of these best practices and commonly used patterns like [language model middleware](/docs/ai-sdk-core/middleware), [multi-step tool calls](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls), [attachments](/docs/ai-sdk-ui/chatbot#attachments-experimental), [telemetry](/docs/ai-sdk-core/telemetry), [provider registry](/docs/ai-sdk-core/provider-management#provider-registry), and many more. These features have been considerately designed into a neat abstraction that you can use to reliably integrate AI into your applications.\\n\\n## Streaming Chat Completions\\n\\n### Basic Setup\\n\\nThe `streamUI` function executes as part of a server action as illustrated below.\\n\\n#### Before: Handle generation and rendering in a single server action\\n\\n```tsx filename=\"@/app/actions.tsx\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { getMutableAIState, streamUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function sendMessage(message: string) {\\n  \\'use server\\';\\n\\n  const messages = getMutableAIState(\\'messages\\');\\n\\n  messages.update([...messages.get(), { role: \\'user\\', content: message }]);\\n\\n  const { value: stream } = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    system: \\'you are a friendly assistant!\\',\\n    messages: messages.get(),\\n    text: async function* ({ content, done }) {\\n      // process text\\n    },\\n    tools: {\\n      // tool definitions\\n    },\\n  });\\n\\n  return stream;\\n}\\n```\\n\\n#### Before: Call server action and update UI state\\n\\nThe chat interface calls the server action. The response is then saved using the `useUIState` hook.\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState, ReactNode } from \\'react\\';\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const { sendMessage } = useActions();\\n  const [input, setInput] = useState(\\'\\');\\n  const [messages, setMessages] = useUIState();\\n\\n  return (\\n    <div>\\n      {messages.map(message => message)}\\n\\n      <form\\n        onSubmit={async () => {\\n          const response: ReactNode = await sendMessage(input);\\n          setMessages(msgs => [...msgs, response]);\\n        }}\\n      >\\n        <input type=\"text\" />\\n        <button type=\"submit\">Submit</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThe `streamUI` function combines generating text and rendering the user interface. To migrate to AI SDK UI, you need to **separate these concerns** –\\xa0streaming generations with `streamText` and rendering the UI with `useChat`.\\n\\n#### After: Replace server action with route handler\\n\\nThe `streamText` function executes as part of a route handler and streams the response to the client. The `useChat` hook on the client decodes this stream and renders the response within the chat interface.\\n\\n```ts filename=\"@/app/api/chat/route.ts\"\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nexport async function POST(request) {\\n  const { messages } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly assistant!\\',\\n    messages,\\n    tools: {\\n      // tool definitions\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n#### After: Update client to use chat hook\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { messages, input, setInput, handleSubmit } = useChat();\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>{message.content}</div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Parallel Tool Calls\\n\\nIn AI SDK RSC, `streamUI` does not support parallel tool calls. You will have to use a combination of `streamText`, `createStreamableUI` and `createStreamableValue`.\\n\\nWith AI SDK UI, `useChat` comes with built-in support for parallel tool calls. You can define multiple tools in the `streamText` and have them called them in parallel. The `useChat` hook will then handle the parallel tool calls for you automatically.\\n\\n### Multi-Step Tool Calls\\n\\nIn AI SDK RSC, `streamUI` does not support multi-step tool calls. You will have to use a combination of `streamText`, `createStreamableUI` and `createStreamableValue`.\\n\\nWith AI SDK UI, `useChat` comes with built-in support for multi-step tool calls. You can set `maxSteps` in the `streamText` function to define the number of steps the language model can make in a single call. The `useChat` hook will then handle the multi-step tool calls for you automatically.\\n\\n### Generative User Interfaces\\n\\nThe `streamUI` function uses `tools` as a way to execute functions based on user input and renders React components based on the function output to go beyond text in the chat interface.\\n\\n#### Before: Render components within the server action and stream to client\\n\\n```tsx filename=\"@/app/actions.tsx\"\\nimport { z } from \\'zod\\';\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { getWeather } from \\'@/utils/queries\\';\\nimport { Weather } from \\'@/components/weather\\';\\n\\nconst { value: stream } = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  system: \\'you are a friendly assistant!\\',\\n  messages,\\n  text: async function* ({ content, done }) {\\n    // process text\\n  },\\n  tools: {\\n    displayWeather: {\\n      description: \\'Display the weather for a location\\',\\n      inputSchema: z.object({\\n        latitude: z.number(),\\n        longitude: z.number(),\\n      }),\\n      generate: async function* ({ latitude, longitude }) {\\n        yield <div>Loading weather...</div>;\\n\\n        const { value, unit } = await getWeather({ latitude, longitude });\\n\\n        return <Weather value={value} unit={unit} />;\\n      },\\n    },\\n  },\\n});\\n```\\n\\nAs mentioned earlier, `streamUI` generates text and renders the React component in a single server action call.\\n\\n#### After: Replace with route handler and stream props data to client\\n\\nThe `streamText` function streams the props data as response to the client, while `useChat` decode the stream as `toolInvocations` and renders the chat interface.\\n\\n```ts filename=\"@/app/api/chat/route.ts\"\\nimport { z } from \\'zod\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { getWeather } from \\'@/utils/queries\\';\\nimport { streamText } from \\'ai\\';\\n\\nexport async function POST(request) {\\n  const { messages } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly assistant!\\',\\n    messages,\\n    tools: {\\n      displayWeather: {\\n        description: \\'Display the weather for a location\\',\\n        parameters: z.object({\\n          latitude: z.number(),\\n          longitude: z.number(),\\n        }),\\n        execute: async function ({ latitude, longitude }) {\\n          const props = await getWeather({ latitude, longitude });\\n          return props;\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n#### After: Update client to use chat hook and render components using tool invocations\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { Weather } from \\'@/components/weather\\';\\n\\nexport default function Page() {\\n  const { messages, input, setInput, handleSubmit } = useChat();\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>{message.content}</div>\\n\\n          <div>\\n            {message.toolInvocations.map(toolInvocation => {\\n              const { toolName, toolCallId, state } = toolInvocation;\\n\\n              if (state === \\'result\\') {\\n                const { result } = toolInvocation;\\n\\n                return (\\n                  <div key={toolCallId}>\\n                    {toolName === \\'displayWeather\\' ? (\\n                      <Weather weatherAtLocation={result} />\\n                    ) : null}\\n                  </div>\\n                );\\n              } else {\\n                return (\\n                  <div key={toolCallId}>\\n                    {toolName === \\'displayWeather\\' ? (\\n                      <div>Loading weather...</div>\\n                    ) : null}\\n                  </div>\\n                );\\n              }\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Handling Client Interactions\\n\\nWith AI SDK RSC, components streamed to the client can trigger subsequent generations by calling the relevant server action using the `useActions` hooks. This is possible as long as the component is a descendant of the `<AI/>` context provider.\\n\\n#### Before: Use actions hook to send messages\\n\\n```tsx filename=\"@/app/components/list-flights.tsx\"\\n\\'use client\\';\\n\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport function ListFlights({ flights }) {\\n  const { sendMessage } = useActions();\\n  const [_, setMessages] = useUIState();\\n\\n  return (\\n    <div>\\n      {flights.map(flight => (\\n        <div\\n          key={flight.id}\\n          onClick={async () => {\\n            const response = await sendMessage(\\n              `I would like to choose flight ${flight.id}!`,\\n            );\\n\\n            setMessages(msgs => [...msgs, response]);\\n          }}\\n        >\\n          {flight.name}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n#### After: Use another chat hook with same ID from the component\\n\\nAfter switching to AI SDK UI, these messages are synced by initializing the `useChat` hook in the component with the same `id` as the parent component.\\n\\n```tsx filename=\"@/app/components/list-flights.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport function ListFlights({ chatId, flights }) {\\n  const { append } = useChat({\\n    id: chatId,\\n    body: { id: chatId },\\n    maxSteps: 5,\\n  });\\n\\n  return (\\n    <div>\\n      {flights.map(flight => (\\n        <div\\n          key={flight.id}\\n          onClick={async () => {\\n            await append({\\n              role: \\'user\\',\\n              content: `I would like to choose flight ${flight.id}!`,\\n            });\\n          }}\\n        >\\n          {flight.name}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n### Loading Indicators\\n\\nIn AI SDK RSC, you can use the `initial` parameter of `streamUI` to define the component to display while the generation is in progress.\\n\\n#### Before: Use `loading` to show loading indicator\\n\\n```tsx filename=\"@/app/actions.tsx\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\n\\nconst { value: stream } = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  system: \\'you are a friendly assistant!\\',\\n  messages,\\n  initial: <div>Loading...</div>,\\n  text: async function* ({ content, done }) {\\n    // process text\\n  },\\n  tools: {\\n    // tool definitions\\n  },\\n});\\n\\nreturn stream;\\n```\\n\\nWith AI SDK UI, you can use the tool invocation state to show a loading indicator while the tool is executing.\\n\\n#### After: Use tool invocation state to show loading indicator\\n\\n```tsx filename=\"@/app/components/message.tsx\"\\n\\'use client\\';\\n\\nexport function Message({ role, content, toolInvocations }) {\\n  return (\\n    <div>\\n      <div>{role}</div>\\n      <div>{content}</div>\\n\\n      {toolInvocations && (\\n        <div>\\n          {toolInvocations.map(toolInvocation => {\\n            const { toolName, toolCallId, state } = toolInvocation;\\n\\n            if (state === \\'result\\') {\\n              const { result } = toolInvocation;\\n\\n              return (\\n                <div key={toolCallId}>\\n                  {toolName === \\'getWeather\\' ? (\\n                    <Weather weatherAtLocation={result} />\\n                  ) : null}\\n                </div>\\n              );\\n            } else {\\n              return (\\n                <div key={toolCallId}>\\n                  {toolName === \\'getWeather\\' ? (\\n                    <Weather isLoading={true} />\\n                  ) : (\\n                    <div>Loading...</div>\\n                  )}\\n                </div>\\n              );\\n            }\\n          })}\\n        </div>\\n      )}\\n    </div>\\n  );\\n}\\n```\\n\\n### Saving Chats\\n\\nBefore implementing `streamUI` as a server action, you should create an `<AI/>` provider and wrap your application at the root layout to sync the AI and UI states. During initialization, you typically use the `onSetAIState` callback function to track updates to the AI state and save it to the database when `done(...)` is called.\\n\\n#### Before: Save chats using callback function of context provider\\n\\n```ts filename=\"@/app/actions.ts\"\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { saveChat } from \\'@/utils/queries\\';\\n\\nexport const AI = createAI({\\n  initialAIState: {},\\n  initialUIState: {},\\n  actions: {\\n    // server actions\\n  },\\n  onSetAIState: async ({ state, done }) => {\\n    \\'use server\\';\\n\\n    if (done) {\\n      await saveChat(state);\\n    }\\n  },\\n});\\n```\\n\\n#### After: Save chats using callback function of `streamText`\\n\\nWith AI SDK UI, you will save chats using the `onFinish` callback function of `streamText` in your route handler.\\n\\n```ts filename=\"@/app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { saveChat } from \\'@/utils/queries\\';\\nimport { streamText, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(request) {\\n  const { id, messages } = await request.json();\\n\\n  const coreMessages = convertToModelMessages(messages);\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly assistant!\\',\\n    messages: coreMessages,\\n    onFinish: async ({ response }) => {\\n      try {\\n        await saveChat({\\n          id,\\n          messages: [...coreMessages, ...response.messages],\\n        });\\n      } catch (error) {\\n        console.error(\\'Failed to save chat\\');\\n      }\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Restoring Chats\\n\\nWhen using AI SDK RSC, the `useUIState` hook contains the UI state of the chat. When restoring a previously saved chat, the UI state needs to be loaded with messages.\\n\\nSimilar to how you typically save chats in AI SDK RSC, you should use the `onGetUIState` callback function to retrieve the chat from the database, convert it into UI state, and return it to be accessible through `useUIState`.\\n\\n#### Before: Load chat from database using callback function of context provider\\n\\n```ts filename=\"@/app/actions.ts\"\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { loadChatFromDB, convertToUIState } from \\'@/utils/queries\\';\\n\\nexport const AI = createAI({\\n  actions: {\\n    // server actions\\n  },\\n  onGetUIState: async () => {\\n    \\'use server\\';\\n\\n    const chat = await loadChatFromDB();\\n    const uiState = convertToUIState(chat);\\n\\n    return uiState;\\n  },\\n});\\n```\\n\\nAI SDK UI uses the `messages` field of `useChat` to store messages. To load messages when `useChat` is mounted, you should use `initialMessages`.\\n\\nAs messages are typically loaded from the database, we can use a server actions inside a Page component to fetch an older chat from the database during static generation and pass the messages as props to the `<Chat/>` component.\\n\\n#### After: Load chat from database during static generation of page\\n\\n```tsx filename=\"@/app/chat/[id]/page.tsx\"\\nimport { Chat } from \\'@/app/components/chat\\';\\nimport { getChatById } from \\'@/utils/queries\\';\\n\\n// link to example implementation: https://github.com/vercel/ai-chatbot/blob/00b125378c998d19ef60b73fe576df0fe5a0e9d4/lib/utils.ts#L87-L127\\nimport { convertToUIMessages } from \\'@/utils/functions\\';\\n\\nexport default async function Page({ params }: { params: any }) {\\n  const { id } = params;\\n  const chatFromDb = await getChatById({ id });\\n\\n  const chat: Chat = {\\n    ...chatFromDb,\\n    messages: convertToUIMessages(chatFromDb.messages),\\n  };\\n\\n  return <Chat key={id} id={chat.id} initialMessages={chat.messages} />;\\n}\\n```\\n\\n#### After: Pass chat messages as props and load into chat hook\\n\\n```tsx filename=\"@/app/components/chat.tsx\"\\n\\'use client\\';\\n\\nimport { Message } from \\'ai\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport function Chat({\\n  id,\\n  initialMessages,\\n}: {\\n  id;\\n  initialMessages: Array<Message>;\\n}) {\\n  const { messages } = useChat({\\n    id,\\n    initialMessages,\\n  });\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>{message.content}</div>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n## Streaming Object Generation\\n\\nThe `createStreamableValue` function streams any serializable data from the server to the client. As a result, this function allows you to stream object generations from the server to the client when paired with `streamObject`.\\n\\n#### Before: Use streamable value to stream object generations\\n\\n```ts filename=\"@/app/actions.ts\"\\nimport { streamObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { notificationsSchema } from \\'@/utils/schemas\\';\\n\\nexport async function generateSampleNotifications() {\\n  \\'use server\\';\\n\\n  const stream = createStreamableValue();\\n\\n  (async () => {\\n    const { partialObjectStream } = streamObject({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      system: \\'generate sample ios messages for testing\\',\\n      prompt: \\'messages from a family group chat during diwali, max 4\\',\\n      schema: notificationsSchema,\\n    });\\n\\n    for await (const partialObject of partialObjectStream) {\\n      stream.update(partialObject);\\n    }\\n  })();\\n\\n  stream.done();\\n\\n  return { partialNotificationsStream: stream.value };\\n}\\n```\\n\\n#### Before: Read streamable value and update object\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { generateSampleNotifications } from \\'@/app/actions\\';\\n\\nexport default function Page() {\\n  const [notifications, setNotifications] = useState(null);\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const { partialNotificationsStream } =\\n            await generateSampleNotifications();\\n\\n          for await (const partialNotifications of readStreamableValue(\\n            partialNotificationsStream,\\n          )) {\\n            if (partialNotifications) {\\n              setNotifications(partialNotifications.notifications);\\n            }\\n          }\\n        }}\\n      >\\n        Generate\\n      </button>\\n    </div>\\n  );\\n}\\n```\\n\\nTo migrate to AI SDK UI, you should use the `useObject` hook and implement `streamObject` within your route handler.\\n\\n#### After: Replace with route handler and stream text response\\n\\n```ts filename=\"@/app/api/object/route.ts\"\\nimport { streamObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { notificationSchema } from \\'@/utils/schemas\\';\\n\\nexport async function POST(req: Request) {\\n  const context = await req.json();\\n\\n  const result = streamObject({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    schema: notificationSchema,\\n    prompt:\\n      `Generate 3 notifications for a messages app in this context:` + context,\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n#### After: Use object hook to decode stream and update object\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\nimport { notificationSchema } from \\'@/utils/schemas\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/object\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <div>\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK RSC\\ndescription: Learn about AI SDK RSC.\\ncollapsed: true\\n---\\n\\n# AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: \\'Overview\\',\\n      description: \\'Learn about AI SDK RSC.\\',\\n      href: \\'/docs/ai-sdk-rsc/overview\\',\\n    },\\n    {\\n      title: \\'Streaming React Components\\',\\n      description: \\'Learn how to stream React components.\\',\\n      href: \\'/docs/ai-sdk-rsc/streaming-react-components\\',\\n    },\\n    {\\n      title: \\'Managing Generative UI State\\',\\n      description: \\'Learn how to manage generative UI state.\\',\\n      href: \\'/docs/ai-sdk-rsc/generative-ui-state\\',\\n    },\\n    {\\n      title: \\'Saving and Restoring States\\',\\n      description: \\'Learn how to save and restore states.\\',\\n      href: \\'/docs/ai-sdk-rsc/saving-and-restoring-states\\',\\n    },\\n    {\\n      title: \\'Multi-step Interfaces\\',\\n      description: \\'Learn how to build multi-step interfaces.\\',\\n      href: \\'/docs/ai-sdk-rsc/multistep-interfaces\\',\\n    },\\n    {\\n      title: \\'Streaming Values\\',\\n      description: \\'Learn how to stream values with AI SDK RSC.\\',\\n      href: \\'/docs/ai-sdk-rsc/streaming-values\\',\\n    },\\n    {\\n      title: \\'Error Handling\\',\\n      description: \\'Learn how to handle errors.\\',\\n      href: \\'/docs/ai-sdk-rsc/error-handling\\',\\n    },\\n    {\\n      title: \\'Authentication\\',\\n      description: \\'Learn how to authenticate users.\\',\\n      href: \\'/docs/ai-sdk-rsc/authentication\\',\\n    },\\n  ]}\\n/>\\n', children=[])]), DocItem(origPath=Path('06-advanced'), name='06-advanced', displayName='06-advanced', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('06-advanced/01-prompt-engineering.mdx'), name='01-prompt-engineering.mdx', displayName='01-prompt-engineering.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Prompt Engineering\\ndescription: Learn how to engineer prompts for LLMs with the AI SDK\\n---\\n\\n# Prompt Engineering\\n\\n## What is a Large Language Model (LLM)?\\n\\nA Large Language Model is essentially a prediction engine that takes a sequence of words as input and aims to predict the most likely sequence to follow. It does this by assigning probabilities to potential next sequences and then selecting one. The model continues to generate sequences until it meets a specified stopping criterion.\\n\\nThese models learn by training on massive text corpuses, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well. However, it\\'s crucial to understand that the generated sequences, while often seeming plausible, can sometimes be random and not grounded in reality. As these models become more accurate, many surprising abilities and applications emerge.\\n\\n## What is a prompt?\\n\\nPrompts are the starting points for LLMs. They are the inputs that trigger the model to generate text. The scope of prompt engineering involves not just crafting these prompts but also understanding related concepts such as hidden prompts, tokens, token limits, and the potential for prompt hacking, which includes phenomena like jailbreaks and leaks.\\n\\n## Why is prompt engineering needed?\\n\\nPrompt engineering currently plays a pivotal role in shaping the responses of LLMs. It allows us to tweak the model to respond more effectively to a broader range of queries. This includes the use of techniques like semantic search, command grammars, and the ReActive model architecture. The performance, context window, and cost of LLMs varies between models and model providers which adds further constraints to the mix. For example, the GPT-4 model is more expensive than GPT-3.5-turbo and significantly slower, but it can also be more effective at certain tasks. And so, like many things in software engineering, there is a trade-offs between cost and performance.\\n\\nTo assist with comparing and tweaking LLMs, we\\'ve built an AI playground that allows you to compare the performance of different models side-by-side online. When you\\'re ready, you can even generate code with the AI SDK to quickly use your prompt and your selected model into your own applications.\\n\\n## Example: Build a Slogan Generator\\n\\n### Start with an instruction\\n\\nImagine you want to build a slogan generator for marketing campaigns. Creating catchy slogans isn\\'t always straightforward!\\n\\nFirst, you\\'ll need a prompt that makes it clear what you want. Let\\'s start with an instruction. Submit this prompt to generate your first completion.\\n\\n<InlinePrompt initialInput=\"Create a slogan for a coffee shop.\" />\\n\\nNot bad! Now, try making your instruction more specific.\\n\\n<InlinePrompt initialInput=\"Create a slogan for an organic coffee shop.\" />\\n\\nIntroducing a single descriptive term to our prompt influences the completion. Essentially, crafting your prompt is the means by which you \"instruct\" or \"program\" the model.\\n\\n### Include examples\\n\\nClear instructions are key for quality outcomes, but that might not always be enough. Let\\'s try to enhance your instruction further.\\n\\n<InlinePrompt initialInput=\"Create three slogans for a coffee shop with live music.\" />\\n\\nThese slogans are fine, but could be even better. It appears the model overlooked the \\'live\\' part in our prompt. Let\\'s change it slightly to generate more appropriate suggestions.\\n\\nOften, it\\'s beneficial to both demonstrate and tell the model your requirements. Incorporating examples in your prompt can aid in conveying patterns or subtleties. Test this prompt that carries a few examples.\\n\\n<InlinePrompt\\n  initialInput={`Create three slogans for a business with unique features.\\n\\nBusiness: Bookstore with cats\\nSlogans: \"Purr-fect Pages\", \"Books and Whiskers\", \"Novels and Nuzzles\"\\nBusiness: Gym with rock climbing\\nSlogans: \"Peak Performance\", \"Reach New Heights\", \"Climb Your Way Fit\"\\nBusiness: Coffee shop with live music\\nSlogans:`}\\n/>\\n\\nGreat! Incorporating examples of expected output for a certain input prompted the model to generate the kind of names we aimed for.\\n\\n### Tweak your settings\\n\\nApart from designing prompts, you can influence completions by tweaking model settings. A crucial setting is the **temperature**.\\n\\nYou might have seen that the same prompt, when repeated, yielded the same or nearly the same completions. This happens when your temperature is at 0.\\n\\nAttempt to re-submit the identical prompt a few times with temperature set to 1.\\n\\n<InlinePrompt\\n  initialInput={`Create three slogans for a business with unique features.\\n\\nBusiness: Bookstore with cats\\nSlogans: \"Purr-fect Pages\", \"Books and Whiskers\", \"Novels and Nuzzles\"\\nBusiness: Gym with rock climbing\\nSlogans: \"Peak Performance\", \"Reach New Heights\", \"Climb Your Way Fit\"\\nBusiness: Coffee shop with live music\\nSlogans:`}\\nshowTemp={true}\\ninitialTemperature={1}\\n/>\\n\\nNotice the difference? With a temperature above 0, the same prompt delivers varied completions each time.\\n\\nKeep in mind that the model forecasts the text most likely to follow the preceding text. Temperature, a value from 0 to 1, essentially governs the model\\'s confidence level in making these predictions. A lower temperature implies lesser risks, leading to more precise and deterministic completions. A higher temperature yields a broader range of completions.\\n\\nFor your slogan generator, you might want a large pool of name suggestions. A moderate temperature of 0.6 should serve well.\\n\\n## Recommended Resources\\n\\nPrompt Engineering is evolving rapidly, with new methods and research papers surfacing every week. Here are some resources that we\\'ve found useful for learning about and experimenting with prompt engineering:\\n\\n- [The Vercel AI Playground](/playground)\\n- [Brex Prompt Engineering](https://github.com/brexhq/prompt-engineering)\\n- [Prompt Engineering Guide by Dair AI](https://www.promptingguide.ai/)\\n', children=[]), DocItem(origPath=Path('06-advanced/02-stopping-streams.mdx'), name='02-stopping-streams.mdx', displayName='02-stopping-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stopping Streams\\ndescription: Learn how to cancel streams with the AI SDK\\n---\\n\\n# Stopping Streams\\n\\nCancelling ongoing streams is often needed.\\nFor example, users might want to stop a stream when they realize that the response is not what they want.\\n\\nThe different parts of the AI SDK support cancelling streams in different ways.\\n\\n## AI SDK Core\\n\\nThe AI SDK functions have an `abortSignal` argument that you can use to cancel a stream.\\nYou would use this if you want to cancel a stream from the server side to the LLM API, e.g. by\\nforwarding the `abortSignal` from the request.\\n\\n```tsx highlight=\"10,11,12-16\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt,\\n    // forward the abort signal:\\n    abortSignal: req.signal,\\n    onAbort: ({ steps }) => {\\n      // Handle cleanup when stream is aborted\\n      console.log(\\'Stream aborted after\\', steps.length, \\'steps\\');\\n      // Persist partial results to database\\n    },\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## AI SDK UI\\n\\nThe hooks, e.g. `useChat` or `useCompletion`, provide a `stop` helper function that can be used to cancel a stream.\\nThis will cancel the stream from the client side to the server.\\n\\n<Note type=\"warning\">\\n  Stream abort functionality is not compatible with stream resumption. If you\\'re\\n  using `resume: true` in `useChat`, the abort functionality will break the\\n  resumption mechanism. Choose either abort or resume functionality, but not\\n  both.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"9,18-20\"\\n\\'use client\\';\\n\\nimport { useCompletion } from \\'@ai-sdk/react\\';\\n\\nexport default function Chat() {\\n  const { input, completion, stop, status, handleSubmit, handleInputChange } =\\n    useCompletion();\\n\\n  return (\\n    <div>\\n      {(status === \\'submitted\\' || status === \\'streaming\\') && (\\n        <button type=\"button\" onClick={() => stop()}>\\n          Stop\\n        </button>\\n      )}\\n      {completion}\\n      <form onSubmit={handleSubmit}>\\n        <input value={input} onChange={handleInputChange} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n## Handling stream abort cleanup\\n\\nWhen streams are aborted, you may need to perform cleanup operations such as persisting partial results or cleaning up resources. The `onAbort` callback provides a way to handle these scenarios on the server side.\\n\\nUnlike `onFinish`, which is called when a stream completes normally, `onAbort` is specifically called when a stream is aborted via `AbortSignal`. This distinction allows you to handle normal completion and aborted streams differently.\\n\\n<Note>\\n  For UI message streams (`toUIMessageStreamResponse`), the `onFinish` callback\\n  also receives an `isAborted` parameter that indicates whether the stream was\\n  aborted. This allows you to handle both completion and abort scenarios in a\\n  single callback.\\n</Note>\\n\\n```tsx highlight=\"8-12\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a long story...\\',\\n  abortSignal: controller.signal,\\n  onAbort: ({ steps }) => {\\n    // Called when stream is aborted - persist partial results\\n    await savePartialResults(steps);\\n    await logAbortEvent(steps.length);\\n  },\\n  onFinish: ({ steps, totalUsage }) => {\\n    // Called when stream completes normally\\n    await saveFinalResults(steps, totalUsage);\\n  },\\n});\\n```\\n\\nThe `onAbort` callback receives:\\n\\n- `steps`: Array of all completed steps before the abort occurred\\n\\nThis is particularly useful for:\\n\\n- Persisting partial conversation history to database\\n- Saving partial progress for later continuation\\n- Cleaning up server-side resources or connections\\n- Logging abort events for analytics\\n\\nYou can also handle abort events directly in the stream using the `abort` stream part:\\n\\n```tsx highlight=\"8-12\"\\nfor await (const part of result.fullStream) {\\n  switch (part.type) {\\n    case \\'text-delta\\':\\n      // Handle text delta content\\n      break;\\n    case \\'abort\\':\\n      // Handle abort event directly in stream\\n      console.log(\\'Stream was aborted\\');\\n      break;\\n    // ... other cases\\n  }\\n}\\n```\\n\\n## UI Message Streams\\n\\nWhen using `toUIMessageStreamResponse`, you need to handle stream abortion slightly differently. The `onFinish` callback receives an `isAborted` parameter, and you should pass the `consumeStream` function to ensure proper abort handling:\\n\\n```tsx highlight=\"5,19,20-24,26\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport {\\n  consumeStream,\\n  convertToModelMessages,\\n  streamText,\\n  UIMessage,\\n} from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    abortSignal: req.signal,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onFinish: async ({ isAborted }) => {\\n      if (isAborted) {\\n        console.log(\\'Stream was aborted\\');\\n        // Handle abort-specific cleanup\\n      } else {\\n        console.log(\\'Stream completed normally\\');\\n        // Handle normal completion\\n      }\\n    },\\n    consumeSseStream: consumeStream,\\n  });\\n}\\n```\\n\\nThe `consumeStream` function is necessary for proper abort handling in UI message streams. It ensures that the stream is properly consumed even when aborted, preventing potential memory leaks or hanging connections.\\n\\n## AI SDK RSC\\n\\n<Note type=\"warning\">\\n  The AI SDK RSC does not currently support stopping streams.\\n</Note>\\n', children=[]), DocItem(origPath=Path('06-advanced/03-backpressure.mdx'), name='03-backpressure.mdx', displayName='03-backpressure.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Backpressure\\ndescription: How to handle backpressure and cancellation when working with the AI SDK\\n---\\n\\n# Stream Back-pressure and Cancellation\\n\\nThis page focuses on understanding back-pressure and cancellation when working with streams. You do not need to know this information to use the AI SDK, but for those interested, it offers a deeper dive on why and how the SDK optimally streams responses.\\n\\nIn the following sections, we\\'ll explore back-pressure and cancellation in the context of a simple example program. We\\'ll discuss the issues that can arise from an eager approach and demonstrate how a lazy approach can resolve them.\\n\\n## Back-pressure and Cancellation with Streams\\n\\nLet\\'s begin by setting up a simple example program:\\n\\n```jsx\\n// A generator that will yield positive integers\\nasync function* integers() {\\n  let i = 1;\\n  while (true) {\\n    console.log(`yielding ${i}`);\\n    yield i++;\\n\\n    await sleep(100);\\n  }\\n}\\nfunction sleep(ms) {\\n  return new Promise(resolve => setTimeout(resolve, ms));\\n}\\n\\n// Wraps a generator into a ReadableStream\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async start(controller) {\\n      for await (const v of iterator) {\\n        controller.enqueue(v);\\n      }\\n      controller.close();\\n    },\\n  });\\n}\\n\\n// Collect data from stream\\nasync function run() {\\n  // Set up a stream of integers\\n  const stream = createStream(integers());\\n\\n  // Read values from our stream\\n  const reader = stream.getReader();\\n  for (let i = 0; i < 10_000; i++) {\\n    // we know our stream is infinite, so there\\'s no need to check `done`.\\n    const { value } = await reader.read();\\n    console.log(`read ${value}`);\\n\\n    await sleep(1_000);\\n  }\\n}\\nrun();\\n```\\n\\nIn this example, we create an async-generator that yields positive integers, a `ReadableStream` that wraps our integer generator, and a reader which will read values out of our stream. Notice, too, that our integer generator logs out `\"yielding ${i}\"`, and our reader logs out `\"read ${value}\"`. Both take an arbitrary amount of time to process data, represented with a 100ms sleep in our generator, and a 1sec sleep in our reader.\\n\\n## Back-pressure\\n\\nIf you were to run this program, you\\'d notice something funny. We\\'ll see roughly 10 \"yield\" logs for every \"read\" log. This might seem obvious, the generator can push values 10x faster than the reader can pull them out. But it represents a problem, our `stream` has to maintain an ever expanding queue of items that have been pushed in but not pulled out.\\n\\nThe problem stems from the way we wrap our generator into a stream. Notice the use of `for await (…)` inside our `start` handler. This is an **eager** for-loop, and it is constantly running to get the next value from our generator to be enqueued in our stream. This means our stream does not respect back-pressure, the signal from the consumer to the producer that more values aren\\'t needed _yet_. We\\'ve essentially spawned a thread that will perpetually push more data into the stream, one that runs as fast as possible to push new data immediately. Worse, there\\'s no way to signal to this thread to stop running when we don\\'t need additional data.\\n\\nTo fix this, `ReadableStream` allows a `pull` handler. `pull` is called every time the consumer attempts to read more data from our stream (if there\\'s no data already queued internally). But it\\'s not enough to just move the `for await(…)` into `pull`, we also need to convert from an eager enqueuing to a **lazy** one. By making these 2 changes, we\\'ll be able to react to the consumer. If they need more data, we can easily produce it, and if they don\\'t, then we don\\'t need to spend any time doing unnecessary work.\\n\\n```jsx\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async pull(controller) {\\n      const { value, done } = await iterator.next();\\n\\n      if (done) {\\n        controller.close();\\n      } else {\\n        controller.enqueue(value);\\n      }\\n    },\\n  });\\n}\\n```\\n\\nOur `createStream` is a little more verbose now, but the new code is important. First, we need to manually call our `iterator.next()` method. This returns a `Promise` for an object with the type signature `{ done: boolean, value: T }`. If `done` is `true`, then we know that our iterator won\\'t yield any more values and we must `close` the stream (this allows the consumer to know that the stream is also finished producing values). Else, we need to `enqueue` our newly produced value.\\n\\nWhen we run this program, we see that our \"yield\" and \"read\" logs are now paired. We\\'re no longer yielding 10x integers for every read! And, our stream now only needs to maintain 1 item in its internal buffer. We\\'ve essentially given control to the consumer, so that it\\'s responsible for producing new values as it needs it. Neato!\\n\\n## Cancellation\\n\\nLet\\'s go back to our initial eager example, with 1 small edit. Now instead of reading 10,000 integers, we\\'re only going to read 3:\\n\\n```jsx\\n// A generator that will yield positive integers\\nasync function* integers() {\\n  let i = 1;\\n  while (true) {\\n    console.log(`yielding ${i}`);\\n    yield i++;\\n\\n    await sleep(100);\\n  }\\n}\\nfunction sleep(ms) {\\n  return new Promise(resolve => setTimeout(resolve, ms));\\n}\\n\\n// Wraps a generator into a ReadableStream\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async start(controller) {\\n      for await (const v of iterator) {\\n        controller.enqueue(v);\\n      }\\n      controller.close();\\n    },\\n  });\\n}\\n// Collect data from stream\\nasync function run() {\\n  // Set up a stream that of integers\\n  const stream = createStream(integers());\\n\\n  // Read values from our stream\\n  const reader = stream.getReader();\\n  // We\\'re only reading 3 items this time:\\n  for (let i = 0; i < 3; i++) {\\n    // we know our stream is infinite, so there\\'s no need to check `done`.\\n    const { value } = await reader.read();\\n    console.log(`read ${value}`);\\n\\n    await sleep(1000);\\n  }\\n}\\nrun();\\n```\\n\\nWe\\'re back to yielding 10x the number of values read. But notice now, after we\\'ve read 3 values, we\\'re continuing to yield new values. We know that our reader will never read another value, but our stream doesn\\'t! The eager `for await (…)` will continue forever, loudly enqueuing new values into our stream\\'s buffer and increasing our memory usage until it consumes all available program memory.\\n\\nThe fix to this is exactly the same: use `pull` and manual iteration. By producing values _**lazily**_, we tie the lifetime of our integer generator to the lifetime of the reader. Once the reads stop, the yields will stop too:\\n\\n```jsx\\n// Wraps a generator into a ReadableStream\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async pull(controller) {\\n      const { value, done } = await iterator.next();\\n\\n      if (done) {\\n        controller.close();\\n      } else {\\n        controller.enqueue(value);\\n      }\\n    },\\n  });\\n}\\n```\\n\\nSince the solution is the same as implementing back-pressure, it shows that they\\'re just 2 facets of the same problem: Pushing values into a stream should be done **lazily**, and doing it eagerly results in expected problems.\\n\\n## Tying Stream Laziness to AI Responses\\n\\nNow let\\'s imagine you\\'re integrating AIBot service into your product. Users will be able to prompt \"count from 1 to infinity\", the browser will fetch your AI API endpoint, and your servers connect to AIBot to get a response. But \"infinity\" is, well, infinite. The response will never end!\\n\\nAfter a few seconds, the user gets bored and navigates away. Or maybe you\\'re doing local development and a hot-module reload refreshes your page. The browser will have ended its connection to the API endpoint, but will your server end its connection with AIBot?\\n\\nIf you used the eager `for await (...)` approach, then the connection is still running and your server is asking for more and more data from AIBot. Our server spawned a \"thread\" and there\\'s no signal when we can end the eager pulls. Eventually, the server is going to run out of memory (remember, there\\'s no active fetch connection to read the buffering responses and free them).\\n\\n{/* When we started writing the streaming code for the AI SDK, we confirm aborting a fetch will end a streamed response from Next.js */}\\n\\nWith the lazy approach, this is taken care of for you. Because the stream will only request new data from AIBot when the consumer requests it, navigating away from the page naturally frees all resources. The fetch connection aborts and the server can clean up the response. The `ReadableStream` tied to that response can now be garbage collected. When that happens, the connection it holds to AIBot can then be freed.\\n', children=[]), DocItem(origPath=Path('06-advanced/04-caching.mdx'), name='04-caching.mdx', displayName='04-caching.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Caching\\ndescription: How to handle caching when working with the AI SDK\\n---\\n\\n# Caching Responses\\n\\nDepending on the type of application you\\'re building, you may want to cache the responses you receive from your AI provider, at least temporarily.\\n\\n## Using Language Model Middleware (Recommended)\\n\\nThe recommended approach to caching responses is using [language model middleware](/docs/ai-sdk-core/middleware)\\nand the [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream) function.\\n\\nLanguage model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model.\\nLet\\'s see how you can use language model middleware to cache responses.\\n\\n```ts filename=\"ai/middleware.ts\"\\nimport { Redis } from \\'@upstash/redis\\';\\nimport {\\n  type LanguageModelV3,\\n  type LanguageModelV3Middleware,\\n  type LanguageModelV3StreamPart,\\n  simulateReadableStream,\\n} from \\'ai\\';\\n\\nconst redis = new Redis({\\n  url: process.env.KV_URL,\\n  token: process.env.KV_TOKEN,\\n});\\n\\nexport const cacheMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    const cacheKey = JSON.stringify(params);\\n\\n    const cached = (await redis.get(cacheKey)) as Awaited<\\n      ReturnType<LanguageModelV3[\\'doGenerate\\']>\\n    > | null;\\n\\n    if (cached !== null) {\\n      return {\\n        ...cached,\\n        response: {\\n          ...cached.response,\\n          timestamp: cached?.response?.timestamp\\n            ? new Date(cached?.response?.timestamp)\\n            : undefined,\\n        },\\n      };\\n    }\\n\\n    const result = await doGenerate();\\n\\n    redis.set(cacheKey, result);\\n\\n    return result;\\n  },\\n  wrapStream: async ({ doStream, params }) => {\\n    const cacheKey = JSON.stringify(params);\\n\\n    // Check if the result is in the cache\\n    const cached = await redis.get(cacheKey);\\n\\n    // If cached, return a simulated ReadableStream that yields the cached result\\n    if (cached !== null) {\\n      // Format the timestamps in the cached response\\n      const formattedChunks = (cached as LanguageModelV3StreamPart[]).map(p => {\\n        if (p.type === \\'response-metadata\\' && p.timestamp) {\\n          return { ...p, timestamp: new Date(p.timestamp) };\\n        } else return p;\\n      });\\n      return {\\n        stream: simulateReadableStream({\\n          initialDelayInMs: 0,\\n          chunkDelayInMs: 10,\\n          chunks: formattedChunks,\\n        }),\\n      };\\n    }\\n\\n    // If not cached, proceed with streaming\\n    const { stream, ...rest } = await doStream();\\n\\n    const fullResponse: LanguageModelV3StreamPart[] = [];\\n\\n    const transformStream = new TransformStream<\\n      LanguageModelV3StreamPart,\\n      LanguageModelV3StreamPart\\n    >({\\n      transform(chunk, controller) {\\n        fullResponse.push(chunk);\\n        controller.enqueue(chunk);\\n      },\\n      flush() {\\n        // Store the full response in the cache after streaming is complete\\n        redis.set(cacheKey, fullResponse);\\n      },\\n    });\\n\\n    return {\\n      stream: stream.pipeThrough(transformStream),\\n      ...rest,\\n    };\\n  },\\n};\\n```\\n\\n<Note>\\n  This example uses `@upstash/redis` to store and retrieve the assistant\\'s\\n  responses but you can use any KV storage provider you would like.\\n</Note>\\n\\n`LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`. `wrapGenerate` is called when using [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`generateObject`](/docs/reference/ai-sdk-core/generate-object), while `wrapStream` is called when using [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object).\\n\\nFor `wrapGenerate`, you can cache the response directly. Instead, for `wrapStream`, you cache an array of the stream parts, which can then be used with [`simulateReadableStream`](/docs/ai-sdk-core/testing#simulate-data-stream-protocol-responses) function to create a simulated `ReadableStream` that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the `initialDelayInMs` and `chunkDelayInMs` parameters of `simulateReadableStream`.\\n\\nYou can see a full example of caching with Redis in a Next.js application in our [Caching Middleware Recipe](/cookbook/next/caching-middleware).\\n\\n## Using Lifecycle Callbacks\\n\\nAlternatively, each AI SDK Core function has special lifecycle callbacks you can use. The one of interest is likely `onFinish`, which is called when the generation is complete. This is where you can cache the full response.\\n\\nHere\\'s an example of how you can implement caching using Vercel KV and Next.js to cache the OpenAI response for 1 hour:\\n\\nThis example uses [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted) and Next.js to cache the response for 1 hour.\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { formatDataStreamPart, streamText, UIMessage } from \\'ai\\';\\nimport { Redis } from \\'@upstash/redis\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nconst redis = new Redis({\\n  url: process.env.KV_URL,\\n  token: process.env.KV_TOKEN,\\n});\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  // come up with a key based on the request:\\n  const key = JSON.stringify(messages);\\n\\n  // Check if we have a cached response\\n  const cached = await redis.get(key);\\n  if (cached != null) {\\n    return new Response(formatDataStreamPart(\\'text\\', cached), {\\n      status: 200,\\n      headers: { \\'Content-Type\\': \\'text/plain\\' },\\n    });\\n  }\\n\\n  // Call the language model:\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    async onFinish({ text }) {\\n      // Cache the response text:\\n      await redis.set(key, text);\\n      await redis.expire(key, 60 * 60);\\n    },\\n  });\\n\\n  // Respond with the stream\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n', children=[]), DocItem(origPath=Path('06-advanced/05-multiple-streamables.mdx'), name='05-multiple-streamables.mdx', displayName='05-multiple-streamables.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Multiple Streamables\\ndescription: Learn to handle multiple streamables in your application.\\n---\\n\\n# Multiple Streams\\n\\n## Multiple Streamable UIs\\n\\nThe AI SDK RSC APIs allow you to compose and return any number of streamable UIs, along with other data, in a single request. This can be useful when you want to decouple the UI into smaller components and stream them separately.\\n\\n```tsx file='app/actions.tsx'\\n'use server';\\n\\nimport { createStreamableUI } from '@ai-sdk/rsc';\\n\\nexport async function getWeather() {\\n  const weatherUI = createStreamableUI();\\n  const forecastUI = createStreamableUI();\\n\\n  weatherUI.update(<div>Loading weather...</div>);\\n  forecastUI.update(<div>Loading forecast...</div>);\\n\\n  getWeatherData().then(weatherData => {\\n    weatherUI.done(<div>{weatherData}</div>);\\n  });\\n\\n  getForecastData().then(forecastData => {\\n    forecastUI.done(<div>{forecastData}</div>);\\n  });\\n\\n  // Return both streamable UIs and other data fields.\\n  return {\\n    requestedAt: Date.now(),\\n    weather: weatherUI.value,\\n    forecast: forecastUI.value,\\n  };\\n}\\n```\\n\\nThe client side code is similar to the previous example, but the [tool call](/docs/ai-sdk-core/tools-and-tool-calling) will return the new data structure with the weather and forecast UIs. Depending on the speed of getting weather and forecast data, these two components might be updated independently.\\n\\n## Nested Streamable UIs\\n\\nYou can stream UI components within other UI components. This allows you to create complex UIs that are built up from smaller, reusable components. In the example below, we pass a `historyChart` streamable as a prop to a `StockCard` component. The StockCard can render the `historyChart` streamable, and it will automatically update as the server responds with new data.\\n\\n```tsx file='app/actions.tsx'\\nasync function getStockHistoryChart({ symbol: string }) {\\n  'use server';\\n\\n  const ui = createStreamableUI(<Spinner />);\\n\\n  // We need to wrap this in an async IIFE to avoid blocking.\\n  (async () => {\\n    const price = await getStockPrice({ symbol });\\n\\n    // Show a spinner as the history chart for now.\\n    const historyChart = createStreamableUI(<Spinner />);\\n    ui.done(<StockCard historyChart={historyChart.value} price={price} />);\\n\\n    // Getting the history data and then update that part of the UI.\\n    const historyData = await fetch('https://my-stock-data-api.com');\\n    historyChart.done(<HistoryChart data={historyData} />);\\n  })();\\n\\n  return ui;\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('06-advanced/06-rate-limiting.mdx'), name='06-rate-limiting.mdx', displayName='06-rate-limiting.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Rate Limiting\\ndescription: Learn how to rate limit your application.\\n---\\n\\n# Rate Limiting\\n\\nRate limiting helps you protect your APIs from abuse. It involves setting a\\nmaximum threshold on the number of requests a client can make within a\\nspecified timeframe. This simple technique acts as a gatekeeper,\\npreventing excessive usage that can degrade service performance and incur\\nunnecessary costs.\\n\\n## Rate Limiting with Vercel KV and Upstash Ratelimit\\n\\nIn this example, you will protect an API endpoint using [Vercel KV](https://vercel.com/storage/kv)\\nand [Upstash Ratelimit](https://github.com/upstash/ratelimit).\\n\\n```tsx filename='app/api/generate/route.ts'\\nimport kv from '@vercel/kv';\\nimport { openai } from '@ai-sdk/openai';\\nimport { streamText } from 'ai';\\nimport { Ratelimit } from '@upstash/ratelimit';\\nimport { NextRequest } from 'next/server';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\n// Create Rate limit\\nconst ratelimit = new Ratelimit({\\n  redis: kv,\\n  limiter: Ratelimit.fixedWindow(5, '30s'),\\n});\\n\\nexport async function POST(req: NextRequest) {\\n  // call ratelimit with request ip\\n  const ip = req.ip ?? 'ip';\\n  const { success, remaining } = await ratelimit.limit(ip);\\n\\n  // block the request if unsuccessfull\\n  if (!success) {\\n    return new Response('Ratelimited!', { status: 429 });\\n  }\\n\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Simplify API Protection\\n\\nWith Vercel KV and Upstash Ratelimit, it is possible to protect your APIs\\nfrom such attacks with ease. To learn more about how Ratelimit works and\\nhow it can be configured to your needs, see [Ratelimit Documentation](https://upstash.com/docs/oss/sdks/ts/ratelimit/overview).\\n\", children=[]), DocItem(origPath=Path('06-advanced/07-rendering-ui-with-language-models.mdx'), name='07-rendering-ui-with-language-models.mdx', displayName='07-rendering-ui-with-language-models.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Rendering UI with Language Models\\ndescription: Rendering UI with Language Models\\n---\\n\\n# Rendering User Interfaces with Language Models\\n\\nLanguage models generate text, so at first it may seem like you would only need to render text in your application.\\n\\n```tsx highlight=\"16\" filename=\"app/actions.tsx\"\\nconst text = generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system: \\'You are a friendly assistant\\',\\n  prompt: \\'What is the weather in SF?\\',\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      parameters: z.object({\\n        city: z.string().describe(\\'The city to get the weather for\\'),\\n        unit: z\\n          .enum([\\'C\\', \\'F\\'])\\n          .describe(\\'The unit to display the temperature in\\'),\\n      }),\\n      execute: async ({ city, unit }) => {\\n        const weather = getWeather({ city, unit });\\n        return `It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;\\n      },\\n    },\\n  },\\n});\\n```\\n\\nAbove, the language model is passed a [tool](/docs/ai-sdk-core/tools-and-tool-calling) called `getWeather` that returns the weather information as text. However, instead of returning text, if you return a JSON object that represents the weather information, you can use it to render a React component instead.\\n\\n```tsx highlight=\"18-23\" filename=\"app/action.ts\"\\nconst text = generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system: \\'You are a friendly assistant\\',\\n  prompt: \\'What is the weather in SF?\\',\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      parameters: z.object({\\n        city: z.string().describe(\\'The city to get the weather for\\'),\\n        unit: z\\n          .enum([\\'C\\', \\'F\\'])\\n          .describe(\\'The unit to display the temperature in\\'),\\n      }),\\n      execute: async ({ city, unit }) => {\\n        const weather = getWeather({ city, unit });\\n        const { temperature, unit, description, forecast } = weather;\\n\\n        return {\\n          temperature,\\n          unit,\\n          description,\\n          forecast,\\n        };\\n      },\\n    },\\n  },\\n});\\n```\\n\\nNow you can use the object returned by the `getWeather` function to conditionally render a React component `<WeatherCard/>` that displays the weather information by passing the object as props.\\n\\n```tsx filename=\"app/page.tsx\"\\nreturn (\\n  <div>\\n    {messages.map(message => {\\n      if (message.role === \\'function\\') {\\n        const { name, content } = message\\n        const { temperature, unit, description, forecast } = content;\\n\\n        return (\\n          <WeatherCard\\n            weather={{\\n              temperature: 47,\\n              unit: \\'F\\',\\n              description: \\'sunny\\'\\n              forecast,\\n            }}\\n          />\\n        )\\n      }\\n    })}\\n  </div>\\n)\\n```\\n\\nHere\\'s a little preview of what that might look like.\\n\\n<div className=\"not-prose flex flex-col2\">\\n  <CardPlayer\\n    type=\"weather\"\\n    title=\"Weather\"\\n    description=\"An example of an assistant that renders the weather information in a streamed component.\"\\n  />\\n</div>\\n\\nRendering interfaces as part of language model generations elevates the user experience of your application, allowing people to interact with language models beyond text.\\n\\nThey also make it easier for you to interpret [sequential tool calls](/docs/ai-sdk-rsc/multistep-interfaces) that take place in multiple steps and help identify and debug where the model reasoned incorrectly.\\n\\n## Rendering Multiple User Interfaces\\n\\nTo recap, an application has to go through the following steps to render user interfaces as part of model generations:\\n\\n1. The user prompts the language model.\\n2. The language model generates a response that includes a tool call.\\n3. The tool call returns a JSON object that represents the user interface.\\n4. The response is sent to the client.\\n5. The client receives the response and checks if the latest message was a tool call.\\n6. If it was a tool call, the client renders the user interface based on the JSON object returned by the tool call.\\n\\nMost applications have multiple tools that are called by the language model, and each tool can return a different user interface.\\n\\nFor example, a tool that searches for courses can return a list of courses, while a tool that searches for people can return a list of people. As this list grows, the complexity of your application will grow as well and it can become increasingly difficult to manage these user interfaces.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n{\\n  message.role === \\'tool\\' ? (\\n    message.name === \\'api-search-course\\' ? (\\n      <Courses courses={message.content} />\\n    ) : message.name === \\'api-search-profile\\' ? (\\n      <People people={message.content} />\\n    ) : message.name === \\'api-meetings\\' ? (\\n      <Meetings meetings={message.content} />\\n    ) : message.name === \\'api-search-building\\' ? (\\n      <Buildings buildings={message.content} />\\n    ) : message.name === \\'api-events\\' ? (\\n      <Events events={message.content} />\\n    ) : message.name === \\'api-meals\\' ? (\\n      <Meals meals={message.content} />\\n    ) : null\\n  ) : (\\n    <div>{message.content}</div>\\n  );\\n}\\n```\\n\\n## Rendering User Interfaces on the Server\\n\\nThe **AI SDK RSC (`@ai-sdk/rsc`)** takes advantage of RSCs to solve the problem of managing all your React components on the client side, allowing you to render React components on the server and stream them to the client.\\n\\nRather than conditionally rendering user interfaces on the client based on the data returned by the language model, you can directly stream them from the server during a model generation.\\n\\n```tsx highlight=\"3,22-31,38\" filename=\"app/action.ts\"\\nimport { createStreamableUI } from \\'@ai-sdk/rsc\\'\\n\\nconst uiStream = createStreamableUI();\\n\\nconst text = generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system: \\'you are a friendly assistant\\'\\n  prompt: \\'what is the weather in SF?\\'\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      parameters: z.object({\\n        city: z.string().describe(\\'The city to get the weather for\\'),\\n        unit: z\\n          .enum([\\'C\\', \\'F\\'])\\n          .describe(\\'The unit to display the temperature in\\')\\n      }),\\n      execute: async ({ city, unit }) => {\\n        const weather = getWeather({ city, unit })\\n        const { temperature, unit, description, forecast } = weather\\n\\n        uiStream.done(\\n          <WeatherCard\\n            weather={{\\n              temperature: 47,\\n              unit: \\'F\\',\\n              description: \\'sunny\\'\\n              forecast,\\n            }}\\n          />\\n        )\\n      }\\n    }\\n  }\\n})\\n\\nreturn {\\n  display: uiStream.value\\n}\\n```\\n\\nThe [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) function belongs to the `@ai-sdk/rsc` module and creates a stream that can send React components to the client.\\n\\nOn the server, you render the `<WeatherCard/>` component with the props passed to it, and then stream it to the client. On the client side, you only need to render the UI that is streamed from the server.\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"4\"\\nreturn (\\n  <div>\\n    {messages.map(message => (\\n      <div>{message.display}</div>\\n    ))}\\n  </div>\\n);\\n```\\n\\nNow the steps involved are simplified:\\n\\n1. The user prompts the language model.\\n2. The language model generates a response that includes a tool call.\\n3. The tool call renders a React component along with relevant props that represent the user interface.\\n4. The response is streamed to the client and rendered directly.\\n\\n> **Note:** You can also render text on the server and stream it to the client using React Server Components. This way, all operations from language model generation to UI rendering can be done on the server, while the client only needs to render the UI that is streamed from the server.\\n\\nCheck out this [example](/examples/next-app/interface/stream-component-updates) for a full illustration of how to stream component updates with React Server Components in Next.js App Router.\\n', children=[]), DocItem(origPath=Path('06-advanced/08-model-as-router.mdx'), name='08-model-as-router.mdx', displayName='08-model-as-router.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Language Models as Routers\\ndescription: Generative User Interfaces and Language Models as Routers\\n---\\n\\n# Generative User Interfaces\\n\\nSince language models can render user interfaces as part of their generations, the resulting model generations are referred to as generative user interfaces.\\n\\nIn this section we will learn more about generative user interfaces and their impact on the way AI applications are built.\\n\\n## Deterministic Routes and Probabilistic Routing\\n\\nGenerative user interfaces are not deterministic in nature because they depend on the model\\'s generation output. Since these generations are probabilistic in nature, it is possible for every user query to result in a different user interface.\\n\\nUsers expect their experience using your application to be predictable, so non-deterministic user interfaces can sound like a bad idea at first. However, language models can be set up to limit their generations to a particular set of outputs using their ability to call functions.\\n\\nWhen language models are provided with a set of function definitions and instructed to execute any of them based on user query, they do either one of the following things:\\n\\n- Execute a function that is most relevant to the user query.\\n- Not execute any function if the user query is out of bounds of the set of functions available to them.\\n\\n```tsx filename=\\'app/actions.ts\\'\\nconst sendMessage = (prompt: string) =>\\n  generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly weather assistant!\\',\\n    prompt,\\n    tools: {\\n      getWeather: {\\n        description: \\'Get the weather in a location\\',\\n        parameters: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }: { location: string }) => ({\\n          location,\\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n        }),\\n      },\\n    },\\n  });\\n\\nsendMessage(\\'What is the weather in San Francisco?\\'); // getWeather is called\\nsendMessage(\\'What is the weather in New York?\\'); // getWeather is called\\nsendMessage(\\'What events are happening in London?\\'); // No function is called\\n```\\n\\nThis way, it is possible to ensure that the generations result in deterministic outputs, while the choice a model makes still remains to be probabilistic.\\n\\nThis emergent ability exhibited by a language model to choose whether a function needs to be executed or not based on a user query is believed to be models emulating \"reasoning\".\\n\\nAs a result, the combination of language models being able to reason which function to execute as well as render user interfaces at the same time gives you the ability to build applications where language models can be used as a router.\\n\\n## Language Models as Routers\\n\\nHistorically, developers had to write routing logic that connected different parts of an application to be navigable by a user and complete a specific task.\\n\\nIn web applications today, most of the routing logic takes place in the form of routes:\\n\\n- `/login` would navigate you to a page with a login form.\\n- `/user/john` would navigate you to a page with profile details about John.\\n- `/api/events?limit=5` would display the five most recent events from an events database.\\n\\nWhile routes help you build web applications that connect different parts of an application into a seamless user experience, it can also be a burden to manage them as the complexity of applications grow.\\n\\nNext.js has helped reduce complexity in developing with routes by introducing:\\n\\n- File-based routing system\\n- Dynamic routing\\n- API routes\\n- Middleware\\n- App router, and so on...\\n\\nWith language models becoming better at reasoning, we believe that there is a future where developers only write core application specific components while models take care of routing them based on the user\\'s state in an application.\\n\\nWith generative user interfaces, the language model decides which user interface to render based on the user\\'s state in the application, giving users the flexibility to interact with your application in a conversational manner instead of navigating through a series of predefined routes.\\n\\n### Routing by parameters\\n\\nFor routes like:\\n\\n- `/profile/[username]`\\n- `/search?q=[query]`\\n- `/media/[id]`\\n\\nthat have segments dependent on dynamic data, the language model can generate the correct parameters and render the user interface.\\n\\nFor example, when you\\'re in a search application, you can ask the language model to search for artworks from different artists. The language model will call the search function with the artist\\'s name as a parameter and render the search results.\\n\\n<div className=\"not-prose\">\\n  <CardPlayer\\n    type=\"media-search\"\\n    title=\"Media Search\"\\n    description=\"Let your users see more than words can say by rendering components directly within your search experience.\"\\n  />\\n</div>\\n\\n### Routing by sequence\\n\\nFor actions that require a sequence of steps to be completed by navigating through different routes, the language model can generate the correct sequence of routes to complete in order to fulfill the user\\'s request.\\n\\nFor example, when you\\'re in a calendar application, you can ask the language model to schedule a happy hour evening with your friends. The language model will then understand your request and will perform the right sequence of [tool calls](/docs/ai-sdk-core/tools-and-tool-calling) to:\\n\\n1. Lookup your calendar\\n2. Lookup your friends\\' calendars\\n3. Determine the best time for everyone\\n4. Search for nearby happy hour spots\\n5. Create an event and send out invites to your friends\\n\\n<div className=\"not-prose\">\\n  <CardPlayer\\n    type=\"event-planning\"\\n    title=\"Planning an Event\"\\n    description=\"The model calls functions and generates interfaces based on user intent, acting like a router.\"\\n  />\\n</div>\\n\\nJust by defining functions to lookup contacts, pull events from a calendar, and search for nearby locations, the model is able to sequentially navigate the routes for you.\\n\\nTo learn more, check out these [examples](/examples/next-app/interface) using the `streamUI` function to stream generative user interfaces to the client based on the response from the language model.\\n', children=[]), DocItem(origPath=Path('06-advanced/09-multistep-interfaces.mdx'), name='09-multistep-interfaces.mdx', displayName='09-multistep-interfaces.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Multistep Interfaces\\ndescription: Concepts behind building multistep interfaces\\n---\\n\\n# Multistep Interfaces\\n\\nMultistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.\\n\\nIn order to understand multistep interfaces, it is important to understand two concepts:\\n\\n- Tool composition\\n- Application context\\n\\n**Tool composition** is the process of combining multiple [tools](/docs/ai-sdk-core/tools-and-tool-calling) to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps.\\n\\n**Application context** refers to the state of the application at any given point in time. This includes the user\\'s input, the output of the language model, and any other relevant information.\\n\\nWhen designing multistep interfaces, you need to consider how the tools in your application can be composed together to form a coherent user experience as well as how the application context changes as the user progresses through the interface.\\n\\n## Application Context\\n\\nThe application context can be thought of as the conversation history between the user and the language model. The richer the context, the more information the model has to generate relevant responses.\\n\\nIn the context of multistep interfaces, the application context becomes even more important. This is because **the user\\'s input in one step may affect the output of the model in the next step**.\\n\\nFor example, consider a meal logging application that helps users track their daily food intake. The language model is provided with the following tools:\\n\\n- `log_meal` takes in parameters like the name of the food, the quantity, and the time of consumption to log a meal.\\n- `delete_meal` takes in the name of the meal to be deleted.\\n\\nWhen the user logs a meal, the model generates a response confirming the meal has been logged.\\n\\n```txt highlight=\"2\"\\nUser: Log a chicken shawarma for lunch.\\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\\nModel: Chicken shawarma has been logged for lunch.\\n```\\n\\nNow when the user decides to delete the meal, the model should be able to reference the previous step to identify the meal to be deleted.\\n\\n```txt highlight=\"7\"\\nUser: Log a chicken shawarma for lunch.\\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\\nModel: Chicken shawarma has been logged for lunch.\\n...\\n...\\nUser: I skipped lunch today, can you update my log?\\nTool: delete_meal(\"chicken shawarma\")\\nModel: Chicken shawarma has been deleted from your log.\\n```\\n\\nIn this example, managing the application context is important for the model to generate the correct response. The model needs to have information about the previous actions in order for it to use generate the parameters for the `delete_meal` tool.\\n\\n## Tool Composition\\n\\nTool composition is the process of combining multiple tools to create a new tool. This involves defining the inputs and outputs of each tool, as well as how they interact with each other.\\n\\nThe design of how these tools can be composed together to form a multistep interface is crucial to both the user experience of your application and the model\\'s ability to generate the correct output.\\n\\nFor example, consider a flight booking assistant that can help users book flights. The assistant can be designed to have the following tools:\\n\\n- `searchFlights`: Searches for flights based on the user\\'s query.\\n- `lookupFlight`: Looks up details of a specific flight based on the flight number.\\n- `bookFlight`: Books a flight based on the user\\'s selection.\\n\\nThe `searchFlights` tool is called when the user wants to lookup flights for a specific route. This would typically mean the tool should be able to take in parameters like the origin and destination of the flight.\\n\\nThe `lookupFlight` tool is called when the user wants to get more details about a specific flight. This would typically mean the tool should be able to take in parameters like the flight number and return information about seat availability.\\n\\nThe `bookFlight` tool is called when the user decides to book a flight. In order to identify the flight to book, the tool should be able to take in parameters like the flight number, trip date, and passenger details.\\n\\nSo the conversation between the user and the model could look like this:\\n\\n```txt highlight=\"8\"\\nUser: I want to book a flight from New York to London.\\nTool: searchFlights(\"New York\", \"London\")\\nModel: Here are the available flights from New York to London.\\nUser: I want to book flight number BA123 on 12th December for myself and my wife.\\nTool: lookupFlight(\"BA123\") -> \"4 seats available\"\\nModel: Sure, there are seats available! Can you provide the names of the passengers?\\nUser: John Doe and Jane Doe.\\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\\nModel: Your flight has been booked!\\n```\\n\\nIn the last tool call, the `bookFlight` tool does not include passenger details as it is not available in the application context. As a result, it requests the user to provide the passenger details before proceeding with the booking.\\n\\nLooking up passenger information could\\'ve been another tool that the model could\\'ve called before calling the `bookFlight` tool assuming that the user is logged into the application. This way, the model does not have to ask the user for the passenger details and can proceed with the booking.\\n\\n```txt highlight=\"5,6\"\\nUser: I want to book a flight from New York to London.\\nTool: searchFlights(\"New York\", \"London\")\\nModel: Here are the available flights from New York to London.\\nUser: I want to book flight number BA123 on 12th December for myself an my wife.\\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\\nModel: Your flight has been booked!\\n```\\n\\nThe `lookupContacts` tool is called before the `bookFlight` tool to ensure that the passenger details are available in the application context when booking the flight. This way, the model can reduce the number of steps required from the user and use its ability to call tools that populate its context and use that information to complete the booking process.\\n\\nNow, let\\'s introduce another tool called `lookupBooking` that can be used to show booking details by taking in the name of the passenger as parameter. This tool can be composed with the existing tools to provide a more complete user experience.\\n\\n```txt highlight=\"2-4\"\\nUser: What\\'s the status of my wife\\'s upcoming flight?\\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\\nTool: lookupBooking(\"Jane Doe\") -> \"BA123 confirmed\"\\nTool: lookupFlight(\"BA123\") -> \"Flight BA123 is scheduled to depart on 12th December.\"\\nModel: Your wife\\'s flight BA123 is confirmed and scheduled to depart on 12th December.\\n```\\n\\nIn this example, the `lookupBooking` tool is used to provide the user with the status of their wife\\'s upcoming flight. By composing this tool with the existing tools, the model is able to generate a response that includes the booking status and the departure date of the flight without requiring the user to provide additional information.\\n\\nAs a result, the more tools you design that can be composed together, the more complex and powerful your application can become.\\n', children=[]), DocItem(origPath=Path('06-advanced/09-sequential-generations.mdx'), name='09-sequential-generations.mdx', displayName='09-sequential-generations.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Sequential Generations\\ndescription: Learn how to implement sequential generations (\"chains\") with the AI SDK\\n---\\n\\n# Sequential Generations\\n\\nWhen working with the AI SDK, you may want to create sequences of generations (often referred to as \"chains\" or \"pipes\"), where the output of one becomes the input for the next. This can be useful for creating more complex AI-powered workflows or for breaking down larger tasks into smaller, more manageable steps.\\n\\n## Example\\n\\nIn a sequential chain, the output of one generation is directly used as input for the next generation. This allows you to create a series of dependent generations, where each step builds upon the previous one.\\n\\nHere\\'s an example of how you can implement sequential actions:\\n\\n```typescript\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nasync function sequentialActions() {\\n  // Generate blog post ideas\\n  const ideasGeneration = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Generate 10 ideas for a blog post about making spaghetti.\\',\\n  });\\n\\n  console.log(\\'Generated Ideas:\\\\n\\', ideasGeneration);\\n\\n  // Pick the best idea\\n  const bestIdeaGeneration = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: `Here are some blog post ideas about making spaghetti:\\n${ideasGeneration}\\n\\nPick the best idea from the list above and explain why it\\'s the best.`,\\n  });\\n\\n  console.log(\\'\\\\nBest Idea:\\\\n\\', bestIdeaGeneration);\\n\\n  // Generate an outline\\n  const outlineGeneration = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: `We\\'ve chosen the following blog post idea about making spaghetti:\\n${bestIdeaGeneration}\\n\\nCreate a detailed outline for a blog post based on this idea.`,\\n  });\\n\\n  console.log(\\'\\\\nBlog Post Outline:\\\\n\\', outlineGeneration);\\n}\\n\\nsequentialActions().catch(console.error);\\n```\\n\\nIn this example, we first generate ideas for a blog post, then pick the best idea, and finally create an outline based on that idea. Each step uses the output from the previous step as input for the next generation.\\n', children=[]), DocItem(origPath=Path('06-advanced/10-vercel-deployment-guide.mdx'), name='10-vercel-deployment-guide.mdx', displayName='10-vercel-deployment-guide.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Vercel Deployment Guide\\ndescription: Learn how to deploy an AI application to production on Vercel\\n---\\n\\n# Vercel Deployment Guide\\n\\nIn this guide, you will deploy an AI application to [Vercel](https://vercel.com) using [Next.js](https://nextjs.org) (App Router).\\n\\nVercel is a platform for developers that provides the tools, workflows, and infrastructure you need to build and deploy your web apps faster, without the need for additional configuration.\\n\\nVercel allows for automatic deployments on every branch push and merges onto the production branch of your GitHub, GitLab, and Bitbucket projects. It is a great option for deploying your AI application.\\n\\n## Before You Begin\\n\\nTo follow along with this guide, you will need:\\n\\n- a Vercel account\\n- an account with a Git provider (this tutorial will use [Github](https://github.com))\\n- an OpenAI API key\\n\\nThis guide will teach you how to deploy the application you built in the Next.js (App Router) quickstart tutorial to Vercel. If you haven’t completed the quickstart guide, you can start with [this repo](https://github.com/vercel-labs/ai-sdk-deployment-guide).\\n\\n## Commit Changes\\n\\nVercel offers a powerful git-centered workflow that automatically deploys your application to production every time you push to your repository’s main branch.\\n\\nBefore committing your local changes, make sure that you have a `.gitignore`. Within your `.gitignore`, ensure that you are excluding your environment variables (`.env`) and your node modules (`node_modules`).\\n\\nIf you have any local changes, you can commit them by running the following commands:\\n\\n```bash\\ngit add .\\ngit commit -m \"init\"\\n```\\n\\n## Create Git Repo\\n\\nYou can create a GitHub repository from within your terminal, or on [github.com](https://github.com/). For this tutorial, you will use the GitHub CLI ([more info here](https://cli.github.com/)).\\n\\nTo create your GitHub repository:\\n\\n1. Navigate to [github.com](http://github.com/)\\n2. In the top right corner, click the \"plus\" icon and select \"New repository\"\\n3. Pick a name for your repository (this can be anything)\\n4. Click \"Create repository\"\\n\\nOnce you have created your repository, GitHub will redirect you to your new repository.\\n\\n1. Scroll down the page and copy the commands under the title \"...or push an existing repository from the command line\"\\n2. Go back to the terminal, paste and then run the commands\\n\\nNote: if you run into the error \"error: remote origin already exists.\", this is because your local repository is still linked to the repository you cloned. To \"unlink\", you can run the following command:\\n\\n```bash\\nrm -rf .git\\ngit init\\ngit add .\\ngit commit -m \"init\"\\n```\\n\\nRerun the code snippet from the previous step.\\n\\n## Import Project in Vercel\\n\\nOn the [New Project](https://vercel.com/new) page, under the **Import Git Repository** section, select the Git provider that you would like to import your project from. Follow the prompts to sign in to your GitHub account.\\n\\nOnce you have signed in, you should see your newly created repository from the previous step in the \"Import Git Repository\" section. Click the \"Import\" button next to that project.\\n\\n### Add Environment Variables\\n\\nYour application stores uses environment secrets to store your OpenAI API key using a `.env.local` file locally in development. To add this API key to your production deployment, expand the \"Environment Variables\" section and paste in your `.env.local` file. Vercel will automatically parse your variables and enter them in the appropriate `key:value` format.\\n\\n### Deploy\\n\\nPress the **Deploy** button. Vercel will create the Project and deploy it based on the chosen configurations.\\n\\n### Enjoy the confetti!\\n\\nTo view your deployment, select the Project in the dashboard and then select the\\xa0**Domain**. This page is now visible to anyone who has the URL.\\n\\n## Considerations\\n\\nWhen deploying an AI application, there are infrastructure-related considerations to be aware of.\\n\\n### Function Duration\\n\\nIn most cases, you will call the large language model (LLM) on the server. By default, Vercel serverless functions have a maximum duration of 10 seconds on the Hobby Tier. Depending on your prompt, it can take an LLM more than this limit to complete a response. If the response is not resolved within this limit, the server will throw an error.\\n\\nYou can specify the maximum duration of your Vercel function using [route segment config](https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config). To update your maximum duration, add the following route segment config to the top of your route handler or the page which is calling your server action.\\n\\n```ts\\nexport const maxDuration = 30;\\n```\\n\\nYou can increase the max duration to 60 seconds on the Hobby Tier. For other tiers, [see the documentation](https://vercel.com/docs/functions/runtimes#max-duration) for limits.\\n\\n## Security Considerations\\n\\nGiven the high cost of calling an LLM, it\\'s important to have measures in place that can protect your application from abuse.\\n\\n### Rate Limit\\n\\nRate limiting is a method used to regulate network traffic by defining a maximum number of requests that a client can send to a server within a given time frame.\\n\\nFollow [this guide](https://vercel.com/guides/securing-ai-app-rate-limiting) to add rate limiting to your application.\\n\\n### Firewall\\n\\nA\\xa0firewall helps protect your applications and websites from DDoS attacks and unauthorized access.\\n\\n[Vercel Firewall](https://vercel.com/docs/security/vercel-firewall) is a set of tools and infrastructure, created specifically with security in mind. It automatically mitigates DDoS attacks and Enterprise teams can get further customization for their site, including dedicated support and custom rules for IP blocking.\\n\\n## Troubleshooting\\n\\n- Streaming not working when [proxied](/docs/troubleshooting/streaming-not-working-when-proxied)\\n- Experiencing [Timeouts](/docs/troubleshooting/timeout-on-vercel)\\n', children=[]), DocItem(origPath=Path('06-advanced/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Advanced\\ndescription: Learn how to use advanced functionality within the AI SDK and RSC API.\\ncollapsed: true\\n---\\n\\n# Advanced\\n\\nThis section covers advanced topics and concepts for the AI SDK and RSC API. Working with LLMs often requires a different mental model compared to traditional software development.\\n\\nAfter these concepts, you should have a better understanding of the paradigms behind the AI SDK and RSC API, and how to use them to build more AI applications.\\n', children=[])]), DocItem(origPath=Path('07-reference'), name='07-reference', displayName='07-reference', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/01-ai-sdk-core'), name='01-ai-sdk-core', displayName='01-ai-sdk-core', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/01-ai-sdk-core/01-generate-text.mdx'), name='01-generate-text.mdx', displayName='01-generate-text.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateText\\ndescription: API Reference for generateText.\\n---\\n\\n# `generateText()`\\n\\nGenerates text and calls tools for a given prompt using a language model.\\n\\nIt is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n\\nconsole.log(text);\\n```\\n\\nTo see `generateText` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { generateText } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4o\\')\",\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Input (parameters) generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'output\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      description:\\n        \\'Tools that are accessible to and can be called by the model. The model needs to support calling tools.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n            },\\n            {\\n              name: \\'execute\\',\\n              isOptional: true,\\n              type: \\'async (parameters: T, options: ToolExecutionOptions) => RESULT\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolExecutionOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'ModelMessage[]\\',\\n                      description:\\n                        \\'Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'abortSignal\\',\\n                      type: \\'AbortSignal\\',\\n                      description:\\n                        \\'An optional abort signal that indicates that the overall operation should be aborted.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      isOptional: true,\\n      type: \\'\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\\',\\n      description:\\n        \\'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.\\',\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'activeTools\\',\\n      type: \\'Array<TOOLNAME>\\',\\n      isOptional: true,\\n      description:\\n        \\'Limits the tools that are available for the model to call without changing the tool call and result types in the result. All tools are active by default.\\',\\n    },\\n    {\\n      name: \\'stopWhen\\',\\n      type: \\'StopCondition<TOOLS> | Array<StopCondition<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Condition for stopping the generation when there are tool results in the last step. When the condition is an array, any of the conditions can be met to stop the generation. Default: stepCountIs(1).\\',\\n    },\\n    {\\n      name: \\'prepareStep\\',\\n      type: \\'(options: PrepareStepOptions) => PrepareStepResult<TOOLS> | Promise<PrepareStepResult<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional function that you can use to provide different settings for a step. You can modify the model, tool choices, active tools, system prompt, and input messages for each step.\\',\\n      properties: [\\n        {\\n          type: \\'PrepareStepFunction<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'options\\',\\n              type: \\'object\\',\\n              description: \\'The options for the step.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareStepOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'steps\\',\\n                      type: \\'Array<StepResult<TOOLS>>\\',\\n                      description: \\'The steps that have been executed so far.\\',\\n                    },\\n                    {\\n                      name: \\'stepNumber\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The number of the step that is being executed.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'LanguageModel\\',\\n                      description: \\'The model that is being used.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ModelMessage>\\',\\n                      description:\\n                        \\'The messages that will be sent to the model for the current step.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'PrepareStepResult<TOOLS>\\',\\n          description:\\n            \\'Return value that can modify settings for the current step.\\',\\n          parameters: [\\n            {\\n              name: \\'model\\',\\n              type: \\'LanguageModel\\',\\n              isOptional: true,\\n              description: \\'Change the model for this step.\\',\\n            },\\n            {\\n              name: \\'toolChoice\\',\\n              type: \\'ToolChoice<TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change the tool choice strategy for this step.\\',\\n            },\\n            {\\n              name: \\'activeTools\\',\\n              type: \\'Array<keyof TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change which tools are active for this step.\\',\\n            },\\n            {\\n              name: \\'system | SystemModelMessage\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'Change the system prompt for this step.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ModelMessage>\\',\\n              isOptional: true,\\n              description: \\'Modify the input messages for this step.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_context\\',\\n      type: \\'unknown\\',\\n      isOptional: true,\\n      description:\\n        \\'Context that is passed into tool execution. Experimental (can break in patch releases).\\',\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_repairToolCall\\',\\n      type: \\'(options: ToolCallRepairOptions) => Promise<LanguageModelV3ToolCall | null>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'ToolCallRepairOptions\\',\\n          parameters: [\\n            {\\n              name: \\'system\\',\\n              type: \\'string | SystemModelMessage | undefined\\',\\n              description: \\'The system prompt.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'ModelMessage[]\\',\\n              description: \\'The messages in the current generation step.\\',\\n            },\\n            {\\n              name: \\'toolCall\\',\\n              type: \\'LanguageModelV3ToolCall\\',\\n              description: \\'The tool call that failed to parse.\\',\\n            },\\n            {\\n              name: \\'tools\\',\\n              type: \\'TOOLS\\',\\n              description: \\'The tools that are available.\\',\\n            },\\n            {\\n              name: \\'parameterSchema\\',\\n              type: \\'(options: { toolName: string }) => JSONSchema7\\',\\n              description:\\n                \\'A function that returns the JSON Schema for a tool.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'NoSuchToolError | InvalidToolInputError\\',\\n              description:\\n                \\'The error that occurred while parsing the tool call.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description:\\n        \\'Specification for parsing structured outputs from the LLM response.\\',\\n      properties: [\\n        {\\n          type: \\'Output\\',\\n          parameters: [\\n            {\\n              name: \\'Output.text()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for text generation (default).\\',\\n            },\\n            {\\n              name: \\'Output.object()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for typed object generation using schemas. When the model generates a text response, it will return an object that matches the schema.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'schema\\',\\n                      type: \\'Schema<OBJECT>\\',\\n                      description: \\'The schema of the object to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.array()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for array generation. When the model generates a text response, it will return an array of elements.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'element\\',\\n                      type: \\'Schema<ELEMENT>\\',\\n                      description:\\n                        \\'The schema of the array elements to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.choice()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for choice generation. When the model generates a text response, it will return a one of the choice options.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'Array<string>\\',\\n                      description: \\'The available choices.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.json()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\' Output specification for unstructured JSON generation. When the model generates a text response, it will return a JSON object.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onStepFinish\\',\\n      type: \\'(result: OnStepFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description: \\'Callback that is called when a step is finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnStepFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description:\\n                \\'The reason the model finished generating the text for the step.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of last step.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'totalUsage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The total token usage from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'modelId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'body\\',\\n                      isOptional: true,\\n                      type: \\'unknown\\',\\n                      description: \\'Optional response body.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'isContinued\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when there will be a continuation step with a continuation text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response and all request tool executions (for tools that have an `execute` function) are finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,Record<string,JSONValue>> | undefined\\',\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'reasoningDetails\\',\\n              type: \\'Array<ReasoningDetail>\\',\\n              description:\\n                \\'The reasoning details of the model (only available for some models).\\',\\n              properties: [\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content (only for type \"text\").\\',\\n                    },\\n                    {\\n                      name: \\'signature\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'Optional signature (only for type \"text\").\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'redacted\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The redacted data content (only for type \"redacted\").\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description:\\n                \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'Files that were generated in the final step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'steps\\',\\n              type: \\'Array<StepResult>\\',\\n              description:\\n                \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'content\\',\\n      type: \\'Array<ContentPart<TOOLS>>\\',\\n      description: \\'The content that was generated in the last step.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      type: \\'string\\',\\n      description: \\'The generated text by the model.\\',\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: \\'Array<ReasoningOutput>\\',\\n      description:\\n        \\'The full reasoning that the model has generated in the last step.\\',\\n      properties: [\\n        {\\n          type: \\'ReasoningOutput\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning\\'\",\\n              description: \\'The type of the message part.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The reasoning text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'reasoningText\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \\'The reasoning text that the model has generated in the last step. Can be undefined if the model has only generated text.\\',\\n    },\\n    {\\n      name: \\'sources\\',\\n      type: \\'Array<Source>\\',\\n      description:\\n        \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n      properties: [\\n        {\\n          type: \\'Source\\',\\n          parameters: [\\n            {\\n              name: \\'sourceType\\',\\n              type: \"\\'url\\'\",\\n              description:\\n                \\'A URL source. This is return by web search RAG models.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'The ID of the source.\\',\\n            },\\n            {\\n              name: \\'url\\',\\n              type: \\'string\\',\\n              description: \\'The URL of the source.\\',\\n            },\\n            {\\n              name: \\'title\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The title of the source.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'files\\',\\n      type: \\'Array<GeneratedFile>\\',\\n      description: \\'Files that were generated in the final step.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'File as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'File as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the file.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolCalls\\',\\n      type: \\'ToolCallArray<TOOLS>\\',\\n      description: \\'The tool calls that were made in the last step.\\',\\n    },\\n    {\\n      name: \\'toolResults\\',\\n      type: \\'ToolResultArray<TOOLS>\\',\\n      description: \\'The results of the tool calls from the last step.\\',\\n    },\\n    {\\n      name: \\'finishReason\\',\\n      type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n      description: \\'The reason the model finished generating the text.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'LanguageModelUsage\\',\\n      description: \\'The token usage of the last step.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'totalUsage\\',\\n      type: \\'CompletionTokenUsage\\',\\n      description:\\n        \\'The total token usage of all steps. When there are multiple steps, the usage is the sum of all step usages.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'LanguageModelRequestMetadata\\',\\n      isOptional: true,\\n      description: \\'Request metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'LanguageModelResponseMetadata\\',\\n      isOptional: true,\\n      description: \\'Response metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'unknown\\',\\n              description: \\'Optional response body.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ResponseMessage>\\',\\n              description:\\n                \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description: \\'Experimental setting for generating structured outputs.\\',\\n    },\\n    {\\n      name: \\'steps\\',\\n      type: \\'Array<StepResult<TOOLS>>\\',\\n      description:\\n        \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n      properties: [\\n        {\\n          type: \\'StepResult\\',\\n          parameters: [\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ContentPart<TOOLS>>\\',\\n              description: \\'The content that was generated in the last step.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The generated text.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'Array<ReasoningPart>\\',\\n              description:\\n                \\'The reasoning that was generated during the generation.\\',\\n              properties: [\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'reasoningText\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text that was generated during the generation.\\',\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description:\\n                \\'The files that were generated during the generation.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description: \\'The sources that were used to generate the text.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCallArray<TOOLS>\\',\\n              description:\\n                \\'The tool calls that were made during the generation.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResultArray<TOOLS>\\',\\n              description: \\'The results of the tool calls.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason why the generation finished.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'request\\',\\n              type: \\'LanguageModelRequestMetadata\\',\\n              description: \\'Additional request information.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelRequestMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'body\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'LanguageModelResponseMetadata\\',\\n              description: \\'Additional response information.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelResponseMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'modelId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'body\\',\\n                      isOptional: true,\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'Response body (available only for providers that use HTTP requests).\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. Response messages can be either assistant messages or tool messages. They contain a generated id.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata | undefined\\',\\n              description:\\n                \\'Additional provider-specific metadata. They are passed through from the provider to the AI SDK and enable provider-specific results that can be fully encapsulated in the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to generate text using a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/generating-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate a chat completion using a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/generating-text\\',\\n    },\\n    {\\n      title: \\'Learn to call tools using a language model in Next.js\\',\\n      link: \\'/examples/next-app/tools/call-tool\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to render a React component as a tool call using a language model in Next.js\\',\\n      link: \\'/examples/next-app/tools/render-interface-during-tool-call\\',\\n    },\\n    {\\n      title: \\'Learn to generate text using a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/generate-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate chat completions using a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/generate-text-with-chat-prompt\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/02-stream-text.mdx'), name='02-stream-text.mdx', displayName='02-stream-text.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamText\\ndescription: API Reference for streamText.\\n---\\n\\n# `streamText()`\\n\\nStreams text generations from a language model.\\n\\nYou can use the streamText function for interactive use cases such as chat bots and other real-time applications. You can also generate UI components with tools.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText } from \\'ai\\';\\n\\nconst { textStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n\\nfor await (const textPart of textStream) {\\n  process.stdout.write(textPart);\\n}\\n```\\n\\nTo see `streamText` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { streamText } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4.1\\')\",\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The IANA media type of the image.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the reasoning part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      description:\\n        \\'Tools that are accessible to and can be called by the model. The model needs to support calling tools.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n            },\\n            {\\n              name: \\'execute\\',\\n              isOptional: true,\\n              type: \\'async (parameters: T, options: ToolExecutionOptions) => RESULT\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolExecutionOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'ModelMessage[]\\',\\n                      description:\\n                        \\'Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'abortSignal\\',\\n                      type: \\'AbortSignal\\',\\n                      description:\\n                        \\'An optional abort signal that indicates that the overall operation should be aborted.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      isOptional: true,\\n      type: \\'\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\\',\\n      description:\\n        \\'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.\\',\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_generateMessageId\\',\\n      type: \\'() => string\\',\\n      isOptional: true,\\n      description:\\n        \\'Function used to generate a unique ID for each message. This is an experimental feature.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_transform\\',\\n      type: \\'StreamTextTransform | Array<StreamTextTransform>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional stream transformations. They are applied in the order they are provided. The stream transformations must maintain the stream structure for streamText to work correctly.\\',\\n      properties: [\\n        {\\n          type: \\'StreamTextTransform\\',\\n          parameters: [\\n            {\\n              name: \\'transform\\',\\n              type: \\'(options: TransformOptions) => TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>\\',\\n              description: \\'A transformation that is applied to the stream.\\',\\n              properties: [\\n                {\\n                  type: \\'TransformOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'stopStream\\',\\n                      type: \\'() => void\\',\\n                      description: \\'A function that stops the stream.\\',\\n                    },\\n                    {\\n                      name: \\'tools\\',\\n                      type: \\'TOOLS\\',\\n                      description: \\'The tools that are available.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'includeRawChunks\\',\\n      type: \\'boolean\\',\\n      isOptional: true,\\n      description:\\n        \\'Whether to include raw chunks from the provider in the stream. When enabled, you will receive raw chunks with type \"raw\" that contain the unprocessed data from the provider. This allows access to cutting-edge provider features not yet wrapped by the AI SDK. Defaults to false.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'activeTools\\',\\n      type: \\'Array<TOOLNAME> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'The tools that are currently active. All tools are active by default.\\',\\n    },\\n    {\\n      name: \\'stopWhen\\',\\n      type: \\'StopCondition<TOOLS> | Array<StopCondition<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Condition for stopping the generation when there are tool results in the last step. When the condition is an array, any of the conditions can be met to stop the generation. Default: stepCountIs(1).\\',\\n    },\\n    {\\n      name: \\'prepareStep\\',\\n      type: \\'(options: PrepareStepOptions) => PrepareStepResult<TOOLS> | Promise<PrepareStepResult<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional function that you can use to provide different settings for a step. You can modify the model, tool choices, active tools, system prompt, and input messages for each step.\\',\\n      properties: [\\n        {\\n          type: \\'PrepareStepFunction<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'options\\',\\n              type: \\'object\\',\\n              description: \\'The options for the step.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareStepOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'steps\\',\\n                      type: \\'Array<StepResult<TOOLS>>\\',\\n                      description: \\'The steps that have been executed so far.\\',\\n                    },\\n                    {\\n                      name: \\'stepNumber\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The number of the step that is being executed.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'LanguageModel\\',\\n                      description: \\'The model that is being used.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ModelMessage>\\',\\n                      description:\\n                        \\'The messages that will be sent to the model for the current step.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'PrepareStepResult<TOOLS>\\',\\n          description:\\n            \\'Return value that can modify settings for the current step.\\',\\n          parameters: [\\n            {\\n              name: \\'model\\',\\n              type: \\'LanguageModel\\',\\n              isOptional: true,\\n              description: \\'Change the model for this step.\\',\\n            },\\n            {\\n              name: \\'toolChoice\\',\\n              type: \\'ToolChoice<TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change the tool choice strategy for this step.\\',\\n            },\\n            {\\n              name: \\'activeTools\\',\\n              type: \\'Array<keyof TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change which tools are active for this step.\\',\\n            },\\n            {\\n              name: \\'system\\',\\n              type: \\'string | SystemModelMessage\\',\\n              isOptional: true,\\n              description: \\'Change the system prompt for this step.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ModelMessage>\\',\\n              isOptional: true,\\n              description: \\'Modify the input messages for this step.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_context\\',\\n      type: \\'unknown\\',\\n      isOptional: true,\\n      description:\\n        \\'Context that is passed into tool execution. Experimental (can break in patch releases).\\',\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_repairToolCall\\',\\n      type: \\'(options: ToolCallRepairOptions) => Promise<LanguageModelV3ToolCall | null>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'ToolCallRepairOptions\\',\\n          parameters: [\\n            {\\n              name: \\'system\\',\\n              type: \\'string | SystemModelMessage | undefined\\',\\n              description: \\'The system prompt.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'ModelMessage[]\\',\\n              description: \\'The messages in the current generation step.\\',\\n            },\\n            {\\n              name: \\'toolCall\\',\\n              type: \\'LanguageModelV3ToolCall\\',\\n              description: \\'The tool call that failed to parse.\\',\\n            },\\n            {\\n              name: \\'tools\\',\\n              type: \\'TOOLS\\',\\n              description: \\'The tools that are available.\\',\\n            },\\n            {\\n              name: \\'parameterSchema\\',\\n              type: \\'(options: { toolName: string }) => JSONSchema7\\',\\n              description:\\n                \\'A function that returns the JSON Schema for a tool.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'NoSuchToolError | InvalidToolInputError\\',\\n              description:\\n                \\'The error that occurred while parsing the tool call.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onChunk\\',\\n      type: \\'(event: OnChunkResult) => Promise<void> |void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.\\',\\n      properties: [\\n        {\\n          type: \\'OnChunkResult\\',\\n          parameters: [\\n            {\\n              name: \\'chunk\\',\\n              type: \\'TextStreamPart\\',\\n              description: \\'The chunk of the stream.\\',\\n              properties: [\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description:\\n                        \\'The type to identify the object as text delta.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text delta.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description:\\n                        \\'The type to identify the object as reasoning.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text delta.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'source\\'\",\\n                      description: \\'The type to identify the object as source.\\',\\n                    },\\n                    {\\n                      name: \\'source\\',\\n                      type: \\'Source\\',\\n                      description: \\'The source.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description:\\n                        \\'The type to identify the object as tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call-streaming-start\\'\",\\n                      description:\\n                        \\'Indicates the start of a tool call streaming. Only available when streaming tool calls.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call-delta\\'\",\\n                      description:\\n                        \\'The type to identify the object as tool call delta. Only available when streaming tool calls.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'argsTextDelta\\',\\n                      type: \\'string\\',\\n                      description: \\'The text delta of the tool call arguments.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  description: \\'The result of a tool call execution.\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description:\\n                        \\'The type to identify the object as tool result.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                    {\\n                      name: \\'output\\',\\n                      type: \\'any\\',\\n                      description:\\n                        \\'The result returned by the tool after execution has completed.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(event: OnErrorResult) => Promise<void> |void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when an error occurs during streaming. You can use it to log errors.\\',\\n      properties: [\\n        {\\n          type: \\'OnErrorResult\\',\\n          parameters: [\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description: \\'The error that occurred.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description:\\n        \\'Specification for parsing structured outputs from the LLM response.\\',\\n      properties: [\\n        {\\n          type: \\'Output\\',\\n          parameters: [\\n            {\\n              name: \\'Output.text()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for text generation (default).\\',\\n            },\\n            {\\n              name: \\'Output.object()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for typed object generation using schemas. When the model generates a text response, it will return an object that matches the schema.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'schema\\',\\n                      type: \\'Schema<OBJECT>\\',\\n                      description: \\'The schema of the object to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.array()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for array generation. When the model generates a text response, it will return an array of elements.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'element\\',\\n                      type: \\'Schema<ELEMENT>\\',\\n                      description:\\n                        \\'The schema of the array elements to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.choice()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for choice generation. When the model generates a text response, it will return a one of the choice options.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'Array<string>\\',\\n                      description: \\'The available choices.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.json()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\' Output specification for unstructured JSON generation. When the model generates a text response, it will return a JSON object.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onStepFinish\\',\\n      type: \\'(result: onStepFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description: \\'Callback that is called when a step is finished.\\',\\n      properties: [\\n        {\\n          type: \\'onStepFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'stepType\\',\\n              type: \\'\"initial\" | \"continue\" | \"tool-result\"\\',\\n              description:\\n                \\'The type of step. The first step is always an \"initial\" step, and subsequent steps are either \"continue\" steps or \"tool-result\" steps.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description:\\n                \\'The reason the model finished generating the text for the step.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the step.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description:\\n                \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'All files that were generated in this step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'isContinued\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when there will be a continuation step with a continuation text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response and all request tool executions (for tools that have an `execute` function) are finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of last step.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'totalUsage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The total token usage from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'reasoningDetails\\',\\n              type: \\'Array<ReasoningDetail>\\',\\n              description:\\n                \\'The reasoning details of the model (only available for some models).\\',\\n              properties: [\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content (only for type \"text\").\\',\\n                    },\\n                    {\\n                      name: \\'signature\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'Optional signature (only for type \"text\").\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'redacted\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The redacted data content (only for type \"redacted\").\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description:\\n                \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'Files that were generated in the final step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'steps\\',\\n              type: \\'Array<StepResult>\\',\\n              description:\\n                \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onAbort\\',\\n      type: \\'(event: OnAbortResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when a stream is aborted via AbortSignal. You can use it to perform cleanup operations.\\',\\n      properties: [\\n        {\\n          type: \\'OnAbortResult\\',\\n          parameters: [\\n            {\\n              name: \\'steps\\',\\n              type: \\'Array<StepResult>\\',\\n              description: \\'Details for all previously finished steps.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'content\\',\\n      type: \\'Promise<Array<ContentPart<TOOLS>>>\\',\\n      description: \\'The content that was generated in the last step. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'finishReason\\',\\n      type: \"Promise<\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'>\",\\n      description:\\n        \\'The reason why the generation finished. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'Promise<LanguageModelUsage>\\',\\n      description:\\n        \\'The token usage of the last step. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'totalUsage\\',\\n      type: \\'Promise<LanguageModelUsage>\\',\\n      description: \\'The total token usage of the generated response. When there are multiple steps, the usage is the sum of all step usages. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'Promise<ProviderMetadata | undefined>\\',\\n      description:\\n        \\'Additional provider-specific metadata from the last step. Metadata is passed through from the provider to the AI SDK and enables provider-specific results that can be fully encapsulated in the provider.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      type: \\'Promise<string>\\',\\n      description:\\n        \\'The full text that has been generated. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: \\'Promise<Array<ReasoningOutput>>\\',\\n      description:\\n        \\'The full reasoning that the model has generated in the last step. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'ReasoningOutput\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning\\'\",\\n              description: \\'The type of the message part.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The reasoning text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'reasoningText\\',\\n      type: \\'Promise<string | undefined>\\',\\n      description:\\n        \\'The reasoning text that the model has generated in the last step. Can be undefined if the model has only generated text. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'sources\\',\\n      type: \\'Promise<Array<Source>>\\',\\n      description:\\n        \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'Source\\',\\n          parameters: [\\n            {\\n              name: \\'sourceType\\',\\n              type: \"\\'url\\'\",\\n              description:\\n                \\'A URL source. This is return by web search RAG models.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'The ID of the source.\\',\\n            },\\n            {\\n              name: \\'url\\',\\n              type: \\'string\\',\\n              description: \\'The URL of the source.\\',\\n            },\\n            {\\n              name: \\'title\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The title of the source.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'files\\',\\n      type: \\'Promise<Array<GeneratedFile>>\\',\\n      description:\\n        \\'Files that were generated in the final step. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'File as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'File as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the file.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolCalls\\',\\n      type: \\'Promise<TypedToolCall<TOOLS>[]>\\',\\n      description:\\n        \\'The tool calls that have been executed. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'toolResults\\',\\n      type: \\'Promise<TypedToolResult<TOOLS>[]>\\',\\n      description:\\n        \\'The tool results that have been generated. Resolved when the all tool executions are finished.\\',\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'Promise<LanguageModelRequestMetadata>\\',\\n      description: \\'Additional request information from the last step.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Promise<LanguageModelResponseMetadata & { messages: Array<ResponseMessage>; }>\\',\\n      description: \\'Additional response information from the last step.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'model\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ResponseMessage>\\',\\n              description:\\n                \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Promise<Warning[] | undefined>\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings) for the first step.\\',\\n    },\\n    {\\n      name: \\'steps\\',\\n      type: \\'Promise<Array<StepResult>>\\',\\n      description:\\n        \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n      properties: [\\n        {\\n          type: \\'StepResult\\',\\n          parameters: [\\n            {\\n              name: \\'stepType\\',\\n              type: \\'\"initial\" | \"continue\" | \"tool-result\"\\',\\n              description:\\n                \\'The type of step. The first step is always an \"initial\" step, and subsequent steps are either \"continue\" steps or \"tool-result\" steps.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The generated text by the model.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description: \\'Sources that have been used as input.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'Files that were generated in this step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'array\\',\\n              description: \\'A list of tool calls made by the model.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'array\\',\\n              description:\\n                \\'A list of tool results returned as responses to earlier tool calls.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'request\\',\\n              type: \\'RequestMetadata\\',\\n              isOptional: true,\\n              description: \\'Request metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'RequestMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'body\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'ResponseMetadata\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'ResponseMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'isContinued\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when there will be a continuation step with a continuation text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'textStream\\',\\n      type: \\'AsyncIterableStream<string>\\',\\n      description:\\n        \\'A text stream that returns only the generated text deltas. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.\\',\\n    },\\n    {\\n      name: \\'fullStream\\',\\n      type: \\'AsyncIterable<TextStreamPart<TOOLS>> & ReadableStream<TextStreamPart<TOOLS>>\\',\\n      description:\\n        \\'A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. Only errors that stop the stream, such as network errors, are thrown.\\',\\n      properties: [\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Text content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'text\\'\",\\n              description: \\'The type to identify the object as text.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The text content.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Reasoning content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning\\'\",\\n              description: \\'The type to identify the object as reasoning.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The reasoning text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Optional provider metadata for the reasoning.\\',\\n            },\\n          ],\\n        },\\n\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Source content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'source\\'\",\\n              description: \\'The type to identify the object as source.\\',\\n            },\\n            {\\n              name: \\'sourceType\\',\\n              type: \"\\'url\\'\",\\n              description: \\'A URL source. This is returned by web search RAG models.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'The ID of the source.\\',\\n            },\\n            {\\n              name: \\'url\\',\\n              type: \\'string\\',\\n              description: \\'The URL of the source.\\',\\n            },\\n            {\\n              name: \\'title\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The title of the source.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'File content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'file\\'\",\\n              description: \\'The type to identify the object as file.\\',\\n            },\\n            {\\n              name: \\'file\\',\\n              type: \\'GeneratedFile\\',\\n              description: \\'The file.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Tool call from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call\\'\",\\n              description: \\'The type to identify the object as tool call.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'input\\',\\n              type: \\'object based on tool parameters\\',\\n              description:\\n                \\'Parameters generated by the model to be used by the tool. The type is inferred from the tool definition.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call-streaming-start\\'\",\\n              description:\\n                \\'Indicates the start of a tool call streaming. Only available when streaming tool calls.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call-delta\\'\",\\n              description:\\n                \\'The type to identify the object as tool call delta. Only available when streaming tool calls.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'argsTextDelta\\',\\n              type: \\'string\\',\\n              description: \\'The text delta of the tool call arguments.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Tool result from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-result\\'\",\\n              description: \\'The type to identify the object as tool result.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'input\\',\\n              type: \\'object based on tool parameters\\',\\n              description:\\n                \\'Parameters that were passed to the tool. The type is inferred from the tool definition.\\',\\n            },\\n            {\\n              name: \\'output\\',\\n              type: \\'tool execution return type\\',\\n              description:\\n                \\'The result returned by the tool after execution has completed. The type is inferred from the tool execute function return type.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'start-step\\'\",\\n              description: \\'Indicates the start of a new step in the stream.\\',\\n            },\\n            {\\n              name: \\'request\\',\\n              type: \\'LanguageModelRequestMetadata\\',\\n              description:\\n                \\'Information about the request that was sent to the language model provider.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelRequestMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'body\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'Raw request HTTP body that was sent to the provider API as a string.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[]\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish-step\\'\",\\n              description:\\n                \\'Indicates the end of the current step in the stream.\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'LanguageModelResponseMetadata\\',\\n              description:\\n                \\'Response metadata from the language model provider.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelResponseMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'The response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'start\\'\",\\n              description: \\'Indicates the start of the stream.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish\\'\",\\n              description: \\'The type to identify the object as finish.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'totalUsage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The total token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning-part-finish\\'\",\\n              description: \\'Indicates the end of a reasoning part.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'error\\'\",\\n              description: \\'The type to identify the object as error.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description:\\n                \\'Describes the error that may have occurred during execution.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'abort\\'\",\\n              description: \\'The type to identify the object as abort.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'partialOutputStream\\',\\n      type: \\'AsyncIterableStream<PARTIAL_OUTPUT>\\',\\n      description:\\n        \\'A stream of partial parsed outputs. It uses the `output` specification. AsyncIterableStream is defined as AsyncIterable<T> & ReadableStream<T>.\\',\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Promise<COMPLETE_OUTPUT>\\',\\n      description:\\n        \\'The complete parsed output. It uses the `output` specification.\\',\\n    },\\n    {\\n      name: \\'consumeStream\\',\\n      type: \\'(options?: ConsumeStreamOptions) => Promise<void>\\',\\n      description:\\n        \\'Consumes the stream without processing the parts. This is useful to force the stream to finish. If an error occurs, it is passed to the optional `onError` callback.\\',\\n      properties: [\\n        {\\n          type: \\'ConsumeStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onError\\',\\n              type: \\'(error: unknown) => void\\',\\n              isOptional: true,\\n              description: \\'The error callback.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toUIMessageStream\\',\\n      type: \\'(options?: UIMessageStreamOptions) => AsyncIterableStream<UIMessageChunk>\\',\\n      description:\\n        \\'Converts the result to a UI message stream. Returns an AsyncIterableStream that can be used as both an AsyncIterable and a ReadableStream.\\',\\n      properties: [\\n        {\\n          type: \\'UIMessageStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'originalMessages\\',\\n              type: \\'UIMessage[]\\',\\n              isOptional: true,\\n              description: \\'The original messages.\\',\\n            },\\n            {\\n              name: \\'onFinish\\',\\n              type: \\'(options: { messages: UIMessage[]; isContinuation: boolean; responseMessage: UIMessage; isAborted: boolean; }) => void\\',\\n              isOptional: true,\\n              description: \\'Callback function called when the stream finishes. Provides the updated list of UI messages, whether the response is a continuation, the response message, and whether the stream was aborted.\\',\\n            },\\n            {\\n              name: \\'messageMetadata\\',\\n              type: \\'(options: { part: TextStreamPart<TOOLS> & { type: \"start\" | \"finish\" | \"start-step\" | \"finish-step\"; }; }) => unknown\\',\\n              isOptional: true,\\n              description: \\'Extracts message metadata that will be sent to the client. Called on start and finish events.\\',\\n            },\\n            {\\n              name: \\'sendReasoning\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send reasoning parts to the client. Defaults to false.\\',\\n            },\\n            {\\n              name: \\'sendSources\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send source parts to the client. Defaults to false.\\',\\n            },\\n            {\\n              name: \\'sendFinish\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send the finish event to the client. Defaults to true.\\',\\n            },\\n            {\\n              name: \\'sendStart\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send the message start event to the client. Set to false if you are using additional streamText calls and the message start event has already been sent. Defaults to true.\\',\\n            },\\n            {\\n              name: \\'onError\\',\\n              type: \\'(error: unknown) => string\\',\\n              isOptional: true,\\n              description:\\n                \\'Process an error, e.g. to log it. Returns error message to include in the data stream. Defaults to () => \"An error occurred.\"\\',\\n            },\\n            {\\n              name: \\'consumeSseStream\\',\\n              type: \\'(stream: ReadableStream) => Promise<void>\\',\\n              isOptional: true,\\n              description:\\n                \\'Function to consume the SSE stream. Required for proper abort handling in UI message streams. Use the `consumeStream` function from the AI SDK.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'pipeUIMessageStreamToResponse\\',\\n      type: \\'(response: ServerResponse, options?: ResponseInit & UIMessageStreamOptions) => void\\',\\n      description:\\n        \\'Writes UI message stream output to a Node.js response-like object.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit & UIMessageStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'HeadersInit\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'pipeTextStreamToResponse\\',\\n      type: \\'(response: ServerResponse, init?: ResponseInit) => void\\',\\n      description:\\n        \\'Writes text delta output to a Node.js response-like object. It sets a `Content-Type` header to `text/plain; charset=utf-8` and writes each text delta as a separate chunk.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toUIMessageStreamResponse\\',\\n      type: \\'(options?: ResponseInit & UIMessageStreamOptions) => Response\\',\\n      description:\\n        \\'Converts the result to a streamed response object with a UI message stream.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit & UIMessageStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'HeadersInit\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toTextStreamResponse\\',\\n      type: \\'(init?: ResponseInit) => Response\\',\\n      description:\\n        \\'Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n\\n]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to stream text generated by a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/streaming-text-generation\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream chat completions generated by a language model in Next.js\\',\\n      link: \\'/examples/next-app/chat/stream-chat-completion\\',\\n    },\\n    {\\n      title: \\'Learn to stream text generated by a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/stream-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream chat completions generated by a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/stream-text-with-chat-prompt\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/03-generate-object.mdx'), name='03-generate-object.mdx', displayName='03-generate-object.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateObject\\ndescription: API Reference for generateObject.\\n---\\n\\n# `generateObject()`\\n\\nGenerates a typed, structured object for a given prompt and schema using a language model.\\n\\nIt can be used to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\\n\\n#### Example: generate an object using a schema\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(z.string()),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n\\nconsole.log(JSON.stringify(object, null, 2));\\n```\\n\\n#### Example: generate an array using a schema\\n\\nFor arrays, you specify the schema of the array items.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'array\\',\\n  schema: z.object({\\n    name: z.string(),\\n    class: z\\n      .string()\\n      .describe(\\'Character class, e.g. warrior, mage, or thief.\\'),\\n    description: z.string(),\\n  }),\\n  prompt: \\'Generate 3 hero descriptions for a fantasy role playing game.\\',\\n});\\n```\\n\\n#### Example: generate an enum\\n\\nWhen you want to generate a specific enum value, you can set the output strategy to `enum`\\nand provide the list of possible values in the `enum` parameter.\\n\\n```ts highlight=\"5-6\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'enum\\',\\n  enum: [\\'action\\', \\'comedy\\', \\'drama\\', \\'horror\\', \\'sci-fi\\'],\\n  prompt:\\n    \\'Classify the genre of this movie plot: \\' +\\n    \\'\"A group of astronauts travel through a wormhole in search of a \\' +\\n    \\'new habitable planet for humanity.\"\\',\\n});\\n```\\n\\n#### Example: generate JSON without a schema\\n\\n```ts highlight=\"6\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'no-schema\\',\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\nTo see `generateObject` in action, check out the [additional examples](#more-examples).\\n\\n## Import\\n\\n<Snippet text={`import { generateObject } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4.1\\')\",\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \"\\'object\\' | \\'array\\' | \\'enum\\' | \\'no-schema\\' | undefined\",\\n      description: \"The type of output to generate. Defaults to \\'object\\'.\",\\n    },\\n    {\\n      name: \\'mode\\',\\n      type: \"\\'auto\\' | \\'json\\' | \\'tool\\'\",\\n      description:\\n        \"The mode to use for object generation. Not every model supports all modes. \\\\\\n        Defaults to \\'auto\\' for \\'object\\' output and to \\'json\\' for \\'no-schema\\' output. \\\\\\n        Must be \\'json\\' for \\'no-schema\\' output.\",\\n    },\\n    {\\n      name: \\'schema\\',\\n      type: \\'Zod Schema | JSON Schema\\',\\n      description:\\n        \"The schema that describes the shape of the object to generate. \\\\\\n        It is sent to the model to generate the object and used to validate the output. \\\\\\n        You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). \\\\\\n        In \\'array\\' mode, the schema is used to describe an array element. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaName\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional name of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaDescription\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional description of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'enum\\',\\n      type: \\'string[]\\',\\n      description:\\n        \"List of possible values to generate. \\\\\\n        Only available with \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'args\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_repairText\\',\\n      type: \\'(options: RepairTextOptions) => Promise<string>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair the raw output of the model to enable JSON parsing. Should return the repaired text or null if the text cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'RepairTextOptions\\',\\n          parameters: [\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The text that was generated by the model.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'JSONParseError | TypeValidationError\\',\\n              description: \\'The error that occurred while parsing the text.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'object\\',\\n      type: \\'based on the schema\\',\\n      description:\\n        \\'The generated object, validated by the schema (if it supports validation).\\',\\n    },\\n    {\\n      name: \\'finishReason\\',\\n      type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n      description: \\'The reason the model finished generating the text.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'LanguageModelUsage\\',\\n      description: \\'The token usage of the generated text.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'LanguageModelRequestMetadata\\',\\n      isOptional: true,\\n      description: \\'Request metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'LanguageModelResponseMetadata\\',\\n      isOptional: true,\\n      description: \\'Response metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'unknown\\',\\n              description: \\'Optional response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \\'The reasoning that was used to generate the object. Concatenated from all reasoning parts.\\',\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'toJsonResponse\\',\\n      type: \\'(init?: ResponseInit) => Response\\',\\n      description:\\n        \\'Converts the object to a JSON response. The response will have a status code of 200 and a content type of `application/json; charset=utf-8`.\\',\\n    },\\n  ]}\\n/>\\n\\n## More Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title:\\n        \\'Learn to generate structured data using a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/generating-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate structured data using a language model in Node.js\\',\\n      link: \\'/examples/node/generating-structured-data/generate-object\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/04-stream-object.mdx'), name='04-stream-object.mdx', displayName='04-stream-object.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamObject\\ndescription: API Reference for streamObject\\n---\\n\\n# `streamObject()`\\n\\nStreams a typed, structured object for a given prompt and schema using a language model.\\n\\nIt can be used to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\\n\\n#### Example: stream an object using a schema\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(z.string()),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n\\nfor await (const partialObject of partialObjectStream) {\\n  console.clear();\\n  console.log(partialObject);\\n}\\n```\\n\\n#### Example: stream an array using a schema\\n\\nFor arrays, you specify the schema of the array items.\\nYou can use `elementStream` to get the stream of complete array elements.\\n\\n```ts highlight=\"7,18\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { elementStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'array\\',\\n  schema: z.object({\\n    name: z.string(),\\n    class: z\\n      .string()\\n      .describe(\\'Character class, e.g. warrior, mage, or thief.\\'),\\n    description: z.string(),\\n  }),\\n  prompt: \\'Generate 3 hero descriptions for a fantasy role playing game.\\',\\n});\\n\\nfor await (const hero of elementStream) {\\n  console.log(hero);\\n}\\n```\\n\\n#### Example: generate JSON without a schema\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'no-schema\\',\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n\\nfor await (const partialObject of partialObjectStream) {\\n  console.clear();\\n  console.log(partialObject);\\n}\\n```\\n\\n#### Example: generate an enum\\n\\nWhen you want to generate a specific enum value, you can set the output strategy to `enum`\\nand provide the list of possible values in the `enum` parameter.\\n\\n```ts highlight=\"5-6\"\\nimport { streamObject } from \\'ai\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'enum\\',\\n  enum: [\\'action\\', \\'comedy\\', \\'drama\\', \\'horror\\', \\'sci-fi\\'],\\n  prompt:\\n    \\'Classify the genre of this movie plot: \\' +\\n    \\'\"A group of astronauts travel through a wormhole in search of a \\' +\\n    \\'new habitable planet for humanity.\"\\',\\n});\\n```\\n\\nTo see `streamObject` in action, check out the [additional examples](#more-examples).\\n\\n## Import\\n\\n<Snippet text={`import { streamObject } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4.1\\')\",\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \"\\'object\\' | \\'array\\' | \\'enum\\' | \\'no-schema\\' | undefined\",\\n      description: \"The type of output to generate. Defaults to \\'object\\'.\",\\n    },\\n    {\\n      name: \\'mode\\',\\n      type: \"\\'auto\\' | \\'json\\' | \\'tool\\'\",\\n      description:\\n        \"The mode to use for object generation. Not every model supports all modes. \\\\\\n        Defaults to \\'auto\\' for \\'object\\' output and to \\'json\\' for \\'no-schema\\' output. \\\\\\n        Must be \\'json\\' for \\'no-schema\\' output.\",\\n    },\\n    {\\n      name: \\'schema\\',\\n      type: \\'Zod Schema | JSON Schema\\',\\n      description:\\n        \"The schema that describes the shape of the object to generate. \\\\\\n        It is sent to the model to generate the object and used to validate the output. \\\\\\n        You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). \\\\\\n        In \\'array\\' mode, the schema is used to describe an array element. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaName\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional name of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaDescription\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional description of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'system | SystemModelMessage\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'args\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_repairText\\',\\n      type: \\'(options: RepairTextOptions) => Promise<string>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair the raw output of the model to enable JSON parsing. Should return the repaired text or null if the text cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'RepairTextOptions\\',\\n          parameters: [\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The text that was generated by the model.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'JSONParseError | TypeValidationError\\',\\n              description: \\'The error that occurred while parsing the text.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(event: OnErrorResult) => Promise<void> |void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when an error occurs during streaming. You can use it to log errors.\\',\\n      properties: [\\n        {\\n          type: \\'OnErrorResult\\',\\n          parameters: [\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description: \\'The error that occurred.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response has finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata | undefined\\',\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n            {\\n              name: \\'object\\',\\n              type: \\'T | undefined\\',\\n              description:\\n                \\'The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown | undefined\\',\\n              description:\\n                \\'Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'usage\\',\\n      type: \\'Promise<LanguageModelUsage>\\',\\n      description:\\n        \\'The token usage of the generated text. Resolved when the response is finished.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'Promise<Record<string,JSONObject> | undefined>\\',\\n      description:\\n        \\'Optional metadata from the provider. Resolved whe the response is finished. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'object\\',\\n      type: \\'Promise<T>\\',\\n      description:\\n        \\'The generated object (typed according to the schema). Resolved when the response is finished.\\',\\n    },\\n    {\\n      name: \\'partialObjectStream\\',\\n      type: \\'AsyncIterableStream<DeepPartial<T>>\\',\\n      description:\\n        \\'Stream of partial objects. It gets more complete as the stream progresses. Note that the partial object is not validated. If you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.\\',\\n    },\\n    {\\n      name: \\'elementStream\\',\\n      type: \\'AsyncIterableStream<ELEMENT>\\',\\n      description: \\'Stream of array elements. Only available in \"array\" mode.\\',\\n    },\\n    {\\n      name: \\'textStream\\',\\n      type: \\'AsyncIterableStream<string>\\',\\n      description:\\n        \\'Text stream of the JSON representation of the generated object. It contains text chunks. When the stream is finished, the object is valid JSON that can be parsed.\\',\\n    },\\n    {\\n      name: \\'fullStream\\',\\n      type: \\'AsyncIterableStream<ObjectStreamPart<T>>\\',\\n      description:\\n        \\'Stream of different types of events, including partial objects, errors, and finish events. Only errors that stop the stream, such as network errors, are thrown.\\',\\n      properties: [\\n        {\\n          type: \\'ObjectPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'object\\'\",\\n            },\\n            {\\n              name: \\'object\\',\\n              type: \\'DeepPartial<T>\\',\\n              description: \\'The partial object that was generated.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextDeltaPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'text-delta\\'\",\\n            },\\n            {\\n              name: \\'textDelta\\',\\n              type: \\'string\\',\\n              description: \\'The text delta for the underlying raw JSON text.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ErrorPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'error\\'\",\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description: \\'The error that occurred.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'FinishPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish\\'\",\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'FinishReason\\',\\n            },\\n            {\\n              name: \\'logprobs\\',\\n              type: \\'Logprobs\\',\\n              isOptional: true,\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'Usage\\',\\n              description: \\'Token usage.\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'Promise<LanguageModelRequestMetadata>\\',\\n      description: \\'Request metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Promise<LanguageModelResponseMetadata>\\',\\n      description: \\'Response metadata. Resolved when the response is finished.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'model\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'CallWarning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'pipeTextStreamToResponse\\',\\n      type: \\'(response: ServerResponse, init?: ResponseInit => void\\',\\n      description:\\n        \\'Writes text delta output to a Node.js response-like object. It sets a `Content-Type` header to `text/plain; charset=utf-8` and writes each text delta as a separate chunk.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toTextStreamResponse\\',\\n      type: \\'(init?: ResponseInit) => Response\\',\\n      description:\\n        \\'Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## More Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Streaming Object Generation with RSC\\',\\n      link: \\'/examples/next-app/basics/streaming-object-generation\\',\\n    },\\n    {\\n      title: \\'Streaming Object Generation with useObject\\',\\n      link: \\'/examples/next-pages/basics/streaming-object-generation\\',\\n    },\\n    {\\n      title: \\'Streaming Partial Objects\\',\\n      link: \\'/examples/node/streaming-structured-data/stream-object\\',\\n    },\\n    {\\n      title: \\'Recording Token Usage\\',\\n      link: \\'/examples/node/streaming-structured-data/token-usage\\',\\n    },\\n    {\\n      title: \\'Recording Final Object\\',\\n      link: \\'/examples/node/streaming-structured-data/object\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/05-embed.mdx'), name='05-embed.mdx', displayName='05-embed.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: embed\\ndescription: API Reference for embed.\\n---\\n\\n# `embed()`\\n\\nGenerate an embedding for a single value using an embedding model.\\n\\nThis is ideal for use cases where you need to embed a single value to e.g. retrieve similar items or to use the embedding in a downstream task.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { embed } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'EmbeddingModel\\',\\n      description:\\n        \"The embedding model to use. Example: openai.embeddingModel(\\'text-embedding-3-small\\')\",\\n    },\\n    {\\n      name: \\'value\\',\\n      type: \\'VALUE\\',\\n      description: \\'The value to embed. The type depends on the model.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n            {\\n              name: \\'tracer\\',\\n              type: \\'Tracer\\',\\n              isOptional: true,\\n              description: \\'A custom tracer to use for the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'VALUE\\',\\n      description: \\'The value that was embedded.\\',\\n    },\\n    {\\n      name: \\'embedding\\',\\n      type: \\'number[]\\',\\n      description: \\'The embedding of the value.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'EmbeddingModelUsage\\',\\n      description: \\'The token usage for generating the embeddings.\\',\\n      properties: [\\n        {\\n          type: \\'EmbeddingModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'tokens\\',\\n              type: \\'number\\',\\n              description: \\'The number of tokens used in the embedding.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      isOptional: true,\\n      description: \\'Optional response data.\\',\\n      properties: [\\n        {\\n          type: \\'Response\\',\\n          parameters: [\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/06-embed-many.mdx'), name='06-embed-many.mdx', displayName='06-embed-many.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: embedMany\\ndescription: API Reference for embedMany.\\n---\\n\\n# `embedMany()`\\n\\nEmbed several values using an embedding model. The type of the value is defined\\nby the embedding model.\\n\\n`embedMany` automatically splits large requests into smaller chunks if the model\\nhas a limit on how many embeddings can be generated in a single call.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embedMany } from \\'ai\\';\\n\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { embedMany } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'EmbeddingModel\\',\\n      description:\\n        \"The embedding model to use. Example: openai.embeddingModel(\\'text-embedding-3-small\\')\",\\n    },\\n    {\\n      name: \\'values\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The values to embed. The type depends on the model.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n            {\\n              name: \\'tracer\\',\\n              type: \\'Tracer\\',\\n              isOptional: true,\\n              description: \\'A custom tracer to use for the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'values\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The values that were embedded.\\',\\n    },\\n    {\\n      name: \\'embeddings\\',\\n      type: \\'number[][]\\',\\n      description: \\'The embeddings. They are in the same order as the values.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'EmbeddingModelUsage\\',\\n      description: \\'The token usage for generating the embeddings.\\',\\n      properties: [\\n        {\\n          type: \\'EmbeddingModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'tokens\\',\\n              type: \\'number\\',\\n              description: \\'The total number of input tokens.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/06-rerank.mdx'), name='06-rerank.mdx', displayName='06-rerank.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: rerank\\ndescription: API Reference for rerank.\\n---\\n\\n# `rerank()`\\n\\nRerank a set of documents based on their relevance to a query using a reranking model.\\n\\nThis is ideal for improving search relevance by reordering documents, emails, or other content based on semantic understanding of the query and documents.\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { rerank } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'RerankingModel\\',\\n      description:\\n        \"The reranking model to use. Example: cohere.reranking(\\'rerank-v3.5\\')\",\\n    },\\n    {\\n      name: \\'documents\\',\\n      type: \\'Array<VALUE>\\',\\n      description:\\n        \\'The documents to rerank. Can be an array of strings or JSON objects.\\',\\n    },\\n    {\\n      name: \\'query\\',\\n      type: \\'string\\',\\n      description: \\'The search query to rank documents against.\\',\\n    },\\n    {\\n      name: \\'topN\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of top documents to return. If not specified, all documents are returned.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'ProviderOptions\\',\\n      isOptional: true,\\n      description: \\'Provider-specific options for the reranking request.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n            {\\n              name: \\'tracer\\',\\n              type: \\'Tracer\\',\\n              isOptional: true,\\n              description: \\'A custom tracer to use for the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'originalDocuments\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The original documents array in their original order.\\',\\n    },\\n    {\\n      name: \\'rerankedDocuments\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The documents sorted by relevance score (descending).\\',\\n    },\\n    {\\n      name: \\'ranking\\',\\n      type: \\'Array<RankingItem<VALUE>>\\',\\n      description: \\'Array of ranking items with scores and indices.\\',\\n      properties: [\\n        {\\n          type: \\'RankingItem<VALUE>\\',\\n          parameters: [\\n            {\\n              name: \\'originalIndex\\',\\n              type: \\'number\\',\\n              description:\\n                \\'The index of the document in the original documents array.\\',\\n            },\\n            {\\n              name: \\'score\\',\\n              type: \\'number\\',\\n              description:\\n                \\'The relevance score for the document (typically 0-1, where higher is more relevant).\\',\\n            },\\n            {\\n              name: \\'document\\',\\n              type: \\'VALUE\\',\\n              description: \\'The document itself.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description: \\'Response data.\\',\\n      properties: [\\n        {\\n          type: \\'Response\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description: \\'The response ID from the provider.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'The timestamp of the response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description: \\'The model ID used for reranking.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The raw response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n### String Documents\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking, rerankedDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n  query: \\'talk about rain\\',\\n  topN: 2,\\n});\\n\\nconsole.log(rerankedDocuments);\\n// [\\'rainy afternoon in the city\\', \\'sunny day at the beach\\']\\n\\nconsole.log(ranking);\\n// [\\n//   { originalIndex: 1, score: 0.9, document: \\'rainy afternoon...\\' },\\n//   { originalIndex: 0, score: 0.3, document: \\'sunny day...\\' }\\n// ]\\n```\\n\\n### Object Documents\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst documents = [\\n  {\\n    from: \\'Paul Doe\\',\\n    subject: \\'Follow-up\\',\\n    text: \\'We are happy to give you a discount of 20%.\\',\\n  },\\n  {\\n    from: \\'John McGill\\',\\n    subject: \\'Missing Info\\',\\n    text: \\'Here is the pricing from Oracle: $5000/month\\',\\n  },\\n];\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'Which pricing did we get from Oracle?\\',\\n  topN: 1,\\n});\\n\\nconsole.log(ranking[0].document);\\n// { from: \\'John McGill\\', subject: \\'Missing Info\\', ... }\\n```\\n\\n### With Provider Options\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  providerOptions: {\\n    cohere: {\\n      maxTokensPerDoc: 1000,\\n    },\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/10-generate-image.mdx'), name='10-generate-image.mdx', displayName='10-generate-image.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateImage\\ndescription: API Reference for generateImage.\\n---\\n\\n# `generateImage()`\\n\\n<Note type=\"warning\">`generateImage` is an experimental feature.</Note>\\n\\nGenerates images based on a given prompt using an image model.\\n\\nIt is ideal for use cases where you need to generate images programmatically,\\nsuch as creating visual content or generating images for data augmentation.\\n\\n```ts\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\n\\nconst { images } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'A futuristic cityscape at sunset\\',\\n  n: 3,\\n  size: \\'1024x1024\\',\\n});\\n\\nconsole.log(images);\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_generateImage as generateImage } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'ImageModelV3\\',\\n      description: \\'The image model to use.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string\\',\\n      description: \\'The input prompt to generate the image from.\\',\\n    },\\n    {\\n      name: \\'n\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Number of images to generate.\\',\\n    },\\n    {\\n      name: \\'size\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'Size of the images to generate. Format: `{width}x{height}`.\\',\\n    },\\n    {\\n      name: \\'aspectRatio\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'Aspect ratio of the images to generate. Format: `{width}:{height}`.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Seed for the image generation.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'ProviderOptions\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific options.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'An optional abort signal to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional HTTP headers for the request.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'image\\',\\n      type: \\'GeneratedFile\\',\\n      description: \\'The first image that was generated.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'Image as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'Image as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the image.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'images\\',\\n      type: \\'Array<GeneratedFile>\\',\\n      description: \\'All images that were generated.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'Image as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'Image as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the image.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[]\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ImageModelProviderMetadata\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. An `images` key is always present in the metadata and is an array with the same length as the top level `images` key. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'responses\\',\\n      type: \\'Array<ImageModelResponseMetadata>\\',\\n      description:\\n        \\'Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.\\',\\n      properties: [\\n        {\\n          type: \\'ImageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'Timestamp for the start of the generated response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the response model that was used to generate the response.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/11-transcribe.mdx'), name='11-transcribe.mdx', displayName='11-transcribe.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: transcribe\\ndescription: API Reference for transcribe.\\n---\\n\\n# `transcribe()`\\n\\n<Note type=\"warning\">`transcribe` is an experimental feature.</Note>\\n\\nGenerates a transcript from an audio file.\\n\\n```ts\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst { text: transcript } = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n});\\n\\nconsole.log(transcript);\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_transcribe as transcribe } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'TranscriptionModelV3\\',\\n      description: \\'The transcription model to use.\\',\\n    },\\n    {\\n      name: \\'audio\\',\\n      type: \\'DataContent (string | Uint8Array | ArrayBuffer | Buffer) | URL\\',\\n      description: \\'The audio file to generate the transcript from.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string, JSONObject>\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific options.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'An optional abort signal to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional HTTP headers for the request.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'text\\',\\n      type: \\'string\\',\\n      description: \\'The complete transcribed text from the audio input.\\',\\n    },\\n    {\\n      name: \\'segments\\',\\n      type: \\'Array<{ text: string; startSecond: number; endSecond: number }>\\',\\n      description:\\n        \\'An array of transcript segments, each containing a portion of the transcribed text along with its start and end times in seconds.\\',\\n    },\\n    {\\n      name: \\'language\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \\'The language of the transcript in ISO-639-1 format e.g. \"en\" for English.\\',\\n    },\\n    {\\n      name: \\'durationInSeconds\\',\\n      type: \\'number | undefined\\',\\n      description: \\'The duration of the transcript in seconds.\\',\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[]\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'responses\\',\\n      type: \\'Array<TranscriptionModelResponseMetadata>\\',\\n      description:\\n        \\'Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.\\',\\n      properties: [\\n        {\\n          type: \\'TranscriptionModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'Timestamp for the start of the generated response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the response model that was used to generate the response.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/12-generate-speech.mdx'), name='12-generate-speech.mdx', displayName='12-generate-speech.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateSpeech\\ndescription: API Reference for generateSpeech.\\n---\\n\\n# `generateSpeech()`\\n\\n<Note type=\"warning\">`generateSpeech` is an experimental feature.</Note>\\n\\nGenerates speech audio from text.\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { audio } = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello from the AI SDK!\\',\\n  voice: \\'alloy\\',\\n});\\n\\nconsole.log(audio);\\n```\\n\\n## Examples\\n\\n### OpenAI\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { audio } = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello from the AI SDK!\\',\\n  voice: \\'alloy\\',\\n});\\n```\\n\\n### ElevenLabs\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { elevenlabs } from \\'@ai-sdk/elevenlabs\\';\\n\\nconst { audio } = await generateSpeech({\\n  model: elevenlabs.speech(\\'eleven_multilingual_v2\\'),\\n  text: \\'Hello from the AI SDK!\\',\\n  voice: \\'your-voice-id\\', // Required: get this from your ElevenLabs account\\n});\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_generateSpeech as generateSpeech } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'SpeechModelV3\\',\\n      description: \\'The speech model to use.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      type: \\'string\\',\\n      description: \\'The text to generate the speech from.\\',\\n    },\\n    {\\n      name: \\'voice\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'The voice to use for the speech.\\',\\n    },\\n    {\\n      name: \\'outputFormat\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'The output format to use for the speech e.g. \"mp3\", \"wav\", etc.\\',\\n    },\\n    {\\n      name: \\'instructions\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'Instructions for the speech generation.\\',\\n    },\\n    {\\n      name: \\'speed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'The speed of the speech generation.\\',\\n    },\\n    {\\n      name: \\'language\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'The language for speech generation. This should be an ISO 639-1 language code (e.g. \"en\", \"es\", \"fr\") or \"auto\" for automatic language detection. Provider support varies.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string, JSONObject>\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific options.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'An optional abort signal to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional HTTP headers for the request.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'audio\\',\\n      type: \\'GeneratedAudioFile\\',\\n      description: \\'The generated audio.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedAudioFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'Audio as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'Audio as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mimeType\\',\\n              type: \\'string\\',\\n              description: \\'MIME type of the audio (e.g. \"audio/mpeg\").\\',\\n            },\\n            {\\n              name: \\'format\\',\\n              type: \\'string\\',\\n              description: \\'Format of the audio (e.g. \"mp3\").\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[]\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'responses\\',\\n      type: \\'Array<SpeechModelResponseMetadata>\\',\\n      description:\\n        \\'Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.\\',\\n      properties: [\\n        {\\n          type: \\'SpeechModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'Timestamp for the start of the generated response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the response model that was used to generate the response.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'unknown\\',\\n              description: \\'Optional response body.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/15-agent.mdx'), name='15-agent.mdx', displayName='15-agent.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Agent (Interface)\\ndescription: API Reference for the Agent interface.\\n---\\n\\n# `Agent` (interface)\\n\\nThe `Agent` interface defines a contract for agents that can generate or stream AI-generated responses in response to prompts. Agents may encapsulate advanced logic such as tool usage, multi-step workflows, or prompt handling, enabling both simple and autonomous AI agents.\\n\\nImplementations of the `Agent` interface—such as `ToolLoopAgent`—fulfill the same contract and integrate seamlessly with all SDK APIs and utilities that expect an agent. This design allows users to supply custom agent classes or wrappers for third-party chains, while maximizing compatibility with AI SDK features.\\n\\n## Interface Definition\\n\\n```ts\\nimport { ModelMessage } from \\'@ai-sdk/provider-utils\\';\\nimport { ToolSet } from \\'../generate-text/tool-set\\';\\nimport { Output } from \\'../generate-text/output\\';\\nimport { GenerateTextResult } from \\'../generate-text/generate-text-result\\';\\nimport { StreamTextResult } from \\'../generate-text/stream-text-result\\';\\n\\nexport type AgentCallParameters<CALL_OPTIONS> = ([CALL_OPTIONS] extends [never]\\n  ? { options?: never }\\n  : { options: CALL_OPTIONS }) &\\n  (\\n    | {\\n        /**\\n         * A prompt. It can be either a text prompt or a list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        prompt: string | Array<ModelMessage>;\\n\\n        /**\\n         * A list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        messages?: never;\\n      }\\n    | {\\n        /**\\n         * A list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        messages: Array<ModelMessage>;\\n\\n        /**\\n         * A prompt. It can be either a text prompt or a list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        prompt?: never;\\n      }\\n  ) & {\\n    /**\\n     * Abort signal.\\n     */\\n    abortSignal?: AbortSignal;\\n  };\\n\\n/**\\n * An Agent receives a prompt (text or messages) and generates or streams an output\\n * that consists of steps, tool calls, data parts, etc.\\n *\\n * You can implement your own Agent by implementing the `Agent` interface,\\n * or use the `ToolLoopAgent` class.\\n */\\nexport interface Agent<\\n  CALL_OPTIONS = never,\\n  TOOLS extends ToolSet = {},\\n  OUTPUT extends Output = never,\\n> {\\n  /**\\n   * The specification version of the agent interface. This will enable\\n   * us to evolve the agent interface and retain backwards compatibility.\\n   */\\n  readonly version: \\'agent-v1\\';\\n\\n  /**\\n   * The id of the agent.\\n   */\\n  readonly id: string | undefined;\\n\\n  /**\\n   * The tools that the agent can use.\\n   */\\n  readonly tools: TOOLS;\\n\\n  /**\\n   * Generates an output from the agent (non-streaming).\\n   */\\n  generate(\\n    options: AgentCallParameters<CALL_OPTIONS>,\\n  ): PromiseLike<GenerateTextResult<TOOLS, OUTPUT>>;\\n\\n  /**\\n   * Streams an output from the agent (streaming).\\n   */\\n  stream(\\n    options: AgentCallParameters<CALL_OPTIONS>,\\n  ): PromiseLike<StreamTextResult<TOOLS, OUTPUT>>;\\n}\\n```\\n\\n## Core Properties & Methods\\n\\n| Name         | Type                                             | Description                                                         |\\n| ------------ | ------------------------------------------------ | ------------------------------------------------------------------- |\\n| `version`    | `\\'agent-v1\\'`                                     | Interface version for compatibility.                                |\\n| `id`         | `string \\\\| undefined`                            | Optional agent identifier.                                          |\\n| `tools`      | `ToolSet`                                        | The set of tools available to this agent.                           |\\n| `generate()` | `PromiseLike<GenerateTextResult<TOOLS, OUTPUT>>` | Generates full, non-streaming output for a text prompt or messages. |\\n| `stream()`   | `PromiseLike<StreamTextResult<TOOLS, OUTPUT>>`   | Streams output (chunks or steps) for a text prompt or messages.     |\\n\\n## Generic Parameters\\n\\n| Parameter      | Default | Description                                                                |\\n| -------------- | ------- | -------------------------------------------------------------------------- |\\n| `CALL_OPTIONS` | `never` | Optional type for additional call options that can be passed to the agent. |\\n| `TOOLS`        | `{}`    | The type of the tool set available to this agent.                          |\\n| `OUTPUT`       | `never` | The type of additional output data that the agent can produce.             |\\n\\n## Method Parameters\\n\\nBoth `generate()` and `stream()` accept an `AgentCallParameters<CALL_OPTIONS>` object with:\\n\\n- `prompt` (optional): A string prompt or array of `ModelMessage` objects\\n- `messages` (optional): An array of `ModelMessage` objects (mutually exclusive with `prompt`)\\n- `options` (optional): Additional call options when `CALL_OPTIONS` is not `never`\\n- `abortSignal` (optional): An `AbortSignal` to cancel the operation\\n\\n## Example: Custom Agent Implementation\\n\\nHere\\'s how you might implement your own Agent:\\n\\n```ts\\nimport { Agent, GenerateTextResult, StreamTextResult } from \\'ai\\';\\nimport type { ModelMessage } from \\'@ai-sdk/provider-utils\\';\\n\\nclass MyEchoAgent implements Agent {\\n  version = \\'agent-v1\\' as const;\\n  id = \\'echo\\';\\n  tools = {};\\n\\n  async generate({ prompt, messages, abortSignal }) {\\n    const text = prompt ?? JSON.stringify(messages);\\n    return { text, steps: [] };\\n  }\\n\\n  async stream({ prompt, messages, abortSignal }) {\\n    const text = prompt ?? JSON.stringify(messages);\\n    return {\\n      textStream: (async function* () {\\n        yield text;\\n      })(),\\n    };\\n  }\\n}\\n```\\n\\n## Usage: Interacting with Agents\\n\\nAll SDK utilities that accept an agent—including [`createAgentUIStream`](/docs/reference/ai-sdk-core/create-agent-ui-stream), [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response), and [`pipeAgentUIStreamToResponse`](/docs/reference/ai-sdk-core/pipe-agent-ui-stream-to-response)—expect an object adhering to the `Agent` interface.\\n\\nYou can use the official [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent) (recommended for multi-step AI workflows with tool use), or supply your own implementation:\\n\\n```ts\\nimport { ToolLoopAgent, createAgentUIStream } from \"ai\";\\n\\nconst agent = new ToolLoopAgent({ ... });\\n\\nconst stream = await createAgentUIStream({\\n  agent,\\n  messages: [{ role: \"user\", content: \"What is the weather in NYC?\" }]\\n});\\n\\nfor await (const chunk of stream) {\\n  console.log(chunk);\\n}\\n```\\n\\n## See Also\\n\\n- [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent) &mdash; Official multi-step agent implementation\\n- [`createAgentUIStream`](/docs/reference/ai-sdk-core/create-agent-ui-stream)\\n- [`GenerateTextResult`](/docs/reference/ai-sdk-core/generate-text)\\n- [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text)\\n\\n## Notes\\n\\n- Agents should define their `tools` property, even if empty (`{}`), for compatibility with SDK utilities.\\n- The interface accepts both plain prompts and message arrays as input, but only one at a time.\\n- The `CALL_OPTIONS` generic parameter allows agents to accept additional call-specific options when needed.\\n- The `abortSignal` parameter enables cancellation of agent operations.\\n- This design is extensible for both complex autonomous agents and simple LLM wrappers.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/16-tool-loop-agent.mdx'), name='16-tool-loop-agent.mdx', displayName='16-tool-loop-agent.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: ToolLoopAgent\\ndescription: API Reference for the ToolLoopAgent class.\\n---\\n\\n# `ToolLoopAgent`\\n\\nCreates a reusable AI agent capable of generating text, streaming responses, and using tools over multiple steps (a reasoning-and-acting loop). `ToolLoopAgent` is ideal for building autonomous, multi-step agents that can take actions, call tools, and reason over the results until a stop condition is reached.\\n\\nUnlike single-step calls like `generateText()`, an agent can iteratively invoke tools, collect tool results, and decide next actions until completion or user approval is required.\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: {\\n    weather: weatherTool,\\n    calculator: calculatorTool,\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'What is the weather in NYC?\\',\\n});\\n\\nconsole.log(result.text);\\n```\\n\\nTo see `ToolLoopAgent` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { ToolLoopAgent } from \"ai\"`} prompt={false} />\\n\\n## Constructor\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      isRequired: true,\\n      description:\\n        \\'The language model instance to use (e.g., from a provider).\\',\\n    },\\n    {\\n      name: \\'instructions\\',\\n      type: \\'string | SystemModelMessage\\',\\n      isOptional: true,\\n      description:\\n        \\'Instructions for the agent, usually used for system prompt/context.\\',\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'Record<string, Tool>\\',\\n      isOptional: true,\\n      description:\\n        \\'A set of tools the agent can call. Keys are tool names. Tools require the underlying model to support tool calling.\\',\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      type: \\'ToolChoice\\',\\n      isOptional: true,\\n      description:\\n        \"Tool call selection strategy. Options: \\'auto\\' | \\'none\\' | \\'required\\' | { type: \\'tool\\', toolName: string }. Default: \\'auto\\'.\",\\n    },\\n    {\\n      name: \\'stopWhen\\',\\n      type: \\'StopCondition | StopCondition[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Condition(s) for ending the agent loop. Default: stepCountIs(20).\\',\\n    },\\n    {\\n      name: \\'activeTools\\',\\n      type: \\'Array<string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Limits the subset of tools that are available in a specific call.\\',\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional structured output specification, for parsing responses into typesafe data.\\',\\n    },\\n    {\\n      name: \\'prepareStep\\',\\n      type: \\'PrepareStepFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional function to mutate step settings or inject state for each agent step.\\',\\n    },\\n    {\\n      name: \\'experimental_repairToolCall\\',\\n      type: \\'ToolCallRepairFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback to attempt automatic recovery when a tool call cannot be parsed.\\',\\n    },\\n    {\\n      name: \\'onStepFinish\\',\\n      type: \\'GenerateTextOnStepFinishCallback\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback invoked after each agent step (LLM/tool call) completes.\\',\\n    },\\n    {\\n      name: \\'experimental_context\\',\\n      type: \\'unknown\\',\\n      isOptional: true,\\n      description:\\n        \\'Experimental: Custom context object passed to each tool call.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Experimental: Optional telemetry configuration.\\',\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'DownloadFunction | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Experimental: Custom download function for fetching files/URLs for tool or model use. By default, files are downloaded if the model does not support the URL for a given media type.\\',\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens the model is allowed to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Sampling temperature, controls randomness. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Top-p (nucleus) sampling parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Top-k sampling parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Presence penalty parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Frequency penalty parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom token sequences which stop the model output. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Seed for deterministic generation (if supported).\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'How many times to retry on failure. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'Optional abort signal to cancel the ongoing request.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'ProviderOptions\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific configuration.\\',\\n    },\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'Custom agent identifier.\\',\\n    },\\n  ]}\\n/>\\n\\n## Methods\\n\\n### `generate()`\\n\\nGenerates a response and triggers tool calls as needed, running the agent loop and returning the final result. Returns a promise resolving to a `GenerateTextResult`.\\n\\n```ts\\nconst result = await agent.generate({\\n  prompt: \\'What is the weather like?\\',\\n});\\n```\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<ModelMessage>\\',\\n      description: \\'A text prompt or message array.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<ModelMessage>\\',\\n      description: \\'A full conversation history as a list of model messages.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n  ]}\\n/>\\n\\n#### Returns\\n\\nThe `generate()` method returns a `GenerateTextResult` object (see [`generateText`](/docs/reference/ai-sdk-core/generate-text#returns) for details).\\n\\n### `stream()`\\n\\nStreams a response from the agent, including agent reasoning and tool calls, as they occur. Returns a `StreamTextResult`.\\n\\n```ts\\nconst stream = agent.stream({\\n  prompt: \\'Tell me a story about a robot.\\',\\n});\\n\\nfor await (const chunk of stream.textStream) {\\n  console.log(chunk);\\n}\\n```\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<ModelMessage>\\',\\n      description: \\'A text prompt or message array.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<ModelMessage>\\',\\n      description: \\'A full conversation history as a list of model messages.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'experimental_transform\\',\\n      type: \\'StreamTextTransform | Array<StreamTextTransform>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional stream transformation(s). They are applied in the order provided and must maintain the stream structure. See `streamText` docs for details.\\',\\n    },\\n  ]}\\n/>\\n\\n#### Returns\\n\\nThe `stream()` method returns a `StreamTextResult` object (see [`streamText`](/docs/reference/ai-sdk-core/stream-text#returns) for details).\\n\\n## Types\\n\\n### `InferAgentUIMessage`\\n\\nInfers the UI message type for the given agent instance. Useful for type-safe UI and message exchanges.\\n\\n#### Basic Example\\n\\n```ts\\nimport { ToolLoopAgent, InferAgentUIMessage } from \\'ai\\';\\n\\nconst weatherAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: { weather: weatherTool },\\n});\\n\\ntype WeatherAgentUIMessage = InferAgentUIMessage<typeof weatherAgent>;\\n```\\n\\n#### Example with Message Metadata\\n\\nYou can provide a second type argument to customize the metadata for each message. This is useful for tracking rich metadata returned by the agent (such as createdAt, tokens, finish reason, etc.).\\n\\n```ts\\nimport { ToolLoopAgent, InferAgentUIMessage } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Example schema for message metadata\\nconst exampleMetadataSchema = z.object({\\n  createdAt: z.number().optional(),\\n  model: z.string().optional(),\\n  totalTokens: z.number().optional(),\\n  finishReason: z.string().optional(),\\n});\\ntype ExampleMetadata = z.infer<typeof exampleMetadataSchema>;\\n\\n// Define agent as usual\\nconst metadataAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  // ...other options\\n});\\n\\n// Type-safe UI message type with custom metadata\\ntype MetadataAgentUIMessage = InferAgentUIMessage<\\n  typeof metadataAgent,\\n  ExampleMetadata\\n>;\\n```\\n\\n## Examples\\n\\n### Basic Agent with Tools\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\nimport { weatherTool, calculatorTool } from \\'./tools\\';\\n\\nconst assistant = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: {\\n    weather: weatherTool,\\n    calculator: calculatorTool,\\n  },\\n  stopWhen: stepCountIs(3),\\n});\\n\\nconst result = await assistant.generate({\\n  prompt: \\'What is the weather in NYC and what is 100 * 25?\\',\\n});\\n\\nconsole.log(result.text);\\nconsole.log(result.steps); // Array of all steps taken by the agent\\n```\\n\\n### Streaming Agent Response\\n\\n```ts\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a creative storyteller.\\',\\n});\\n\\nconst stream = agent.stream({\\n  prompt: \\'Tell me a short story about a time traveler.\\',\\n});\\n\\nfor await (const chunk of stream.textStream) {\\n  process.stdout.write(chunk);\\n}\\n```\\n\\n### Agent with Output Parsing\\n\\n```ts\\nimport { z } from \\'zod\\';\\n\\nconst analysisAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: {\\n    schema: z.object({\\n      sentiment: z.enum([\\'positive\\', \\'negative\\', \\'neutral\\']),\\n      score: z.number(),\\n      summary: z.string(),\\n    }),\\n  },\\n});\\n\\nconst result = await analysisAgent.generate({\\n  prompt: \\'Analyze this review: \"The product exceeded my expectations!\"\\',\\n});\\n\\nconsole.log(result.output);\\n// Typed as { sentiment: \\'positive\\' | \\'negative\\' | \\'neutral\\', score: number, summary: string }\\n```\\n\\n### Example: Approved Tool Execution\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are an agent with access to a weather API.\\',\\n  tools: {\\n    weather: openai.tools.weather({\\n      /* ... */\\n    }),\\n  },\\n  // Optionally require approval, etc.\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Is it raining in Paris today?\\',\\n});\\nconsole.log(result.text);\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/17-create-agent-ui-stream.mdx'), name='17-create-agent-ui-stream.mdx', displayName='17-create-agent-ui-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createAgentUIStream\\ndescription: API Reference for the createAgentUIStream utility.\\n---\\n\\n# `createAgentUIStream`\\n\\nThe `createAgentUIStream` function runs an [Agent](/docs/reference/ai-sdk-core/agent) and returns a streaming UI message stream as an async iterable. This allows you to consume an agent\\'s reasoning and UI messages incrementally in your own server, edge function, or background job—ideal for building custom streaming interfaces or piping the data to different outputs.\\n\\n## Import\\n\\n<Snippet text={`import { createAgentUIStream } from \"ai\"`} prompt={false} />\\n\\n## Usage\\n\\n```ts\\nimport { ToolLoopAgent, createAgentUIStream } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: { weather: weatherTool, calculator: calculatorTool },\\n});\\n\\nexport async function* streamAgent(\\n  messages: unknown[],\\n  abortSignal?: AbortSignal,\\n) {\\n  const stream = await createAgentUIStream({\\n    agent,\\n    messages,\\n    abortSignal,\\n    // ...other options\\n  });\\n\\n  for await (const chunk of stream) {\\n    yield chunk; // UI message chunk object (see UIMessageStream)\\n  }\\n}\\n```\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'agent\\',\\n      type: \\'Agent\\',\\n      isRequired: true,\\n      description:\\n        \\'The agent instance to run. Must define its tools and implement `.stream({ prompt })`.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'unknown[]\\',\\n      isRequired: true,\\n      description:\\n        \\'Array of input UI messages sent to the agent (e.g., from user/assistant).\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isRequired: false,\\n      description:\\n        \\'Optional abort signal to cancel the agent streaming, e.g., in response to client disconnection.\\',\\n    },\\n    {\\n      name: \\'...options\\',\\n      type: \\'UIMessageStreamOptions\\',\\n      isRequired: false,\\n      description:\\n        \\'Additional options for customizing UI message streaming, such as source inclusion or error formatting.\\',\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\nA `Promise<AsyncIterableStream<UIMessageChunk>>`, where each yielded value is a UI message chunk representing incremental agent UI output. This stream can be piped to HTTP responses, processed for dashboards, or logged.\\n\\n## Example\\n\\n```ts\\nimport { createAgentUIStream } from \\'ai\\';\\n\\nconst controller = new AbortController();\\n\\nconst stream = await createAgentUIStream({\\n  agent,\\n  messages: [{ role: \\'user\\', content: \\'What is the weather in SF today?\\' }],\\n  abortSignal: controller.signal,\\n  sendStart: true,\\n});\\n\\nfor await (const chunk of stream) {\\n  // Process each UI message chunk (e.g., send to client)\\n  console.log(chunk);\\n}\\n\\n// Call controller.abort() to stop streaming early if needed.\\n```\\n\\n## How It Works\\n\\n1. **Message Validation:** The incoming array of `messages` is validated and normalized according to the agent\\'s tools and requirements. Invalid messages will cause an error.\\n2. **Model Message Conversion:** The validated UI messages are converted into the model message format the agent expects.\\n3. **Agent Streaming:** The agent\\'s `.stream({ prompt, abortSignal })` method is invoked to produce a low-level result stream. If an `abortSignal` is provided and triggered, streaming will be canceled promptly.\\n4. **UI Message Stream:** That result stream is exposed as a streaming async iterable of UI message chunks.\\n\\n## Notes\\n\\n- The agent **must** define its `tools` and a `.stream({ prompt })` method.\\n- This utility returns an async iterator for full streaming flexibility. To produce an HTTP response, see [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response) or [`pipeAgentUIStreamToResponse`](/docs/reference/ai-sdk-core/pipe-agent-ui-stream-to-response).\\n- You can provide UI message stream options (see [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)) for fine-grained control over the output.\\n- To cancel a streaming agent operation, supply an [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) via the `abortSignal` option.\\n\\n## See Also\\n\\n- [`Agent`](/docs/reference/ai-sdk-core/agent)\\n- [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent)\\n- [`UIMessage`](/docs/reference/ai-sdk-core/ui-message)\\n- [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)\\n- [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response)\\n- [`pipeAgentUIStreamToResponse`](/docs/reference/ai-sdk-core/pipe-agent-ui-stream-to-response)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/18-create-agent-ui-stream-response.mdx'), name='18-create-agent-ui-stream-response.mdx', displayName='18-create-agent-ui-stream-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createAgentUIStreamResponse\\ndescription: API Reference for the createAgentUIStreamResponse utility.\\n---\\n\\n# `createAgentUIStreamResponse`\\n\\nThe `createAgentUIStreamResponse` function executes an [Agent](/docs/reference/ai-sdk-core/agent) and streams its output as a UI message stream in an HTTP [Response](https://developer.mozilla.org/en-US/docs/Web/API/Response) body. This is designed for building API endpoints that deliver real-time streaming results from an agent (for example, chat or tool-use applications).\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createAgentUIStreamResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Usage\\n\\n```ts\\nimport { ToolLoopAgent, createAgentUIStreamResponse } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: { weather: weatherTool, calculator: calculatorTool },\\n});\\n\\n// Typical usage with streaming options\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  // Optional: Use abortSignal for cancellation support (client-side disconnects)\\n  const abortController = new AbortController();\\n\\n  return createAgentUIStreamResponse({\\n    agent,\\n    messages,\\n    abortSignal: abortController.signal, // optional\\n    // ...other UIMessageStreamOptions like sendSources, includeUsage, experimental_transform, etc.\\n  });\\n}\\n```\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'agent\\',\\n      type: \\'Agent\\',\\n      isRequired: true,\\n      description:\\n        \\'The agent instance to use for streaming responses. Must implement `.stream({ prompt })` and define tools.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'unknown[]\\',\\n      isRequired: true,\\n      description:\\n        \\'Array of input UI messages sent to the agent (typically user and assistant message objects).\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isRequired: false,\\n      description:\\n        \\'Optional abort signal to cancel streaming, e.g., when client disconnects. Useful for long-running or cancelable requests.\\',\\n    },\\n    {\\n      name: \\'...options\\',\\n      type: \\'UIMessageStreamOptions\\',\\n      isRequired: false,\\n      description:\\n        \\'Additional UI message streaming options, such as `sendSources`, `includeUsage`, `experimental_transform`, etc. See [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options) for details.\\',\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\nA `Promise<Response>` whose body is a stream of UI messages from the agent—suitable as an HTTP response in server-side API routes (Next.js, Express, serverless, or edge handlers).\\n\\n## Example: Next.js API Route Handler\\n\\n```ts\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\nimport { MyCustomAgent } from \\'@/agent/my-custom-agent\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: MyCustomAgent,\\n    messages,\\n    sendSources: true, // optionally include sources in the UI message stream\\n    includeUsage: true, // include token usage details if enabled by the agent\\n    // Optionally, provide abortSignal for cancellation and other stream options\\n  });\\n}\\n```\\n\\n## How It Works\\n\\nUnder the hood, this function uses the internal `createAgentUIStream` utility and wraps its result as an HTTP `Response` readable stream:\\n\\n1. **Message Validation:** The incoming array of `messages` is validated and normalized according to the agent\\'s tools and requirements. Messages not meeting spec will trigger an error.\\n2. **Conversion:** Validated messages are transformed to the internal model message format expected by the agent.\\n3. **Streaming:** The agent\\'s `.stream({ prompt })` method is called, producing a stream of UI message chunks representing the agent\\'s process and outputs.\\n4. **HTTP Response:** The stream of UI message chunks is returned as a streaming `Response` object suitable for consumption by server-side API clients (such as a chat UI or live tool interface).\\n\\n## Notes\\n\\n- The agent **must** define its `tools` and implement `.stream({ prompt })`.\\n- **Do not use in the browser**; call this from backend/API/server code only.\\n- You can provide additional UI message streaming options (see [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)) to customize the response, including experimental stream transforms.\\n- The returned `Response` leverages [Readable Streams](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream). Make sure your client or framework can consume streamed HTTP responses.\\n\\n## See Also\\n\\n- [`Agent`](/docs/reference/ai-sdk-core/agent)\\n- [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent)\\n- [`UIMessage`](/docs/reference/ai-sdk-core/ui-message)\\n- [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)\\n- [`createAgentUIStream`](/docs/reference/ai-sdk-core/create-agent-ui-stream)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/18-pipe-agent-ui-stream-to-response.mdx'), name='18-pipe-agent-ui-stream-to-response.mdx', displayName='18-pipe-agent-ui-stream-to-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: pipeAgentUIStreamToResponse\\ndescription: API Reference for the pipeAgentUIStreamToResponse utility.\\n---\\n\\n# `pipeAgentUIStreamToResponse`\\n\\nThe `pipeAgentUIStreamToResponse` function executes an [Agent](/docs/reference/ai-sdk-core/agent) and streams its output as a UI message stream directly to a Node.js [`ServerResponse`](https://nodejs.org/api/http.html#class-httpserverresponse) object. This is ideal for building API endpoints in Node.js servers (such as Express, Hono, or custom servers) that require low-latency, real-time UI message streaming from an Agent (e.g., for chat- or tool-use-based applications).\\n\\n## Import\\n\\n<Snippet\\n  text={`import { pipeAgentUIStreamToResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Usage\\n\\n```ts\\nimport { pipeAgentUIStreamToResponse } from \\'ai\\';\\nimport { MyCustomAgent } from \\'./agent\\';\\n\\nexport async function handler(req, res) {\\n  const { messages } = JSON.parse(req.body);\\n\\n  // Optional: Use abortSignal for request cancellation support\\n  const abortController = new AbortController();\\n\\n  await pipeAgentUIStreamToResponse({\\n    response: res, // Node.js ServerResponse\\n    agent: MyCustomAgent,\\n    messages,\\n    abortSignal: abortController.signal, // optional, see notes\\n    // ...other optional streaming options\\n  });\\n}\\n```\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'ServerResponse\\',\\n      isRequired: true,\\n      description:\\n        \\'The Node.js ServerResponse object to which the UI message stream will be piped.\\',\\n    },\\n    {\\n      name: \\'agent\\',\\n      type: \\'Agent\\',\\n      isRequired: true,\\n      description:\\n        \\'The agent instance to use for streaming responses. Must implement `.stream({ prompt })` and define tools.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'unknown[]\\',\\n      isRequired: true,\\n      description:\\n        \\'Array of input UI messages sent to the agent (typically user and assistant message objects).\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isRequired: false,\\n      description:\\n        \\'Optional abort signal to cancel streaming (e.g., when the client disconnects). Useful for enabling cancellation of long-running or streaming agent responses. Provide an instance of [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) (for example, from an `AbortController`).\\',\\n    },\\n    {\\n      name: \\'...options\\',\\n      type: \\'UIMessageStreamResponseInit & UIMessageStreamOptions\\',\\n      isRequired: false,\\n      description:\\n        \\'Options for response headers, status, and additional streaming configuration.\\',\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\nA `Promise<void>`. This function returns a promise that resolves when piping the UI message stream to the ServerResponse is complete.\\n\\n## Example: Hono/Express Route Handler\\n\\n```ts\\nimport { pipeAgentUIStreamToResponse } from \\'ai\\';\\nimport { openaiWebSearchAgent } from \\'./openai-web-search-agent\\';\\n\\napp.post(\\'/chat\\', async (req, res) => {\\n  const { messages } = await getJsonBody(req);\\n\\n  // Optionally use abortSignal for cancellation on disconnect, etc.\\n  const abortController = new AbortController();\\n  // e.g., tie abortController to request lifecycle/disconnect detection as needed\\n\\n  await pipeAgentUIStreamToResponse({\\n    response: res,\\n    agent: openaiWebSearchAgent,\\n    messages,\\n    abortSignal: abortController.signal, // optional\\n    // status: 200,\\n    // headers: { \\'X-Custom\\': \\'foo\\' },\\n    // ...additional streaming options\\n  });\\n});\\n```\\n\\n## How It Works\\n\\n1. **Streams Output:** The function creates a UI message stream from the agent and efficiently pipes it to the provided Node.js `ServerResponse`, setting appropriate HTTP headers (including content type and streaming-friendly headers) and status.\\n2. **Abort Support:** If you provide an `abortSignal`, you can cancel the streaming response (for example, when a client disconnects or a timeout occurs), improving resource usage and responsiveness.\\n3. **No Response Object:** Unlike serverless `Response`-returning APIs, this function does _not_ return a Response object. It writes streaming bytes directly to the Node.js response. This is more memory- and latency-efficient for Node.js server frameworks.\\n\\n## Notes\\n\\n- **abortSignal for Cancellation:** Use `abortSignal` to stop agent and stream processing early, improving robustness for long-running connections. In frameworks like Express or Hono, tie this to your server\\'s disconnect or timeout events when possible.\\n- **Only for Node.js:** This function is intended for use in Node.js environments with access to `ServerResponse` objects, not for Edge/serverless/server-side frameworks using web `Response` objects.\\n- **Streaming Support:** Ensure your client and reverse proxy/server infrastructure support streaming HTTP responses.\\n- Supports both Hono (`@hono/node-server`), Express, and similar Node.js frameworks.\\n\\n## See Also\\n\\n- [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response)\\n- [`Agent`](/docs/reference/ai-sdk-core/agent)\\n- [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)\\n- [`UIMessage`](/docs/reference/ai-sdk-core/ui-message)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/20-tool.mdx'), name='20-tool.mdx', displayName='20-tool.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: tool\\ndescription: Helper function for tool type inference\\n---\\n\\n# `tool()`\\n\\nTool is a helper function that infers the tool input for its `execute` method.\\n\\nIt does not have any runtime behavior, but it helps TypeScript infer the types of the input for the `execute` method.\\n\\nWithout this helper function, TypeScript is unable to connect the `inputSchema` property to the `execute` method,\\nand the argument types of `execute` cannot be inferred.\\n\\n```ts highlight={\"1,4,9,10\"}\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const weatherTool = tool({\\n  description: \\'Get the weather in a location\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The location to get the weather for\\'),\\n  }),\\n  // location below is inferred to be a string:\\n  execute: async ({ location }) => ({\\n    location,\\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n  }),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { tool } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tool\\',\\n      type: \\'Tool\\',\\n      description: \\'The tool definition.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n            },\\n            {\\n              name: \\'execute\\',\\n              isOptional: true,\\n              type: \\'async (input: INPUT, options: ToolCallOptions) => RESULT | Promise<RESULT> | AsyncIterable<RESULT>\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result or a results iterable. If an iterable is provided, all results but the last one are considered preliminary. If not provided, the tool will not be executed automatically.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolCallOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'ModelMessage[]\\',\\n                      description:\\n                        \\'Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'abortSignal\\',\\n                      type: \\'AbortSignal\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'An optional abort signal that indicates that the overall operation should be aborted.\\',\\n                    },\\n                    {\\n                      name: \\'experimental_context\\',\\n                      type: \\'unknown\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Context that is passed into tool execution. Experimental (can break in patch releases).\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'outputSchema\\',\\n              isOptional: true,\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the output that the tool produces. Used for validation and type inference.\\',\\n            },\\n            {\\n              name: \\'toModelOutput\\',\\n              isOptional: true,\\n              type: \"(output: RESULT) => LanguageModelV3ToolResultPart[\\'output\\']\",\\n              description:\\n                \\'Optional conversion function that maps the tool result to an output that can be used by the language model. If not provided, the tool result will be sent as a JSON object.\\',\\n            },\\n            {\\n              name: \\'onInputStart\\',\\n              isOptional: true,\\n              type: \\'(options: ToolCallOptions) => void | PromiseLike<void>\\',\\n              description:\\n                \\'Optional function that is called when the argument streaming starts. Only called when the tool is used in a streaming context.\\',\\n            },\\n            {\\n              name: \\'onInputDelta\\',\\n              isOptional: true,\\n              type: \\'(options: { inputTextDelta: string } & ToolCallOptions) => void | PromiseLike<void>\\',\\n              description:\\n                \\'Optional function that is called when an argument streaming delta is available. Only called when the tool is used in a streaming context.\\',\\n            },\\n            {\\n              name: \\'onInputAvailable\\',\\n              isOptional: true,\\n              type: \\'(options: { input: INPUT } & ToolCallOptions) => void | PromiseLike<void>\\',\\n              description:\\n                \\'Optional function that is called when a tool call can be started, even if the execute function is not provided.\\',\\n            },\\n            {\\n              name: \\'providerOptions\\',\\n              isOptional: true,\\n              type: \\'ProviderOptions\\',\\n              description:\\n                \\'Additional provider-specific metadata. They are passed through to the provider from the AI SDK and enable provider-specific functionality that can be fully encapsulated in the provider.\\',\\n            },\\n            {\\n              name: \\'type\\',\\n              isOptional: true,\\n              type: \"\\'function\\' | \\'provider-defined\\'\",\\n              description:\\n                \\'The type of the tool. Defaults to \"function\" for regular tools. Use \"provider-defined\" for provider-specific tools.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the tool for provider-defined tools. Should follow the format `<provider-name>.<unique-tool-name>`. Required when type is \"provider-defined\".\\',\\n            },\\n            {\\n              name: \\'name\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool that the user must use in the tool set. Required when type is \"provider-defined\".\\',\\n            },\\n            {\\n              name: \\'args\\',\\n              isOptional: true,\\n              type: \\'Record<string, unknown>\\',\\n              description:\\n                \\'The arguments for configuring the tool. Must match the expected arguments defined by the provider for this tool. Required when type is \"provider-defined\".\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe tool that was passed in.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/22-dynamic-tool.mdx'), name='22-dynamic-tool.mdx', displayName='22-dynamic-tool.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: dynamicTool\\ndescription: Helper function for creating dynamic tools with unknown types\\n---\\n\\n# `dynamicTool()`\\n\\nThe `dynamicTool` function creates tools where the input and output types are not known at compile time. This is useful for scenarios such as:\\n\\n- MCP (Model Context Protocol) tools without schemas\\n- User-defined functions loaded at runtime\\n- Tools loaded from external sources or databases\\n- Dynamic tool generation based on user input\\n\\nUnlike the regular `tool` function, `dynamicTool` accepts and returns `unknown` types, allowing you to work with tools that have runtime-determined schemas.\\n\\n```ts highlight={\"1,4,9,10,11\"}\\nimport { dynamicTool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const customTool = dynamicTool({\\n  description: \\'Execute a custom user-defined function\\',\\n  inputSchema: z.object({}),\\n  // input is typed as \\'unknown\\'\\n  execute: async input => {\\n    const { action, parameters } = input as any;\\n\\n    // Execute your dynamic logic\\n    return {\\n      result: `Executed ${action} with ${JSON.stringify(parameters)}`,\\n    };\\n  },\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { dynamicTool } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tool\\',\\n      type: \\'Object\\',\\n      description: \\'The dynamic tool definition.\\',\\n      properties: [\\n        {\\n          type: \\'Object\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\'\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'FlexibleSchema<unknown>\\',\\n              description:\\n                \\'The schema of the input that the tool expects. While the type is unknown, a schema is still required for validation. You can use Zod schemas with z.unknown() or z.any() for fully dynamic inputs.\\'\\n            },\\n            {\\n              name: \\'execute\\',\\n              type: \\'ToolExecuteFunction<unknown, unknown>\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call. The input is typed as unknown and must be validated/cast at runtime.\\',\\n                properties: [\\n                  {\\n                    type: \"ToolCallOptions\",\\n                    parameters: [\\n                      {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the tool call.\\',\\n                    },\\n                    {\\n                        name: \"messages\",\\n                        type: \"ModelMessage[]\",\\n                        description: \"Messages that were sent to the language model.\"\\n                      },\\n                      {\\n                        name: \"abortSignal\",\\n                        type: \"AbortSignal\",\\n                        isOptional: true,\\n                        description: \"An optional abort signal.\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n            },\\n            {\\n              name: \\'toModelOutput\\',\\n              isOptional: true,\\n              type: \\'(output: unknown) => LanguageModelV3ToolResultPart[\\\\\\'output\\\\\\']\\',\\n              description: \\'Optional conversion function that maps the tool result to an output that can be used by the language model.\\'\\n            },\\n            {\\n              name: \\'providerOptions\\',\\n              isOptional: true,\\n              type: \\'ProviderOptions\\',\\n              description: \\'Additional provider-specific metadata.\\'\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n\\n]}\\n/>\\n\\n### Returns\\n\\nA `Tool<unknown, unknown>` with `type: \\'dynamic\\'` that can be used with `generateText`, `streamText`, and other AI SDK functions.\\n\\n## Type-Safe Usage\\n\\nWhen using dynamic tools alongside static tools, you need to check the `dynamic` flag for proper type narrowing:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // Static tool with known types\\n    weather: weatherTool,\\n    // Dynamic tool with unknown types\\n    custom: dynamicTool({\\n      /* ... */\\n    }),\\n  },\\n  onStepFinish: ({ toolCalls, toolResults }) => {\\n    for (const toolCall of toolCalls) {\\n      if (toolCall.dynamic) {\\n        // Dynamic tool: input/output are \\'unknown\\'\\n        console.log(\\'Dynamic tool:\\', toolCall.toolName);\\n        console.log(\\'Input:\\', toolCall.input);\\n        continue;\\n      }\\n\\n      // Static tools have full type inference\\n      switch (toolCall.toolName) {\\n        case \\'weather\\':\\n          // TypeScript knows the exact types\\n          console.log(toolCall.input.location); // string\\n          break;\\n      }\\n    }\\n  },\\n});\\n```\\n\\n## Usage with `useChat`\\n\\nWhen used with useChat (`UIMessage` format), dynamic tools appear as `dynamic-tool` parts:\\n\\n```tsx\\n{\\n  message.parts.map(part => {\\n    switch (part.type) {\\n      case \\'dynamic-tool\\':\\n        return (\\n          <div>\\n            <h4>Tool: {part.toolName}</h4>\\n            <pre>{JSON.stringify(part.input, null, 2)}</pre>\\n          </div>\\n        );\\n      // ... handle other part types\\n    }\\n  });\\n}\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/23-create-mcp-client.mdx'), name='23-create-mcp-client.mdx', displayName='23-create-mcp-client.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: experimental_createMCPClient\\ndescription: Create a client for connecting to MCP servers\\n---\\n\\n# `experimental_createMCPClient()`\\n\\nCreates a lightweight Model Context Protocol (MCP) client that connects to an MCP server. The client provides:\\n\\n- **Tools**: Automatic conversion between MCP tools and AI SDK tools\\n- **Resources**: Methods to list, read, and discover resource templates from MCP servers\\n- **Prompts**: Methods to list available prompts and retrieve prompt messages\\n- **Elicitation**: Support for handling server requests for additional input during tool execution\\n\\nIt currently does not support accepting notifications from an MCP server, and custom configuration of the client.\\n\\nThis feature is experimental and may change or be removed in the future.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_createMCPClient } from \"@ai-sdk/mcp\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'config\\',\\n      type: \\'MCPClientConfig\\',\\n      description: \\'Configuration for the MCP client.\\',\\n      properties: [\\n        {\\n          type: \\'MCPClientConfig\\',\\n          parameters: [\\n            {\\n              name: \\'transport\\',\\n              type: \\'TransportConfig = MCPTransport | McpSSEServerConfig\\',\\n              description: \\'Configuration for the message transport layer.\\',\\n              properties: [\\n                {\\n                  type: \\'MCPTransport\\',\\n                  description:\\n                    \\'A client transport instance, used explicitly for stdio or custom transports\\',\\n                  parameters: [\\n                    {\\n                      name: \\'start\\',\\n                      type: \\'() => Promise<void>\\',\\n                      description: \\'A method that starts the transport\\',\\n                    },\\n                    {\\n                      name: \\'send\\',\\n                      type: \\'(message: JSONRPCMessage) => Promise<void>\\',\\n                      description:\\n                        \\'A method that sends a message through the transport\\',\\n                    },\\n                    {\\n                      name: \\'close\\',\\n                      type: \\'() => Promise<void>\\',\\n                      description: \\'A method that closes the transport\\',\\n                    },\\n                    {\\n                      name: \\'onclose\\',\\n                      type: \\'() => void\\',\\n                      description:\\n                        \\'A method that is called when the transport is closed\\',\\n                    },\\n                    {\\n                      name: \\'onerror\\',\\n                      type: \\'(error: Error) => void\\',\\n                      description:\\n                        \\'A method that is called when the transport encounters an error\\',\\n                    },\\n                    {\\n                      name: \\'onmessage\\',\\n                      type: \\'(message: JSONRPCMessage) => void\\',\\n                      description:\\n                        \\'A method that is called when the transport receives a message\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'MCPTransportConfig\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'sse\\' | \\'http\",\\n                      description: \\'Use Server-Sent Events for communication\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'URL of the MCP server\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      type: \\'Record<string, string>\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional HTTP headers to be sent with requests.\\',\\n                    },\\n                    {\\n                      name: \\'authProvider\\',\\n                      type: \\'OAuthClientProvider\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Optional OAuth provider for authorization to access protected remote MCP servers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'name\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'Client name. Defaults to \"ai-sdk-mcp-client\"\\',\\n            },\\n            {\\n              name: \\'onUncaughtError\\',\\n              type: \\'(error: unknown) => void\\',\\n              isOptional: true,\\n              description: \\'Handler for uncaught errors\\',\\n            },\\n            {\\n              name: \\'capabilities\\',\\n              type: \\'ClientCapabilities\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional client capabilities to advertise during initialization. For example, set { elicitation: {} } to enable handling elicitation requests from the server.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a Promise that resolves to an `MCPClient` with the following methods:\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tools\\',\\n      type: `async (options?: {\\n        schemas?: TOOL_SCHEMAS\\n      }) => Promise<McpToolSet<TOOL_SCHEMAS>>`,\\n      description: \\'Gets the tools available from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'schemas\\',\\n              type: \\'TOOL_SCHEMAS\\',\\n              isOptional: true,\\n              description:\\n                \\'Schema definitions for compile-time type checking. When not provided, schemas are inferred from the server.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'listResources\\',\\n      type: `async (options?: {\\n        params?: PaginatedRequest[\\'params\\'];\\n        options?: RequestOptions;\\n      }) => Promise<ListResourcesResult>`,\\n      description: \\'Lists all available resources from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'params\\',\\n              type: \"PaginatedRequest[\\'params\\']\",\\n              isOptional: true,\\n              description: \\'Optional pagination parameters including cursor.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'readResource\\',\\n      type: `async (args: {\\n        uri: string;\\n        options?: RequestOptions;\\n      }) => Promise<ReadResourceResult>`,\\n      description: \\'Reads the contents of a specific resource by URI.\\',\\n      properties: [\\n        {\\n          type: \\'args\\',\\n          parameters: [\\n            {\\n              name: \\'uri\\',\\n              type: \\'string\\',\\n              description: \\'The URI of the resource to read.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'listResourceTemplates\\',\\n      type: `async (options?: {\\n        options?: RequestOptions;\\n      }) => Promise<ListResourceTemplatesResult>`,\\n      description:\\n        \\'Lists all available resource templates from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'listPrompts\\',\\n      type: `async (options?: {\\n        params?: PaginatedRequest[\\'params\\'];\\n        options?: RequestOptions;\\n      }) => Promise<ListPromptsResult>`,\\n      description: \\'Lists available prompts from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'params\\',\\n              type: \"PaginatedRequest[\\'params\\']\",\\n              isOptional: true,\\n              description: \\'Optional pagination parameters including cursor.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'getPrompt\\',\\n      type: `async (args: {\\n        name: string;\\n        arguments?: Record<string, unknown>;\\n        options?: RequestOptions;\\n      }) => Promise<GetPromptResult>`,\\n      description: \\'Retrieves a prompt by name, optionally passing arguments.\\',\\n      properties: [\\n        {\\n          type: \\'args\\',\\n          parameters: [\\n            {\\n              name: \\'name\\',\\n              type: \\'string\\',\\n              description: \\'Prompt name to retrieve.\\',\\n            },\\n            {\\n              name: \\'arguments\\',\\n              type: \\'Record<string, unknown>\\',\\n              isOptional: true,\\n              description: \\'Optional arguments to fill into the prompt.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onElicitationRequest\\',\\n      type: `(\\n        schema: typeof ElicitationRequestSchema,\\n        handler: (request: ElicitationRequest) => Promise<ElicitResult> | ElicitResult\\n      ) => void`,\\n      description:\\n        \\'Registers a handler for elicitation requests from the MCP server. The handler receives requests when the server needs additional input during tool execution.\\',\\n      properties: [\\n        {\\n          type: \\'parameters\\',\\n          parameters: [\\n            {\\n              name: \\'schema\\',\\n              type: \\'typeof ElicitationRequestSchema\\',\\n              description:\\n                \\'The schema to validate requests against. Must be ElicitationRequestSchema.\\',\\n            },\\n            {\\n              name: \\'handler\\',\\n              type: \\'(request: ElicitationRequest) => Promise<ElicitResult> | ElicitResult\\',\\n              description:\\n                \\'A function that handles the elicitation request. The request contains a message and requestedSchema. The handler must return an object with an action (\"accept\", \"decline\", or \"cancel\") and optionally content when accepting.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'close\\',\\n      type: \\'async () => void\\',\\n      description:\\n        \\'Closes the connection to the MCP server and cleans up resources.\\',\\n    },\\n  ]}\\n/>\\n\\n## Example\\n\\n```typescript\\nimport {\\n  experimental_createMCPClient as createMCPClient,\\n  generateText,\\n} from \\'@ai-sdk/mcp\\';\\nimport { Experimental_StdioMCPTransport } from \\'@ai-sdk/mcp/mcp-stdio\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nlet client;\\n\\ntry {\\n  client = await createMCPClient({\\n    transport: new Experimental_StdioMCPTransport({\\n      command: \\'node server.js\\',\\n    }),\\n  });\\n\\n  const tools = await client.tools();\\n\\n  const response = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    tools,\\n    messages: [{ role: \\'user\\', content: \\'Query the data\\' }],\\n  });\\n\\n  console.log(response);\\n} catch (error) {\\n  console.error(\\'Error:\\', error);\\n} finally {\\n  // ensure the client is closed even if an error occurs\\n  if (client) {\\n    await client.close();\\n  }\\n}\\n```\\n\\n## Error Handling\\n\\nThe client throws `MCPClientError` for:\\n\\n- Client initialization failures\\n- Protocol version mismatches\\n- Missing server capabilities\\n- Connection failures\\n\\nFor tool execution, errors are propagated as `CallToolError` errors.\\n\\nFor unknown errors, the client exposes an `onUncaughtError` callback that can be used to manually log or handle errors that are not covered by known error types.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/24-mcp-stdio-transport.mdx'), name='24-mcp-stdio-transport.mdx', displayName='24-mcp-stdio-transport.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Experimental_StdioMCPTransport\\ndescription: Create a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams\\n---\\n\\n# `Experimental_StdioMCPTransport`\\n\\nCreates a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams. This transport is only supported in Node.js environments.\\n\\nThis feature is experimental and may change or be removed in the future.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { Experimental_StdioMCPTransport } from \"ai/mcp-stdio\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'config\\',\\n      type: \\'StdioConfig\\',\\n      description: \\'Configuration for the MCP client.\\',\\n      properties: [\\n        {\\n          type: \\'StdioConfig\\',\\n          parameters: [\\n            {\\n              name: \\'command\\',\\n              type: \\'string\\',\\n              description: \\'The command to run the MCP server.\\',\\n            },\\n            {\\n              name: \\'args\\',\\n              type: \\'string[]\\',\\n              isOptional: true,\\n              description: \\'The arguments to pass to the MCP server.\\',\\n            },\\n            {\\n              name: \\'env\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description:\\n                \\'The environment variables to set for the MCP server.\\',\\n            },\\n            {\\n              name: \\'stderr\\',\\n              type: \\'IOType | Stream | number\\',\\n              isOptional: true,\\n              description: \"The stream to write the MCP server\\'s stderr to.\",\\n            },\\n            {\\n              name: \\'cwd\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The current working directory for the MCP server.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/25-json-schema.mdx'), name='25-json-schema.mdx', displayName='25-json-schema.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: jsonSchema\\ndescription: Helper function for creating JSON schemas\\n---\\n\\n# `jsonSchema()`\\n\\n`jsonSchema` is a helper function that creates a JSON schema object that is compatible with the AI SDK.\\nIt takes the JSON schema and an optional validation function as inputs, and can be typed.\\n\\nYou can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n`jsonSchema` is an alternative to using Zod schemas that provides you with flexibility in dynamic situations\\n(e.g. when using OpenAPI definitions) or for using other validation libraries.\\n\\n```ts\\nimport { jsonSchema } from \\'ai\\';\\n\\nconst mySchema = jsonSchema<{\\n  recipe: {\\n    name: string;\\n    ingredients: { name: string; amount: string }[];\\n    steps: string[];\\n  };\\n}>({\\n  type: \\'object\\',\\n  properties: {\\n    recipe: {\\n      type: \\'object\\',\\n      properties: {\\n        name: { type: \\'string\\' },\\n        ingredients: {\\n          type: \\'array\\',\\n          items: {\\n            type: \\'object\\',\\n            properties: {\\n              name: { type: \\'string\\' },\\n              amount: { type: \\'string\\' },\\n            },\\n            required: [\\'name\\', \\'amount\\'],\\n          },\\n        },\\n        steps: {\\n          type: \\'array\\',\\n          items: { type: \\'string\\' },\\n        },\\n      },\\n      required: [\\'name\\', \\'ingredients\\', \\'steps\\'],\\n    },\\n  },\\n  required: [\\'recipe\\'],\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { jsonSchema } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'schema\\',\\n      type: \\'JSONSchema7\\',\\n      description: \\'The JSON schema definition.\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'SchemaOptions\\',\\n      description: \\'Additional options for the JSON schema.\\',\\n      properties: [\\n        {\\n          type: \\'SchemaOptions\\',\\n          parameters: [\\n            {\\n              name: \\'validate\\',\\n              isOptional: true,\\n              type: \\'(value: unknown) => { success: true; value: OBJECT } | { success: false; error: Error };\\',\\n              description:\\n                \\'A function that validates the value against the JSON schema. If the value is valid, the function should return an object with a `success` property set to `true` and a `value` property set to the validated value. If the value is invalid, the function should return an object with a `success` property set to `false` and an `error` property set to the error.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA JSON schema object that is compatible with the AI SDK.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/26-zod-schema.mdx'), name='26-zod-schema.mdx', displayName='26-zod-schema.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: zodSchema\\ndescription: Helper function for creating Zod schemas\\n---\\n\\n# `zodSchema()`\\n\\n`zodSchema` is a helper function that converts a Zod schema into a JSON schema object that is compatible with the AI SDK.\\nIt takes a Zod schema and optional configuration as inputs, and returns a typed schema.\\n\\nYou can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n<Note>\\n  You can also pass Zod objects directly to the AI SDK functions. Internally,\\n  the AI SDK will convert the Zod schema to a JSON schema using `zodSchema()`.\\n  However, if you want to specify options such as `useReferences`, you can pass\\n  the `zodSchema()` helper function instead.\\n</Note>\\n\\n<Note type=\"warning\">\\n  When using `.meta()` or `.describe()` to add metadata to your Zod schemas,\\n  make sure these methods are called **at the end** of the schema chain.\\n  \\n  metadata is attached to a specific schema\\n  instance, and most schema methods (`.min()`, `.optional()`, `.extend()`, etc.)\\n  return a new schema instance that does not inherit metadata from the previous one.\\n  Due to Zod\\'s immutability, metadata is only included in the JSON schema output\\n  if `.meta()` or `.describe()` is the last method in the chain.\\n\\n```ts\\n// ❌ Metadata will be lost - .min() returns a new instance without metadata\\nz.string().meta({ describe: \\'first name\\' }).min(1);\\n\\n// ✅ Metadata is preserved - .meta() is the final method\\nz.string().min(1).meta({ describe: \\'first name\\' });\\n```\\n\\n</Note>\\n\\n## Example with recursive schemas\\n\\n```ts\\nimport { zodSchema } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Define a base category schema\\nconst baseCategorySchema = z.object({\\n  name: z.string(),\\n});\\n\\n// Define the recursive Category type\\ntype Category = z.infer<typeof baseCategorySchema> & {\\n  subcategories: Category[];\\n};\\n\\n// Create the recursive schema using z.lazy\\nconst categorySchema: z.ZodType<Category> = baseCategorySchema.extend({\\n  subcategories: z.lazy(() => categorySchema.array()),\\n});\\n\\n// Create the final schema with useReferences enabled for recursive support\\nconst mySchema = zodSchema(\\n  z.object({\\n    category: categorySchema,\\n  }),\\n  { useReferences: true },\\n);\\n```\\n\\n## Import\\n\\n<Snippet text={`import { zodSchema } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'zodSchema\\',\\n      type: \\'z.Schema\\',\\n      description: \\'The Zod schema definition.\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'object\\',\\n      description: \\'Additional options for the schema conversion.\\',\\n      properties: [\\n        {\\n          type: \\'object\\',\\n          parameters: [\\n            {\\n              name: \\'useReferences\\',\\n              isOptional: true,\\n              type: \\'boolean\\',\\n              description:\\n                \\'Enables support for references in the schema. This is required for recursive schemas, e.g. with `z.lazy`. However, not all language models and providers support such references. Defaults to `false`.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/27-valibot-schema.mdx'), name='27-valibot-schema.mdx', displayName='27-valibot-schema.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: valibotSchema\\ndescription: Helper function for creating Valibot schemas\\n---\\n\\n# `valibotSchema()`\\n\\n<Note type=\"warning\">`valibotSchema` is currently experimental.</Note>\\n\\n`valibotSchema` is a helper function that converts a Valibot schema into a JSON schema object that is compatible with the AI SDK.\\nIt takes a Valibot schema as input, and returns a typed schema.\\n\\nYou can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n## Example\\n\\n```ts\\nimport { valibotSchema } from \\'@ai-sdk/valibot\\';\\nimport { object, string, array } from \\'valibot\\';\\n\\nconst recipeSchema = valibotSchema(\\n  object({\\n    name: string(),\\n    ingredients: array(\\n      object({\\n        name: string(),\\n        amount: string(),\\n      }),\\n    ),\\n    steps: array(string()),\\n  }),\\n);\\n```\\n\\n## Import\\n\\n<Snippet text={`import { valibotSchema } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'valibotSchema\\',\\n      type: \\'GenericSchema<unknown, T>\\',\\n      description: \\'The Valibot schema definition.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/30-model-message.mdx'), name='30-model-message.mdx', displayName='30-model-message.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: ModelMessage\\ndescription: Message types for AI SDK Core (API Reference)\\n---\\n\\n# `ModelMessage`\\n\\n`ModelMessage` represents the fundamental message structure used with AI SDK Core functions.\\nIt encompasses various message types that can be used in the `messages` field of any AI SDK Core functions.\\n\\nYou can access the Zod schema for `ModelMessage` with the `modelMessageSchema` export.\\n\\n## `ModelMessage` Types\\n\\n### `SystemModelMessage`\\n\\nA system message that can contain system information.\\n\\n```typescript\\ntype SystemModelMessage = {\\n  role: \\'system\\';\\n  content: string;\\n};\\n```\\n\\nYou can access the Zod schema for `SystemModelMessage` with the `systemModelMessageSchema` export.\\n\\n<Note>\\n  Using the \"system\" property instead of a system message is recommended to\\n  enhance resilience against prompt injection attacks.\\n</Note>\\n\\n### `UserModelMessage`\\n\\nA user message that can contain text or a combination of text, images, and files.\\n\\n```typescript\\ntype UserModelMessage = {\\n  role: \\'user\\';\\n  content: UserContent;\\n};\\n\\ntype UserContent = string | Array<TextPart | ImagePart | FilePart>;\\n```\\n\\nYou can access the Zod schema for `UserModelMessage` with the `userModelMessageSchema` export.\\n\\n### `AssistantModelMessage`\\n\\nAn assistant message that can contain text, tool calls, or a combination of both.\\n\\n```typescript\\ntype AssistantModelMessage = {\\n  role: \\'assistant\\';\\n  content: AssistantContent;\\n};\\n\\ntype AssistantContent = string | Array<TextPart | ToolCallPart>;\\n```\\n\\nYou can access the Zod schema for `AssistantModelMessage` with the `assistantModelMessageSchema` export.\\n\\n### `ToolModelMessage`\\n\\nA tool message that contains the result of one or more tool calls.\\n\\n```typescript\\ntype ToolModelMessage = {\\n  role: \\'tool\\';\\n  content: ToolContent;\\n};\\n\\ntype ToolContent = Array<ToolResultPart>;\\n```\\n\\nYou can access the Zod schema for `ToolModelMessage` with the `toolModelMessageSchema` export.\\n\\n## `ModelMessage` Parts\\n\\n### `TextPart`\\n\\nRepresents a text content part of a prompt. It contains a string of text.\\n\\n```typescript\\nexport interface TextPart {\\n  type: \\'text\\';\\n  /**\\n   * The text content.\\n   */\\n  text: string;\\n}\\n```\\n\\n### `ImagePart`\\n\\nRepresents an image part in a user message.\\n\\n```typescript\\nexport interface ImagePart {\\n  type: \\'image\\';\\n\\n  /**\\n   * Image data. Can either be:\\n   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\\n   * - URL: a URL that points to the image\\n   */\\n  image: DataContent | URL;\\n\\n  /**\\n   * Optional IANA media type of the image.\\n   * We recommend leaving this out as it will be detected automatically.\\n   */\\n  mediaType?: string;\\n}\\n```\\n\\n### `FilePart`\\n\\nRepresents an file part in a user message.\\n\\n```typescript\\nexport interface FilePart {\\n  type: \\'file\\';\\n\\n  /**\\n   * File data. Can either be:\\n   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\\n   * - URL: a URL that points to the file\\n   */\\n  data: DataContent | URL;\\n\\n  /**\\n   * Optional filename of the file.\\n   */\\n  filename?: string;\\n\\n  /**\\n   * IANA media type of the file.\\n   */\\n  mediaType: string;\\n}\\n```\\n\\n### `ToolCallPart`\\n\\nRepresents a tool call content part of a prompt, typically generated by the AI model.\\n\\n```typescript\\nexport interface ToolCallPart {\\n  type: \\'tool-call\\';\\n\\n  /**\\n   * ID of the tool call. This ID is used to match the tool call with the tool result.\\n   */\\n  toolCallId: string;\\n\\n  /**\\n   * Name of the tool that is being called.\\n   */\\n  toolName: string;\\n\\n  /**\\n   * Arguments of the tool call. This is a JSON-serializable object that matches the tool\\'s input schema.\\n   */\\n  args: unknown;\\n}\\n```\\n\\n### `ToolResultPart`\\n\\nRepresents the result of a tool call in a tool message.\\n\\n```typescript\\nexport interface ToolResultPart {\\n  type: \\'tool-result\\';\\n\\n  /**\\n   * ID of the tool call that this result is associated with.\\n   */\\n  toolCallId: string;\\n\\n  /**\\n   * Name of the tool that generated this result.\\n   */\\n  toolName: string;\\n\\n  /**\\n   * Result of the tool call. This is a JSON-serializable object.\\n   */\\n  output: LanguageModelV3ToolResultOutput;\\n\\n  /**\\n  Additional provider-specific metadata. They are passed through\\n  to the provider from the AI SDK and enable provider-specific\\n  functionality that can be fully encapsulated in the provider.\\n  */\\n  providerOptions?: ProviderOptions;\\n}\\n```\\n\\n### `LanguageModelV3ToolResultOutput`\\n\\n```ts\\n/**\\n * Output of a tool result.\\n */\\nexport type ToolResultOutput =\\n  | {\\n      /**\\n       * Text tool output that should be directly sent to the API.\\n       */\\n      type: \\'text\\';\\n      value: string;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'json\\';\\n      value: JSONValue;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      /**\\n       * Type when the user has denied the execution of the tool call.\\n       */\\n      type: \\'execution-denied\\';\\n\\n      /**\\n       * Optional reason for the execution denial.\\n       */\\n      reason?: string;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'error-text\\';\\n      value: string;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'error-json\\';\\n      value: JSONValue;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'content\\';\\n      value: Array<\\n        | {\\n            type: \\'text\\';\\n\\n            /**\\nText content.\\n*/\\n            text: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * @deprecated Use image-data or file-data instead.\\n             */\\n            type: \\'media\\';\\n            data: string;\\n            mediaType: string;\\n          }\\n        | {\\n            type: \\'file-data\\';\\n\\n            /**\\nBase-64 encoded media data.\\n*/\\n            data: string;\\n\\n            /**\\nIANA media type.\\n@see https://www.iana.org/assignments/media-types/media-types.xhtml\\n*/\\n            mediaType: string;\\n\\n            /**\\n             * Optional filename of the file.\\n             */\\n            filename?: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            type: \\'file-url\\';\\n\\n            /**\\n             * URL of the file.\\n             */\\n            url: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            type: \\'file-id\\';\\n\\n            /**\\n             * ID of the file.\\n             *\\n             * If you use multiple providers, you need to\\n             * specify the provider specific ids using\\n             * the Record option. The key is the provider\\n             * name, e.g. \\'openai\\' or \\'anthropic\\'.\\n             */\\n            fileId: string | Record<string, string>;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Images that are referenced using base64 encoded data.\\n             */\\n            type: \\'image-data\\';\\n\\n            /**\\nBase-64 encoded image data.\\n*/\\n            data: string;\\n\\n            /**\\nIANA media type.\\n@see https://www.iana.org/assignments/media-types/media-types.xhtml\\n*/\\n            mediaType: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Images that are referenced using a URL.\\n             */\\n            type: \\'image-url\\';\\n\\n            /**\\n             * URL of the image.\\n             */\\n            url: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Images that are referenced using a provider file id.\\n             */\\n            type: \\'image-file-id\\';\\n\\n            /**\\n             * Image that is referenced using a provider file id.\\n             *\\n             * If you use multiple providers, you need to\\n             * specify the provider specific ids using\\n             * the Record option. The key is the provider\\n             * name, e.g. \\'openai\\' or \\'anthropic\\'.\\n             */\\n            fileId: string | Record<string, string>;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Custom content part. This can be used to implement\\n             * provider-specific content parts.\\n             */\\n            type: \\'custom\\';\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n      >;\\n    };\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/31-ui-message.mdx'), name='31-ui-message.mdx', displayName='31-ui-message.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: UIMessage\\ndescription: API Reference for UIMessage\\n---\\n\\n# `UIMessage`\\n\\n`UIMessage` serves as the source of truth for your application's state, representing the complete message history including metadata, data parts, and all contextual information. In contrast to `ModelMessage`, which represents the state or context passed to the model, `UIMessage` contains the full application state needed for UI rendering and client-side functionality.\\n\\n## Type Safety\\n\\n`UIMessage` is designed to be type-safe and accepts three generic parameters to ensure proper typing throughout your application:\\n\\n1. **`METADATA`** - Custom metadata type for additional message information\\n2. **`DATA_PARTS`** - Custom data part types for structured data components\\n3. **`TOOLS`** - Tool definitions for type-safe tool interactions\\n\\n## Creating Your Own UIMessage Type\\n\\nHere's an example of how to create a custom typed UIMessage for your application:\\n\\n```typescript\\nimport { InferUITools, ToolSet, UIMessage, tool } from 'ai';\\nimport z from 'zod';\\n\\nconst metadataSchema = z.object({\\n  someMetadata: z.string().datetime(),\\n});\\n\\ntype MyMetadata = z.infer<typeof metadataSchema>;\\n\\nconst dataPartSchema = z.object({\\n  someDataPart: z.object({}),\\n  anotherDataPart: z.object({}),\\n});\\n\\ntype MyDataPart = z.infer<typeof dataPartSchema>;\\n\\nconst tools = {\\n  someTool: tool({}),\\n} satisfies ToolSet;\\n\\ntype MyTools = InferUITools<typeof tools>;\\n\\nexport type MyUIMessage = UIMessage<MyMetadata, MyDataPart, MyTools>;\\n```\\n\\n## `UIMessage` Interface\\n\\n```typescript\\ninterface UIMessage<\\n  METADATA = unknown,\\n  DATA_PARTS extends UIDataTypes = UIDataTypes,\\n  TOOLS extends UITools = UITools,\\n> {\\n  /**\\n   * A unique identifier for the message.\\n   */\\n  id: string;\\n\\n  /**\\n   * The role of the message.\\n   */\\n  role: 'system' | 'user' | 'assistant';\\n\\n  /**\\n   * The metadata of the message.\\n   */\\n  metadata?: METADATA;\\n\\n  /**\\n   * The parts of the message. Use this for rendering the message in the UI.\\n   */\\n  parts: Array<UIMessagePart<DATA_PARTS, TOOLS>>;\\n}\\n```\\n\\n## `UIMessagePart` Types\\n\\n### `TextUIPart`\\n\\nA text part of a message.\\n\\n```typescript\\ntype TextUIPart = {\\n  type: 'text';\\n  /**\\n   * The text content.\\n   */\\n  text: string;\\n  /**\\n   * The state of the text part.\\n   */\\n  state?: 'streaming' | 'done';\\n};\\n```\\n\\n### `ReasoningUIPart`\\n\\nA reasoning part of a message.\\n\\n```typescript\\ntype ReasoningUIPart = {\\n  type: 'reasoning';\\n  /**\\n   * The reasoning text.\\n   */\\n  text: string;\\n  /**\\n   * The state of the reasoning part.\\n   */\\n  state?: 'streaming' | 'done';\\n  /**\\n   * The provider metadata.\\n   */\\n  providerMetadata?: Record<string, any>;\\n};\\n```\\n\\n### `ToolUIPart`\\n\\nA tool part of a message that represents tool invocations and their results.\\n\\n<Note>\\n  The type is based on the name of the tool (e.g., `tool-someTool` for a tool\\n  named `someTool`).\\n</Note>\\n\\n```typescript\\ntype ToolUIPart<TOOLS extends UITools = UITools> = ValueOf<{\\n  [NAME in keyof TOOLS & string]: {\\n    type: `tool-${NAME}`;\\n    toolCallId: string;\\n  } & (\\n    | {\\n        state: 'input-streaming';\\n        input: DeepPartial<TOOLS[NAME]['input']> | undefined;\\n        providerExecuted?: boolean;\\n        output?: never;\\n        errorText?: never;\\n      }\\n    | {\\n        state: 'input-available';\\n        input: TOOLS[NAME]['input'];\\n        providerExecuted?: boolean;\\n        output?: never;\\n        errorText?: never;\\n      }\\n    | {\\n        state: 'output-available';\\n        input: TOOLS[NAME]['input'];\\n        output: TOOLS[NAME]['output'];\\n        errorText?: never;\\n        providerExecuted?: boolean;\\n      }\\n    | {\\n        state: 'output-error';\\n        input: TOOLS[NAME]['input'];\\n        output?: never;\\n        errorText: string;\\n        providerExecuted?: boolean;\\n      }\\n  );\\n}>;\\n```\\n\\n### `SourceUrlUIPart`\\n\\nA source URL part of a message.\\n\\n```typescript\\ntype SourceUrlUIPart = {\\n  type: 'source-url';\\n  sourceId: string;\\n  url: string;\\n  title?: string;\\n  providerMetadata?: Record<string, any>;\\n};\\n```\\n\\n### `SourceDocumentUIPart`\\n\\nA document source part of a message.\\n\\n```typescript\\ntype SourceDocumentUIPart = {\\n  type: 'source-document';\\n  sourceId: string;\\n  mediaType: string;\\n  title: string;\\n  filename?: string;\\n  providerMetadata?: Record<string, any>;\\n};\\n```\\n\\n### `FileUIPart`\\n\\nA file part of a message.\\n\\n```typescript\\ntype FileUIPart = {\\n  type: 'file';\\n  /**\\n   * IANA media type of the file.\\n   */\\n  mediaType: string;\\n  /**\\n   * Optional filename of the file.\\n   */\\n  filename?: string;\\n  /**\\n   * The URL of the file.\\n   * It can either be a URL to a hosted file or a Data URL.\\n   */\\n  url: string;\\n};\\n```\\n\\n### `DataUIPart`\\n\\nA data part of a message for custom data types.\\n\\n<Note>\\n  The type is based on the name of the data part (e.g., `data-someDataPart` for\\n  a data part named `someDataPart`).\\n</Note>\\n\\n```typescript\\ntype DataUIPart<DATA_TYPES extends UIDataTypes> = ValueOf<{\\n  [NAME in keyof DATA_TYPES & string]: {\\n    type: `data-${NAME}`;\\n    id?: string;\\n    data: DATA_TYPES[NAME];\\n  };\\n}>;\\n```\\n\\n### `StepStartUIPart`\\n\\nA step boundary part of a message.\\n\\n```typescript\\ntype StepStartUIPart = {\\n  type: 'step-start';\\n};\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/32-validate-ui-messages.mdx'), name='32-validate-ui-messages.mdx', displayName='32-validate-ui-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: validateUIMessages\\ndescription: API Reference for validateUIMessages\\n---\\n\\n# `validateUIMessages`\\n\\n`validateUIMessages` is an async function that validates UI messages against schemas for metadata, data parts, and tools. It ensures type safety and data integrity for your message arrays before processing or rendering.\\n\\n## Basic Usage\\n\\nSimple validation without custom schemas:\\n\\n```typescript\\nimport { validateUIMessages } from 'ai';\\n\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    parts: [{ type: 'text', text: 'Hello!' }],\\n  },\\n];\\n\\nconst validatedMessages = await validateUIMessages({\\n  messages,\\n});\\n```\\n\\n## Advanced Usage\\n\\nComprehensive validation with custom metadata, data parts, and tools:\\n\\n```typescript\\nimport { validateUIMessages, tool } from 'ai';\\nimport { z } from 'zod';\\n\\n// Define schemas\\nconst metadataSchema = z.object({\\n  timestamp: z.string().datetime(),\\n  userId: z.string(),\\n});\\n\\nconst dataSchemas = {\\n  chart: z.object({\\n    data: z.array(z.number()),\\n    labels: z.array(z.string()),\\n  }),\\n  image: z.object({\\n    url: z.string().url(),\\n    caption: z.string(),\\n  }),\\n};\\n\\nconst tools = {\\n  weather: tool({\\n    description: 'Get weather info',\\n    parameters: z.object({\\n      location: z.string(),\\n    }),\\n    execute: async ({ location }) => `Weather in ${location}: sunny`,\\n  }),\\n};\\n\\n// Messages with custom parts\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    metadata: { timestamp: '2024-01-01T00:00:00Z', userId: 'user123' },\\n    parts: [\\n      { type: 'text', text: 'Show me a chart' },\\n      {\\n        type: 'data-chart',\\n        data: { data: [1, 2, 3], labels: ['A', 'B', 'C'] },\\n      },\\n    ],\\n  },\\n  {\\n    id: '2',\\n    role: 'assistant',\\n    parts: [\\n      {\\n        type: 'tool-weather',\\n        toolCallId: 'call_123',\\n        state: 'output-available',\\n        input: { location: 'San Francisco' },\\n        output: 'Weather in San Francisco: sunny',\\n      },\\n    ],\\n  },\\n];\\n\\n// Validate with all schemas\\nconst validatedMessages = await validateUIMessages({\\n  messages,\\n  metadataSchema,\\n  dataSchemas,\\n  tools,\\n});\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/33-safe-validate-ui-messages.mdx'), name='33-safe-validate-ui-messages.mdx', displayName='33-safe-validate-ui-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: safeValidateUIMessages\\ndescription: API Reference for safeValidateUIMessages\\n---\\n\\n# `safeValidateUIMessages`\\n\\n`safeValidateUIMessages` is an async function that validates UI messages like [`validateUIMessages`](https://ai-sdk.dev/docs/reference/ai-sdk-core/validate-ui-messages), but instead of throwing it returns an object with a `success` key and either `data` or `error`.\\n\\n## Basic Usage\\n\\nSimple validation without custom schemas:\\n\\n```typescript\\nimport { safeValidateUIMessages } from 'ai';\\n\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    parts: [{ type: 'text', text: 'Hello!' }],\\n  },\\n];\\n\\nconst result = await safeValidateUIMessages({\\n  messages,\\n});\\n\\nif (!result.success) {\\n  console.error(result.error.message);\\n} else {\\n  const validatedMessages = result.data;\\n}\\n```\\n\\n## Advanced Usage\\n\\nComprehensive validation with custom metadata, data parts, and tools:\\n\\n```typescript\\nimport { safeValidateUIMessages, tool } from 'ai';\\nimport { z } from 'zod';\\n\\n// Define schemas\\nconst metadataSchema = z.object({\\n  timestamp: z.string().datetime(),\\n  userId: z.string(),\\n});\\n\\nconst dataSchemas = {\\n  chart: z.object({\\n    data: z.array(z.number()),\\n    labels: z.array(z.string()),\\n  }),\\n  image: z.object({\\n    url: z.string().url(),\\n    caption: z.string(),\\n  }),\\n};\\n\\nconst tools = {\\n  weather: tool({\\n    description: 'Get weather info',\\n    parameters: z.object({\\n      location: z.string(),\\n    }),\\n    execute: async ({ location }) => `Weather in ${location}: sunny`,\\n  }),\\n};\\n\\n// Messages with custom parts\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    metadata: { timestamp: '2024-01-01T00:00:00Z', userId: 'user123' },\\n    parts: [\\n      { type: 'text', text: 'Show me a chart' },\\n      {\\n        type: 'data-chart',\\n        data: { data: [1, 2, 3], labels: ['A', 'B', 'C'] },\\n      },\\n    ],\\n  },\\n  {\\n    id: '2',\\n    role: 'assistant',\\n    parts: [\\n      {\\n        type: 'tool-weather',\\n        toolCallId: 'call_123',\\n        state: 'output-available',\\n        input: { location: 'San Francisco' },\\n        output: 'Weather in San Francisco: sunny',\\n      },\\n    ],\\n  },\\n];\\n\\n// Validate with all schemas\\nconst result = await safeValidateUIMessages({\\n  messages,\\n  metadataSchema,\\n  dataSchemas,\\n  tools,\\n});\\n\\nif (!result.success) {\\n  console.error(result.error.message);\\n} else {\\n  const validatedMessages = result.data;\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/40-provider-registry.mdx'), name='40-provider-registry.mdx', displayName='40-provider-registry.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createProviderRegistry\\ndescription: Registry for managing multiple providers and models (API Reference)\\n---\\n\\n# `createProviderRegistry()`\\n\\nWhen you work with multiple providers and models, it is often desirable to manage them\\nin a central place and access the models through simple string ids.\\n\\n`createProviderRegistry` lets you create a registry with multiple providers that you\\ncan access by their ids in the format `providerId:modelId`.\\n\\n### Setup\\n\\nYou can create a registry with multiple providers and models using `createProviderRegistry`.\\n\\n```ts\\nimport { anthropic } from \\'@ai-sdk/anthropic\\';\\nimport { createOpenAI } from \\'@ai-sdk/openai\\';\\nimport { createProviderRegistry } from \\'ai\\';\\n\\nexport const registry = createProviderRegistry({\\n  // register provider with prefix and default setup:\\n  anthropic,\\n\\n  // register provider with prefix and custom setup:\\n  openai: createOpenAI({\\n    apiKey: process.env.OPENAI_API_KEY,\\n  }),\\n});\\n```\\n\\n### Custom Separator\\n\\nBy default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator by passing a `separator` option:\\n\\n```ts\\nconst registry = createProviderRegistry(\\n  {\\n    anthropic,\\n    openai,\\n  },\\n  { separator: \\' > \\' },\\n);\\n\\n// Now you can use the custom separator\\nconst model = registry.languageModel(\\'anthropic > claude-3-opus-20240229\\');\\n```\\n\\n### Language models\\n\\nYou can access language models by using the `languageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateText } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { text } = await generateText({\\n  model: registry.languageModel(\\'openai:gpt-4.1\\'),\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\n### Text embedding models\\n\\nYou can access text embedding models by using the `.embeddingModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { embed } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { embedding } = await embed({\\n  model: registry.embeddingModel(\\'openai:text-embedding-3-small\\'),\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n### Image models\\n\\nYou can access image models by using the `imageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateImage } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { image } = await generateImage({\\n  model: registry.imageModel(\\'openai:dall-e-3\\'),\\n  prompt: \\'A beautiful sunset over a calm ocean\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { createProviderRegistry } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'providers\\',\\n      type: \\'Record<string, Provider>\\',\\n      description:\\n        \\'The unique identifier for the provider. It should be unique within the registry.\\',\\n      properties: [\\n        {\\n          type: \\'Provider\\',\\n          parameters: [\\n            {\\n              name: \\'languageModel\\',\\n              type: \\'(id: string) => LanguageModel\\',\\n              description:\\n                \\'A function that returns a language model by its id.\\',\\n            },\\n            {\\n              name: \\'embeddingModel\\',,\\n              type: \\'(id: string) => EmbeddingModel<string>\\',\\n              description:\\n                \\'A function that returns a text embedding model by its id.\\',\\n            },\\n            {\\n              name: \\'imageModel\\',\\n              type: \\'(id: string) => ImageModel\\',\\n              description: \\'A function that returns an image model by its id.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'object\\',\\n      description: \\'Optional configuration for the registry.\\',\\n      properties: [\\n        {\\n          type: \\'Options\\',\\n          parameters: [\\n            {\\n              name: \\'separator\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Custom separator between provider and model IDs. Defaults to \":\".\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe `createProviderRegistry` function returns a `Provider` instance. It has the following methods:\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'languageModel\\',\\n      type: \\'(id: string) => LanguageModel\\',\\n      description:\\n        \\'A function that returns a language model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'embeddingModel\\',,\\n      type: \\'(id: string) => EmbeddingModel<string>\\',\\n      description:\\n        \\'A function that returns a text embedding model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'imageModel\\',\\n      type: \\'(id: string) => ImageModel\\',\\n      description:\\n        \\'A function that returns an image model by its id (format: providerId:modelId)\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/42-custom-provider.mdx'), name='42-custom-provider.mdx', displayName='42-custom-provider.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: customProvider\\ndescription: Custom provider that uses models from a different provider (API Reference)\\n---\\n\\n# `customProvider()`\\n\\nWith a custom provider, you can map ids to any model.\\nThis allows you to set up custom model configurations, alias names, and more.\\nThe custom provider also supports a fallback provider, which is useful for\\nwrapping existing providers and adding additional functionality.\\n\\n### Example: custom model settings\\n\\nYou can create a custom provider using `customProvider`.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { customProvider } from \\'ai\\';\\n\\n// custom provider with different model settings:\\nexport const myOpenAI = customProvider({\\n  languageModels: {\\n    // replacement model with custom settings:\\n    \\'gpt-4\\': wrapLanguageModel({\\n      model: openai(\\'gpt-4\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n    // alias model with custom settings:\\n    \\'gpt-4o-reasoning-high\\': wrapLanguageModel({\\n      model: openai(\\'gpt-4o\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n  },\\n  fallbackProvider: openai,\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import {  customProvider } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'languageModels\\',\\n      type: \\'Record<string, LanguageModel>\\',\\n      isOptional: true,\\n      description:\\n        \\'A record of language models, where keys are model IDs and values are LanguageModel instances.\\',\\n    },\\n    {\\n      name: \\'.embeddingModels\\',\\n      type: \\'Record<string, EmbeddingModel<string>>\\',\\n      isOptional: true,\\n      description:\\n        \\'A record of text embedding models, where keys are model IDs and values are EmbeddingModel<string> instances.\\',\\n    },\\n    {\\n      name: \\'imageModels\\',\\n      type: \\'Record<string, ImageModel>\\',\\n      isOptional: true,\\n      description:\\n        \\'A record of image models, where keys are model IDs and values are image model instances.\\',\\n    },\\n    {\\n      name: \\'fallbackProvider\\',\\n      type: \\'Provider\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional fallback provider to use when a requested model is not found in the custom provider.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe `customProvider` function returns a `Provider` instance. It has the following methods:\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'languageModel\\',\\n      type: \\'(id: string) => LanguageModel\\',\\n      description:\\n        \\'A function that returns a language model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'embeddingModel\\',,\\n      type: \\'(id: string) => EmbeddingModel<string>\\',\\n      description:\\n        \\'A function that returns a text embedding model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'imageModel\\',\\n      type: \\'(id: string) => ImageModel\\',\\n      description:\\n        \\'A function that returns an image model by its id (format: providerId:modelId)\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/50-cosine-similarity.mdx'), name='50-cosine-similarity.mdx', displayName='50-cosine-similarity.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: cosineSimilarity\\ndescription: Calculate the cosine similarity between two vectors (API Reference)\\n---\\n\\n# `cosineSimilarity()`\\n\\nWhen you want to compare the similarity of embeddings, standard vector similarity metrics\\nlike cosine similarity are often used.\\n\\n`cosineSimilarity` calculates the cosine similarity between two vectors.\\nA high value (close to 1) indicates that the vectors are very similar, while a low value (close to -1) indicates that they are different.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { cosineSimilarity, embedMany } from \\'ai\\';\\n\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n});\\n\\nconsole.log(\\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\\n);\\n```\\n\\n## Import\\n\\n<Snippet text={`import { cosineSimilarity } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'vector1\\',\\n      type: \\'number[]\\',\\n      description: \\'The first vector to compare\\',\\n    },\\n    {\\n      name: \\'vector2\\',\\n      type: \\'number[]\\',\\n      description: \\'The second vector to compare\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA number between -1 and 1 representing the cosine similarity between the two vectors.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/60-wrap-language-model.mdx'), name='60-wrap-language-model.mdx', displayName='60-wrap-language-model.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: wrapLanguageModel\\ndescription: Function for wrapping a language model with middleware (API Reference)\\n---\\n\\n# `wrapLanguageModel()`\\n\\nThe `wrapLanguageModel` function provides a way to enhance the behavior of language models\\nby wrapping them with middleware.\\nSee [Language Model Middleware](/docs/ai-sdk-core/middleware) for more information on middleware.\\n\\n```ts\\nimport { wrapLanguageModel } from \\'ai\\';\\n\\nconst wrappedLanguageModel = wrapLanguageModel({\\n  model: \\'openai/gpt-4.1\\',\\n  middleware: yourLanguageModelMiddleware,\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { wrapLanguageModel } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModelV3\\',\\n      description: \\'The original LanguageModelV3 instance to be wrapped.\\',\\n    },\\n    {\\n      name: \\'middleware\\',\\n      type: \\'LanguageModelV3Middleware | LanguageModelV3Middleware[]\\',\\n      description:\\n        \\'The middleware to be applied to the language model. When multiple middlewares are provided, the first middleware will transform the input first, and the last middleware will be wrapped directly around the model.\\',\\n    },\\n    {\\n      name: \\'modelId\\',\\n      type: \\'string\\',\\n      description:\\n        \"Optional custom model ID to override the original model\\'s ID.\",\\n    },\\n    {\\n      name: \\'providerId\\',\\n      type: \\'string\\',\\n      description:\\n        \"Optional custom provider ID to override the original model\\'s provider.\",\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA new `LanguageModelV3` instance with middleware applied.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/65-language-model-v2-middleware.mdx'), name='65-language-model-v2-middleware.mdx', displayName='65-language-model-v2-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: LanguageModelV3Middleware\\ndescription: Middleware for enhancing language model behavior (API Reference)\\n---\\n\\n# `LanguageModelV3Middleware`\\n\\n<Note type=\"warning\">\\n  Language model middleware is an experimental feature.\\n</Note>\\n\\nLanguage model middleware provides a way to enhance the behavior of language models\\nby intercepting and modifying the calls to the language model. It can be used to add\\nfeatures like guardrails, RAG, caching, and logging in a language model agnostic way.\\n\\nSee [Language Model Middleware](/docs/ai-sdk-core/middleware) for more information.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { LanguageModelV3Middleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'transformParams\\',\\n      type: \\'({ type: \"generate\" | \"stream\", params: LanguageModelV3CallOptions }) => Promise<LanguageModelV3CallOptions>\\',\\n      description:\\n        \\'Transforms the parameters before they are passed to the language model.\\',\\n    },\\n    {\\n      name: \\'wrapGenerate\\',\\n      type: \\'({ doGenerate: DoGenerateFunction, params: LanguageModelV3CallOptions, model: LanguageModelV3 }) => Promise<DoGenerateResult>\\',\\n      description: \\'Wraps the generate operation of the language model.\\',\\n    },\\n    {\\n      name: \\'wrapStream\\',\\n      type: \\'({ doStream: DoStreamFunction, params: LanguageModelV3CallOptions, model: LanguageModelV3 }) => Promise<DoStreamResult>\\',\\n      description: \\'Wraps the stream operation of the language model.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/66-extract-reasoning-middleware.mdx'), name='66-extract-reasoning-middleware.mdx', displayName='66-extract-reasoning-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: extractReasoningMiddleware\\ndescription: Middleware that extracts XML-tagged reasoning sections from generated text\\n---\\n\\n# `extractReasoningMiddleware()`\\n\\n`extractReasoningMiddleware` is a middleware function that extracts XML-tagged reasoning sections from generated text and exposes them separately from the main text content. This is particularly useful when you want to separate an AI model\\'s reasoning process from its final output.\\n\\n```ts\\nimport { extractReasoningMiddleware } from \\'ai\\';\\n\\nconst middleware = extractReasoningMiddleware({\\n  tagName: \\'reasoning\\',\\n  separator: \\'\\\\n\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { extractReasoningMiddleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tagName\\',\\n      type: \\'string\\',\\n      isOptional: false,\\n      description:\\n        \\'The name of the XML tag to extract reasoning from (without angle brackets)\\',\\n    },\\n    {\\n      name: \\'separator\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'The separator to use between reasoning and text sections. Defaults to \"\\\\\\\\n\"\\',\\n    },\\n    {\\n      name: \\'startWithReasoning\\',\\n      type: \\'boolean\\',\\n      isOptional: true,\\n      description:\\n        \\'Starts with reasoning tokens. Set to true when the response always starts with reasoning and the initial tag is omitted. Defaults to false.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a middleware object that:\\n\\n- Processes both streaming and non-streaming responses\\n- Extracts content between specified XML tags as reasoning\\n- Removes the XML tags and reasoning from the main text\\n- Adds a `reasoning` property to the result containing the extracted content\\n- Maintains proper separation between text sections using the specified separator\\n\\n### Type Parameters\\n\\nThe middleware works with the `LanguageModelV3StreamPart` type for streaming responses.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx'), name='67-simulate-streaming-middleware.mdx', displayName='67-simulate-streaming-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: simulateStreamingMiddleware\\ndescription: Middleware that simulates streaming for non-streaming language models\\n---\\n\\n# `simulateStreamingMiddleware()`\\n\\n`simulateStreamingMiddleware` is a middleware function that simulates streaming behavior with responses from non-streaming language models. This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\\n\\n```ts\\nimport { simulateStreamingMiddleware } from \\'ai\\';\\n\\nconst middleware = simulateStreamingMiddleware();\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { simulateStreamingMiddleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\nThis middleware doesn\\'t accept any parameters.\\n\\n### Returns\\n\\nReturns a middleware object that:\\n\\n- Takes a complete response from a language model\\n- Converts it into a simulated stream of chunks\\n- Properly handles various response components including:\\n  - Text content\\n  - Reasoning (as string or array of objects)\\n  - Tool calls\\n  - Metadata and usage information\\n  - Warnings\\n\\n### Usage Example\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\nimport { wrapLanguageModel } from \\'ai\\';\\nimport { simulateStreamingMiddleware } from \\'ai\\';\\n\\n// Example with a non-streaming model\\nconst result = streamText({\\n  model: wrapLanguageModel({\\n    model: nonStreamingModel,\\n    middleware: simulateStreamingMiddleware(),\\n  }),\\n  prompt: \\'Your prompt here\\',\\n});\\n\\n// Now you can use the streaming interface\\nfor await (const chunk of result.fullStream) {\\n  // Process streaming chunks\\n}\\n```\\n\\n## How It Works\\n\\nThe middleware:\\n\\n1. Awaits the complete response from the language model\\n2. Creates a `ReadableStream` that emits chunks in the correct sequence\\n3. Simulates streaming by breaking down the response into appropriate chunk types\\n4. Preserves all metadata, reasoning, tool calls, and other response properties\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/68-default-settings-middleware.mdx'), name='68-default-settings-middleware.mdx', displayName='68-default-settings-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: defaultSettingsMiddleware\\ndescription: Middleware that applies default settings for language models\\n---\\n\\n# `defaultSettingsMiddleware()`\\n\\n`defaultSettingsMiddleware` is a middleware function that applies default settings to language model calls. This is useful when you want to establish consistent default parameters across multiple model invocations.\\n\\n```ts\\nimport { defaultSettingsMiddleware } from \\'ai\\';\\n\\nconst middleware = defaultSettingsMiddleware({\\n  settings: {\\n    temperature: 0.7,\\n    maxOutputTokens: 1000,\\n    // other settings...\\n  },\\n});\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { defaultSettingsMiddleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\nThe middleware accepts a configuration object with the following properties:\\n\\n- `settings`: An object containing default parameter values to apply to language model calls. These can include any valid `LanguageModelV3CallOptions` properties and optional provider metadata.\\n\\n### Returns\\n\\nReturns a middleware object that:\\n\\n- Merges the default settings with the parameters provided in each model call\\n- Ensures that explicitly provided parameters take precedence over defaults\\n- Merges provider metadata objects\\n\\n### Usage Example\\n\\n```ts\\nimport { streamText, wrapLanguageModel, defaultSettingsMiddleware } from \\'ai\\';\\n\\n// Create a model with default settings\\nconst modelWithDefaults = wrapLanguageModel({\\n  model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n  middleware: defaultSettingsMiddleware({\\n    settings: {\\n      providerOptions: {\\n        openai: {\\n          reasoningEffort: \\'high\\',\\n        },\\n      },\\n    },\\n  }),\\n});\\n\\n// Use the model - default settings will be applied\\nconst result = await streamText({\\n  model: modelWithDefaults,\\n  prompt: \\'Your prompt here\\',\\n  // These parameters will override the defaults\\n  temperature: 0.8,\\n});\\n```\\n\\n## How It Works\\n\\nThe middleware:\\n\\n1. Takes a set of default settings as configuration\\n2. Merges these defaults with the parameters provided in each model call\\n3. Ensures that explicitly provided parameters take precedence over defaults\\n4. Merges provider metadata objects from both sources\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/70-step-count-is.mdx'), name='70-step-count-is.mdx', displayName='70-step-count-is.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: stepCountIs\\ndescription: API Reference for stepCountIs.\\n---\\n\\n# `stepCountIs()`\\n\\nCreates a stop condition that stops when the number of steps reaches a specified count.\\n\\nThis function is used with `stopWhen` in `generateText` and `streamText` to control when a tool-calling loop should stop based on the number of steps executed.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText, stepCountIs } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  // Stop after 5 steps\\n  stopWhen: stepCountIs(5),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { stepCountIs } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'count\\',\\n      type: \\'number\\',\\n      description:\\n        \\'The maximum number of steps to execute before stopping the tool-calling loop.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `StopCondition` function that returns `true` when the step count reaches the specified number. The function can be used with the `stopWhen` parameter in `generateText` and `streamText`.\\n\\n## Examples\\n\\n### Basic Usage\\n\\nStop after 3 steps:\\n\\n```ts\\nimport { generateText, stepCountIs } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: yourTools,\\n  stopWhen: stepCountIs(3),\\n});\\n```\\n\\n### Combining with Other Conditions\\n\\nYou can combine multiple stop conditions in an array:\\n\\n```ts\\nimport { generateText, stepCountIs, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: yourTools,\\n  // Stop after 10 steps OR when finalAnswer tool is called\\n  stopWhen: [stepCountIs(10), hasToolCall(\\'finalAnswer\\')],\\n});\\n```\\n\\n## See also\\n\\n- [`hasToolCall()`](/docs/reference/ai-sdk-core/has-tool-call)\\n- [`generateText()`](/docs/reference/ai-sdk-core/generate-text)\\n- [`streamText()`](/docs/reference/ai-sdk-core/stream-text)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/71-has-tool-call.mdx'), name='71-has-tool-call.mdx', displayName='71-has-tool-call.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: hasToolCall\\ndescription: API Reference for hasToolCall.\\n---\\n\\n# `hasToolCall()`\\n\\nCreates a stop condition that stops when a specific tool is called.\\n\\nThis function is used with `stopWhen` in `generateText` and `streamText` to control when a tool-calling loop should stop based on whether a particular tool has been invoked.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: weatherTool,\\n    finalAnswer: finalAnswerTool,\\n  },\\n  // Stop when the finalAnswer tool is called\\n  stopWhen: hasToolCall(\\'finalAnswer\\'),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { hasToolCall } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'toolName\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The name of the tool that should trigger the stop condition when called.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `StopCondition` function that returns `true` when the specified tool is called in the current step. The function can be used with the `stopWhen` parameter in `generateText` and `streamText`.\\n\\n## Examples\\n\\n### Basic Usage\\n\\nStop when a specific tool is called:\\n\\n```ts\\nimport { generateText, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: {\\n    submitAnswer: submitAnswerTool,\\n    search: searchTool,\\n  },\\n  stopWhen: hasToolCall(\\'submitAnswer\\'),\\n});\\n```\\n\\n### Combining with Other Conditions\\n\\nYou can combine multiple stop conditions in an array:\\n\\n```ts\\nimport { generateText, hasToolCall, stepCountIs } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: {\\n    weather: weatherTool,\\n    search: searchTool,\\n    finalAnswer: finalAnswerTool,\\n  },\\n  // Stop when weather tool is called OR finalAnswer is called OR after 5 steps\\n  stopWhen: [\\n    hasToolCall(\\'weather\\'),\\n    hasToolCall(\\'finalAnswer\\'),\\n    stepCountIs(5),\\n  ],\\n});\\n```\\n\\n### Agent Pattern\\n\\nCommon pattern for agents that run until they provide a final answer:\\n\\n```ts\\nimport { generateText, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: {\\n    search: searchTool,\\n    calculate: calculateTool,\\n    finalAnswer: {\\n      description: \\'Provide the final answer to the user\\',\\n      parameters: z.object({\\n        answer: z.string(),\\n      }),\\n      execute: async ({ answer }) => answer,\\n    },\\n  },\\n  stopWhen: hasToolCall(\\'finalAnswer\\'),\\n});\\n```\\n\\n## See also\\n\\n- [`stepCountIs()`](/docs/reference/ai-sdk-core/step-count-is)\\n- [`generateText()`](/docs/reference/ai-sdk-core/generate-text)\\n- [`streamText()`](/docs/reference/ai-sdk-core/stream-text)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx'), name='75-simulate-readable-stream.mdx', displayName='75-simulate-readable-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: simulateReadableStream\\ndescription: Create a ReadableStream that emits values with configurable delays\\n---\\n\\n# `simulateReadableStream()`\\n\\n`simulateReadableStream` is a utility function that creates a ReadableStream which emits provided values sequentially with configurable delays. This is particularly useful for testing streaming functionality or simulating time-delayed data streams.\\n\\n```ts\\nimport { simulateReadableStream } from \\'ai\\';\\n\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n  initialDelayInMs: 100,\\n  chunkDelayInMs: 50,\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { simulateReadableStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'chunks\\',\\n      type: \\'T[]\\',\\n      isOptional: false,\\n      description: \\'Array of values to be emitted by the stream\\',\\n    },\\n    {\\n      name: \\'initialDelayInMs\\',\\n      type: \\'number | null\\',\\n      isOptional: true,\\n      description:\\n        \\'Initial delay in milliseconds before emitting the first value. Defaults to 0. Set to null to skip the initial delay entirely.\\',\\n    },\\n    {\\n      name: \\'chunkDelayInMs\\',\\n      type: \\'number | null\\',\\n      isOptional: true,\\n      description:\\n        \\'Delay in milliseconds between emitting each value. Defaults to 0. Set to null to skip delays between chunks.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a `ReadableStream<T>` that:\\n\\n- Emits each value from the provided `chunks` array sequentially\\n- Waits for `initialDelayInMs` before emitting the first value (if not `null`)\\n- Waits for `chunkDelayInMs` between emitting subsequent values (if not `null`)\\n- Closes automatically after all chunks have been emitted\\n\\n### Type Parameters\\n\\n- `T`: The type of values contained in the chunks array and emitted by the stream\\n\\n## Examples\\n\\n### Basic Usage\\n\\n```ts\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n});\\n```\\n\\n### With Delays\\n\\n```ts\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n  initialDelayInMs: 1000, // Wait 1 second before first chunk\\n  chunkDelayInMs: 500, // Wait 0.5 seconds between chunks\\n});\\n```\\n\\n### Without Delays\\n\\n```ts\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n  initialDelayInMs: null, // No initial delay\\n  chunkDelayInMs: null, // No delay between chunks\\n});\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/80-smooth-stream.mdx'), name='80-smooth-stream.mdx', displayName='80-smooth-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: smoothStream\\ndescription: Stream transformer for smoothing text output\\n---\\n\\n# `smoothStream()`\\n\\n`smoothStream` is a utility function that creates a TransformStream\\nfor the `streamText` `transform` option\\nto smooth out text streaming by buffering and releasing complete words with configurable delays.\\nThis creates a more natural reading experience when streaming text responses.\\n\\n```ts highlight={\"6-9\"}\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: smoothStream({\\n    delayInMs: 20, // optional: defaults to 10ms\\n    chunking: \\'line\\', // optional: defaults to \\'word\\'\\n  }),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { smoothStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'delayInMs\\',\\n      type: \\'number | null\\',\\n      isOptional: true,\\n      description:\\n        \\'The delay in milliseconds between outputting each chunk. Defaults to 10ms. Set to `null` to disable delays.\\',\\n    },\\n    {\\n      name: \\'chunking\\',\\n      type: \\'\"word\" | \"line\" | RegExp | (buffer: string) => string | undefined | null\\',\\n      isOptional: true,\\n      description:\\n        \\'Controls how the text is chunked for streaming. Use \"word\" to stream word by word (default), \"line\" to stream line by line, or provide a custom callback or RegExp pattern for custom chunking.\\',\\n    },\\n  ]}\\n/>\\n\\n#### Word chunking caveats with non-latin languages\\n\\nThe word based chunking **does not work well** with the following languages that do not delimit words with spaces:\\n\\nFor these languages we recommend using a custom regex, like the following:\\n\\n- Chinese - `/[\\\\u4E00-\\\\u9FFF]|\\\\S+\\\\s+/`\\n- Japanese - `/[\\\\u3040-\\\\u309F\\\\u30A0-\\\\u30FF]|\\\\S+\\\\s+/`\\n\\n```tsx filename=\"Japanese example\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Your prompt here\\',\\n  experimental_transform: smoothStream({\\n    chunking: /[\\\\u3040-\\\\u309F\\\\u30A0-\\\\u30FF]|\\\\S+\\\\s+/,\\n  }),\\n});\\n```\\n\\n```tsx filename=\"Chinese example\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Your prompt here\\',\\n  experimental_transform: smoothStream({\\n    chunking: /[\\\\u4E00-\\\\u9FFF]|\\\\S+\\\\s+/,\\n  }),\\n});\\n```\\n\\nFor these languages you could pass your own language aware chunking function:\\n\\n- Vietnamese\\n- Thai\\n- Javanese (Aksara Jawa)\\n\\n#### Regex based chunking\\n\\nTo use regex based chunking, pass a `RegExp` to the `chunking` option.\\n\\n```ts\\n// To split on underscores:\\nsmoothStream({\\n  chunking: /_+/,\\n});\\n\\n// Also can do it like this, same behavior\\nsmoothStream({\\n  chunking: /[^_]*_/,\\n});\\n```\\n\\n#### Custom callback chunking\\n\\nTo use a custom callback for chunking, pass a function to the `chunking` option.\\n\\n```ts\\nsmoothStream({\\n  chunking: text => {\\n    const findString = \\'some string\\';\\n    const index = text.indexOf(findString);\\n\\n    if (index === -1) {\\n      return null;\\n    }\\n\\n    return text.slice(0, index) + findString;\\n  },\\n});\\n```\\n\\n### Returns\\n\\nReturns a `TransformStream` that:\\n\\n- Buffers incoming text chunks\\n- Releases text when the chunking pattern is encountered\\n- Adds configurable delays between chunks for smooth output\\n- Passes through non-text chunks (like step-finish events) immediately\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/90-generate-id.mdx'), name='90-generate-id.mdx', displayName='90-generate-id.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateId\\ndescription: Generate a unique identifier (API Reference)\\n---\\n\\n# `generateId()`\\n\\nGenerates a unique identifier. You can optionally provide the length of the ID.\\n\\nThis is the same id generator used by the AI SDK.\\n\\n```ts\\nimport { generateId } from \\'ai\\';\\n\\nconst id = generateId();\\n```\\n\\n## Import\\n\\n<Snippet text={`import { generateId } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'size\\',\\n      type: \\'number\\',\\n      description:\\n        \\'The length of the generated ID. It defaults to 16. This parameter is deprecated and will be removed in the next major version.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA string representing the generated ID.\\n\\n## See also\\n\\n- [`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/91-create-id-generator.mdx'), name='91-create-id-generator.mdx', displayName='91-create-id-generator.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createIdGenerator\\ndescription: Create a customizable unique identifier generator (API Reference)\\n---\\n\\n# `createIdGenerator()`\\n\\nCreates a customizable ID generator function. You can configure the alphabet, prefix, separator, and default size of the generated IDs.\\n\\n```ts\\nimport { createIdGenerator } from \\'ai\\';\\n\\nconst generateCustomId = createIdGenerator({\\n  prefix: \\'user\\',\\n  separator: \\'_\\',\\n});\\n\\nconst id = generateCustomId(); // Example: \"user_1a2b3c4d5e6f7g8h\"\\n```\\n\\n## Import\\n\\n<Snippet text={`import { createIdGenerator } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'options\\',\\n      type: \\'object\\',\\n      description:\\n        \\'Optional configuration object with the following properties:\\',\\n    },\\n    {\\n      name: \\'options.alphabet\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The characters to use for generating the random part of the ID. Defaults to alphanumeric characters (0-9, A-Z, a-z).\\',\\n    },\\n    {\\n      name: \\'options.prefix\\',\\n      type: \\'string\\',\\n      description:\\n        \\'A string to prepend to all generated IDs. Defaults to none.\\',\\n    },\\n    {\\n      name: \\'options.separator\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The character(s) to use between the prefix and the random part. Defaults to \"-\".\\',\\n    },\\n    {\\n      name: \\'options.size\\',\\n      type: \\'number\\',\\n      description:\\n        \\'The default length of the random part of the ID. Defaults to 16.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a function that generates IDs based on the configured options.\\n\\n### Notes\\n\\n- The generator uses non-secure random generation and should not be used for security-critical purposes.\\n- The separator character must not be part of the alphabet to ensure reliable prefix checking.\\n\\n## Example\\n\\n```ts\\n// Create a custom ID generator for user IDs\\nconst generateUserId = createIdGenerator({\\n  prefix: \\'user\\',\\n  separator: \\'_\\',\\n  size: 8,\\n});\\n\\n// Generate IDs\\nconst id1 = generateUserId(); // e.g., \"user_1a2b3c4d\"\\n```\\n\\n## See also\\n\\n- [`generateId()`](/docs/reference/ai-sdk-core/generate-id)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK Core\\ndescription: Reference documentation for the AI SDK Core\\ncollapsed: true\\n---\\n\\n# AI SDK Core\\n\\n[AI SDK Core](/docs/ai-sdk-core) is a set of functions that allow you to interact with language models and other AI models.\\nThese functions are designed to be easy-to-use and flexible, allowing you to generate text, structured data,\\nand embeddings from language models and other AI models.\\n\\nAI SDK Core contains the following main functions:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'generateText()',\\n      description: 'Generate text and call tools from a language model.',\\n      href: '/docs/reference/ai-sdk-core/generate-text',\\n    },\\n    {\\n      title: 'streamText()',\\n      description: 'Stream text and call tools from a language model.',\\n      href: '/docs/reference/ai-sdk-core/stream-text',\\n    },\\n    {\\n      title: 'generateObject()',\\n      description: 'Generate structured data from a language model.',\\n      href: '/docs/reference/ai-sdk-core/generate-object',\\n    },\\n    {\\n      title: 'streamObject()',\\n      description: 'Stream structured data from a language model.',\\n      href: '/docs/reference/ai-sdk-core/stream-object',\\n    },\\n    {\\n      title: 'embed()',\\n      description:\\n        'Generate an embedding for a single value using an embedding model.',\\n      href: '/docs/reference/ai-sdk-core/embed',\\n    },\\n    {\\n      title: 'embedMany()',\\n      description:\\n        'Generate embeddings for several values using an embedding model (batch embedding).',\\n      href: '/docs/reference/ai-sdk-core/embed-many',\\n    },\\n    {\\n      title: 'experimental_generateImage()',\\n      description:\\n        'Generate images based on a given prompt using an image model.',\\n      href: '/docs/reference/ai-sdk-core/generate-image',\\n    },\\n    {\\n      title: 'experimental_transcribe()',\\n      description: 'Generate a transcript from an audio file.',\\n      href: '/docs/reference/ai-sdk-core/transcribe',\\n    },\\n    {\\n      title: 'experimental_generateSpeech()',\\n      description: 'Generate speech audio from text.',\\n      href: '/docs/reference/ai-sdk-core/generate-speech',\\n    },\\n  ]}\\n/>\\n\\nIt also contains the following helper functions:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'tool()',\\n      description: 'Type inference helper function for tools.',\\n      href: '/docs/reference/ai-sdk-core/tool',\\n    },\\n    {\\n      title: 'experimental_createMCPClient()',\\n      description: 'Creates a client for connecting to MCP servers.',\\n      href: '/docs/reference/ai-sdk-core/create-mcp-client',\\n    },\\n    {\\n      title: 'jsonSchema()',\\n      description: 'Creates AI SDK compatible JSON schema objects.',\\n      href: '/docs/reference/ai-sdk-core/json-schema',\\n    },\\n    {\\n      title: 'zodSchema()',\\n      description: 'Creates AI SDK compatible Zod schema objects.',\\n      href: '/docs/reference/ai-sdk-core/zod-schema',\\n    },\\n    {\\n      title: 'createProviderRegistry()',\\n      description:\\n        'Creates a registry for using models from multiple providers.',\\n      href: '/docs/reference/ai-sdk-core/provider-registry',\\n    },\\n    {\\n      title: 'cosineSimilarity()',\\n      description:\\n        'Calculates the cosine similarity between two vectors, e.g. embeddings.',\\n      href: '/docs/reference/ai-sdk-core/cosine-similarity',\\n    },\\n    {\\n      title: 'simulateReadableStream()',\\n      description:\\n        'Creates a ReadableStream that emits values with configurable delays.',\\n      href: '/docs/reference/ai-sdk-core/simulate-readable-stream',\\n    },\\n    {\\n      title: 'wrapLanguageModel()',\\n      description: 'Wraps a language model with middleware.',\\n      href: '/docs/reference/ai-sdk-core/wrap-language-model',\\n    },\\n    {\\n      title: 'extractReasoningMiddleware()',\\n      description:\\n        'Extracts reasoning from the generated text and exposes it as a `reasoning` property on the result.',\\n      href: '/docs/reference/ai-sdk-core/extract-reasoning-middleware',\\n    },\\n    {\\n      title: 'simulateStreamingMiddleware()',\\n      description:\\n        'Simulates streaming behavior with responses from non-streaming language models.',\\n      href: '/docs/reference/ai-sdk-core/simulate-streaming-middleware',\\n    },\\n    {\\n      title: 'defaultSettingsMiddleware()',\\n      description: 'Applies default settings to a language model.',\\n      href: '/docs/reference/ai-sdk-core/default-settings-middleware',\\n    },\\n    {\\n      title: 'smoothStream()',\\n      description: 'Smooths text streaming output.',\\n      href: '/docs/reference/ai-sdk-core/smooth-stream',\\n    },\\n    {\\n      title: 'generateId()',\\n      description: 'Helper function for generating unique IDs',\\n      href: '/docs/reference/ai-sdk-core/generate-id',\\n    },\\n    {\\n      title: 'createIdGenerator()',\\n      description: 'Creates an ID generator',\\n      href: '/docs/reference/ai-sdk-core/create-id-generator',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui'), name='02-ai-sdk-ui', displayName='02-ai-sdk-ui', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/02-ai-sdk-ui/01-use-chat.mdx'), name='01-use-chat.mdx', displayName='01-use-chat.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat\\ndescription: API reference for the useChat hook.\\n---\\n\\n# `useChat()`\\n\\nAllows you to easily create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages are received.\\n\\n<Note>\\n  The `useChat` API has been significantly updated in AI SDK 5.0. It now uses a\\n  transport-based architecture and no longer manages input state internally. See\\n  the [migration\\n  guide](/docs/migration-guides/migration-guide-5-0#usechat-changes) for\\n  details.\\n</Note>\\n\\n## Import\\n\\n<Tabs items={[\\'React\\', \\'Svelte\\', \\'Vue\\']}>\\n  <Tab>\\n    <Snippet\\n      text=\"import { useChat } from \\'@ai-sdk/react\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"import { Chat } from \\'@ai-sdk/svelte\\'\" dark prompt={false} />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"import { Chat } from \\'@ai-sdk/vue\\'\" dark prompt={false} />\\n  </Tab>\\n</Tabs>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'chat\\',\\n      type: \\'Chat<UIMessage>\\',\\n      isOptional: true,\\n      description:\\n        \\'An existing Chat instance to use. If provided, other parameters are ignored.\\',\\n    },\\n    {\\n      name: \\'transport\\',\\n      type: \\'ChatTransport\\',\\n      isOptional: true,\\n      description:\\n        \\'The transport to use for sending messages. Defaults to DefaultChatTransport with `/api/chat` endpoint.\\',\\n      properties: [\\n        {\\n          type: \\'DefaultChatTransport\\',\\n          parameters: [\\n            {\\n              name: \\'api\\',\\n              type: \"string = \\'/api/chat\\'\",\\n              isOptional: true,\\n              description: \\'The API endpoint for chat requests.\\',\\n            },\\n            {\\n              name: \\'credentials\\',\\n              type: \\'RequestCredentials\\',\\n              isOptional: true,\\n              description: \\'The credentials mode for fetch requests.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string> | Headers\\',\\n              isOptional: true,\\n              description: \\'HTTP headers to send with requests.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'object\\',\\n              isOptional: true,\\n              description: \\'Extra body object to send with requests.\\',\\n            },\\n            {\\n              name: \\'prepareSendMessagesRequest\\',\\n              type: \\'PrepareSendMessagesRequest\\',\\n              isOptional: true,\\n              description:\\n                \\'A function to customize the request before chat API calls.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareSendMessagesRequest\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'PrepareSendMessageRequestOptions\\',\\n                      description: \\'Options for preparing the request\\',\\n                      properties: [\\n                        {\\n                          type: \\'PrepareSendMessageRequestOptions\\',\\n                          parameters: [\\n                            {\\n                              name: \\'id\\',\\n                              type: \\'string\\',\\n                              description: \\'The chat ID\\',\\n                            },\\n                            {\\n                              name: \\'messages\\',\\n                              type: \\'UIMessage[]\\',\\n                              description: \\'Current messages in the chat\\',\\n                            },\\n                            {\\n                              name: \\'requestMetadata\\',\\n                              type: \\'unknown\\',\\n                              description: \\'The request metadata\\',\\n                            },\\n                            {\\n                              name: \\'body\\',\\n                              type: \\'Record<string, any> | undefined\\',\\n                              description: \\'The request body\\',\\n                            },\\n                            {\\n                              name: \\'credentials\\',\\n                              type: \\'RequestCredentials | undefined\\',\\n                              description: \\'The request credentials\\',\\n                            },\\n                            {\\n                              name: \\'headers\\',\\n                              type: \\'HeadersInit | undefined\\',\\n                              description: \\'The request headers\\',\\n                            },\\n                            {\\n                              name: \\'api\\',\\n                              type: \\'string\\',\\n                              description: `The API endpoint to use for the request. If not specified, it defaults to the transport’s API endpoint: /api/chat.`,\\n                            },\\n                            {\\n                              name: \\'trigger\\',\\n                              type: \"\\'submit-message\\' | \\'regenerate-message\\'\",\\n                              description: \\'The trigger for the request\\',\\n                            },\\n                            {\\n                              name: \\'messageId\\',\\n                              type: \\'string | undefined\\',\\n                              description: \\'The message ID if applicable\\',\\n                            },\\n                          ],\\n                        },\\n                      ],\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'prepareReconnectToStreamRequest\\',\\n              type: \\'PrepareReconnectToStreamRequest\\',\\n              isOptional: true,\\n              description:\\n                \\'A function to customize the request before reconnect API call.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareReconnectToStreamRequest\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'PrepareReconnectToStreamRequestOptions\\',\\n                      description:\\n                        \\'Options for preparing the reconnect request\\',\\n                      properties: [\\n                        {\\n                          type: \\'PrepareReconnectToStreamRequestOptions\\',\\n                          parameters: [\\n                            {\\n                              name: \\'id\\',\\n                              type: \\'string\\',\\n                              description: \\'The chat ID\\',\\n                            },\\n                            {\\n                              name: \\'requestMetadata\\',\\n                              type: \\'unknown\\',\\n                              description: \\'The request metadata\\',\\n                            },\\n                            {\\n                              name: \\'body\\',\\n                              type: \\'Record<string, any> | undefined\\',\\n                              description: \\'The request body\\',\\n                            },\\n                            {\\n                              name: \\'credentials\\',\\n                              type: \\'RequestCredentials | undefined\\',\\n                              description: \\'The request credentials\\',\\n                            },\\n                            {\\n                              name: \\'headers\\',\\n                              type: \\'HeadersInit | undefined\\',\\n                              description: \\'The request headers\\',\\n                            },\\n                            {\\n                              name: \\'api\\',\\n                              type: \\'string\\',\\n                              description: `The API endpoint to use for the request. If not specified, it defaults to the transport’s API endpoint combined with the chat ID: /api/chat/{chatId}/stream.`,\\n                            },\\n                          ],\\n                        },\\n                      ],\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'A unique identifier for the chat. If not provided, a random one will be generated.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'UIMessage[]\\',\\n      isOptional: true,\\n      description: \\'Initial chat messages to populate the conversation with.\\',\\n    },\\n    {\\n      name: \\'onToolCall\\',\\n      type: \\'({toolCall: ToolCall}) => void | Promise<void>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback function that is invoked when a tool call is received. You must call addToolOutput to provide the tool result.\\',\\n    },\\n    {\\n      name: \\'sendAutomaticallyWhen\\',\\n      type: \\'(options: { messages: UIMessage[] }) => boolean | PromiseLike<boolean>\\',\\n      isOptional: true,\\n      description:\\n        \\'When provided, this function will be called when the stream is finished or a tool call is added to determine if the current messages should be resubmitted. You can use the lastAssistantMessageIsCompleteWithToolCalls helper for common scenarios.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(options: OnFinishOptions) => void\\',\\n      isOptional: true,\\n      description: \\'Called when the assistant response has finished streaming.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishOptions\\',\\n          parameters: [\\n            {\\n              name: \\'message\\',\\n              type: \\'UIMessage\\',\\n              description: \\'The response message.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'UIMessage[]\\',\\n              description: \\'All messages including the response message\\',\\n            },\\n            {\\n              name: \\'isAbort\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when the request has been aborted by the client.\\',\\n            },\\n            {\\n              name: \\'isDisconnect\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True if the server has been disconnected, e.g. because of a network error.\\',\\n            },\\n            {\\n              name: \\'isError\\',\\n              type: \\'boolean\\',\\n              description: `True if errors during streaming caused the response to stop early.`,\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              isOptional: true,\\n              description:\\n                \\'The reason why the model finished generating the response. Undefined if the finish reason was not provided by the model.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: Error) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback function to be called when an error is encountered.\\',\\n    },\\n    {\\n      name: \\'onData\\',\\n      type: \\'(dataPart: DataUIPart) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback function that is called when a data part is received.\\',\\n    },\\n    {\\n      name: \\'experimental_throttle\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom throttle wait in ms for the chat messages and data updates. Default is undefined, which disables throttling.\\',\\n    },\\n    {\\n      name: \\'resume\\',\\n      type: \\'boolean\\',\\n      isOptional: true,\\n      description:\\n        \\'Whether to resume an ongoing chat generation stream. Defaults to false.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      description: \\'The id of the chat.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'UIMessage[]\\',\\n      description: \\'The current array of chat messages.\\',\\n      properties: [\\n        {\\n          type: \\'UIMessage\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'A unique identifier for the message.\\',\\n            },\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\' | \\'user\\' | \\'assistant\\'\",\\n              description: \\'The role of the message.\\',\\n            },\\n            {\\n              name: \\'parts\\',\\n              type: \\'UIMessagePart[]\\',\\n              description:\\n                \\'The parts of the message. Use this for rendering the message in the UI.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The metadata of the message.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'status\\',\\n      type: \"\\'submitted\\' | \\'streaming\\' | \\'ready\\' | \\'error\\'\",\\n      description:\\n        \\'The current status of the chat: \"ready\" (idle), \"submitted\" (request sent), \"streaming\" (receiving response), or \"error\" (request failed).\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'Error | undefined\\',\\n      description: \\'The error object if an error occurred.\\',\\n    },\\n    {\\n      name: \\'sendMessage\\',\\n      type: \\'(message: CreateUIMessage | string, options?: ChatRequestOptions) => void\\',\\n      description:\\n        \\'Function to send a new message to the chat. This will trigger an API call to generate the assistant response.\\',\\n      properties: [\\n        {\\n          type: \\'ChatRequestOptions\\',\\n          parameters: [\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string> | Headers\\',\\n              description:\\n                \\'Additional headers that should be to be passed to the API endpoint.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'object\\',\\n              description:\\n                \\'Additional body JSON properties that should be sent to the API endpoint.\\',\\n            },\\n            {\\n              name: \\'data\\',\\n              type: \\'JSONValue\\',\\n              description: \\'Additional data to be sent to the API endpoint.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'regenerate\\',\\n      type: \\'(options?: { messageId?: string }) => void\\',\\n      description:\\n        \\'Function to regenerate the last assistant message or a specific message. If no messageId is provided, regenerates the last assistant message.\\',\\n    },\\n    {\\n      name: \\'stop\\',\\n      type: \\'() => void\\',\\n      description:\\n        \\'Function to abort the current streaming response from the assistant.\\',\\n    },\\n    {\\n      name: \\'clearError\\',\\n      type: \\'() => void\\',\\n      description: \\'Clears the error state.\\',\\n    },\\n    {\\n      name: \\'resumeStream\\',\\n      type: \\'() => void\\',\\n      description:\\n        \\'Function to resume an interrupted streaming response. Useful when a network error occurs during streaming.\\',\\n    },\\n    {\\n      name: \\'addToolOutput\\',\\n      type: \\'(options: { tool: string; toolCallId: string; output: unknown } | { tool: string; toolCallId: string; state: \"output-error\", errorText: string }) => void\\',\\n      description:\\n        \\'Function to add a tool result to the chat. This will update the chat messages with the tool result. If sendAutomaticallyWhen is configured, it may trigger an automatic submission.\\',\\n    },\\n    {\\n      name: \\'setMessages\\',\\n      type: \\'(messages: UIMessage[] | ((messages: UIMessage[]) => UIMessage[])) => void\\',\\n      description:\\n        \\'Function to update the messages state locally without triggering an API call. Useful for optimistic updates.\\',\\n    },\\n  ]}\\n/>\\n\\n## Learn more\\n\\n- [Chatbot](/docs/ai-sdk-ui/chatbot)\\n- [Chatbot with Tools](/docs/ai-sdk-ui/chatbot-with-tool-calling)\\n- [UIMessage](/docs/reference/ai-sdk-core/ui-message)\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/02-use-completion.mdx'), name='02-use-completion.mdx', displayName='02-use-completion.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useCompletion\\ndescription: API reference for the useCompletion hook.\\n---\\n\\n# `useCompletion()`\\n\\nAllows you to create text completion based capabilities for your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\\n\\n## Import\\n\\n<Tabs items={[\\'React\\', \\'Svelte\\', \\'Vue\\']}>\\n  <Tab>\\n    <Snippet\\n      text=\"import { useCompletion } from \\'@ai-sdk/react\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n  <Tab>\\n    <Snippet\\n      text=\"import { Completion } from \\'@ai-sdk/svelte\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n  <Tab>\\n    <Snippet\\n      text=\"import { useCompletion } from \\'@ai-sdk/vue\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n\\n</Tabs>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'api\\',\\n      type: \"string = \\'/api/completion\\'\",\\n      description:\\n        \\'The API endpoint that is called to generate text. It can be a relative path (starting with `/`) or an absolute URL.\\',\\n    },\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      description:\\n        \\'An unique identifier for the completion. If not provided, a random one will be generated. When provided, the `useCompletion` hook with the same `id` will have shared states across components. This is useful when you have multiple components showing the same chat stream\\',\\n    },\\n    {\\n      name: \\'initialInput\\',\\n      type: \\'string\\',\\n      description: \\'An optional string for the initial prompt input.\\',\\n    },\\n    {\\n      name: \\'initialCompletion\\',\\n      type: \\'string\\',\\n      description: \\'An optional string for the initial completion result.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(prompt: string, completion: string) => void\\',\\n      description:\\n        \\'An optional callback function that is called when the completion stream ends.\\',\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: Error) => void\\',\\n      description:\\n        \\'An optional callback that will be called when the chat stream encounters an error.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string> | Headers\\',\\n      description:\\n        \\'An optional object of headers to be passed to the API endpoint.\\',\\n    },\\n    {\\n      name: \\'body\\',\\n      type: \\'any\\',\\n      description:\\n        \\'An optional, additional body object to be passed to the API endpoint.\\',\\n    },\\n    {\\n      name: \\'credentials\\',\\n      type: \"\\'omit\\' | \\'same-origin\\' | \\'include\\'\",\\n      description:\\n        \\'An optional literal that sets the mode of credentials to be used on the request. Defaults to same-origin.\\',\\n    },\\n    {\\n      name: \\'streamProtocol\\',\\n      type: \"\\'text\\' | \\'data\\'\",\\n      isOptional: true,\\n      description:\\n        \\'An optional literal that sets the type of stream to be used. Defaults to `data`. If set to `text`, the stream will be treated as a text stream.\\',\\n    },\\n    {\\n      name: \\'fetch\\',\\n      type: \\'FetchFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional. A custom fetch function to be used for the API call. Defaults to the global fetch function.\\',\\n    },\\n    {\\n      name: \\'experimental_throttle\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'React only. Custom throttle wait time in milliseconds for the completion and data updates. When specified, throttles how often the UI updates during streaming. Default is undefined, which disables throttling.\\',\\n    },\\n\\n]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'completion\\',\\n      type: \\'string\\',\\n      description: \\'The current text completion.\\',\\n    },\\n    {\\n      name: \\'complete\\',\\n      type: \\'(prompt: string, options: { headers, body }) => void\\',\\n      description:\\n        \\'Function to execute text completion based on the provided prompt.\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'undefined | Error\\',\\n      description: \\'The error thrown during the completion process, if any.\\',\\n    },\\n    {\\n      name: \\'setCompletion\\',\\n      type: \\'(completion: string) => void\\',\\n      description: \\'Function to update the `completion` state.\\',\\n    },\\n    {\\n      name: \\'stop\\',\\n      type: \\'() => void\\',\\n      description: \\'Function to abort the current API request.\\',\\n    },\\n    {\\n      name: \\'input\\',\\n      type: \\'string\\',\\n      description: \\'The current value of the input field.\\',\\n    },\\n    {\\n      name: \\'setInput\\',\\n      type: \\'React.Dispatch<React.SetStateAction<string>>\\',\\n      description: \\'The current value of the input field.\\',\\n    },\\n    {\\n      name: \\'handleInputChange\\',\\n      type: \\'(event: any) => void\\',\\n      description:\\n        \"Handler for the `onChange` event of the input field to control the input\\'s value.\",\\n    },\\n    {\\n      name: \\'handleSubmit\\',\\n      type: \\'(event?: { preventDefault?: () => void }) => void\\',\\n      description:\\n        \\'Form submission handler that automatically resets the input field and appends a user message.\\',\\n    },\\n    {\\n      name: \\'isLoading\\',\\n      type: \\'boolean\\',\\n      description:\\n        \\'Boolean flag indicating whether a fetch operation is currently in progress.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/03-use-object.mdx'), name='03-use-object.mdx', displayName='03-use-object.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useObject\\ndescription: API reference for the useObject hook.\\n---\\n\\n# `experimental_useObject()`\\n\\n<Note>\\n  `useObject` is an experimental feature and only available in React, Svelte,\\n  and Vue.\\n</Note>\\n\\nAllows you to consume text streams that represent a JSON object and parse them into a complete object based on a schema.\\nYou can use it together with [`streamObject`](/docs/reference/ai-sdk-core/stream-object) in the backend.\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/use-object\\',\\n    schema: z.object({ content: z.string() }),\\n  });\\n\\n  return (\\n    <div>\\n      <button onClick={() => submit(\\'example input\\')}>Generate</button>\\n      {object?.content && <p>{object.content}</p>}\\n    </div>\\n  );\\n}\\n```\\n\\n## Import\\n\\n<Snippet\\n  text=\"import { experimental_useObject as useObject } from \\'@ai-sdk/react\\'\"\\n  dark\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'api\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The API endpoint that is called to generate objects. It should stream JSON that matches the schema as chunked text. It can be a relative path (starting with `/`) or an absolute URL.\\',\\n    },\\n    {\\n      name: \\'schema\\',\\n      type: \\'Zod Schema | JSON Schema\\',\\n      description:\\n        \\'A schema that defines the shape of the complete object. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n    },\\n    {\\n      name: \\'id?\\',\\n      type: \\'string\\',\\n      description:\\n        \\'A unique identifier. If not provided, a random one will be generated. When provided, the `useObject` hook with the same `id` will have shared states across components.\\',\\n    },\\n    {\\n      name: \\'initialValue\\',\\n      type: \\'DeepPartial<RESULT> | undefined\\',\\n      isOptional: true,\\n      description: \\'An value for the initial object. Optional.\\',\\n    },\\n    {\\n      name: \\'fetch\\',\\n      type: \\'FetchFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'A custom fetch function to be used for the API call. Defaults to the global fetch function. Optional.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string> | Headers\\',\\n      isOptional: true,\\n      description:\\n        \\'A headers object to be passed to the API endpoint. Optional.\\',\\n    },\\n    {\\n      name: \\'credentials\\',\\n      type: \\'RequestCredentials\\',\\n      isOptional: true,\\n      description:\\n        \\'The credentials mode to be used for the fetch request. Possible values are: \"omit\", \"same-origin\", \"include\". Optional.\\',\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: Error) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback function to be called when an error is encountered. Optional.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => void\\',\\n      isOptional: true,\\n      description: \\'Called when the streaming response has finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'object\\',\\n              type: \\'T | undefined\\',\\n              description:\\n                \\'The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown | undefined\\',\\n              description:\\n                \\'Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'submit\\',\\n      type: \\'(input: INPUT) => void\\',\\n      description: \\'Calls the API with the provided input as JSON body.\\',\\n    },\\n    {\\n      name: \\'object\\',\\n      type: \\'DeepPartial<RESULT> | undefined\\',\\n      description:\\n        \\'The current value for the generated object. Updated as the API streams JSON chunks.\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'Error | unknown\\',\\n      description: \\'The error object if the API call fails.\\',\\n    },\\n    {\\n      name: \\'isLoading\\',\\n      type: \\'boolean\\',\\n      description:\\n        \\'Boolean flag indicating whether a request is currently in progress.\\',\\n    },\\n    {\\n      name: \\'stop\\',\\n      type: \\'() => void\\',\\n      description: \\'Function to abort the current API request.\\',\\n    },\\n    {\\n      name: \\'clear\\',\\n      type: \\'() => void\\',\\n      description: \\'Function to clear the object state.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Streaming Object Generation with useObject\\',\\n      link: \\'/examples/next-pages/basics/streaming-object-generation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/31-convert-to-model-messages.mdx'), name='31-convert-to-model-messages.mdx', displayName='31-convert-to-model-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: convertToModelMessages\\ndescription: Convert useChat messages to ModelMessages for AI functions (API Reference)\\n---\\n\\n# `convertToModelMessages()`\\n\\nThe `convertToModelMessages` function is used to transform an array of UI messages from the `useChat` hook into an array of `ModelMessage` objects. These `ModelMessage` objects are compatible with AI core functions like `streamText`.\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Import\\n\\n<Snippet text={`import { convertToModelMessages } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'messages\\',\\n      type: \\'Message[]\\',\\n      description:\\n        \\'An array of UI messages from the useChat hook to be converted\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'{ tools?: ToolSet, convertDataPart?: (part: DataUIPart) => TextPart | FilePart | null }\\',\\n      description:\\n        \\'Optional configuration object. Provide tools to enable multi-modal tool responses, and convertDataPart to transform custom data parts into model-compatible content.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn array of [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) objects.\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'ModelMessage[]\\',\\n      type: \\'Array\\',\\n      description: \\'An array of ModelMessage objects\\',\\n    },\\n  ]}\\n/>\\n\\n## Multi-modal Tool Responses\\n\\nThe `convertToModelMessages` function supports tools that can return multi-modal content. This is useful when tools need to return non-text content like images.\\n\\n```ts\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst screenshotTool = tool({\\n  parameters: z.object({}),\\n  execute: async () => \\'imgbase64\\',\\n  toModelOutput: result => [{ type: \\'image\\', data: result }],\\n});\\n\\nconst result = streamText({\\n  model: openai(\\'gpt-4\\'),\\n  messages: convertToModelMessages(messages, {\\n    tools: {\\n      screenshot: screenshotTool,\\n    },\\n  }),\\n});\\n```\\n\\nTools can implement the optional `toModelOutput` method to transform their results into multi-modal content. The content is an array of content parts, where each part has a `type` (e.g., \\'text\\', \\'image\\') and corresponding data.\\n\\n## Custom Data Part Conversion\\n\\nThe `convertToModelMessages` function supports converting custom data parts attached to user messages. This is useful when users need to include additional context (URLs, code files, JSON configs) with their messages.\\n\\n### Basic Usage\\n\\nBy default, data parts in user messages are filtered out during conversion. To include them, provide a `convertDataPart` callback that transforms data parts into text or file parts that the model can understand:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\n\\ntype CustomUIMessage = UIMessage<\\n  never,\\n  {\\n    url: { url: string; title: string; content: string };\\n    \\'code-file\\': { filename: string; code: string; language: string };\\n  }\\n>;\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages<CustomUIMessage>(messages, {\\n      convertDataPart: part => {\\n        // Convert URL attachments to text\\n        if (part.type === \\'data-url\\') {\\n          return {\\n            type: \\'text\\',\\n            text: `[Reference: ${part.data.title}](${part.data.url})\\\\n\\\\n${part.data.content}`,\\n          };\\n        }\\n\\n        // Convert code file attachments\\n        if (part.type === \\'data-code-file\\') {\\n          return {\\n            type: \\'text\\',\\n            text: `\\\\`\\\\`\\\\`${part.data.language}\\\\n// ${part.data.filename}\\\\n${part.data.code}\\\\n\\\\`\\\\`\\\\``,\\n          };\\n        }\\n\\n        // Other data parts are ignored\\n      },\\n    }),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Use Cases\\n\\n**Attaching URL Content**\\nAllow users to attach URLs to their messages, with the content fetched and formatted for the model:\\n\\n```ts\\n// Client side\\nsendMessage({\\n  parts: [\\n    { type: \\'text\\', text: \\'Analyze this article\\' },\\n    {\\n      type: \\'data-url\\',\\n      data: {\\n        url: \\'https://example.com/article\\',\\n        title: \\'Important Article\\',\\n        content: \\'...\\',\\n      },\\n    },\\n  ],\\n});\\n```\\n\\n**Including Code Files as Context**\\nLet users reference code files in their conversations:\\n\\n```ts\\nconvertDataPart: part => {\\n  if (part.type === \\'data-code-file\\') {\\n    return {\\n      type: \\'text\\',\\n      text: `\\\\`\\\\`\\\\`${part.data.language}\\\\n${part.data.code}\\\\n\\\\`\\\\`\\\\``,\\n    };\\n  }\\n};\\n```\\n\\n**Selective Inclusion**\\nOnly data parts for which you return a text or file model message part are included,\\nall other data parts are ignored.\\n\\n```ts\\nconst result = convertToModelMessages<\\n  UIMessage<\\n    unknown,\\n    {\\n      url: { url: string; title: string };\\n      code: { code: string; language: string };\\n      note: { text: string };\\n    }\\n  >\\n>(messages, {\\n  convertDataPart: part => {\\n    if (part.type === \\'data-url\\') {\\n      return {\\n        type: \\'text\\',\\n        text: `[${part.data.title}](${part.data.url})`,\\n      };\\n    }\\n\\n    // data-code and data-node are ignored\\n  },\\n});\\n```\\n\\n### Type Safety\\n\\nThe generic parameter ensures full type safety for your custom data parts:\\n\\n```ts\\ntype MyUIMessage = UIMessage<\\n  unknown,\\n  {\\n    url: { url: string; content: string };\\n    config: { key: string; value: string };\\n  }\\n>;\\n\\n// TypeScript knows the exact shape of part.data\\nconvertToModelMessages<MyUIMessage>(messages, {\\n  convertDataPart: part => {\\n    if (part.type === \\'data-url\\') {\\n      // part.data is typed as { url: string; content: string }\\n      return { type: \\'text\\', text: part.data.url };\\n    }\\n    return null;\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/32-prune-messages.mdx'), name='32-prune-messages.mdx', displayName='32-prune-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: pruneMessages\\ndescription: API Reference for pruneMessages.\\n---\\n\\n# `pruneMessages()`\\n\\nThe `pruneMessages` function is used to prune or filter an array of `ModelMessage` objects. This is useful for reducing message context (to save tokens), removing intermediate reasoning, or trimming tool calls and empty messages before sending to an LLM.\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { pruneMessages, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const prunedMessages = pruneMessages({\\n    messages,\\n    reasoning: \\'before-last-message\\',\\n    toolCalls: \\'before-last-2-messages\\',\\n    emptyMessages: \\'remove\\',\\n  });\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: prunedMessages,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Import\\n\\n<Snippet text={`import { pruneMessages } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'messages\\',\\n      type: \\'ModelMessage[]\\',\\n      description: \\'An array of ModelMessage objects to prune.\\',\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: `\\'all\\' | \\'before-last-message\\' | \\'none\\'`,\\n      description:\\n        \\'How to remove reasoning content from assistant messages. Default: \"none\".\\',\\n    },\\n    {\\n      name: \\'toolCalls\\',\\n      type: `\\'all\\' | \\'before-last-message\\' | \\'before-last-\\\\${number}-messages\\\\\\' | \\'none\\' | PruneToolCallsOption[]`,\\n      description:\\n        \\'How to prune tool call/results/approval content. Can specify strategy or a list with tools.\\',\\n    },\\n    {\\n      name: \\'emptyMessages\\',\\n      type: `\\'keep\\' | \\'remove\\'`,\\n      description:\\n        \\'Whether to keep or remove messages whose content is empty after pruning. Default: \"remove\".\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn array of [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) objects, pruned according to the provided options.\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'ModelMessage[]\\',\\n      type: \\'Array\\',\\n      description: \\'The pruned list of ModelMessage objects\\',\\n    },\\n  ]}\\n/>\\n\\n## Example Usage\\n\\n```ts\\nimport { pruneMessages } from \\'ai\\';\\n\\nconst pruned = pruneMessages({\\n  messages,\\n  reasoning: \\'all\\', // Remove all reasoning parts\\n  toolCalls: \\'before-last-message\\', // Remove tool calls except those in the last message\\n});\\n```\\n\\n## Pruning Options\\n\\n- **reasoning:** Removes reasoning parts from assistant messages. Use `\\'all\\'` to remove all, `\\'before-last-message\\'` to keep reasoning in the last message, or `\\'none\\'` to retain all reasoning.\\n- **toolCalls:** Prune tool-call, tool-result, and tool-approval chunks from assistant/tool messages. Options include:\\n  - `\\'all\\'`: Prune all such content.\\n  - `\\'before-last-message\\'`: Prune except in the last message.\\n  - `before-last-N-messages`: Prune except in the last N messages.\\n  - `\\'none\\'`: Do not prune.\\n  - Or provide an array for per-tool fine control.\\n- **emptyMessages:** Set to `\\'remove\\'` (default) to exclude messages that have no content after pruning.\\n\\n> **Tip**: `pruneMessages` is typically used prior to sending a context window to an LLM to reduce message/token count, especially after a series of tool-calls and approvals.\\n\\nFor advanced usage and the full list of possible message parts, see [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) and [`pruneMessages` implementation](https://github.com/vercel/ai/blob/main/packages/ai/src/generate-text/prune-messages.ts).\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/40-create-ui-message-stream.mdx'), name='40-create-ui-message-stream.mdx', displayName='40-create-ui-message-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createUIMessageStream\\ndescription: API Reference for createUIMessageStream.\\n---\\n\\n# `createUIMessageStream`\\n\\nThe `createUIMessageStream` function allows you to create a readable stream for UI messages with advanced features like message merging, error handling, and finish callbacks.\\n\\n## Import\\n\\n<Snippet text={`import { createUIMessageStream } from \"ai\"`} prompt={false} />\\n\\n## Example\\n\\n```tsx\\nconst existingMessages: UIMessage[] = [\\n  /* ... */\\n];\\n\\nconst stream = createUIMessageStream({\\n  async execute({ writer }) {\\n    // Start a text message\\n    // Note: The id must be consistent across text-start, text-delta, and text-end steps\\n    // This allows the system to correctly identify they belong to the same text block\\n    writer.write({\\n      type: \\'text-start\\',\\n      id: \\'example-text\\',\\n    });\\n\\n    // Write a message chunk\\n    writer.write({\\n      type: \\'text-delta\\',\\n      id: \\'example-text\\',\\n      delta: \\'Hello\\',\\n    });\\n\\n    // End the text message\\n    writer.write({\\n      type: \\'text-end\\',\\n      id: \\'example-text\\',\\n    });\\n\\n    // Merge another stream from streamText\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      prompt: \\'Write a haiku about AI\\',\\n    });\\n\\n    writer.merge(result.toUIMessageStream());\\n  },\\n  onError: error => `Custom error: ${error.message}`,\\n  originalMessages: existingMessages,\\n  onFinish: ({ messages, isContinuation, responseMessage }) => {\\n    console.log(\\'Stream finished with messages:\\', messages);\\n  },\\n});\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'execute\\',\\n      type: \\'(options: { writer: UIMessageStreamWriter }) => Promise<void> | void\\',\\n      description:\\n        \\'A function that receives a writer instance and can use it to write UI message chunks to the stream.\\',\\n      properties: [\\n        {\\n          type: \\'UIMessageStreamWriter\\',\\n          parameters: [\\n            {\\n              name: \\'write\\',\\n              type: \\'(part: UIMessageChunk) => void\\',\\n              description: \\'Writes a UI message chunk to the stream.\\',\\n            },\\n            {\\n              name: \\'merge\\',\\n              type: \\'(stream: ReadableStream<UIMessageChunk>) => void\\',\\n              description:\\n                \\'Merges the contents of another UI message stream into this stream.\\',\\n            },\\n            {\\n              name: \\'onError\\',\\n              type: \\'(error: unknown) => string\\',\\n              description:\\n                \\'Error handler that is used by the stream writer for handling errors in merged streams.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: unknown) => string\\',\\n      description:\\n        \\'A function that handles errors and returns an error message string. By default, it returns the error message.\\',\\n    },\\n    {\\n      name: \\'originalMessages\\',\\n      type: \\'UIMessage[] | undefined\\',\\n      description:\\n        \\'The original messages. If provided, persistence mode is assumed and a message ID is provided for the response message.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(options: { messages: UIMessage[]; isContinuation: boolean; responseMessage: UIMessage }) => void | undefined\\',\\n      description:\\n        \\'A callback function that is called when the stream finishes.\\',\\n      properties: [\\n        {\\n          type: \\'FinishOptions\\',\\n          parameters: [\\n            {\\n              name: \\'messages\\',\\n              type: \\'UIMessage[]\\',\\n              description: \\'The updated list of UI messages.\\',\\n            },\\n            {\\n              name: \\'isContinuation\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'Indicates whether the response message is a continuation of the last original message, or if a new message was created.\\',\\n            },\\n            {\\n              name: \\'responseMessage\\',\\n              type: \\'UIMessage\\',\\n              description:\\n                \\'The message that was sent to the client as a response (including the original message if it was extended).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'generateId\\',\\n      type: \\'IdGenerator | undefined\\',\\n      description:\\n        \\'A function to generate unique IDs for messages. Uses the default ID generator if not provided.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n`ReadableStream<UIMessageChunk>`\\n\\nA readable stream that emits UI message chunks. The stream automatically handles error propagation, merging of multiple streams, and proper cleanup when all operations are complete.\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/41-create-ui-message-stream-response.mdx'), name='41-create-ui-message-stream-response.mdx', displayName='41-create-ui-message-stream-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createUIMessageStreamResponse\\ndescription: API Reference for createUIMessageStreamResponse.\\n---\\n\\n# `createUIMessageStreamResponse`\\n\\nThe `createUIMessageStreamResponse` function creates a Response object that streams UI messages to the client.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createUIMessageStreamResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\n```tsx\\nimport {\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n  streamText,\\n} from \\'ai\\';\\n\\nconst response = createUIMessageStreamResponse({\\n  status: 200,\\n  statusText: \\'OK\\',\\n  headers: {\\n    \\'Custom-Header\\': \\'value\\',\\n  },\\n  stream: createUIMessageStream({\\n    execute({ writer }) {\\n      // Write custom data\\n      writer.write({\\n        type: \\'data\\',\\n        value: { message: \\'Hello\\' },\\n      });\\n\\n      // Write text content\\n      writer.write({\\n        type: \\'text\\',\\n        value: \\'Hello, world!\\',\\n      });\\n\\n      // Write source information\\n      writer.write({\\n        type: \\'source-url\\',\\n        value: {\\n          type: \\'source\\',\\n          id: \\'source-1\\',\\n          url: \\'https://example.com\\',\\n          title: \\'Example Source\\',\\n        },\\n      });\\n\\n      // Merge with LLM stream\\n      const result = streamText({\\n        model: \\'anthropic/claude-sonnet-4.5\\',\\n        prompt: \\'Say hello\\',\\n      });\\n\\n      writer.merge(result.toUIMessageStream());\\n    },\\n  }),\\n});\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream<UIMessageChunk>\\',\\n      description: \\'The UI message stream to send to the client.\\',\\n    },\\n    {\\n      name: \\'status\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'The status code for the response. Defaults to 200.\\',\\n    },\\n    {\\n      name: \\'statusText\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'The status text for the response.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Headers | Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional headers for the response.\\',\\n    },\\n    {\\n      name: \\'consumeSseStream\\',\\n      type: \\'(options: { stream: ReadableStream<string> }) => PromiseLike<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback to consume the Server-Sent Events stream.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n`Response`\\n\\nA Response object that streams UI message chunks with the specified status, headers, and content.\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/42-pipe-ui-message-stream-to-response.mdx'), name='42-pipe-ui-message-stream-to-response.mdx', displayName='42-pipe-ui-message-stream-to-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: pipeUIMessageStreamToResponse\\ndescription: Learn to use pipeUIMessageStreamToResponse helper function to pipe streaming data to a ServerResponse object.\\n---\\n\\n# `pipeUIMessageStreamToResponse`\\n\\nThe `pipeUIMessageStreamToResponse` function pipes streaming data to a Node.js ServerResponse object (see [Streaming Data](/docs/ai-sdk-ui/streaming-data)).\\n\\n## Import\\n\\n<Snippet\\n  text={`import { pipeUIMessageStreamToResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\n```tsx\\npipeUIMessageStreamToResponse({\\n  response: serverResponse,\\n  status: 200,\\n  statusText: \\'OK\\',\\n  headers: {\\n    \\'Custom-Header\\': \\'value\\',\\n  },\\n  stream: myUIMessageStream,\\n  consumeSseStream: ({ stream }) => {\\n    // Optional: consume the SSE stream independently\\n    console.log(\\'Consuming SSE stream:\\', stream);\\n  },\\n});\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'ServerResponse\\',\\n      description: \\'The Node.js ServerResponse object to pipe the data to.\\',\\n    },\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream<UIMessageChunk>\\',\\n      description: \\'The UI message stream to pipe to the response.\\',\\n    },\\n    {\\n      name: \\'status\\',\\n      type: \\'number\\',\\n      description: \\'The status code for the response.\\',\\n    },\\n    {\\n      name: \\'statusText\\',\\n      type: \\'string\\',\\n      description: \\'The status text for the response.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Headers | Record<string, string>\\',\\n      description: \\'Additional headers for the response.\\',\\n    },\\n    {\\n      name: \\'consumeSseStream\\',\\n      type: \\'({ stream }: { stream: ReadableStream }) => void\\',\\n      description:\\n        \\'Optional function to consume the SSE stream independently. The stream is teed and this function receives a copy.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/43-read-ui-message-stream.mdx'), name='43-read-ui-message-stream.mdx', displayName='43-read-ui-message-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: readUIMessageStream\\ndescription: API Reference for readUIMessageStream.\\n---\\n\\n# readUIMessageStream\\n\\nTransforms a stream of `UIMessageChunk`s into an `AsyncIterableStream` of `UIMessage`s.\\n\\nUI message streams are useful outside of Chat use cases, e.g. for terminal UIs, custom stream consumption on the client, or RSC (React Server Components).\\n\\n## Import\\n\\n```tsx\\nimport { readUIMessageStream } from 'ai';\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: 'message',\\n      type: 'UIMessage',\\n      isOptional: true,\\n      description:\\n        'The last assistant message to use as a starting point when the conversation is resumed. Otherwise undefined.',\\n    },\\n    {\\n      name: 'stream',\\n      type: 'ReadableStream<UIMessageChunk>',\\n      description: 'The stream of UIMessageChunk objects to read.',\\n    },\\n    {\\n      name: 'onError',\\n      type: '(error: unknown) => void',\\n      isOptional: true,\\n      description:\\n        'A function that is called when an error occurs during stream processing.',\\n    },\\n    {\\n      name: 'terminateOnError',\\n      type: 'boolean',\\n      isOptional: true,\\n      description:\\n        'Whether to terminate the stream if an error occurs. Defaults to false.',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn `AsyncIterableStream` of `UIMessage`s. Each stream part represents a different state of the same message as it is being completed.\\n\\nFor comprehensive examples and use cases, see [Reading UI Message Streams](/docs/ai-sdk-ui/reading-ui-message-streams).\\n\", children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/46-infer-ui-tools.mdx'), name='46-infer-ui-tools.mdx', displayName='46-infer-ui-tools.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: InferUITools\\ndescription: API Reference for InferUITools.\\n---\\n\\n# InferUITools\\n\\nInfers the input and output types of a `ToolSet`.\\n\\nThis type helper is useful when working with tools in TypeScript to ensure type safety for your tool inputs and outputs in `UIMessage`s.\\n\\n## Import\\n\\n```tsx\\nimport { InferUITools } from 'ai';\\n```\\n\\n## API Signature\\n\\n### Type Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: 'TOOLS',\\n      type: 'ToolSet',\\n      description: 'The tool set to infer types from.',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA type that maps each tool in the tool set to its inferred input and output types.\\n\\nThe resulting type has the shape:\\n\\n```typescript\\n{\\n  [NAME in keyof TOOLS & string]: {\\n    input: InferToolInput<TOOLS[NAME]>;\\n    output: InferToolOutput<TOOLS[NAME]>;\\n  };\\n}\\n```\\n\\n## Examples\\n\\n### Basic Usage\\n\\n```tsx\\nimport { InferUITools } from 'ai';\\nimport { z } from 'zod';\\n\\nconst tools = {\\n  weather: {\\n    description: 'Get the current weather',\\n    parameters: z.object({\\n      location: z.string().describe('The city and state'),\\n    }),\\n    execute: async ({ location }) => {\\n      return `The weather in ${location} is sunny.`;\\n    },\\n  },\\n  calculator: {\\n    description: 'Perform basic arithmetic',\\n    parameters: z.object({\\n      operation: z.enum(['add', 'subtract', 'multiply', 'divide']),\\n      a: z.number(),\\n      b: z.number(),\\n    }),\\n    execute: async ({ operation, a, b }) => {\\n      switch (operation) {\\n        case 'add':\\n          return a + b;\\n        case 'subtract':\\n          return a - b;\\n        case 'multiply':\\n          return a * b;\\n        case 'divide':\\n          return a / b;\\n      }\\n    },\\n  },\\n};\\n\\n// Infer the types from the tool set\\ntype MyUITools = InferUITools<typeof tools>;\\n// This creates a type with:\\n// {\\n//   weather: { input: { location: string }; output: string };\\n//   calculator: { input: { operation: 'add' | 'subtract' | 'multiply' | 'divide'; a: number; b: number }; output: number };\\n// }\\n```\\n\\n## Related\\n\\n- [`InferUITool`](/docs/reference/ai-sdk-ui/infer-ui-tool) - Infer types for a single tool\\n- [`useChat`](/docs/reference/ai-sdk-ui/use-chat) - Chat hook that supports typed tools\\n\", children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/47-infer-ui-tool.mdx'), name='47-infer-ui-tool.mdx', displayName='47-infer-ui-tool.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: InferUITool\\ndescription: API Reference for InferUITool.\\n---\\n\\n# InferUITool\\n\\nInfers the input and output types of a tool.\\n\\nThis type helper is useful when working with individual tools to ensure type safety for your tool inputs and outputs in `UIMessage`s.\\n\\n## Import\\n\\n```tsx\\nimport { InferUITool } from 'ai';\\n```\\n\\n## API Signature\\n\\n### Type Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: 'TOOL',\\n      type: 'Tool',\\n      description: 'The tool to infer types from.',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA type that contains the inferred input and output types of the tool.\\n\\nThe resulting type has the shape:\\n\\n```typescript\\n{\\n  input: InferToolInput<TOOL>;\\n  output: InferToolOutput<TOOL>;\\n}\\n```\\n\\n## Examples\\n\\n### Basic Usage\\n\\n```tsx\\nimport { InferUITool } from 'ai';\\nimport { z } from 'zod';\\n\\nconst weatherTool = {\\n  description: 'Get the current weather',\\n  parameters: z.object({\\n    location: z.string().describe('The city and state'),\\n  }),\\n  execute: async ({ location }) => {\\n    return `The weather in ${location} is sunny.`;\\n  },\\n};\\n\\n// Infer the types from the tool\\ntype WeatherUITool = InferUITool<typeof weatherTool>;\\n// This creates a type with:\\n// {\\n//   input: { location: string };\\n//   output: string;\\n// }\\n```\\n\\n## Related\\n\\n- [`InferUITools`](/docs/reference/ai-sdk-ui/infer-ui-tools) - Infer types for a tool set\\n- [`ToolUIPart`](/docs/reference/ai-sdk-ui/tool-ui-part) - Tool part type for UI messages\\n\", children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK UI\\ndescription: Reference documentation for the AI SDK UI\\ncollapsed: true\\n---\\n\\n# AI SDK UI\\n\\n[AI SDK UI](/docs/ai-sdk-ui) is designed to help you build interactive chat, completion, and assistant applications with ease.\\nIt is framework-agnostic toolkit, streamlining the integration of advanced AI functionalities into your applications.\\n\\nAI SDK UI contains the following hooks:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'useChat',\\n      description:\\n        'Use a hook to interact with language models in a chat interface.',\\n      href: '/docs/reference/ai-sdk-ui/use-chat',\\n    },\\n    {\\n      title: 'useCompletion',\\n      description:\\n        'Use a hook to interact with language models in a completion interface.',\\n      href: '/docs/reference/ai-sdk-ui/use-completion',\\n    },\\n    {\\n      title: 'useObject',\\n      description: 'Use a hook for consuming a streamed JSON objects.',\\n      href: '/docs/reference/ai-sdk-ui/use-object',\\n    },\\n    {\\n      title: 'convertToModelMessages',\\n      description:\\n        'Convert useChat messages to ModelMessages for AI functions.',\\n      href: '/docs/reference/ai-sdk-ui/convert-to-model-messages',\\n    },\\n    {\\n      title: 'pruneMessages',\\n      description: 'Prunes model messages from a list of model messages.',\\n      href: '/docs/reference/ai-sdk-ui/prune-messages',\\n    },\\n    {\\n      title: 'createUIMessageStream',\\n      description:\\n        'Create a UI message stream to stream additional data to the client.',\\n      href: '/docs/reference/ai-sdk-ui/create-ui-message-stream',\\n    },\\n    {\\n      title: 'createUIMessageStreamResponse',\\n      description:\\n        'Create a response object to stream UI messages to the client.',\\n      href: '/docs/reference/ai-sdk-ui/create-ui-message-stream-response',\\n    },\\n    {\\n      title: 'pipeUIMessageStreamToResponse',\\n      description:\\n        'Pipe a UI message stream to a Node.js ServerResponse object.',\\n      href: '/docs/reference/ai-sdk-ui/pipe-ui-message-stream-to-response',\\n    },\\n    {\\n      title: 'readUIMessageStream',\\n      description:\\n        'Transform a stream of UIMessageChunk objects into an AsyncIterableStream of UIMessage objects.',\\n      href: '/docs/reference/ai-sdk-ui/read-ui-message-stream',\\n    },\\n  ]}\\n/>\\n\\n## UI Framework Support\\n\\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), and [Vue.js](https://vuejs.org/).\\nHere is a comparison of the supported functions across these frameworks:\\n\\n| Function                                                  | React               | Svelte                               | Vue.js              |\\n| --------------------------------------------------------- | ------------------- | ------------------------------------ | ------------------- |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)             | <Check size={18} /> | <Check size={18} /> Chat             | <Check size={18} /> |\\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion) | <Check size={18} /> | <Check size={18} /> Completion       | <Check size={18} /> |\\n| [useObject](/docs/reference/ai-sdk-ui/use-object)         | <Check size={18} /> | <Check size={18} /> StructuredObject | <Cross size={18} /> |\\n\\n<Note>\\n  [Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\\n  welcome to implement missing features for non-React frameworks.\\n</Note>\\n\", children=[])]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc'), name='03-ai-sdk-rsc', displayName='03-ai-sdk-rsc', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/01-stream-ui.mdx'), name='01-stream-ui.mdx', displayName='01-stream-ui.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamUI\\ndescription: Reference for the streamUI function from the AI SDK RSC\\n---\\n\\n# `streamUI`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nA helper function to create a streamable UI from LLM providers. This function is similar to AI SDK Core APIs and supports the same model interfaces.\\n\\nTo see `streamUI` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { streamUI } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \\'The language model to use. Example: openai(\"gpt-4.1\")\\',\\n    },\\n    {\\n      name: \\'initial\\',\\n      isOptional: true,\\n      type: \\'ReactNode\\',\\n      description: \\'The initial UI to render.\\',\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'CoreSystemMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'CoreUserMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'CoreAssistantMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'args\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'CoreToolMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      description:\\n        \\'Tools that are accessible to and can be called by the model.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'parameters\\',\\n              type: \\'zod schema\\',\\n              description:\\n                \\'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\\',\\n            },\\n            {\\n              name: \\'generate\\',\\n              isOptional: true,\\n              type: \\'(async (parameters) => ReactNode) | AsyncGenerator<ReactNode, ReactNode, void>\\',\\n              description:\\n                \\'A function or a generator function that is called with the arguments from the tool call and yields React nodes as the UI.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      isOptional: true,\\n      type: \\'\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\\',\\n      description:\\n        \\'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      isOptional: true,\\n      type: \\'(Text) => ReactNode\\',\\n      description: \\'Callback to handle the generated tokens from the model.\\',\\n      properties: [\\n        {\\n          type: \\'Text\\',\\n          parameters: [\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The full content of the completion.\\',\\n            },\\n            { name: \\'delta\\', type: \\'string\\', description: \\'The delta.\\' },\\n            { name: \\'done\\', type: \\'boolean\\', description: \\'Is it done?\\' },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response and all request tool executions (for tools that have a `generate` function) are finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'usage\\',\\n              type: \\'TokenUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'TokenUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'promptTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens in the prompt.\\',\\n                    },\\n                    {\\n                      name: \\'completionTokens\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The total number of tokens in the completion.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens generated.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'value\\',\\n              type: \\'ReactNode\\',\\n              description: \\'The final ui node that was generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              description: \\'Optional response data.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'ReactNode\\',\\n      description: \\'The user interface based on the stream output.\\',\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      isOptional: true,\\n      description: \\'Optional response data.\\',\\n      properties: [\\n        {\\n          type: \\'Response\\',\\n          parameters: [\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'stream\\',\\n      type: \\'AsyncIterable<StreamPart> & ReadableStream<StreamPart>\\',\\n      description:\\n        \\'A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.\\',\\n      properties: [\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'text-delta\\'\",\\n              description: \\'The type to identify the object as text delta.\\',\\n            },\\n            {\\n              name: \\'textDelta\\',\\n              type: \\'string\\',\\n              description: \\'The text delta.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call\\'\",\\n              description: \\'The type to identify the object as tool call.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'args\\',\\n              type: \\'object based on zod schema\\',\\n              description:\\n                \\'Parameters generated by the model to be used by the tool.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'error\\'\",\\n              description: \\'The type to identify the object as error.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'Error\\',\\n              description:\\n                \\'Describes the error that may have occurred during execution.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish\\'\",\\n              description: \\'The type to identify the object as finish.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'TokenUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'TokenUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'promptTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens in the prompt.\\',\\n                    },\\n                    {\\n                      name: \\'completionTokens\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The total number of tokens in the completion.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens generated.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title:\\n        \\'Learn to render a React component as a function call using a language model in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n    {\\n      title: \\'Learn to persist and restore states UI/AI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/save-and-restore-states\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to route React components using a language model in Next.js\\',\\n      link: \\'/examples/next-app/interface/route-components\\',\\n    },\\n    {\\n      title: \\'Learn to stream component updates to the client in Next.js\\',\\n      link: \\'/examples/next-app/interface/stream-component-updates\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/02-create-ai.mdx'), name='02-create-ai.mdx', displayName='02-create-ai.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createAI\\ndescription: Reference for the createAI function from the AI SDK RSC\\n---\\n\\n# `createAI`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nCreates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.\\n\\n## Import\\n\\n<Snippet text={`import { createAI } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'actions\\',\\n      type: \\'Record<string, Action>\\',\\n      description: \\'Server side actions that can be called from the client.\\',\\n    },\\n    {\\n      name: \\'initialAIState\\',\\n      type: \\'any\\',\\n      description: \\'Initial AI state to be used in the client.\\',\\n    },\\n    {\\n      name: \\'initialUIState\\',\\n      type: \\'any\\',\\n      description: \\'Initial UI state to be used in the client.\\',\\n    },\\n    {\\n      name: \\'onGetUIState\\',\\n      type: \\'() => UIState\\',\\n      description: \\'is called during SSR to compare and update UI state.\\',\\n    },\\n    {\\n      name: \\'onSetAIState\\',\\n      type: \\'(Event) => void\\',\\n      description:\\n        \\'is triggered whenever an update() or done() is called by the mutable AI state in your action, so you can safely store your AI state in the database.\\',\\n      properties: [\\n        {\\n          type: \\'Event\\',\\n          parameters: [\\n            {\\n              name: \\'state\\',\\n              type: \\'AIState\\',\\n              description: \\'The resulting AI state after the update.\\',\\n            },\\n            {\\n              name: \\'done\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'Whether the AI state updates have been finalized or not.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nIt returns an `<AI/>` context provider.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to manage AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n    {\\n      title: \\'Learn to persist and restore states UI/AI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/save-and-restore-states\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/03-create-streamable-ui.mdx'), name='03-create-streamable-ui.mdx', displayName='03-create-streamable-ui.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createStreamableUI\\ndescription: Reference for the createStreamableUI function from the AI SDK RSC\\n---\\n\\n# `createStreamableUI`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nCreate a stream that sends UI from the server to the client. On the client side, it can be rendered as a normal React node.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createStreamableUI } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'initialValue\\',\\n      type: \\'ReactNode\\',\\n      isOptional: true,\\n      description: \\'The initial value of the streamable UI.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'ReactNode\\',\\n      description:\\n        \\'The value of the streamable UI. This can be returned from a Server Action and received by the client.\\',\\n    },\\n  ]}\\n/>\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'update\\',\\n      type: \\'(ReactNode) => void\\',\\n      description:\\n        \\'Updates the current UI node. It takes a new UI node and replaces the old one.\\',\\n    },\\n    {\\n      name: \\'append\\',\\n      type: \\'(ReactNode) => void\\',\\n      description:\\n        \\'Appends a new UI node to the end of the old one. Once appended a new UI node, the previous UI node cannot be updated anymore.\\',\\n    },\\n    {\\n      name: \\'done\\',\\n      type: \\'(ReactNode | null) => void\\',\\n      description:\\n        \\'Marks the UI node as finalized and closes the stream. Once called, the UI node cannot be updated or appended anymore. This method is always required to be called, otherwise the response will be stuck in a loading state.\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'(Error) => void\\',\\n      description:\\n        \\'Signals that there is an error in the UI stream. It will be thrown on the client side and caught by the nearest error boundary component.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Render a React component during a tool call\\',\\n      link: \\'/examples/next-app/tools/render-interface-during-tool-call\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/04-create-streamable-value.mdx'), name='04-create-streamable-value.mdx', displayName='04-create-streamable-value.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createStreamableValue\\ndescription: Reference for the createStreamableValue function from the AI SDK RSC\\n---\\n\\n# `createStreamableValue`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nCreate a stream that sends values from the server to the client. The value can be any serializable data.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createStreamableValue } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'any\\',\\n      description: \\'Any data that RSC supports. Example, JSON.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'streamable\\',\\n      description:\\n        \\'This creates a special value that can be returned from Actions to the client. It holds the data inside and can be updated via the update method.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx'), name='05-read-streamable-value.mdx', displayName='05-read-streamable-value.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: readStreamableValue\\ndescription: Reference for the readStreamableValue function from the AI SDK RSC\\n---\\n\\n# `readStreamableValue`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a function that helps you read the streamable value from the client that was originally created using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) on the server.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { readStreamableValue } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\n```ts filename=\"app/actions.ts\"\\nasync function generate() {\\n  \\'use server\\';\\n  const streamable = createStreamableValue();\\n\\n  streamable.update(1);\\n  streamable.update(2);\\n  streamable.done(3);\\n\\n  return streamable.value;\\n}\\n```\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"12\"\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [generation, setGeneration] = useState(\\'\\');\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const stream = await generate();\\n\\n          for await (const delta of readStreamableValue(stream)) {\\n            setGeneration(generation => generation + delta);\\n          }\\n        }}\\n      >\\n        Generate\\n      </button>\\n    </div>\\n  );\\n}\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'StreamableValue\\',\\n      description: \\'The streamable value to read from.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nIt returns an async iterator that contains the values emitted by the streamable value.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/06-get-ai-state.mdx'), name='06-get-ai-state.mdx', displayName='06-get-ai-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: getAIState\\ndescription: Reference for the getAIState function from the AI SDK RSC\\n---\\n\\n# `getAIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nGet the current AI state.\\n\\n## Import\\n\\n<Snippet text={`import { getAIState } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'key\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \"Returns the value of the specified key in the AI state, if it\\'s an object.\",\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe AI state.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title:\\n        \\'Learn to render a React component during a tool call made by a language model in Next.js\\',\\n      link: \\'/examples/next-app/tools/render-interface-during-tool-call\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/07-get-mutable-ai-state.mdx'), name='07-get-mutable-ai-state.mdx', displayName='07-get-mutable-ai-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: getMutableAIState\\ndescription: Reference for the getMutableAIState function from the AI SDK RSC\\n---\\n\\n# `getMutableAIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nGet a mutable copy of the AI state. You can use this to update the state in the server.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { getMutableAIState } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'key\\',\\n      isOptional: true,\\n      type: \\'string\\',\\n      description:\\n        \"Returns the value of the specified key in the AI state, if it\\'s an object.\",\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe mutable AI state.\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'update\\',\\n      type: \\'(newState: any) => void\\',\\n      description: \\'Updates the AI state with the new state.\\',\\n    },\\n    {\\n      name: \\'done\\',\\n      type: \\'(newState: any) => void\\',\\n      description:\\n        \\'Updates the AI state with the new state, marks it as finalized and closes the stream.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to persist and restore states AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/save-and-restore-states\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/08-use-ai-state.mdx'), name='08-use-ai-state.mdx', displayName='08-use-ai-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useAIState\\ndescription: Reference for the useAIState function from the AI SDK RSC\\n---\\n\\n# `useAIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a hook that enables you to read and update the AI state. The AI state is shared globally between all `useAIState` hooks under the same `<AI/>` provider.\\n\\nThe AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.\\n\\n## Import\\n\\n<Snippet text={`import { useAIState } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Returns\\n\\nA single element array where the first element is the current AI state.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/09-use-actions.mdx'), name='09-use-actions.mdx', displayName='09-use-actions.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useActions\\ndescription: Reference for the useActions function from the AI SDK RSC\\n---\\n\\n# `useActions`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a hook to help you access your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.\\n\\nIt is required to access these server actions via this hook because they are patched when passed through the context. Accessing them directly may result in a [Cannot find Client Component error](/docs/troubleshooting/common-issues/server-actions-in-client-components).\\n\\n## Import\\n\\n<Snippet text={`import { useActions } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Returns\\n\\n`Record<string, Action>`, a dictionary of server actions.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to manage AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to route React components using a language model in Next.js\\',\\n      link: \\'/examples/next-app/interface/route-components\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/10-use-ui-state.mdx'), name='10-use-ui-state.mdx', displayName='10-use-ui-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useUIState\\ndescription: Reference for the useUIState function from the AI SDK RSC\\n---\\n\\n# `useUIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a hook that enables you to read and update the UI State. The state is client-side and can contain functions, React nodes, and other data. UIState is the visual representation of the AI state.\\n\\n## Import\\n\\n<Snippet text={`import { useUIState } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Returns\\n\\nSimilar to useState, it is an array, where the first element is the current UI state and the second element is the function that updates the state.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to manage AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx'), name='11-use-streamable-value.mdx', displayName='11-use-streamable-value.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useStreamableValue\\ndescription: Reference for the useStreamableValue function from the AI SDK RSC\\n---\\n\\n# `useStreamableValue`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a React hook that takes a streamable value created using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) and returns the current value, error, and pending state.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { useStreamableValue } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\nThis is useful for consuming streamable values received from a component\\'s props.\\n\\n```tsx\\nfunction MyComponent({ streamableValue }) {\\n  const [data, error, pending] = useStreamableValue(streamableValue);\\n\\n  if (pending) return <div>Loading...</div>;\\n  if (error) return <div>Error: {error.message}</div>;\\n\\n  return <div>Data: {data}</div>;\\n}\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\nIt accepts a streamable value created using `createStreamableValue`.\\n\\n### Returns\\n\\nIt is an array, where the first element contains the data, the second element contains an error if it is thrown anytime during the stream, and the third is a boolean indicating if the value is pending.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/20-render.mdx'), name='20-render.mdx', displayName='20-render.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: render (Removed)\\ndescription: Reference for the render function from the AI SDK RSC\\n---\\n\\n# `render` (Removed)\\n\\n<Note type=\"warning\">\"render\" has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nA helper function to create a streamable UI from LLM providers. This function is similar to AI SDK Core APIs and supports the same model interfaces.\\n\\n> **Note**: `render` has been deprecated in favor of [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui). During migration, please ensure that the `messages` parameter follows the updated [specification](/docs/reference/ai-sdk-rsc/stream-ui#messages).\\n\\n## Import\\n\\n<Snippet text={`import { render } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'string\\',\\n      description: \\'Model identifier, must be OpenAI SDK compatible.\\',\\n    },\\n    {\\n      name: \\'provider\\',\\n      type: \\'provider client\\',\\n      description:\\n        \\'Currently the only provider available is OpenAI. This needs to match the model name.\\',\\n    },\\n    {\\n      name: \\'initial\\',\\n      isOptional: true,\\n      type: \\'ReactNode\\',\\n      description: \\'The initial UI to render.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemMessage | UserMessage | AssistantMessage | ToolMessage>\\',\\n      description: \\'A list of messages that represent a conversation.\\',\\n      properties: [\\n        {\\n          type: \\'SystemMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n            {\\n              name: \\'tool_calls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'A list of tool calls made by the model.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolCall\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'function\\'\",\\n                      description: \\'The type of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'function\\',\\n                      type: \\'Function\\',\\n                      description: \\'The function to call.\\',\\n                      properties: [\\n                        {\\n                          type: \\'Function\\',\\n                          parameters: [\\n                            {\\n                              name: \\'name\\',\\n                              type: \\'string\\',\\n                              description: \\'The name of the function.\\',\\n                            },\\n                            {\\n                              name: \\'arguments\\',\\n                              type: \\'string\\',\\n                              description: \\'The arguments of the function.\\',\\n                            },\\n                          ],\\n                        },\\n                      ],\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the tool message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'functions\\',\\n      type: \\'ToolSet\\',\\n      isOptional: true,\\n      description:\\n        \\'Tools that are accessible to and can be called by the model.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'parameters\\',\\n              type: \\'zod schema\\',\\n              description:\\n                \\'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\\',\\n            },\\n            {\\n              name: \\'render\\',\\n              isOptional: true,\\n              type: \\'async (parameters) => any\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      isOptional: true,\\n      description:\\n        \\'Tools that are accessible to and can be called by the model.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'parameters\\',\\n              type: \\'zod schema\\',\\n              description:\\n                \\'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\\',\\n            },\\n            {\\n              name: \\'render\\',\\n              isOptional: true,\\n              type: \\'async (parameters) => any\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'text\\',\\n      isOptional: true,\\n      type: \\'(Text) => ReactNode\\',\\n      description: \\'Callback to handle the generated tokens from the model.\\',\\n      properties: [\\n        {\\n          type: \\'Text\\',\\n          parameters: [\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The full content of the completion.\\',\\n            },\\n            { name: \\'delta\\', type: \\'string\\', description: \\'The delta.\\' },\\n            { name: \\'done\\', type: \\'boolean\\', description: \\'Is it done?\\' },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'temperature\\',\\n      isOptional: true,\\n      type: \\'number\\',\\n      description: \\'The temperature to use for the model.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nIt can return any valid ReactNode.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK RSC\\ndescription: Reference documentation for the AI SDK UI\\ncollapsed: true\\n---\\n\\n# AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: \\'streamUI\\',\\n      description:\\n        \\'Use a helper function that streams React Server Components on tool execution.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/stream-ui\\',\\n    },\\n    {\\n      title: \\'createAI\\',\\n      description:\\n        \\'Create a context provider that wraps your application and shares state between the client and language model on the server.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/create-ai\\',\\n    },\\n    {\\n      title: \\'createStreamableUI\\',\\n      description:\\n        \\'Create a streamable UI component that can be rendered on the server and streamed to the client.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/create-streamable-ui\\',\\n    },\\n    {\\n      title: \\'createStreamableValue\\',\\n      description:\\n        \\'Create a streamable value that can be rendered on the server and streamed to the client.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/create-streamable-value\\',\\n    },\\n    {\\n      title: \\'getAIState\\',\\n      description: \\'Read the AI state on the server.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/get-ai-state\\',\\n    },\\n    {\\n      title: \\'getMutableAIState\\',\\n      description: \\'Read and update the AI state on the server.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/get-mutable-ai-state\\',\\n    },\\n    {\\n      title: \\'useAIState\\',\\n      description: \\'Get the AI state on the client from the context provider.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/use-ai-state\\',\\n    },\\n    {\\n      title: \\'useUIState\\',\\n      description: \\'Get the UI state on the client from the context provider.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/use-ui-state\\',\\n    },\\n    {\\n      title: \\'useActions\\',\\n      description: \\'Call server actions from the client.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/use-actions\\',\\n    },\\n  ]}\\n/>\\n', children=[])]), DocItem(origPath=Path('07-reference/04-stream-helpers'), name='04-stream-helpers', displayName='04-stream-helpers', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/04-stream-helpers/01-ai-stream.mdx'), name='01-ai-stream.mdx', displayName='01-ai-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AIStream\\ndescription: Learn to use AIStream helper function in your application.\\n---\\n\\n# `AIStream`\\n\\n<Note type=\"warning\">\\n  AIStream has been removed in AI SDK 4.0. Use\\n  `streamText.toDataStreamResponse()` instead.\\n</Note>\\n\\nCreates a readable stream for AI responses. This is based on the responses returned\\nby fetch and serves as the basis for the OpenAIStream and AnthropicStream. It allows\\nyou to handle AI response streams in a controlled and customized manner that will\\nwork with useChat and useCompletion.\\n\\nAIStream will throw an error if response doesn\\'t have a 2xx status code. This is to ensure that the stream is only created for successful responses.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AIStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \"This is the response object returned by fetch. It\\'s used as the source of the readable stream.\",\\n    },\\n    {\\n      name: \\'customParser\\',\\n      type: \\'(AIStreamParser) => void\\',\\n      description:\\n        \\'This is a function that is used to parse the events in the stream. It should return a function that receives a stringified chunk from the LLM and extracts the message content. The function is expected to return nothing (void) or a string.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamParser\\',\\n          parameters: [\\n            {\\n              name: \\'\\',\\n              type: \\'(data: string) => string | void\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/02-streaming-text-response.mdx'), name='02-streaming-text-response.mdx', displayName='02-streaming-text-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: StreamingTextResponse\\ndescription: Learn to use StreamingTextResponse helper function in your application.\\n---\\n\\n# `StreamingTextResponse`\\n\\n<Note type=\"warning\">\\n  `StreamingTextResponse` has been removed in AI SDK 4.0. Use\\n  [`streamText.toDataStreamResponse()`](/docs/reference/ai-sdk-core/stream-text)\\n  instead.\\n</Note>\\n\\nIt is a utility class that simplifies the process of returning a ReadableStream of text in HTTP responses.\\nIt is a lightweight wrapper around the native Response class, automatically setting the status code to 200 and the Content-Type header to \\'text/plain; charset=utf-8\\'.\\n\\n## Import\\n\\n<Snippet text={`import { StreamingTextResponse } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream\\',\\n      description: \\'The stream of content which represents the HTTP response.\\',\\n    },\\n    {\\n      name: \\'init\\',\\n      isOptional: true,\\n      type: \\'ResponseInit\\',\\n      description:\\n        \\'It can be used to customize the properties of the HTTP response. It is an object that corresponds to the ResponseInit object used in the Response constructor.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description:\\n                \\'The status code for the response. StreamingTextResponse will overwrite this value with 200.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'The status message associated with the status code.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'HeadersInit\\',\\n              isOptional: true,\\n              description:\\n                \"Any headers you want to add to your response. StreamingTextResponse will add \\'Content-Type\\': \\'text/plain; charset=utf-8\\' to these headers.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'data\\',\\n      isOptional: true,\\n      type: \\'StreamData\\',\\n      description:\\n        \\'StreamData object that you are using to generate additional data for the response.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn instance of Response with the provided ReadableStream as the body, the status set to 200, and the Content-Type header set to \\'text/plain; charset=utf-8\\'. Additional headers and properties can be added using the init parameter\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/05-stream-to-response.mdx'), name='05-stream-to-response.mdx', displayName='05-stream-to-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamToResponse\\ndescription: Learn to use streamToResponse helper function in your application.\\n---\\n\\n# `streamToResponse`\\n\\n<Note type=\"warning\">\\n  `streamToResponse` has been removed in AI SDK 4.0. Use\\n  `pipeDataStreamToResponse` from\\n  [streamText](/docs/reference/ai-sdk-core/stream-text) instead.\\n</Note>\\n\\n`streamToResponse` pipes a data stream to a Node.js `ServerResponse` object and sets the status code and headers.\\n\\nThis is useful to create data stream responses in environments that use `ServerResponse` objects, such as Node.js HTTP servers.\\n\\nThe status code and headers can be configured using the `options` parameter.\\nBy default, the status code is set to 200 and the Content-Type header is set to `text/plain; charset=utf-8`.\\n\\n## Import\\n\\n<Snippet text={`import { streamToResponse } from \"ai\"`} prompt={false} />\\n\\n## Example\\n\\nYou can e.g. use `streamToResponse` to pipe a data stream to a Node.js HTTP server response:\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { StreamData, streamText, streamToResponse } from \\'ai\\';\\nimport { createServer } from \\'http\\';\\n\\ncreateServer(async (req, res) => {\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'What is the weather in San Francisco?\\',\\n  });\\n\\n  // use stream data\\n  const data = new StreamData();\\n\\n  data.append(\\'initialized call\\');\\n\\n  streamToResponse(\\n    result.toAIStream({\\n      onFinal() {\\n        data.append(\\'call completed\\');\\n        data.close();\\n      },\\n    }),\\n    res,\\n    {},\\n    data,\\n  );\\n}).listen(8080);\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream\\',\\n      description:\\n        \\'The Web Stream to pipe to the response. It can be the return value of OpenAIStream, HuggingFaceStream, AnthropicStream, or an AIStream instance.\\',\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'ServerResponse\\',\\n      description:\\n        \\'The Node.js ServerResponse object to pipe the stream to. This is usually the second argument of a Node.js HTTP request handler.\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'Options\\',\\n      description: \\'Configure the response\\',\\n      properties: [\\n        {\\n          type: \\'Options\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              description:\\n                \\'The status code to set on the response. Defaults to `200`.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              description:\\n                \"Additional headers to set on the response. Defaults to `{ \\'Content-Type\\': \\'text/plain; charset=utf-8\\' }`.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'data\\',\\n      type: \\'StreamData\\',\\n      description:\\n        \\'StreamData object for forwarding additional data to the client.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/07-openai-stream.mdx'), name='07-openai-stream.mdx', displayName='07-openai-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: OpenAIStream\\ndescription: Learn to use OpenAIStream helper function in your application.\\n---\\n\\n# `OpenAIStream`\\n\\n<Note type=\"warning\">OpenAIStream has been removed in AI SDK 4.0</Note>\\n\\n<Note type=\"warning\">\\n  OpenAIStream is part of the legacy OpenAI integration. It is not compatible\\n  with the AI SDK 3.1 functions. It is recommended to use the [AI SDK OpenAI\\n  Provider](/providers/ai-sdk-providers/openai) instead.\\n</Note>\\n\\nTransforms the response from OpenAI\\'s language models into a ReadableStream.\\n\\nNote: Prior to v4, the official OpenAI API SDK does not support the Edge Runtime and only works in serverless environments. The openai-edge package is based on fetch instead of axios (and thus works in the Edge Runtime) so we recommend using openai v4+ or openai-edge.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { OpenAIStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/08-anthropic-stream.mdx'), name='08-anthropic-stream.mdx', displayName='08-anthropic-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AnthropicStream\\ndescription: Learn to use AnthropicStream helper function in your application.\\n---\\n\\n# `AnthropicStream`\\n\\n<Note type=\"warning\">AnthropicStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  AnthropicStream is part of the legacy Anthropic integration. It is not\\n  compatible with the AI SDK 3.1 functions. It is recommended to use the [AI SDK\\n  Anthropic Provider](/providers/ai-sdk-providers/anthropic) instead.\\n</Note>\\n\\nIt is a utility function that transforms the output from Anthropic\\'s SDK into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Anthropic\\'s response data structure.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AnthropicStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/09-aws-bedrock-stream.mdx'), name='09-aws-bedrock-stream.mdx', displayName='09-aws-bedrock-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockStream\\ndescription: Learn to use AWSBedrockStream helper function in your application.\\n---\\n\\n# `AWSBedrockStream`\\n\\n<Note type=\"warning\">AWSBedrockStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockStream is part of the legacy AWS Bedrock integration. It is not\\n  compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AWSBedrockStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/10-aws-bedrock-anthropic-stream.mdx'), name='10-aws-bedrock-anthropic-stream.mdx', displayName='10-aws-bedrock-anthropic-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockAnthropicStream\\ndescription: Learn to use AWSBedrockAnthropicStream helper function in your application.\\n---\\n\\n# `AWSBedrockAnthropicStream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicStream is part of the legacy AWS Bedrock integration. It is\\n  not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet\\n  text={`import { AWSBedrockAnthropicStream } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/10-aws-bedrock-messages-stream.mdx'), name='10-aws-bedrock-messages-stream.mdx', displayName='10-aws-bedrock-messages-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockAnthropicMessagesStream\\ndescription: Learn to use AWSBedrockAnthropicMessagesStream helper function in your application.\\n---\\n\\n# `AWSBedrockAnthropicMessagesStream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicMessagesStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicMessagesStream is part of the legacy AWS Bedrock\\n  integration. It is not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet\\n  text={`import { AWSBedrockAnthropicMessagesStream } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/11-aws-bedrock-cohere-stream.mdx'), name='11-aws-bedrock-cohere-stream.mdx', displayName='11-aws-bedrock-cohere-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockCohereStream\\ndescription: Learn to use AWSBedrockCohereStream helper function in your application.\\n---\\n\\n# `AWSBedrockCohereStream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockCohereStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockCohereStream is part of the legacy AWS Bedrock integration. It is\\n  not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\n## Import\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handles parsing Bedrock\\'s response.\\n\\n### React\\n\\n<Snippet text={`import { AWSBedrockCohereStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/12-aws-bedrock-llama-2-stream.mdx'), name='12-aws-bedrock-llama-2-stream.mdx', displayName='12-aws-bedrock-llama-2-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockLlama2Stream\\ndescription: Learn to use AWSBedrockLlama2Stream helper function in your application.\\n---\\n\\n# `AWSBedrockLlama2Stream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockLlama2Stream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockLlama2Stream is part of the legacy AWS Bedrock integration. It is\\n  not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AWSBedrockLlama2Stream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/13-cohere-stream.mdx'), name='13-cohere-stream.mdx', displayName='13-cohere-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: CohereStream\\ndescription: Learn to use CohereStream helper function in your application.\\n---\\n\\n# `CohereStream`\\n\\n<Note type=\"warning\">CohereStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  CohereStream is part of the legacy Cohere integration. It is not compatible\\n  with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe CohereStream function is a utility that transforms the output from Cohere\\'s API into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Cohere\\'s response data structure. This works with the official Cohere API, and it\\'s supported in both Node.js, the Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { CohereStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/14-google-generative-ai-stream.mdx'), name='14-google-generative-ai-stream.mdx', displayName='14-google-generative-ai-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: GoogleGenerativeAIStream\\ndescription: Learn to use GoogleGenerativeAIStream helper function in your application.\\n---\\n\\n# `GoogleGenerativeAIStream`\\n\\n<Note type=\"warning\">\\n  GoogleGenerativeAIStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  GoogleGenerativeAIStream is part of the legacy Google Generative AI\\n  integration. It is not compatible with the AI SDK 3.1 functions. It is\\n  recommended to use the [AI SDK Google Generative AI\\n  Provider](/providers/ai-sdk-providers/google-generative-ai) instead.\\n</Note>\\n\\nThe GoogleGenerativeAIStream function is a utility that transforms the output from Google\\'s Generative AI SDK into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Google\\'s response data structure. This works with the official Generative AI SDK, and it\\'s supported in both Node.js, Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet\\n  text={`import { GoogleGenerativeAIStream } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'{ stream: AsyncIterable<GenerateContentResponse> }\\',\\n      description:\\n        \\'The response object returned by the Google Generative AI API.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/15-hugging-face-stream.mdx'), name='15-hugging-face-stream.mdx', displayName='15-hugging-face-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: HuggingFaceStream\\ndescription: Learn to use HuggingFaceStream helper function in your application.\\n---\\n\\n# `HuggingFaceStream`\\n\\n<Note type=\"warning\">HuggingFaceStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  HuggingFaceStream is part of the legacy Hugging Face integration. It is not\\n  compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nConverts the output from language models hosted on Hugging Face into a ReadableStream.\\n\\nWhile HuggingFaceStream is compatible with most Hugging Face language models, the rapidly evolving landscape of models may result in certain new or niche models not being supported. If you encounter a model that isn\\'t supported, we encourage you to open an issue.\\n\\nTo ensure that AI responses are comprised purely of text without any delimiters that could pose issues when rendering in chat or completion modes, we standardize and remove special end-of-response tokens. If your use case requires a different handling of responses, you can fork and modify this stream to meet your specific needs.\\n\\nCurrently, `</s>` and `<|endoftext|>` are recognized as end-of-stream tokens.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { HuggingFaceStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'iter\\',\\n      type: \\'AsyncGenerator<any>\\',\\n      description:\\n        \\'This parameter should be the generator function returned by the hf.textGenerationStream method in the Hugging Face Inference SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/16-langchain-adapter.mdx'), name='16-langchain-adapter.mdx', displayName='16-langchain-adapter.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: \\'@ai-sdk/langchain Adapter\\'\\ndescription: API Reference for the LangChain Adapter.\\n---\\n\\n# `@ai-sdk/langchain`\\n\\nThe `@ai-sdk/langchain` module provides helper functions to transform LangChain output streams into data streams and data stream responses.\\nSee the [LangChain Adapter documentation](/providers/adapters/langchain) for more information.\\n\\nIt supports:\\n\\n- LangChain StringOutputParser streams\\n- LangChain AIMessageChunk streams\\n- LangChain StreamEvents v2 streams\\n\\n## Import\\n\\n<Snippet\\n  text={`import { toDataStreamResponse } from \"@ai-sdk/langchain\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'toDataStream\\',\\n      type: \\'(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, AIStreamCallbacksAndOptions) => AIStream\\',\\n      description: \\'Converts LangChain output streams to data stream.\\',\\n    },\\n    {\\n      name: \\'toDataStreamResponse\\',\\n      type: \\'(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}) => Response\\',\\n      description: \\'Converts LangChain output streams to data stream response.\\',\\n    },\\n    {\\n      name: \\'mergeIntoDataStream\\',\\n      type: \\'(stream: ReadableStream<LangChainStreamEvent> | ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }) => void\\',\\n      description:\\n        \\'Merges LangChain output streams into an existing data stream.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n### Convert LangChain Expression Language Stream\\n\\n```tsx filename=\"app/api/completion/route.ts\" highlight={\"13\"}\\nimport { toUIMessageStream } from \\'@ai-sdk/langchain\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\nimport { createUIMessageStreamResponse } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const model = new ChatOpenAI({\\n    model: \\'gpt-3.5-turbo-0125\\',\\n    temperature: 0,\\n  });\\n\\n  const stream = await model.stream(prompt);\\n\\n  return createUIMessageStreamResponse({\\n    stream: toUIMessageStream(stream),\\n  });\\n}\\n```\\n\\n### Convert StringOutputParser Stream\\n\\n```tsx filename=\"app/api/completion/route.ts\" highlight={\"16\"}\\nimport { toUIMessageStream } from \\'@ai-sdk/langchain\\';\\nimport { StringOutputParser } from \\'@langchain/core/output_parsers\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\nimport { createUIMessageStreamResponse } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const model = new ChatOpenAI({\\n    model: \\'gpt-3.5-turbo-0125\\',\\n    temperature: 0,\\n  });\\n\\n  const parser = new StringOutputParser();\\n\\n  const stream = await model.pipe(parser).stream(prompt);\\n\\n  return createUIMessageStreamResponse({\\n    stream: toUIMessageStream(stream),\\n  });\\n}\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/16-llamaindex-adapter.mdx'), name='16-llamaindex-adapter.mdx', displayName='16-llamaindex-adapter.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: \\'@ai-sdk/llamaindex Adapter\\'\\ndescription: API Reference for the LlamaIndex Adapter.\\n---\\n\\n# `@ai-sdk/llamaindex`\\n\\nThe `@ai-sdk/llamaindex` package provides helper functions to transform LlamaIndex output streams into data streams and data stream responses.\\nSee the [LlamaIndex Adapter documentation](/providers/adapters/llamaindex) for more information.\\n\\nIt supports:\\n\\n- LlamaIndex ChatEngine streams\\n- LlamaIndex QueryEngine streams\\n\\n## Import\\n\\n<Snippet\\n  text={`import { toDataResponse } from \"@ai-sdk/llamaindex\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'toDataStream\\',\\n      type: \\'(stream: AsyncIterable<EngineResponse>, AIStreamCallbacksAndOptions) => AIStream\\',\\n      description: \\'Converts LlamaIndex output streams to data stream.\\',\\n    },\\n    {\\n      name: \\'toDataStreamResponse\\',\\n      type: \\'(stream: AsyncIterable<EngineResponse>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}) => Response\\',\\n      description:\\n        \\'Converts LlamaIndex output streams to data stream response.\\',\\n    },\\n    {\\n      name: \\'mergeIntoDataStream\\',\\n      type: \\'(stream: AsyncIterable<EngineResponse>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }) => void\\',\\n      description:\\n        \\'Merges LlamaIndex output streams into an existing data stream.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n### Convert LlamaIndex ChatEngine Stream\\n\\n```tsx filename=\"app/api/completion/route.ts\" highlight=\"15\"\\nimport { OpenAI, SimpleChatEngine } from \\'llamaindex\\';\\nimport { toDataStreamResponse } from \\'@ai-sdk/llamaindex\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const llm = new OpenAI({ model: \\'gpt-4o\\' });\\n  const chatEngine = new SimpleChatEngine({ llm });\\n\\n  const stream = await chatEngine.chat({\\n    message: prompt,\\n    stream: true,\\n  });\\n\\n  return toDataStreamResponse(stream);\\n}\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/17-mistral-stream.mdx'), name='17-mistral-stream.mdx', displayName='17-mistral-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: MistralStream\\ndescription: Learn to use MistralStream helper function in your application.\\n---\\n\\n# `MistralStream`\\n\\n<Note type=\"warning\">MistralStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  MistralStream is part of the legacy Mistral integration. It is not compatible\\n  with the AI SDK 3.1 functions. It is recommended to use the [AI SDK Mistral\\n  Provider](/providers/ai-sdk-providers/mistral) instead.\\n</Note>\\n\\nTransforms the output from Mistral\\'s language models into a ReadableStream.\\n\\nThis works with the official Mistral API, and it\\'s supported in both Node.js, the Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { MistralStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/18-replicate-stream.mdx'), name='18-replicate-stream.mdx', displayName='18-replicate-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: ReplicateStream\\ndescription: Learn to use ReplicateStream helper function in your application.\\n---\\n\\n# `ReplicateStream`\\n\\n<Note type=\"warning\">ReplicateStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  ReplicateStream is part of the legacy Replicate integration. It is not\\n  compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe ReplicateStream function is a utility that handles extracting the stream from the output of [Replicate](https://replicate.com)\\'s API. It expects a Prediction object as returned by the [Replicate JavaScript SDK](https://github.com/replicate/replicate-javascript), and returns a ReadableStream. Unlike other wrappers, ReplicateStream returns a Promise because it makes a fetch call to the [Replicate streaming API](https://github.com/replicate/replicate-javascript#streaming) under the hood.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { ReplicateStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'pre\\',\\n      type: \\'Prediction\\',\\n      description: \\'Object returned by the Replicate JavaScript SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'{ headers?: Record<string, string> }\\',\\n      isOptiona: true,\\n      description: \\'An optional parameter for passing additional headers.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream` wrapped in a promise.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/19-inkeep-stream.mdx'), name='19-inkeep-stream.mdx', displayName='19-inkeep-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: InkeepStream\\ndescription: Learn to use InkeepStream helper function in your application.\\n---\\n\\n# `InkeepStream`\\n\\n<Note type=\"warning\">InkeepStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  InkeepStream is part of the legacy Inkeep integration. It is not compatible\\n  with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe InkeepStream function is a utility that transforms the output from [Inkeep](https://inkeep.com)\\'s API into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Inkeep\\'s response data structure.\\n\\nThis works with the official Inkeep API, and it\\'s supported in both Node.js, the Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { InkeepStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stream Helpers\\ndescription: Learn to use help functions that help stream generations from different providers.\\ncollapsed: true\\n---\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: \\'AIStream\\',\\n      description: \\'Create a readable stream for AI responses.\\',\\n      href: \\'/docs/reference/stream-helpers/ai-stream\\',\\n    },\\n    {\\n      title: \\'StreamingTextResponse\\',\\n      description: \\'Create a streaming response for text generations.\\',\\n      href: \\'/docs/reference/stream-helpers/streaming-text-response\\',\\n    },\\n    {\\n      title: \\'streamtoResponse\\',\\n      description: \\'Pipe a ReadableStream to a Node.js ServerResponse object.\\',\\n      href: \\'/docs/reference/stream-helpers/stream-to-response\\',\\n    },\\n    {\\n      title: \\'OpenAIStream\\',\\n      description:\\n        \"Transforms the response from OpenAI\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/openai-stream\\',\\n    },\\n    {\\n      title: \\'AnthropicStream\\',\\n      description:\\n        \"Transforms the response from Anthropic\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/anthropic-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockStream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockMessagesStream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock Message\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-messages-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockCohereStream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock Cohere\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-cohere-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockLlama-2Stream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock Llama-2\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-llama-2-stream\\',\\n    },\\n    {\\n      title: \\'CohereStream\\',\\n      description:\\n        \"Transforms the response from Cohere\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/cohere-stream\\',\\n    },\\n    {\\n      title: \\'GoogleGenerativeAIStream\\',\\n      description:\\n        \"Transforms the response from Google\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/google-generative-ai-stream\\',\\n    },\\n    {\\n      title: \\'HuggingFaceStream\\',\\n      description:\\n        \"Transforms the response from Hugging Face\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/hugging-face-stream\\',\\n    },\\n    {\\n      title: \\'LangChainStream\\',\\n      description:\\n        \"Transforms the response from LangChain\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/langchain-adapter\\',\\n    },\\n    {\\n      title: \\'MistralStream\\',\\n      description:\\n        \"Transforms the response from Mistral\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/mistral-stream\\',\\n    },\\n    {\\n      title: \\'ReplicateStream\\',\\n      description:\\n        \"Transforms the response from Replicate\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/replicate-stream\\',\\n    },\\n    {\\n      title: \\'InkeepsStream\\',\\n      description:\\n        \"Transforms the response from Inkeeps\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/inkeep-stream\\',\\n    },\\n  ]}\\n/>\\n', children=[])]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors'), name='05-ai-sdk-errors', displayName='05-ai-sdk-errors', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-api-call-error.mdx'), name='ai-api-call-error.mdx', displayName='ai-api-call-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_APICallError\\ndescription: Learn how to fix AI_APICallError\\n---\\n\\n# AI_APICallError\\n\\nThis error occurs when an API call fails.\\n\\n## Properties\\n\\n- `url`: The URL of the API request that failed\\n- `requestBodyValues`: The request body values sent to the API\\n- `statusCode`: The HTTP status code returned by the API\\n- `responseHeaders`: The response headers returned by the API\\n- `responseBody`: The response body returned by the API\\n- `isRetryable`: Whether the request can be retried based on the status code\\n- `data`: Any additional data associated with the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_APICallError` using:\\n\\n```typescript\\nimport { APICallError } from 'ai';\\n\\nif (APICallError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-download-error.mdx'), name='ai-download-error.mdx', displayName='ai-download-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_DownloadError\\ndescription: Learn how to fix AI_DownloadError\\n---\\n\\n# AI_DownloadError\\n\\nThis error occurs when a download fails.\\n\\n## Properties\\n\\n- `url`: The URL that failed to download\\n- `statusCode`: The HTTP status code returned by the server\\n- `statusText`: The HTTP status text returned by the server\\n- `message`: The error message containing details about the download failure\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_DownloadError` using:\\n\\n```typescript\\nimport { DownloadError } from 'ai';\\n\\nif (DownloadError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-empty-response-body-error.mdx'), name='ai-empty-response-body-error.mdx', displayName='ai-empty-response-body-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_EmptyResponseBodyError\\ndescription: Learn how to fix AI_EmptyResponseBodyError\\n---\\n\\n# AI_EmptyResponseBodyError\\n\\nThis error occurs when the server returns an empty response body.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_EmptyResponseBodyError` using:\\n\\n```typescript\\nimport { EmptyResponseBodyError } from 'ai';\\n\\nif (EmptyResponseBodyError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-argument-error.mdx'), name='ai-invalid-argument-error.mdx', displayName='ai-invalid-argument-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidArgumentError\\ndescription: Learn how to fix AI_InvalidArgumentError\\n---\\n\\n# AI_InvalidArgumentError\\n\\nThis error occurs when an invalid argument was provided.\\n\\n## Properties\\n\\n- `parameter`: The name of the parameter that is invalid\\n- `value`: The invalid value\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidArgumentError` using:\\n\\n```typescript\\nimport { InvalidArgumentError } from 'ai';\\n\\nif (InvalidArgumentError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-data-content-error.mdx'), name='ai-invalid-data-content-error.mdx', displayName='ai-invalid-data-content-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidDataContentError\\ndescription: How to fix AI_InvalidDataContentError\\n---\\n\\n# AI_InvalidDataContentError\\n\\nThis error occurs when the data content provided in a multi-modal message part is invalid. Check out the [ prompt examples for multi-modal messages ](/docs/foundations/prompts#message-prompts).\\n\\n## Properties\\n\\n- `content`: The invalid content value\\n- `message`: The error message describing the expected and received content types\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidDataContentError` using:\\n\\n```typescript\\nimport { InvalidDataContentError } from 'ai';\\n\\nif (InvalidDataContentError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-data-content.mdx'), name='ai-invalid-data-content.mdx', displayName='ai-invalid-data-content.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidDataContent\\ndescription: Learn how to fix AI_InvalidDataContent\\n---\\n\\n# AI_InvalidDataContent\\n\\nThis error occurs when invalid data content is provided.\\n\\n## Properties\\n\\n- `content`: The invalid content value\\n- `message`: The error message\\n- `cause`: The cause of the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidDataContent` using:\\n\\n```typescript\\nimport { InvalidDataContent } from 'ai';\\n\\nif (InvalidDataContent.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-message-role-error.mdx'), name='ai-invalid-message-role-error.mdx', displayName='ai-invalid-message-role-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidMessageRoleError\\ndescription: Learn how to fix AI_InvalidMessageRoleError\\n---\\n\\n# AI_InvalidMessageRoleError\\n\\nThis error occurs when an invalid message role is provided.\\n\\n## Properties\\n\\n- `role`: The invalid role value\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidMessageRoleError` using:\\n\\n```typescript\\nimport { InvalidMessageRoleError } from 'ai';\\n\\nif (InvalidMessageRoleError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-prompt-error.mdx'), name='ai-invalid-prompt-error.mdx', displayName='ai-invalid-prompt-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidPromptError\\ndescription: Learn how to fix AI_InvalidPromptError\\n---\\n\\n# AI_InvalidPromptError\\n\\nThis error occurs when the prompt provided is invalid.\\n\\n## Potential Causes\\n\\n### UI Messages\\n\\nYou are passing a `UIMessage[]` as messages into e.g. `streamText`.\\n\\nYou need to first convert them to a `ModelMessage[]` using `convertToModelMessages()`.\\n\\n```typescript\\nimport { type UIMessage, generateText, convertToModelMessages } from 'ai';\\n\\nconst messages: UIMessage[] = [\\n  /* ... */\\n];\\n\\nconst result = await generateText({\\n  // ...\\n  messages: convertToModelMessages(messages),\\n});\\n```\\n\\n## Properties\\n\\n- `prompt`: The invalid prompt value\\n- `message`: The error message\\n- `cause`: The cause of the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidPromptError` using:\\n\\n```typescript\\nimport { InvalidPromptError } from 'ai';\\n\\nif (InvalidPromptError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-response-data-error.mdx'), name='ai-invalid-response-data-error.mdx', displayName='ai-invalid-response-data-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidResponseDataError\\ndescription: Learn how to fix AI_InvalidResponseDataError\\n---\\n\\n# AI_InvalidResponseDataError\\n\\nThis error occurs when the server returns a response with invalid data content.\\n\\n## Properties\\n\\n- `data`: The invalid response data value\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidResponseDataError` using:\\n\\n```typescript\\nimport { InvalidResponseDataError } from 'ai';\\n\\nif (InvalidResponseDataError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-tool-input-error.mdx'), name='ai-invalid-tool-input-error.mdx', displayName='ai-invalid-tool-input-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidToolInputError\\ndescription: Learn how to fix AI_InvalidToolInputError\\n---\\n\\n# AI_InvalidToolInputError\\n\\nThis error occurs when invalid tool input was provided.\\n\\n## Properties\\n\\n- `toolName`: The name of the tool with invalid inputs\\n- `toolInput`: The invalid tool inputs\\n- `message`: The error message\\n- `cause`: The cause of the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidToolInputError` using:\\n\\n```typescript\\nimport { InvalidToolInputError } from 'ai';\\n\\nif (InvalidToolInputError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-json-parse-error.mdx'), name='ai-json-parse-error.mdx', displayName='ai-json-parse-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_JSONParseError\\ndescription: Learn how to fix AI_JSONParseError\\n---\\n\\n# AI_JSONParseError\\n\\nThis error occurs when JSON fails to parse.\\n\\n## Properties\\n\\n- `text`: The text value that could not be parsed\\n- `message`: The error message including parse error details\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_JSONParseError` using:\\n\\n```typescript\\nimport { JSONParseError } from 'ai';\\n\\nif (JSONParseError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-load-api-key-error.mdx'), name='ai-load-api-key-error.mdx', displayName='ai-load-api-key-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_LoadAPIKeyError\\ndescription: Learn how to fix AI_LoadAPIKeyError\\n---\\n\\n# AI_LoadAPIKeyError\\n\\nThis error occurs when API key is not loaded successfully.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_LoadAPIKeyError` using:\\n\\n```typescript\\nimport { LoadAPIKeyError } from 'ai';\\n\\nif (LoadAPIKeyError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-load-setting-error.mdx'), name='ai-load-setting-error.mdx', displayName='ai-load-setting-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_LoadSettingError\\ndescription: Learn how to fix AI_LoadSettingError\\n---\\n\\n# AI_LoadSettingError\\n\\nThis error occurs when a setting is not loaded successfully.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_LoadSettingError` using:\\n\\n```typescript\\nimport { LoadSettingError } from 'ai';\\n\\nif (LoadSettingError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-message-conversion-error.mdx'), name='ai-message-conversion-error.mdx', displayName='ai-message-conversion-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_MessageConversionError\\ndescription: Learn how to fix AI_MessageConversionError\\n---\\n\\n# AI_MessageConversionError\\n\\nThis error occurs when message conversion fails.\\n\\n## Properties\\n\\n- `originalMessage`: The original message that failed conversion\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_MessageConversionError` using:\\n\\n```typescript\\nimport { MessageConversionError } from 'ai';\\n\\nif (MessageConversionError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-content-generated-error.mdx'), name='ai-no-content-generated-error.mdx', displayName='ai-no-content-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoContentGeneratedError\\ndescription: Learn how to fix AI_NoContentGeneratedError\\n---\\n\\n# AI_NoContentGeneratedError\\n\\nThis error occurs when the AI provider fails to generate content.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoContentGeneratedError` using:\\n\\n```typescript\\nimport { NoContentGeneratedError } from 'ai';\\n\\nif (NoContentGeneratedError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-image-generated-error.mdx'), name='ai-no-image-generated-error.mdx', displayName='ai-no-image-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoImageGeneratedError\\ndescription: Learn how to fix AI_NoImageGeneratedError\\n---\\n\\n# AI_NoImageGeneratedError\\n\\nThis error occurs when the AI provider fails to generate an image.\\nIt can arise due to the following reasons:\\n\\n- The model failed to generate a response.\\n- The model generated an invalid response.\\n\\n## Properties\\n\\n- `message`: The error message.\\n- `responses`: Metadata about the image model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling.\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoImageGeneratedError` using:\\n\\n```typescript\\nimport { generateImage, NoImageGeneratedError } from 'ai';\\n\\ntry {\\n  await generateImage({ model, prompt });\\n} catch (error) {\\n  if (NoImageGeneratedError.isInstance(error)) {\\n    console.log('NoImageGeneratedError');\\n    console.log('Cause:', error.cause);\\n    console.log('Responses:', error.responses);\\n  }\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-object-generated-error.mdx'), name='ai-no-object-generated-error.mdx', displayName='ai-no-object-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoObjectGeneratedError\\ndescription: Learn how to fix AI_NoObjectGeneratedError\\n---\\n\\n# AI_NoObjectGeneratedError\\n\\nThis error occurs when the AI provider fails to generate a parsable object that conforms to the schema.\\nIt can arise due to the following reasons:\\n\\n- The model failed to generate a response.\\n- The model generated a response that could not be parsed.\\n- The model generated a response that could not be validated against the schema.\\n\\n## Properties\\n\\n- `message`: The error message.\\n- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.\\n- `response`: Metadata about the language model response, including response id, timestamp, and model.\\n- `usage`: Request token usage.\\n- `finishReason`: Request finish reason. For example 'length' if model generated maximum number of tokens, this could result in a JSON parsing error.\\n- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoObjectGeneratedError` using:\\n\\n```typescript\\nimport { generateObject, NoObjectGeneratedError } from 'ai';\\n\\ntry {\\n  await generateObject({ model, schema, prompt });\\n} catch (error) {\\n  if (NoObjectGeneratedError.isInstance(error)) {\\n    console.log('NoObjectGeneratedError');\\n    console.log('Cause:', error.cause);\\n    console.log('Text:', error.text);\\n    console.log('Response:', error.response);\\n    console.log('Usage:', error.usage);\\n    console.log('Finish Reason:', error.finishReason);\\n  }\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-speech-generated-error.mdx'), name='ai-no-speech-generated-error.mdx', displayName='ai-no-speech-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSpeechGeneratedError\\ndescription: Learn how to fix AI_NoSpeechGeneratedError\\n---\\n\\n# AI_NoSpeechGeneratedError\\n\\nThis error occurs when no audio could be generated from the input.\\n\\n## Properties\\n\\n- `responses`: Array of responses\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSpeechGeneratedError` using:\\n\\n```typescript\\nimport { NoSpeechGeneratedError } from 'ai';\\n\\nif (NoSpeechGeneratedError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-such-model-error.mdx'), name='ai-no-such-model-error.mdx', displayName='ai-no-such-model-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSuchModelError\\ndescription: Learn how to fix AI_NoSuchModelError\\n---\\n\\n# AI_NoSuchModelError\\n\\nThis error occurs when a model ID is not found.\\n\\n## Properties\\n\\n- `modelId`: The ID of the model that was not found\\n- `modelType`: The type of model\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSuchModelError` using:\\n\\n```typescript\\nimport { NoSuchModelError } from 'ai';\\n\\nif (NoSuchModelError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-such-provider-error.mdx'), name='ai-no-such-provider-error.mdx', displayName='ai-no-such-provider-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSuchProviderError\\ndescription: Learn how to fix AI_NoSuchProviderError\\n---\\n\\n# AI_NoSuchProviderError\\n\\nThis error occurs when a provider ID is not found.\\n\\n## Properties\\n\\n- `providerId`: The ID of the provider that was not found\\n- `availableProviders`: Array of available provider IDs\\n- `modelId`: The ID of the model\\n- `modelType`: The type of model\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSuchProviderError` using:\\n\\n```typescript\\nimport { NoSuchProviderError } from 'ai';\\n\\nif (NoSuchProviderError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-such-tool-error.mdx'), name='ai-no-such-tool-error.mdx', displayName='ai-no-such-tool-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSuchToolError\\ndescription: Learn how to fix AI_NoSuchToolError\\n---\\n\\n# AI_NoSuchToolError\\n\\nThis error occurs when a model tries to call an unavailable tool.\\n\\n## Properties\\n\\n- `toolName`: The name of the tool that was not found\\n- `availableTools`: Array of available tool names\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSuchToolError` using:\\n\\n```typescript\\nimport { NoSuchToolError } from 'ai';\\n\\nif (NoSuchToolError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-transcript-generated-error.mdx'), name='ai-no-transcript-generated-error.mdx', displayName='ai-no-transcript-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoTranscriptGeneratedError\\ndescription: Learn how to fix AI_NoTranscriptGeneratedError\\n---\\n\\n# AI_NoTranscriptGeneratedError\\n\\nThis error occurs when no transcript could be generated from the input.\\n\\n## Properties\\n\\n- `responses`: Array of responses\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoTranscriptGeneratedError` using:\\n\\n```typescript\\nimport { NoTranscriptGeneratedError } from 'ai';\\n\\nif (NoTranscriptGeneratedError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-retry-error.mdx'), name='ai-retry-error.mdx', displayName='ai-retry-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_RetryError\\ndescription: Learn how to fix AI_RetryError\\n---\\n\\n# AI_RetryError\\n\\nThis error occurs when a retry operation fails.\\n\\n## Properties\\n\\n- `reason`: The reason for the retry failure\\n- `lastError`: The most recent error that occurred during retries\\n- `errors`: Array of all errors that occurred during retry attempts\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_RetryError` using:\\n\\n```typescript\\nimport { RetryError } from 'ai';\\n\\nif (RetryError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-too-many-embedding-values-for-call-error.mdx'), name='ai-too-many-embedding-values-for-call-error.mdx', displayName='ai-too-many-embedding-values-for-call-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_TooManyEmbeddingValuesForCallError\\ndescription: Learn how to fix AI_TooManyEmbeddingValuesForCallError\\n---\\n\\n# AI_TooManyEmbeddingValuesForCallError\\n\\nThis error occurs when too many values are provided in a single embedding call.\\n\\n## Properties\\n\\n- `provider`: The AI provider name\\n- `modelId`: The ID of the embedding model\\n- `maxEmbeddingsPerCall`: The maximum number of embeddings allowed per call\\n- `values`: The array of values that was provided\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_TooManyEmbeddingValuesForCallError` using:\\n\\n```typescript\\nimport { TooManyEmbeddingValuesForCallError } from 'ai';\\n\\nif (TooManyEmbeddingValuesForCallError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-tool-call-repair-error.mdx'), name='ai-tool-call-repair-error.mdx', displayName='ai-tool-call-repair-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: ToolCallRepairError\\ndescription: Learn how to fix AI SDK ToolCallRepairError\\n---\\n\\n# ToolCallRepairError\\n\\nThis error occurs when there is a failure while attempting to repair an invalid tool call.\\nThis typically happens when the AI attempts to fix either\\na `NoSuchToolError` or `InvalidToolInputError`.\\n\\n## Properties\\n\\n- `originalError`: The original error that triggered the repair attempt (either `NoSuchToolError` or `InvalidToolInputError`)\\n- `message`: The error message\\n- `cause`: The underlying error that caused the repair to fail\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `ToolCallRepairError` using:\\n\\n```typescript\\nimport { ToolCallRepairError } from 'ai';\\n\\nif (ToolCallRepairError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-type-validation-error.mdx'), name='ai-type-validation-error.mdx', displayName='ai-type-validation-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_TypeValidationError\\ndescription: Learn how to fix AI_TypeValidationError\\n---\\n\\n# AI_TypeValidationError\\n\\nThis error occurs when type validation fails.\\n\\n## Properties\\n\\n- `value`: The value that failed validation\\n- `message`: The error message including validation details\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_TypeValidationError` using:\\n\\n```typescript\\nimport { TypeValidationError } from 'ai';\\n\\nif (TypeValidationError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-unsupported-functionality-error.mdx'), name='ai-unsupported-functionality-error.mdx', displayName='ai-unsupported-functionality-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_UnsupportedFunctionalityError\\ndescription: Learn how to fix AI_UnsupportedFunctionalityError\\n---\\n\\n# AI_UnsupportedFunctionalityError\\n\\nThis error occurs when functionality is not unsupported.\\n\\n## Properties\\n\\n- `functionality`: The name of the unsupported functionality\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_UnsupportedFunctionalityError` using:\\n\\n```typescript\\nimport { UnsupportedFunctionalityError } from 'ai';\\n\\nif (UnsupportedFunctionalityError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK Errors\\ndescription: Troubleshooting information for common AI SDK errors.\\ncollapsed: true\\n---\\n\\n# AI SDK Errors\\n\\n- [AI_APICallError](/docs/reference/ai-sdk-errors/ai-api-call-error)\\n- [AI_DownloadError](/docs/reference/ai-sdk-errors/ai-download-error)\\n- [AI_EmptyResponseBodyError](/docs/reference/ai-sdk-errors/ai-empty-response-body-error)\\n- [AI_InvalidArgumentError](/docs/reference/ai-sdk-errors/ai-invalid-argument-error)\\n- [AI_InvalidDataContent](/docs/reference/ai-sdk-errors/ai-invalid-data-content)\\n- [AI_InvalidDataContentError](/docs/reference/ai-sdk-errors/ai-invalid-data-content-error)\\n- [AI_InvalidMessageRoleError](/docs/reference/ai-sdk-errors/ai-invalid-message-role-error)\\n- [AI_InvalidPromptError](/docs/reference/ai-sdk-errors/ai-invalid-prompt-error)\\n- [AI_InvalidResponseDataError](/docs/reference/ai-sdk-errors/ai-invalid-response-data-error)\\n- [AI_InvalidToolInputError](/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error)\\n- [AI_JSONParseError](/docs/reference/ai-sdk-errors/ai-json-parse-error)\\n- [AI_LoadAPIKeyError](/docs/reference/ai-sdk-errors/ai-load-api-key-error)\\n- [AI_LoadSettingError](/docs/reference/ai-sdk-errors/ai-load-setting-error)\\n- [AI_MessageConversionError](/docs/reference/ai-sdk-errors/ai-message-conversion-error)\\n- [AI_NoSpeechGeneratedError](/docs/reference/ai-sdk-errors/ai-no-speech-generated-error)\\n- [AI_NoContentGeneratedError](/docs/reference/ai-sdk-errors/ai-no-content-generated-error)\\n- [AI_NoImageGeneratedError](/docs/reference/ai-sdk-errors/ai-no-image-generated-error)\\n- [AI_NoTranscriptGeneratedError](/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error)\\n- [AI_NoObjectGeneratedError](/docs/reference/ai-sdk-errors/ai-no-object-generated-error)\\n- [AI_NoOutputSpecifiedError](/docs/reference/ai-sdk-errors/ai-no-output-specified-error)\\n- [AI_NoSuchModelError](/docs/reference/ai-sdk-errors/ai-no-such-model-error)\\n- [AI_NoSuchProviderError](/docs/reference/ai-sdk-errors/ai-no-such-provider-error)\\n- [AI_NoSuchToolError](/docs/reference/ai-sdk-errors/ai-no-such-tool-error)\\n- [AI_RetryError](/docs/reference/ai-sdk-errors/ai-retry-error)\\n- [AI_ToolCallRepairError](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error)\\n- [AI_TooManyEmbeddingValuesForCallError](/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error)\\n- [AI_TypeValidationError](/docs/reference/ai-sdk-errors/ai-type-validation-error)\\n- [AI_UnsupportedFunctionalityError](/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error)\\n', children=[])]), DocItem(origPath=Path('07-reference/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Reference\\ndescription: Reference documentation for the AI SDK\\n---\\n\\n# API Reference\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'AI SDK Core',\\n      description: 'Switch between model providers without changing your code.',\\n      href: '/docs/reference/ai-sdk-core',\\n    },\\n    {\\n      title: 'AI SDK RSC',\\n      description:\\n        'Use React Server Components to stream user interfaces to the client.',\\n      href: '/docs/reference/ai-sdk-rsc',\\n    },\\n    {\\n      title: 'AI SDK UI',\\n      description:\\n        'Use hooks to integrate user interfaces that interact with language models.',\\n      href: '/docs/reference/ai-sdk-ui',\\n    },\\n    {\\n      title: 'Stream Helpers',\\n      description:\\n        'Use special functions that help stream model generations from various providers.',\\n      href: '/docs/reference/stream-helpers',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('09-troubleshooting'), name='09-troubleshooting', displayName='09-troubleshooting', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('09-troubleshooting/01-azure-stream-slow.mdx'), name='01-azure-stream-slow.mdx', displayName='01-azure-stream-slow.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Azure OpenAI Slow to Stream\\ndescription: Learn to troubleshoot Azure OpenAI slow to stream issues.\\n---\\n\\n# Azure OpenAI Slow To Stream\\n\\n## Issue\\n\\nWhen using OpenAI hosted on Azure, streaming is slow and in big chunks.\\n\\n## Cause\\n\\nThis is a Microsoft Azure issue. Some users have reported the following solutions:\\n\\n- **Update Content Filtering Settings**:\\n  Inside [Azure AI Studio](https://ai.azure.com/), within \"Shared resources\" > \"Content filters\", create a new\\n  content filter and set the \"Streaming mode (Preview)\" under \"Output filter\" from \"Default\"\\n  to \"Asynchronous Filter\".\\n\\n## Solution\\n\\nYou can use the [`smoothStream` transformation](/docs/ai-sdk-core/generating-text#smoothing-streams) to stream each word individually.\\n\\n```tsx highlight=\"6\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: smoothStream(),\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/02-client-side-function-calls-not-invoked.mdx'), name='02-client-side-function-calls-not-invoked.mdx', displayName='02-client-side-function-calls-not-invoked.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Client-Side Function Calls Not Invoked\\ndescription: Troubleshooting client-side function calls not being invoked.\\n---\\n\\n# Client-Side Function Calls Not Invoked\\n\\n## Issue\\n\\nI upgraded the AI SDK to v3.0.20 or newer. I am using [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream). Client-side function calls are no longer invoked.\\n\\n## Solution\\n\\nYou will need to add a stub for `experimental_onFunctionCall` to [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream) to enable the correct forwarding of the function calls to the client.\\n\\n```tsx\\nconst stream = OpenAIStream(response, {\\n  async experimental_onFunctionCall() {\\n    return;\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/03-server-actions-in-client-components.mdx'), name='03-server-actions-in-client-components.mdx', displayName='03-server-actions-in-client-components.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Server Actions in Client Components\\ndescription: Troubleshooting errors related to server actions in client components.\\n---\\n\\n# Server Actions in Client Components\\n\\nYou may use Server Actions in client components, but sometimes you may encounter the following issues.\\n\\n## Issue\\n\\nIt is not allowed to define inline `\"use server\"` annotated Server Actions in Client Components.\\n\\n## Solution\\n\\nTo use Server Actions in a Client Component, you can either:\\n\\n- Export them from a separate file with `\"use server\"` at the top.\\n- Pass them down through props from a Server Component.\\n- Implement a combination of [`createAI`](/docs/reference/ai-sdk-rsc/create-ai) and [`useActions`](/docs/reference/ai-sdk-rsc/use-actions) hooks to access them.\\n\\nLearn more about [Server Actions and Mutations](https://nextjs.org/docs/app/api-reference/functions/server-actions#with-client-components).\\n\\n```ts file=\\'actions.ts\\'\\n\\'use server\\';\\n\\nimport { generateText } from \\'ai\\';\\n\\nexport async function getAnswer(question: string) {\\n  \\'use server\\';\\n\\n  const { text } = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: question,\\n  });\\n\\n  return { answer: text };\\n}\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/04-strange-stream-output.mdx'), name='04-strange-stream-output.mdx', displayName='04-strange-stream-output.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat/useCompletion stream output contains 0:... instead of text\\ndescription: How to fix strange stream output in the UI\\n---\\n\\n# useChat/useCompletion stream output contains 0:... instead of text\\n\\n## Issue\\n\\nI am using custom client code to process a server response that is sent using [`StreamingTextResponse`](/docs/reference/stream-helpers/streaming-text-response). I am using version `3.0.20` or newer of the AI SDK. When I send a query, the UI streams text such as `0: \"Je\"`, `0: \" suis\"`, `0: \"des\"...` instead of the text that I’m looking for.\\n\\n## Background\\n\\nThe AI SDK has switched to the stream data protocol in version `3.0.20`. It sends different stream parts to support data, tool calls, etc. What you see is the raw stream data protocol response.\\n\\n## Solution\\n\\nYou have several options:\\n\\n1. Use the AI Core [`streamText`](/docs/reference/ai-sdk-core/stream-text) function to send a raw text stream:\\n\\n   ```tsx\\n   export async function POST(req: Request) {\\n     const { prompt } = await req.json();\\n\\n     const result = streamText({\\n       model: openai.completion(\\'gpt-3.5-turbo-instruct\\'),\\n       maxOutputTokens: 2000,\\n       prompt,\\n     });\\n\\n     return result.toTextStreamResponse();\\n   }\\n   ```\\n\\n2. Pin the AI SDK version to `3.0.19` . This will keep the raw text stream.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/05-streamable-ui-errors.mdx'), name='05-streamable-ui-errors.mdx', displayName='05-streamable-ui-errors.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streamable UI Errors\\ndescription: Troubleshooting errors related to streamable UI.\\n---\\n\\n# Streamable UI Component Error\\n\\n## Issue\\n\\n- Variable Not Found\\n- Cannot find `div`\\n- `Component` refers to a value, but is being used as a type\\n\\n## Solution\\n\\nIf you encounter these errors when working with streamable UIs within server actions, it is likely because the file ends in `.ts` instead of `.tsx`.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/05-tool-invocation-missing-result.mdx'), name='05-tool-invocation-missing-result.mdx', displayName='05-tool-invocation-missing-result.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Tool Invocation Missing Result Error\\ndescription: How to fix the \"ToolInvocation must have a result\" error when using tools without execute functions\\n---\\n\\n# Tool Invocation Missing Result Error\\n\\n## Issue\\n\\nWhen using `generateText()` or `streamText()`, you may encounter the error \"ToolInvocation must have a result\" when a tool without an `execute` function is called.\\n\\n## Cause\\n\\nThe error occurs when you define a tool without an `execute` function and don\\'t provide the result through other means (like `useChat`\\'s `onToolCall` or `addToolOutput` functions).\\n\\nEach time a tool is invoked, the model expects to receive a result before continuing the conversation. Without a result, the model cannot determine if the tool call succeeded or failed and the conversation state becomes invalid.\\n\\n## Solution\\n\\nYou have two options for handling tool results:\\n\\n1. Server-side execution using tools with an `execute` function:\\n\\n```tsx\\nconst tools = {\\n  weather: tool({\\n    description: \\'Get the weather in a location\\',\\n    parameters: z.object({\\n      location: z\\n        .string()\\n        .describe(\\'The city and state, e.g. \"San Francisco, CA\"\\'),\\n    }),\\n    execute: async ({ location }) => {\\n      // Fetch and return weather data\\n      return { temperature: 72, conditions: \\'sunny\\', location };\\n    },\\n  }),\\n};\\n```\\n\\n2. Client-side execution with `useChat` (omitting the `execute` function), you must provide results using `addToolOutput`:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport {\\n  DefaultChatTransport,\\n  lastAssistantMessageIsCompleteWithToolCalls,\\n} from \\'ai\\';\\n\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  // Automatically submit when all tool results are available\\n  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n\\n  // Handle tool calls in onToolCall\\n  onToolCall: async ({ toolCall }) => {\\n    if (toolCall.toolName === \\'getLocation\\') {\\n      try {\\n        const result = await getLocationData();\\n\\n        // Important: Don\\'t await inside onToolCall to avoid deadlocks\\n        addToolOutput({\\n          tool: \\'getLocation\\',\\n          toolCallId: toolCall.toolCallId,\\n          output: result,\\n        });\\n      } catch (err) {\\n        // Important: Don\\'t await inside onToolCall to avoid deadlocks\\n        addToolOutput({\\n          tool: \\'getLocation\\',\\n          toolCallId: toolCall.toolCallId,\\n          state: \\'output-error\\',\\n          errorText: \\'Failed to get location\\',\\n        });\\n      }\\n    }\\n  },\\n});\\n```\\n\\n```tsx\\n// For interactive UI elements:\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  transport: new DefaultChatTransport({ api: \\'/api/chat\\' }),\\n  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n});\\n\\n// Inside your JSX, when rendering tool calls:\\n<button\\n  onClick={() =>\\n    addToolOutput({\\n      tool: \\'myTool\\',\\n      toolCallId, // must provide tool call ID\\n      output: {\\n        /* your tool result */\\n      },\\n    })\\n  }\\n>\\n  Confirm\\n</button>;\\n```\\n\\n<Note type=\"warning\">\\n  Whether handling tools on the server or client, each tool call must have a\\n  corresponding result before the conversation can continue.\\n</Note>\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/06-streaming-not-working-when-deployed.mdx'), name='06-streaming-not-working-when-deployed.mdx', displayName='06-streaming-not-working-when-deployed.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Streaming Not Working When Deployed\\ndescription: Troubleshooting streaming issues in deployed apps.\\n---\\n\\n# Streaming Not Working When Deployed\\n\\n## Issue\\n\\nStreaming with the AI SDK works in my local development environment.\\nHowever, when deploying, streaming does not work in the deployed app.\\nInstead of streaming, only the full response is returned after a while.\\n\\n## Cause\\n\\nThe causes of this issue are varied and depend on the deployment environment.\\n\\n## Solution\\n\\nYou can try the following:\\n\\n- add `'Transfer-Encoding': 'chunked'` and/or `Connection: 'keep-alive'` headers\\n\\n  ```tsx\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      'Transfer-Encoding': 'chunked',\\n      Connection: 'keep-alive',\\n    },\\n  });\\n  ```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/06-streaming-not-working-when-proxied.mdx'), name='06-streaming-not-working-when-proxied.mdx', displayName='06-streaming-not-working-when-proxied.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Streaming Not Working When Proxied\\ndescription: Troubleshooting streaming issues in proxied apps.\\n---\\n\\n# Streaming Not Working When Proxied\\n\\n## Issue\\n\\nStreaming with the AI SDK doesn't work in local development environment, or deployed in some proxy environments.\\nInstead of streaming, only the full response is returned after a while.\\n\\n## Cause\\n\\nThe causes of this issue are caused by the proxy middleware.\\n\\nIf the middleware is configured to compress the response, it will cause the streaming to fail.\\n\\n## Solution\\n\\nYou can try the following, the solution only affects the streaming API:\\n\\n- add `'Content-Encoding': 'none'` headers\\n\\n  ```tsx\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      'Content-Encoding': 'none',\\n    },\\n  });\\n  ```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/06-timeout-on-vercel.mdx'), name='06-timeout-on-vercel.mdx', displayName='06-timeout-on-vercel.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Getting Timeouts When Deploying on Vercel\\ndescription: Learn how to fix timeouts and cut off responses when deploying to Vercel.\\n---\\n\\n# Getting Timeouts When Deploying on Vercel\\n\\n## Issue\\n\\nStreaming with the AI SDK works in my local development environment.\\nHowever, when I\\'m deploying to Vercel, longer responses get chopped off in the UI and I\\'m seeing timeouts in the Vercel logs or I\\'m seeing the error: `Uncaught (in promise) Error: Connection closed`.\\n\\n## Solution\\n\\nWith Vercel\\'s [Fluid Compute](https://vercel.com/docs/fluid-compute), the default function duration is now **5 minutes (300 seconds)** across all plans. This should be sufficient for most streaming applications.\\n\\nIf you need to extend the timeout for longer-running processes, you can increase the `maxDuration` setting:\\n\\n### Next.js (App Router)\\n\\nAdd the following to your route file or the page you are calling your Server Action from:\\n\\n```tsx\\nexport const maxDuration = 600;\\n```\\n\\n<Note>\\n  Setting `maxDuration` above 300 seconds requires a Pro or Enterprise plan.\\n</Note>\\n\\n### Other Frameworks\\n\\nFor other frameworks, you can set timeouts in your `vercel.json` file:\\n\\n```json\\n{\\n  \"functions\": {\\n    \"api/chat/route.ts\": {\\n      \"maxDuration\": 600\\n    }\\n  }\\n}\\n```\\n\\n<Note>\\n  Setting `maxDuration` above 300 seconds requires a Pro or Enterprise plan.\\n</Note>\\n\\n### Maximum Duration Limits\\n\\nThe maximum duration you can set depends on your Vercel plan:\\n\\n- **Hobby**: Up to 300 seconds (5 minutes)\\n- **Pro**: Up to 800 seconds (~13 minutes)\\n- **Enterprise**: Up to 800 seconds (~13 minutes)\\n\\n## Learn more\\n\\n- [Fluid Compute Default Settings](https://vercel.com/docs/fluid-compute#default-settings-by-plan)\\n- [Configuring Maximum Duration for Vercel Functions](https://vercel.com/docs/functions/configuring-functions/duration)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/07-unclosed-streams.mdx'), name='07-unclosed-streams.mdx', displayName='07-unclosed-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Unclosed Streams\\ndescription: Troubleshooting errors related to unclosed streams.\\n---\\n\\n# Unclosed Streams\\n\\nSometimes streams are not closed properly, which can lead to unexpected behavior. The following are some common issues that can occur when streams are not closed properly.\\n\\n## Issue\\n\\nThe streamable UI has been slow to update.\\n\\n## Solution\\n\\nThis happens when you create a streamable UI using [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) and fail to close the stream.\\nIn order to fix this, you must ensure you close the stream by calling the [`.done()`](/docs/reference/ai-sdk-rsc/create-streamable-ui#done) method.\\nThis will ensure the stream is closed.\\n\\n```tsx file='app/actions.tsx'\\nimport { createStreamableUI } from '@ai-sdk/rsc';\\n\\nconst submitMessage = async () => {\\n  'use server';\\n\\n  const stream = createStreamableUI('1');\\n\\n  stream.update('2');\\n  stream.append('3');\\n  stream.done('4'); // [!code ++]\\n\\n  return stream.value;\\n};\\n```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/08-use-chat-failed-to-parse-stream.mdx'), name='08-use-chat-failed-to-parse-stream.mdx', displayName='08-use-chat-failed-to-parse-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat Failed to Parse Stream\\ndescription: Troubleshooting errors related to the Use Chat Failed to Parse Stream error.\\n---\\n\\n# `useChat` \"Failed to Parse Stream String\" Error\\n\\n## Issue\\n\\nI am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) or [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and I am getting a `\"Failed to parse stream string. Invalid code\"` error. I am using version `3.0.20` or newer of the AI SDK.\\n\\n## Background\\n\\nThe AI SDK has switched to the stream data protocol in version `3.0.20`.\\n[`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) expect stream parts that support data, tool calls, etc.\\nWhat you see is a failure to parse the stream.\\nThis can be caused by using an older version of the AI SDK in the backend, by providing a text stream using a custom provider, or by using a raw LangChain stream result.\\n\\n## Solution\\n\\nYou can switch [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) to raw text stream processing with the [`streamProtocol`](/docs/reference/ai-sdk-ui/use-completion#stream-protocol) parameter.\\nSet it to `text` as follows:\\n\\n```tsx\\nconst { messages, append } = useChat({ streamProtocol: \\'text\\' });\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/09-client-stream-error.mdx'), name='09-client-stream-error.mdx', displayName='09-client-stream-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Server Action Plain Objects Error\\ndescription: Troubleshooting errors related to using AI SDK Core functions with Server Actions.\\n---\\n\\n# \"Only plain objects can be passed from client components\" Server Action Error\\n\\n## Issue\\n\\nI am using [`streamText`](/docs/reference/ai-sdk-core/stream-text) or [`streamObject`](/docs/reference/ai-sdk-core/stream-object) with Server Actions, and I am getting a `\"only plain objects and a few built ins can be passed from client components\"` error.\\n\\n## Background\\n\\nThis error occurs when you\\'re trying to return a non-serializable object from a Server Action to a Client Component. The streamText function likely returns an object with methods or complex structures that can\\'t be directly serialized and passed to the client.\\n\\n## Solution\\n\\nTo fix this issue, you need to ensure that you\\'re only returning serializable data from your Server Action. Here\\'s how you can modify your approach:\\n\\n1. Instead of returning the entire result object from streamText, extract only the necessary serializable data.\\n2. Use the [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) function to create a streamable value that can be safely passed to the client.\\n\\nHere\\'s an example that demonstrates how to implement this solution: [Streaming Text Generation](/examples/next-app/basics/streaming-text-generation).\\n\\nThis approach ensures that only serializable data (the text) is passed to the client, avoiding the \"only plain objects\" error.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/10-use-chat-tools-no-response.mdx'), name='10-use-chat-tools-no-response.mdx', displayName='10-use-chat-tools-no-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat No Response\\ndescription: Troubleshooting errors related to the Use Chat Failed to Parse Stream error.\\n---\\n\\n# `useChat` No Response\\n\\n## Issue\\n\\nI am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\nWhen I log the incoming messages on the server, I can see the tool call and the tool result, but the model does not respond with anything.\\n\\n## Solution\\n\\nTo resolve this issue, convert the incoming messages to the `ModelMessage` format using the [`convertToModelMessages`](/docs/reference/ai-sdk-ui/convert-to-model-messages) function.\\n\\n```tsx highlight=\"9\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/11-use-chat-custom-request-options.mdx'), name='11-use-chat-custom-request-options.mdx', displayName='11-use-chat-custom-request-options.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Custom headers, body, and credentials not working with useChat\\ndescription: Troubleshooting errors related to custom request configuration in useChat hook\\n---\\n\\n# Custom headers, body, and credentials not working with useChat\\n\\n## Issue\\n\\nWhen using the `useChat` hook, custom request options like headers, body fields, and credentials configured directly on the hook are not being sent with the request:\\n\\n```tsx\\n// These options are not sent with the request\\nconst { messages, sendMessage } = useChat({\\n  headers: {\\n    Authorization: 'Bearer token123',\\n  },\\n  body: {\\n    user_id: '123',\\n  },\\n  credentials: 'include',\\n});\\n```\\n\\n## Background\\n\\nThe `useChat` hook has changed its API for configuring request options. Direct options like `headers`, `body`, and `credentials` on the hook itself are no longer supported. Instead, you need to use the `transport` configuration with `DefaultChatTransport` or pass options at the request level.\\n\\n## Solution\\n\\nThere are three ways to properly configure request options with `useChat`:\\n\\n### Option 1: Request-Level Configuration (Recommended for Dynamic Values)\\n\\nFor dynamic values that change over time, the recommended approach is to pass options when calling `sendMessage`:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat();\\n\\n// Send options with each message\\nsendMessage(\\n  { text: input },\\n  {\\n    headers: {\\n      Authorization: `Bearer ${getAuthToken()}`, // Dynamic auth token\\n      'X-Request-ID': generateRequestId(),\\n    },\\n    body: {\\n      temperature: 0.7,\\n      max_tokens: 100,\\n      user_id: getCurrentUserId(), // Dynamic user ID\\n      sessionId: getCurrentSessionId(), // Dynamic session\\n    },\\n  },\\n);\\n```\\n\\nThis approach ensures that the most up-to-date values are always sent with each request.\\n\\n### Option 2: Hook-Level Configuration with Static Values\\n\\nFor static values that don't change during the component lifecycle, use the `DefaultChatTransport`:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: {\\n      'X-API-Version': 'v1', // Static API version\\n      'X-App-ID': 'my-app', // Static app identifier\\n    },\\n    body: {\\n      model: 'gpt-5.1', // Default model\\n      stream: true, // Static configuration\\n    },\\n    credentials: 'include', // Static credentials policy\\n  }),\\n});\\n```\\n\\n### Option 3: Hook-Level Configuration with Resolvable Functions\\n\\nIf you need dynamic values at the hook level, you can use functions that return configuration values. However, request-level configuration is generally preferred for better reliability:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: () => ({\\n      Authorization: `Bearer ${getAuthToken()}`,\\n      'X-User-ID': getCurrentUserId(),\\n    }),\\n    body: () => ({\\n      sessionId: getCurrentSessionId(),\\n      preferences: getUserPreferences(),\\n    }),\\n    credentials: () => (isAuthenticated() ? 'include' : 'same-origin'),\\n  }),\\n});\\n```\\n\\n<Note>\\n  For component state that changes over time, request-level configuration\\n  (Option 1) is recommended. If using hook-level functions, consider using\\n  `useRef` to store current values and reference `ref.current` in your\\n  configuration function.\\n</Note>\\n\\n### Combining Hook and Request Level Options\\n\\nRequest-level options take precedence over hook-level options:\\n\\n```tsx\\n// Hook-level default configuration\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: {\\n      'X-API-Version': 'v1',\\n    },\\n    body: {\\n      model: 'gpt-5.1',\\n    },\\n  }),\\n});\\n\\n// Override or add options per request\\nsendMessage(\\n  { text: input },\\n  {\\n    headers: {\\n      'X-API-Version': 'v2', // This overrides the hook-level header\\n      'X-Request-ID': '123', // This is added\\n    },\\n    body: {\\n      model: 'gpt-5-mini', // This overrides the hook-level body field\\n      temperature: 0.5, // This is added\\n    },\\n  },\\n);\\n```\\n\\nFor more details on request configuration, see the [Request Configuration](/docs/ai-sdk-ui/chatbot#request-configuration) documentation.\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/12-typescript-performance-zod.mdx'), name='12-typescript-performance-zod.mdx', displayName='12-typescript-performance-zod.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: TypeScript performance issues with Zod and AI SDK 5\\ndescription: Troubleshooting TypeScript server crashes and slow performance when using Zod with AI SDK 5\\n---\\n\\n# TypeScript performance issues with Zod and AI SDK 5\\n\\n## Issue\\n\\nWhen using the AI SDK 5 with Zod, you may experience:\\n\\n- TypeScript server crashes or hangs\\n- Extremely slow type checking in files that import AI SDK functions\\n- Error messages like \"Type instantiation is excessively deep and possibly infinite\"\\n- IDE becoming unresponsive when working with AI SDK code\\n\\n## Background\\n\\nThe AI SDK 5 has specific compatibility requirements with Zod versions. When importing Zod using the standard import path (`import { z } from \\'zod\\'`), TypeScript\\'s type inference can become excessively complex, leading to performance degradation or crashes.\\n\\n## Solution\\n\\n### Upgrade Zod to 4.1.8 or Later\\n\\nThe primary solution is to upgrade to Zod version 4.1.8 or later, which includes a fix for this module resolution issue:\\n\\n```bash\\npnpm add zod@^4.1.8\\n```\\n\\nThis version resolves the underlying problem where different module resolution settings were causing TypeScript to load the same Zod declarations twice, leading to expensive structural comparisons.\\n\\n### Alternative: Update TypeScript Configuration\\n\\nIf upgrading Zod isn\\'t possible, you can update your `tsconfig.json` to use `moduleResolution: \"nodenext\"`:\\n\\n```json\\n{\\n  \"compilerOptions\": {\\n    \"moduleResolution\": \"nodenext\"\\n    // ... other options\\n  }\\n}\\n```\\n\\nThis resolves the TypeScript performance issues while allowing you to continue using the standard Zod import.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/12-use-chat-an-error-occurred.mdx'), name='12-use-chat-an-error-occurred.mdx', displayName='12-use-chat-an-error-occurred.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat \"An error occurred\"\\ndescription: Troubleshooting errors related to the \"An error occurred\" error in useChat.\\n---\\n\\n# `useChat` \"An error occurred\"\\n\\n## Issue\\n\\nI am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and I get the error \"An error occurred\".\\n\\n## Background\\n\\nError messages from `streamText` are masked by default when using `toDataStreamResponse` for security reasons (secure-by-default).\\nThis prevents leaking sensitive information to the client.\\n\\n## Solution\\n\\nTo forward error details to the client or to log errors, use the `getErrorMessage` function when calling `toDataStreamResponse`.\\n\\n```tsx\\nexport function errorHandler(error: unknown) {\\n  if (error == null) {\\n    return \\'unknown error\\';\\n  }\\n\\n  if (typeof error === \\'string\\') {\\n    return error;\\n  }\\n\\n  if (error instanceof Error) {\\n    return error.message;\\n  }\\n\\n  return JSON.stringify(error);\\n}\\n```\\n\\n```tsx\\nconst result = streamText({\\n  // ...\\n});\\n\\nreturn result.toUIMessageStreamResponse({\\n  getErrorMessage: errorHandler,\\n});\\n```\\n\\nIn case you are using `createDataStreamResponse`, you can use the `onError` function when calling `toDataStreamResponse`:\\n\\n```tsx\\nconst response = createDataStreamResponse({\\n  // ...\\n  async execute(dataStream) {\\n    // ...\\n  },\\n  onError: errorHandler,\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/13-repeated-assistant-messages.mdx'), name='13-repeated-assistant-messages.mdx', displayName='13-repeated-assistant-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Repeated assistant messages in useChat\\ndescription: Troubleshooting duplicate assistant messages when using useChat with streamText\\n---\\n\\n# Repeated assistant messages in useChat\\n\\n## Issue\\n\\nWhen using `useChat` with `streamText` on the server, the assistant's messages appear duplicated in the UI - showing both the previous message and the new message, or showing the same message multiple times. This can occur when using tool calls or complex message flows.\\n\\n```tsx\\n// Server-side code that may experience assistant message duplication on the client\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'openai/gpt-5-mini',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: {\\n        description: 'Get the weather for a location',\\n        parameters: z.object({\\n          location: z.string(),\\n        }),\\n        execute: async ({ location }) => {\\n          return { temperature: 72, condition: 'sunny' };\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Background\\n\\nThe duplication occurs because `toUIMessageStreamResponse` generates new message IDs for each new message.\\n\\n## Solution\\n\\nPass the original messages array to `toUIMessageStreamResponse` using the `originalMessages` option. By passing `originalMessages`, the method can reuse existing message IDs instead of generating new ones, ensuring the client properly updates existing messages rather than creating duplicates.\\n\\n```tsx\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'openai/gpt-5-mini',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: {\\n        description: 'Get the weather for a location',\\n        parameters: z.object({\\n          location: z.string(),\\n        }),\\n        execute: async ({ location }) => {\\n          return { temperature: 72, condition: 'sunny' };\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages, // Pass the original messages here\\n    generateMessageId: generateId,\\n    onFinish: ({ messages }) => {\\n      saveChat({ id, messages });\\n    },\\n  });\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/14-stream-abort-handling.mdx'), name='14-stream-abort-handling.mdx', displayName='14-stream-abort-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: onFinish not called when stream is aborted\\ndescription: Troubleshooting onFinish callback not executing when streams are aborted with toUIMessageStreamResponse\\n---\\n\\n# onFinish not called when stream is aborted\\n\\n## Issue\\n\\nWhen using `toUIMessageStreamResponse` with an `onFinish` callback, the callback may not execute when the stream is aborted. This happens because the abort handler immediately terminates the response, preventing the `onFinish` callback from being triggered.\\n\\n```tsx\\n// Server-side code where onFinish isn't called on abort\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages: convertToModelMessages(messages),\\n    abortSignal: req.signal,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onFinish: async ({ isAborted }) => {\\n      // This isn't called when the stream is aborted!\\n      if (isAborted) {\\n        console.log('Stream was aborted');\\n        // Handle abort-specific cleanup\\n      } else {\\n        console.log('Stream completed normally');\\n        // Handle normal completion\\n      }\\n    },\\n  });\\n}\\n```\\n\\n## Background\\n\\nWhen a stream is aborted, the response is immediately terminated. Without proper handling, the `onFinish` callback has no chance to execute, preventing important cleanup operations like saving partial results or logging abort events.\\n\\n## Solution\\n\\nAdd `consumeStream` to the `toUIMessageStreamResponse` configuration. This ensures that abort events are properly captured and forwarded to the `onFinish` callback, allowing it to execute even when the stream is aborted.\\n\\n```tsx\\n// other imports...\\nimport { consumeStream } from 'ai';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages: convertToModelMessages(messages),\\n    abortSignal: req.signal,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onFinish: async ({ isAborted }) => {\\n      // Now this WILL be called even when aborted!\\n      if (isAborted) {\\n        console.log('Stream was aborted');\\n        // Handle abort-specific cleanup\\n      } else {\\n        console.log('Stream completed normally');\\n        // Handle normal completion\\n      }\\n    },\\n    consumeSseStream: consumeStream, // This enables onFinish to be called on abort\\n  });\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/14-tool-calling-with-structured-outputs.mdx'), name='14-tool-calling-with-structured-outputs.mdx', displayName='14-tool-calling-with-structured-outputs.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Tool calling with generateObject and streamObject\\ndescription: Troubleshooting tool calling when combined with generateObject and streamObject\\n---\\n\\n# Tool calling with generateObject and streamObject (structured outputs)\\n\\n## Issue\\n\\nYou may want to combine tool calling with structured output generation. While `generateObject` and `streamObject` are designed specifically for structured outputs, they don't support tool calling.\\n\\n## Background\\n\\nTo use tool calling with structured outputs, use `generateText` or `streamText` with the `output` option.\\n\\n**Important**: When using `output` with tool calling, the structured output generation counts as an additional step in the execution flow.\\n\\n## Solution\\n\\nWhen using `output` with tool calling, adjust your `stopWhen` condition to account for the additional step required for structured output generation:\\n\\n```tsx\\nconst result = await generateText({\\n  model: 'anthropic/claude-sonnet-4.5',\\n  output: Output.object({\\n    schema: z.object({\\n      summary: z.string(),\\n      sentiment: z.enum(['positive', 'neutral', 'negative']),\\n    }),\\n  }),\\n  tools: {\\n    analyze: tool({\\n      description: 'Analyze data',\\n      inputSchema: z.object({\\n        data: z.string(),\\n      }),\\n      execute: async ({ data }) => {\\n        return { result: 'analyzed' };\\n      }),\\n    },\\n  },\\n  // Add at least 1 to your intended step count to account for structured output\\n  stopWhen: stepCountIs(3), // Now accounts for: tool call + tool result + structured output\\n  prompt: 'Analyze the data and provide a summary',\\n});\\n```\\n\\nFor more information about using structured outputs with `generateText` and `streamText` see [Generating Structured Data](/docs/ai-sdk-core/generating-structured-data#structured-outputs-with-generatetext-and-streamtext).\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/15-abort-breaks-resumable-streams.mdx'), name='15-abort-breaks-resumable-streams.mdx', displayName='15-abort-breaks-resumable-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Abort breaks resumable streams\\ndescription: Troubleshooting stream resumption failures when using abort functionality\\n---\\n\\n# Abort breaks resumable streams\\n\\n## Issue\\n\\nWhen using `useChat` with `resume: true` for stream resumption, the abort functionality breaks. Closing a tab, refreshing the page, or calling the `stop()` function will trigger an abort signal that interferes with the resumption mechanism, preventing streams from being properly resumed.\\n\\n```tsx\\n// This configuration will cause conflicts\\nconst { messages, stop } = useChat({\\n  id: chatId,\\n  resume: true, // Stream resumption enabled\\n});\\n\\n// Closing the tab will trigger abort and stop resumption\\n```\\n\\n## Background\\n\\nWhen a page is closed or refreshed, the browser automatically sends an abort signal, which breaks the resumption flow.\\n\\n## Current limitations\\n\\nWe're aware of this incompatibility and are exploring solutions. **In the meantime, please choose either stream resumption or abort functionality based on your application's requirements**, but not both.\\n\\n### Option 1: Use stream resumption without abort\\n\\nIf you need to support long-running generations that persist across page reloads:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat({\\n  id: chatId,\\n  resume: true,\\n});\\n```\\n\\n### Option 2: Use abort without stream resumption\\n\\nIf you need to allow users to stop streams manually:\\n\\n```tsx\\nconst { messages, sendMessage, stop } = useChat({\\n  id: chatId,\\n  resume: false, // Disable stream resumption (default behaviour)\\n});\\n```\\n\\n## Related\\n\\n- [Chatbot Resume Streams](/docs/ai-sdk-ui/chatbot-resume-streams)\\n- [Stopping Streams](/docs/advanced/stopping-streams)\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/15-stream-text-not-working.mdx'), name='15-stream-text-not-working.mdx', displayName='15-stream-text-not-working.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamText fails silently\\ndescription: Troubleshooting errors related to the streamText function not working.\\n---\\n\\n# `streamText` is not working\\n\\n## Issue\\n\\nI am using [`streamText`](/docs/reference/ai-sdk-core/stream-text) function, and it does not work.\\nIt does not throw any errors and the stream is only containing error parts.\\n\\n## Background\\n\\n`streamText` immediately starts streaming to enable sending data without waiting for the model.\\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\\n\\n## Solution\\n\\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\\n\\n```tsx highlight=\"6-8\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onError({ error }) {\\n    console.error(error); // your error logging logic here\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/16-streaming-status-delay.mdx'), name='16-streaming-status-delay.mdx', displayName='16-streaming-status-delay.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming Status Shows But No Text Appears\\ndescription: Why useChat shows \"streaming\" status without any visible content\\n---\\n\\n# Streaming Status Shows But No Text Appears\\n\\n## Issue\\n\\nWhen using `useChat`, the status changes to \"streaming\" immediately, but no text appears for several seconds.\\n\\n## Background\\n\\nThe status changes to \"streaming\" as soon as the connection to the server is established and streaming begins - this includes metadata streaming, not just the LLM\\'s generated tokens.\\n\\n## Solution\\n\\nCreate a custom loading state that checks if the last assistant message actually contains content:\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { messages, status } = useChat();\\n\\n  const lastMessage = messages.at(-1);\\n\\n  const showLoader =\\n    status === \\'streaming\\' &&\\n    lastMessage?.role === \\'assistant\\' &&\\n    lastMessage?.parts?.length === 0;\\n\\n  return (\\n    <>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      {showLoader && <div>Loading...</div>}\\n    </>\\n  );\\n}\\n```\\n\\nYou can also check for specific part types if you\\'re waiting for something specific:\\n\\n```tsx\\nconst showLoader =\\n  status === \\'streaming\\' &&\\n  lastMessage?.role === \\'assistant\\' &&\\n  !lastMessage?.parts?.some(part => part.type === \\'text\\');\\n```\\n\\n## Related Issues\\n\\n- [GitHub Issue #7586](https://github.com/vercel/ai/issues/7586)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/17-use-chat-stale-body-data.mdx'), name='17-use-chat-stale-body-data.mdx', displayName='17-use-chat-stale-body-data.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stale body values with useChat\\ndescription: Troubleshooting stale values when passing information via the body parameter of useChat\\n---\\n\\n# Stale body values with useChat\\n\\n## Issue\\n\\nWhen using `useChat` and passing dynamic information via the `body` parameter at the hook level, the data remains stale and only reflects the value from the initial component render. This occurs because the body configuration is captured once when the hook is initialized and doesn\\'t update with subsequent component re-renders.\\n\\n```tsx\\n// Problematic code - body data will be stale\\nexport default function Chat() {\\n  const [temperature, setTemperature] = useState(0.7);\\n  const [userId, setUserId] = useState(\\'user123\\');\\n\\n  // This body configuration is captured once and won\\'t update\\n  const { messages, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n      body: {\\n        temperature, // Always the initial value (0.7)\\n        userId, // Always the initial value (\\'user123\\')\\n      },\\n    }),\\n  });\\n\\n  // Even if temperature or userId change, the body in requests will still use initial values\\n  return (\\n    <div>\\n      <input\\n        type=\"range\"\\n        value={temperature}\\n        onChange={e => setTemperature(parseFloat(e.target.value))}\\n      />\\n      {/* Chat UI */}\\n    </div>\\n  );\\n}\\n```\\n\\n## Background\\n\\nThe hook-level body configuration is evaluated once during the initial render and doesn\\'t re-evaluate when component state changes.\\n\\n## Solution\\n\\nPass dynamic variables via the second argument of the `sendMessage` function instead of at the hook level. Request-level options are evaluated on each call and take precedence over hook-level options.\\n\\n```tsx\\nexport default function Chat() {\\n  const [temperature, setTemperature] = useState(0.7);\\n  const [userId, setUserId] = useState(\\'user123\\');\\n  const [input, setInput] = useState(\\'\\');\\n\\n  const { messages, sendMessage } = useChat({\\n    // Static configuration only\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return (\\n    <div>\\n      <input\\n        type=\"range\"\\n        value={temperature}\\n        onChange={e => setTemperature(parseFloat(e.target.value))}\\n      />\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            // Pass dynamic values as request-level options\\n            sendMessage(\\n              { text: input },\\n              {\\n                body: {\\n                  temperature, // Current value at request time\\n                  userId, // Current value at request time\\n                },\\n              },\\n            );\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Alternative: Dynamic Hook-Level Configuration\\n\\nIf you need hook-level configuration that responds to changes, you can use functions that return configuration values. However, for component state, you\\'ll need to use `useRef` to access current values:\\n\\n```tsx\\nexport default function Chat() {\\n  const temperatureRef = useRef(0.7);\\n\\n  const { messages, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n      body: () => ({\\n        temperature: temperatureRef.current, // Access via ref.current\\n        sessionId: getCurrentSessionId(), // Function calls work directly\\n      }),\\n    }),\\n  });\\n\\n  // ...\\n}\\n```\\n\\n**Recommendation:** Request-level configuration is simpler and more reliable for component state. Use it whenever you need to pass dynamic values that change during the component lifecycle.\\n\\n### Server-side handling\\n\\nOn your server side, retrieve the custom fields by destructuring the request body:\\n\\n```tsx\\n// app/api/chat/route.ts\\nexport async function POST(req: Request) {\\n  const { messages, temperature, userId } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(messages),\\n    temperature, // Use the dynamic temperature from the request\\n    // ... other configuration\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nFor more information, see [chatbot request configuration documentation](/docs/ai-sdk-ui/chatbot#request-configuration).\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/18-ontoolcall-type-narrowing.mdx'), name='18-ontoolcall-type-narrowing.mdx', displayName='18-ontoolcall-type-narrowing.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Type Error with onToolCall\\ndescription: How to handle TypeScript type errors when using the onToolCall callback\\n---\\n\\n# Type Error with onToolCall\\n\\nWhen using the `onToolCall` callback with TypeScript, you may encounter type errors when trying to pass tool properties directly to `addToolOutput`.\\n\\n## Problem\\n\\nTypeScript cannot automatically narrow the type of `toolCall.toolName` when you have both static and dynamic tools, leading to type errors:\\n\\n```tsx\\n// ❌ This causes a TypeScript error\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  async onToolCall({ toolCall }) {\\n    addToolOutput({\\n      tool: toolCall.toolName, // Type \\'string\\' is not assignable to type \\'\"yourTool\" | \"yourOtherTool\"\\'\\n      toolCallId: toolCall.toolCallId,\\n      output: someOutput,\\n    });\\n  },\\n});\\n```\\n\\nThe error occurs because:\\n\\n- Static tools have specific literal types for their names (e.g., `\"getWeatherInformation\"`)\\n- Dynamic tools have `toolName` as a generic `string`\\n- TypeScript can\\'t guarantee that `toolCall.toolName` matches your specific tool names\\n\\n## Solution\\n\\nCheck if the tool is dynamic first to enable proper type narrowing:\\n\\n```tsx\\n// ✅ Correct approach with type narrowing\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  async onToolCall({ toolCall }) {\\n    // Check if it\\'s a dynamic tool first\\n    if (toolCall.dynamic) {\\n      return;\\n    }\\n\\n    // Now TypeScript knows this is a static tool with the correct type\\n    addToolOutput({\\n      tool: toolCall.toolName, // No type error!\\n      toolCallId: toolCall.toolCallId,\\n      output: someOutput,\\n    });\\n  },\\n});\\n```\\n\\n<Note>\\n  If you\\'re still using the deprecated `addToolResult` method, this solution\\n  applies the same way. Consider migrating to `addToolOutput` for consistency\\n  with the latest API.\\n</Note>\\n\\n## Related\\n\\n- [Chatbot Tool Usage](/docs/ai-sdk-ui/chatbot-tool-usage)\\n- [Dynamic Tools](/docs/reference/ai-sdk-core/dynamic-tool)\\n- [useChat Reference](/docs/reference/ai-sdk-ui/use-chat)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/19-unsupported-model-version.mdx'), name='19-unsupported-model-version.mdx', displayName='19-unsupported-model-version.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Unsupported model version error\\ndescription: Troubleshooting the AI_UnsupportedModelVersionError when migrating to AI SDK 5\\n---\\n\\n# Unsupported model version error\\n\\n## Issue\\n\\nWhen migrating to AI SDK 5, you might encounter an error stating that your model uses an unsupported version:\\n\\n```\\nAI_UnsupportedModelVersionError: Unsupported model version v1 for provider \"ollama.chat\" and model \"gamma3:4b\".\\nAI SDK 5 only supports models that implement specification version \"v2\".\\n```\\n\\nThis error occurs because the version of the provider package you\\'re using implements the older (v1) model specification.\\n\\n## Background\\n\\nAI SDK 5 requires all provider packages to implement specification version \"v2\". When you upgrade to AI SDK 5 but don\\'t update your provider packages to compatible versions, they continue using the older \"v1\" specification, causing this error.\\n\\n## Solution\\n\\n### Update provider packages to AI SDK 5 compatible versions\\n\\nUpdate all your `@ai-sdk/*` provider packages to compatible version `2.0.0` or later. These versions implement the v2 specification required by AI SDK 5.\\n\\n```bash\\npnpm install ai@latest @ai-sdk/openai@latest @ai-sdk/anthropic@latest\\n```\\n\\nFor AI SDK 5 compatibility, you need:\\n\\n- `ai` package: `5.0.0` or later\\n- `@ai-sdk/*` packages: `2.0.0` or later (for example, `@ai-sdk/openai`, `@ai-sdk/anthropic`, `@ai-sdk/google`)\\n- `@ai-sdk/provider` package: `2.0.0` or later\\n- `zod` package: `4.1.8` or later\\n\\n### Check provider compatibility\\n\\nIf you\\'re using a third-party or custom provider, verify that it has been updated to support AI SDK 5. Not all providers may have v2-compatible versions available yet.\\n\\nTo check if a provider supports AI SDK 5:\\n\\n1. Check the provider\\'s package.json for `@ai-sdk/provider` peer dependency version `2.0.0` or later\\n2. Review the provider\\'s changelog or migration guide\\n3. Check the provider\\'s repository for AI SDK 5 support\\n\\nFor more information on migrating to AI SDK 5, see the [AI SDK 5.0 migration guide](/docs/migration-guides/migration-guide-5-0).\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/20-no-object-generated-content-filter.mdx'), name='20-no-object-generated-content-filter.mdx', displayName='20-no-object-generated-content-filter.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Object generation failed with OpenAI\\ndescription: Troubleshooting NoObjectGeneratedError with finish-reason content-filter caused by incompatible Zod schema types when using OpenAI structured outputs\\n---\\n\\n# Object generation failed with OpenAI\\n\\n## Issue\\n\\nWhen using `generateObject` or `streamObject` with OpenAI\\'s structured output generation, you may encounter a `NoObjectGeneratedError` with the finish reason `content-filter`. This error occurs when your Zod schema contains incompatible types that OpenAI\\'s structured output feature cannot process.\\n\\n```typescript\\n// Problematic code - incompatible schema types\\nimport { generateObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = await generateObject({\\n  model: openai(\\'gpt-4o-2024-08-06\\'),\\n  schema: z.object({\\n    name: z.string().nullish(), // ❌ .nullish() is not supported\\n    email: z.string().optional(), // ❌ .optional() is not supported\\n    age: z.number().nullable(), // ✅ .nullable() is supported\\n  }),\\n  prompt: \\'Generate a user profile\\',\\n});\\n\\n// Error: NoObjectGeneratedError: No object generated.\\n// Finish reason: content-filter\\n```\\n\\n## Background\\n\\nOpenAI\\'s structured output generation uses JSON Schema under the hood and has specific requirements for schema compatibility. The Zod methods `.nullish()` and `.optional()` generate JSON Schema patterns that are incompatible with OpenAI\\'s implementation, causing the model to reject the schema and return a content-filter finish reason.\\n\\n## Solution\\n\\nReplace `.nullish()` and `.optional()` with `.nullable()` in your Zod schemas when using structured output generation with OpenAI models.\\n\\n```typescript\\nimport { generateObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\n// Correct approach - use .nullable()\\nconst result = await generateObject({\\n  model: openai(\\'gpt-4o-2024-08-06\\'),\\n  schema: z.object({\\n    name: z.string().nullable(), // ✅ Use .nullable() instead of .nullish()\\n    email: z.string().nullable(), // ✅ Use .nullable() instead of .optional()\\n    age: z.number().nullable(),\\n  }),\\n  prompt: \\'Generate a user profile\\',\\n});\\n\\nconsole.log(result.object);\\n// { name: \"John Doe\", email: \"john@example.com\", age: 30 }\\n// or { name: null, email: null, age: 25 }\\n```\\n\\n### Schema Type Comparison\\n\\n| Zod Type      | Compatible | JSON Schema Behavior                                   |\\n| ------------- | ---------- | ------------------------------------------------------ |\\n| `.nullable()` | ✅ Yes     | Allows `null` or the specified type                    |\\n| `.optional()` | ❌ No      | Field can be omitted (not supported)                   |\\n| `.nullish()`  | ❌ No      | Allows `null`, `undefined`, or omitted (not supported) |\\n\\n## Related Information\\n\\n- For more details on structured output generation, see [Generating Structured Data](/docs/ai-sdk-core/generating-structured-data)\\n- For OpenAI-specific structured output configuration, see [OpenAI Provider - Structured Outputs](/providers/ai-sdk-providers/openai#structured-outputs)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/30-model-is-not-assignable-to-type.mdx'), name='30-model-is-not-assignable-to-type.mdx', displayName='30-model-is-not-assignable-to-type.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Model is not assignable to type \"LanguageModelV1\"\\ndescription: Troubleshooting errors related to incompatible models.\\n---\\n\\n# Model is not assignable to type \"LanguageModelV1\"\\n\\n## Issue\\n\\nI have updated the AI SDK and now I get the following error: `Type \\'SomeModel\\' is not assignable to type \\'LanguageModelV1\\'.`\\n\\n<Note>Similar errors can occur with `EmbeddingModelV3` as well.</Note>\\n\\n## Background\\n\\nSometimes new features are being added to the model specification.\\nThis can cause incompatibilities with older provider versions.\\n\\n## Solution\\n\\nUpdate your provider packages and the AI SDK to the latest version.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/40-typescript-cannot-find-namespace-jsx.mdx'), name='40-typescript-cannot-find-namespace-jsx.mdx', displayName='40-typescript-cannot-find-namespace-jsx.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: TypeScript error \"Cannot find namespace \\'JSX\\'\"\\ndescription: Troubleshooting errors related to TypeScript and JSX.\\n---\\n\\n# TypeScript error \"Cannot find namespace \\'JSX\\'\"\\n\\n## Issue\\n\\nI am using the AI SDK in a project without React, e.g. an Hono server, and I get the following error:\\n`error TS2503: Cannot find namespace \\'JSX\\'.`\\n\\n## Background\\n\\nThe AI SDK has a dependency on `@types/react` which defines the `JSX` namespace.\\nIt will be removed in the next major version of the AI SDK.\\n\\n## Solution\\n\\nYou can install the `@types/react` package as a dependency to fix the error.\\n\\n```bash\\nnpm install @types/react\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx'), name='50-react-maximum-update-depth-exceeded.mdx', displayName='50-react-maximum-update-depth-exceeded.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: React error \"Maximum update depth exceeded\"\\ndescription: Troubleshooting errors related to the \"Maximum update depth exceeded\" error.\\n---\\n\\n# React error \"Maximum update depth exceeded\"\\n\\n## Issue\\n\\nI am using the AI SDK in a React project with the `useChat` or `useCompletion` hooks\\nand I get the following error when AI responses stream in: `Maximum update depth exceeded`.\\n\\n## Background\\n\\nBy default, the UI is re-rendered on every chunk that arrives.\\nThis can overload the rendering, especially on slower devices or when complex components\\nneed updating (e.g. Markdown). Throttling can mitigate this.\\n\\n## Solution\\n\\nUse the `experimental_throttle` option to throttle the UI updates:\\n\\n### `useChat`\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { messages, ... } = useChat({\\n  // Throttle the messages and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n\\n### `useCompletion`\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { completion, ... } = useCompletion({\\n  // Throttle the completion and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/60-jest-cannot-find-module-ai-rsc.mdx'), name='60-jest-cannot-find-module-ai-rsc.mdx', displayName='60-jest-cannot-find-module-ai-rsc.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: \"Jest: cannot find module \\'@ai-sdk/rsc\\'\"\\ndescription: \"Troubleshooting AI SDK errors related to the Jest: cannot find module \\'@ai-sdk/rsc\\' error\"\\n---\\n\\n# Jest: cannot find module \\'@ai-sdk/rsc\\'\\n\\n## Issue\\n\\nI am using AI SDK RSC and am writing tests for my RSC components with Jest.\\n\\nI am getting the following error: `Cannot find module \\'@ai-sdk/rsc\\'`.\\n\\n## Solution\\n\\nConfigure the module resolution via `jest config update` in `moduleNameMapper`:\\n\\n```json filename=\"jest.config.js\"\\n\"moduleNameMapper\": {\\n  \"^@ai-sdk/rsc$\": \"<rootDir>/node_modules/@ai-sdk/rsc/dist\"\\n}\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Troubleshooting\\ndescription: Troubleshooting information for common issues encountered with the AI SDK.\\ncollapsed: true\\n---\\n\\n# Troubleshooting\\n\\nThis section is designed to help you quickly identify and resolve common issues encountered with the AI SDK, ensuring a smoother and more efficient development experience.\\n\\n<Support />\\n', children=[])])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = build_markdown_doc_tree(clone_dir/source.doc_dir, extensions=['.mdx'], exclude=[\"08-migration-guides\"])\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocItem(origPath=Path('00-introduction/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK by Vercel\\ndescription: The AI SDK is the TypeScript toolkit for building AI applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.\\n---\\n\\n# AI SDK\\n\\nThe AI SDK is the TypeScript toolkit designed to help developers build AI-powered applications and agents with React, Next.js, Vue, Svelte, Node.js, and more.\\n\\n## Why use the AI SDK?\\n\\nIntegrating large language models (LLMs) into applications is complicated and heavily dependent on the specific model provider you use.\\n\\nThe AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.\\n\\nFor example, here’s how you can generate text with various models using the AI SDK:\\n\\n<PreviewSwitchProviders />\\n\\nThe AI SDK has two main libraries:\\n\\n- **[AI SDK Core](/docs/ai-sdk-core):** A unified API for generating text, structured objects, tool calls, and building agents with LLMs.\\n- **[AI SDK UI](/docs/ai-sdk-ui):** A set of framework-agnostic hooks for quickly building chat and generative user interface.\\n\\n## Model Providers\\n\\nThe AI SDK supports [multiple model providers](/providers).\\n\\n<OfficialModelCards />\\n\\n## Templates\\n\\nWe\\'ve built some [templates](https://vercel.com/templates?type=ai) that include AI SDK integrations for different use cases, providers, and frameworks. You can use these templates to get started with your AI-powered application.\\n\\n### Starter Kits\\n\\n<Templates type=\"starter-kits\" />\\n\\n### Feature Exploration\\n\\n<Templates type=\"feature-exploration\" />\\n\\n### Frameworks\\n\\n<Templates type=\"frameworks\" />\\n\\n### Generative UI\\n\\n<Templates type=\"generative-ui\" />\\n\\n### Security\\n\\n<Templates type=\"security\" />\\n\\n## Join our Community\\n\\nIf you have questions about anything related to the AI SDK, you\\'re always welcome to ask our community on [the Vercel Community](https://community.vercel.com/c/ai-sdk/62).\\n\\n## `llms.txt` (for Cursor, Windsurf, Copilot, Claude etc.)\\n\\nYou can access the entire AI SDK documentation in Markdown format at [ai-sdk.dev/llms.txt](/llms.txt). This can be used to ask any LLM (assuming it has a big enough context window) questions about the AI SDK based on the most up-to-date documentation.\\n\\n### Example Usage\\n\\nFor instance, to prompt an LLM with questions about the AI SDK:\\n\\n1. Copy the documentation contents from [ai-sdk.dev/llms.txt](/llms.txt)\\n2. Use the following prompt format:\\n\\n```prompt\\nDocumentation:\\n{paste documentation here}\\n---\\nBased on the above documentation, answer the following:\\n{your question}\\n```\\n', children=[]),\n",
       " DocItem(origPath=Path('01-announcing-ai-sdk-6-beta/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK 6 Beta\\ndescription: Get started with the Beta version of AI SDK 6.\\n---\\n\\n# Announcing AI SDK 6 Beta\\n\\n<Note type=\"warning\">\\n  AI SDK 6 is in beta — while more stable than alpha, AI SDK 6 is still in\\n  active development and APIs may still change. Pin to specific versions as\\n  breaking changes may occur in patch releases.\\n</Note>\\n\\n## Why AI SDK 6?\\n\\nAI SDK 6 is a **major version** due to the introduction of the **v3 Language Model Specification** that powers new capabilities like agents and tool approval. However, unlike AI SDK 5, **this release is not expected to have major breaking changes** for most users.\\n\\nThe version bump reflects improvements to the specification, not a complete redesign of the SDK. If you\\'re using AI SDK 5, migrating to v6 should be straightforward with minimal code changes.\\n\\n## Beta Version Guidance\\n\\nThe AI SDK 6 Beta is intended for:\\n\\n- **Trying out new features** and giving us feedback on the developer experience\\n- **Experimenting with agents** and tool approval workflows\\n\\nYour feedback during this beta phase directly shapes the final stable release. Share your experiences through [GitHub issues](https://github.com/vercel/ai/issues/new/choose).\\n\\n## Installation\\n\\nTo install the AI SDK 6 Beta, run the following command:\\n\\n```bash\\nnpm install ai@beta @ai-sdk/openai@beta @ai-sdk/react@beta\\n```\\n\\n<Note type=\"warning\">\\n  APIs may still change during beta. Pin to specific versions as breaking\\n  changes may occur in patch releases.\\n</Note>\\n\\n## What\\'s New in AI SDK 6?\\n\\nAI SDK 6 introduces several features (with more to come soon!):\\n\\n### Agent Abstraction\\n\\nA new unified interface for building agents with full control over execution flow, tool loops, and state management.\\n\\n### Tool Execution Approval\\n\\nRequest user confirmation before executing tools, enabling native human-in-the-loop patterns.\\n\\n### Structured Output (Stable)\\n\\nGenerate structured data alongside tool calling with `generateText` and `streamText` - now stable and production-ready.\\n\\n### Reranking Support\\n\\nImprove search relevance by reordering documents based on their relationship to a query using specialized reranking models.\\n\\n### Image Editing Support\\n\\nNative support for image editing (coming soon).\\n\\n## Agent Abstraction\\n\\nAI SDK 6 introduces a powerful new `Agent` interface that provides a standardized way to build agents.\\n\\n### Default Implementation: ToolLoopAgent\\n\\nThe `ToolLoopAgent` class provides a default implementation out of the box:\\n\\n```typescript\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { weatherTool } from \\'@/tool/weather\\';\\n\\nexport const weatherAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful weather assistant.\\',\\n  tools: {\\n    weather: weatherTool,\\n  },\\n});\\n\\n// Use the agent\\nconst result = await weatherAgent.generate({\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\nThe agent automatically handles the tool execution loop:\\n\\n1. Calls the LLM with your prompt\\n2. Executes any requested tool calls\\n3. Adds results back to the conversation\\n4. Repeats until complete (default `stopWhen: stepCountIs(20)`)\\n\\n### Configuring Call Options\\n\\nCall options let you pass type-safe runtime inputs to dynamically configure your agents. Use them to inject retrieved documents for RAG, select models based on request complexity, customize tool behavior per request, or adjust any agent setting based on context.\\n\\nWithout call options, you\\'d need to create multiple agents or handle configuration logic outside the agent. With call options, you define a schema once and modify agent behavior at runtime:\\n\\n```typescript\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst supportAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    userId: z.string(),\\n    accountType: z.enum([\\'free\\', \\'pro\\', \\'enterprise\\']),\\n  }),\\n  instructions: \\'You are a helpful customer support agent.\\',\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    instructions:\\n      settings.instructions +\\n      `\\\\nUser context:\\n- Account type: ${options.accountType}\\n- User ID: ${options.userId}\\n\\nAdjust your response based on the user\\'s account level.`,\\n  }),\\n});\\n\\n// Pass options when calling the agent\\nconst result = await supportAgent.generate({\\n  prompt: \\'How do I upgrade my account?\\',\\n  options: {\\n    userId: \\'user_123\\',\\n    accountType: \\'free\\',\\n  },\\n});\\n```\\n\\nThe `options` parameter is type-safe and will error if you don\\'t provide it or pass incorrect types.\\n\\nCall options enable dynamic agent configuration for several scenarios:\\n\\n- **RAG**: Fetch relevant documents and inject them into prompts at runtime\\n- **Dynamic model selection**: Choose faster or more capable models based on request complexity\\n- **Tool configuration**: Adjust tools per request\\n- **Provider options**: Set reasoning effort, temperature, or other provider-specific settings dynamically\\n\\nLearn more in the [Configuring Call Options](/docs/agents/configuring-call-options) documentation.\\n\\n### UI Integration\\n\\nAgents integrate seamlessly with React and other UI frameworks:\\n\\n```typescript\\n// Server-side API route\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: weatherAgent,\\n    messages,\\n  });\\n}\\n```\\n\\n```typescript\\n// Client-side with type safety\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { InferAgentUIMessage } from \\'ai\\';\\nimport { weatherAgent } from \\'@/agent/weather-agent\\';\\n\\ntype WeatherAgentUIMessage = InferAgentUIMessage<typeof weatherAgent>;\\n\\nconst { messages, sendMessage } = useChat<WeatherAgentUIMessage>();\\n```\\n\\n### Custom Agent Implementations\\n\\nIn AI SDK 6, `Agent` is an interface rather than a concrete class. While `ToolLoopAgent` provides a solid default implementation for most use cases, you can implement the `Agent` interface to build custom agent architectures:\\n\\n```typescript\\nimport { Agent } from \\'ai\\';\\n\\n// Build your own multi-agent orchestrator that delegates to specialists\\nclass Orchestrator implements Agent {\\n  constructor(private subAgents: Record<string, Agent>) {\\n    /* Implementation */\\n  }\\n}\\n\\nconst orchestrator = new Orchestrator({\\n  subAgents: {\\n    // your subagents\\n  },\\n});\\n```\\n\\nThis approach enables you to experiment with orchestrators, memory layers, custom stop conditions, and agent patterns tailored to your specific use case.\\n\\n## Tool Execution Approval\\n\\nAI SDK 6 introduces a tool approval system that gives you control over when tools are executed.\\n\\nEnable approval for a tool by setting `needsApproval`:\\n\\n```typescript\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const weatherTool = tool({\\n  description: \\'Get the weather in a location\\',\\n  inputSchema: z.object({\\n    city: z.string(),\\n  }),\\n  needsApproval: true, // Require user approval\\n  execute: async ({ city }) => {\\n    const weather = await fetchWeather(city);\\n    return weather;\\n  },\\n});\\n```\\n\\n### Dynamic Approval\\n\\nMake approval decisions based on tool input:\\n\\n```typescript\\nexport const paymentTool = tool({\\n  description: \\'Process a payment\\',\\n  inputSchema: z.object({\\n    amount: z.number(),\\n    recipient: z.string(),\\n  }),\\n  // Only require approval for large transactions\\n  needsApproval: async ({ amount }) => amount > 1000,\\n  execute: async ({ amount, recipient }) => {\\n    return await processPayment(amount, recipient);\\n  },\\n});\\n```\\n\\n### Client-Side Approval UI\\n\\nHandle approval requests in your UI:\\n\\n```tsx\\nexport function WeatherToolView({ invocation, addToolApprovalResponse }) {\\n  if (invocation.state === \\'approval-requested\\') {\\n    return (\\n      <div>\\n        <p>Can I retrieve the weather for {invocation.input.city}?</p>\\n        <button\\n          onClick={() =>\\n            addToolApprovalResponse({\\n              id: invocation.approval.id,\\n              approved: true,\\n            })\\n          }\\n        >\\n          Approve\\n        </button>\\n        <button\\n          onClick={() =>\\n            addToolApprovalResponse({\\n              id: invocation.approval.id,\\n              approved: false,\\n            })\\n          }\\n        >\\n          Deny\\n        </button>\\n      </div>\\n    );\\n  }\\n\\n  if (invocation.state === \\'output-available\\') {\\n    return (\\n      <div>\\n        Weather: {invocation.output.weather}\\n        Temperature: {invocation.output.temperature}°F\\n      </div>\\n    );\\n  }\\n\\n  // Handle other states...\\n}\\n```\\n\\n### Auto-Submit After Approvals\\n\\nAutomatically continue the conversation once approvals are handled:\\n\\n```typescript\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { lastAssistantMessageIsCompleteWithApprovalResponses } from \\'ai\\';\\n\\nconst { messages, addToolApprovalResponse } = useChat({\\n  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithApprovalResponses,\\n});\\n```\\n\\n## Structured Output (Stable)\\n\\nAI SDK 6 stabilizes structured output support for agents, enabling you to generate structured data alongside multi-step tool calling.\\n\\nPreviously, you could only generate structured outputs with `generateObject` and `streamObject`, which didn\\'t support tool calling. Now `ToolLoopAgent` (and `generateText` / `streamText`) can combine both capabilities using the `output` parameter:\\n\\n```typescript\\nimport { Output, ToolLoopAgent, tool } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        city: z.string(),\\n      }),\\n      execute: async ({ city }) => {\\n        return { temperature: 72, condition: \\'sunny\\' };\\n      },\\n    }),\\n  },\\n  output: Output.object({\\n    schema: z.object({\\n      summary: z.string(),\\n      temperature: z.number(),\\n      recommendation: z.string(),\\n    }),\\n  }),\\n});\\n\\nconst { output } = await agent.generate({\\n  prompt: \\'What is the weather in San Francisco and what should I wear?\\',\\n});\\n// The agent calls the weather tool AND returns structured output\\nconsole.log(output);\\n// {\\n//   summary: \"It\\'s sunny in San Francisco\",\\n//   temperature: 72,\\n//   recommendation: \"Wear light clothing and sunglasses\"\\n// }\\n```\\n\\n### Output Types\\n\\nThe `Output` object provides multiple strategies for structured generation:\\n\\n- **`Output.object()`**: Generate structured objects with Zod schemas\\n- **`Output.array()`**: Generate arrays of structured objects\\n- **`Output.choice()`**: Select from a specific set of options\\n- **`Output.text()`**: Generate plain text (default behavior)\\n\\n### Streaming Structured Output\\n\\nUse `agent.stream()` to stream structured output as it\\'s being generated:\\n\\n```typescript\\nimport { ToolLoopAgent, Output } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst profileAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'Generate realistic person profiles.\\',\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number(),\\n      occupation: z.string(),\\n    }),\\n  }),\\n});\\n\\nconst { partialOutputStream } = await profileAgent.stream({\\n  prompt: \\'Generate a person profile.\\',\\n});\\n\\nfor await (const partial of partialOutputStream) {\\n  console.log(partial);\\n  // { name: \"John\" }\\n  // { name: \"John\", age: 30 }\\n  // { name: \"John\", age: 30, occupation: \"Engineer\" }\\n}\\n```\\n\\n### Support in `generateText` and `streamText`\\n\\nStructured outputs are also supported in `generateText` and `streamText` functions, allowing you to use this feature outside of agents when needed.\\n\\n<Note>\\n  When using structured output with `generateText` or `streamText`, you must\\n  configure multiple steps with `stopWhen` because generating the structured\\n  output is itself a step. For example: `stopWhen: stepCountIs(2)` to allow tool\\n  calling and output generation.\\n</Note>\\n\\n## Reranking Support\\n\\nAI SDK 6 introduces native support for reranking, a technique that improves search relevance by reordering documents based on their relationship to a query.\\n\\nUnlike embedding-based similarity search, reranking models are specifically trained to understand query-document relationships, producing more accurate relevance scores:\\n\\n```typescript\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  \\'sunny day at the beach\\',\\n  \\'rainy afternoon in the city\\',\\n  \\'snowy night in the mountains\\',\\n];\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'talk about rain\\',\\n  topN: 2,\\n});\\n\\nconsole.log(ranking);\\n// [\\n//   { originalIndex: 1, score: 0.9, document: \\'rainy afternoon in the city\\' },\\n//   { originalIndex: 0, score: 0.3, document: \\'sunny day at the beach\\' }\\n// ]\\n```\\n\\n### Structured Document Reranking\\n\\nReranking also supports structured documents, making it ideal for searching through databases, emails, or other structured content:\\n\\n```typescript\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  {\\n    from: \\'Paul Doe\\',\\n    subject: \\'Follow-up\\',\\n    text: \\'We are happy to give you a discount of 20% on your next order.\\',\\n  },\\n  {\\n    from: \\'John McGill\\',\\n    subject: \\'Missing Info\\',\\n    text: \\'Sorry, but here is the pricing information from Oracle: $5000/month\\',\\n  },\\n];\\n\\nconst { rerankedDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'Which pricing did we get from Oracle?\\',\\n  topN: 1,\\n});\\n\\nconsole.log(rerankedDocuments[0]);\\n// { from: \\'John McGill\\', subject: \\'Missing Info\\', text: \\'...\\' }\\n```\\n\\n### Supported Providers\\n\\nSeveral providers offer reranking models:\\n\\n- [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)\\n- [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models)\\n- [Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models)\\n\\n## Image Editing Support\\n\\nNative support for image editing and generation workflows is coming soon. This will enable:\\n\\n- Image-to-image transformations\\n- Multi-modal editing with text prompts\\n\\n## Migration from AI SDK 5.x\\n\\nAI SDK 6 is expected to have minimal breaking changes. The version bump is due to the v3 Language Model Specification, but most AI SDK 5 code will work with little or no modification.\\n\\n## Timeline\\n\\n**AI SDK 6 Beta**: Available now\\n\\n**Stable Release**: End of 2025\\n', children=[]),\n",
       " DocItem(origPath=Path('02-foundations'), name='02-foundations', displayName='02-foundations', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('02-foundations/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of foundational concepts critical to understanding the AI SDK\\n---\\n\\n# Overview\\n\\n<Note>\\n  This page is a beginner-friendly introduction to high-level artificial\\n  intelligence (AI) concepts. To dive right into implementing the AI SDK, feel\\n  free to skip ahead to our [quickstarts](/docs/getting-started) or learn about\\n  our [supported models and providers](/docs/foundations/providers-and-models).\\n</Note>\\n\\nThe AI SDK standardizes integrating artificial intelligence (AI) models across [supported providers](/docs/foundations/providers-and-models). This enables developers to focus on building great AI applications, not waste time on technical details.\\n\\nFor example, here’s how you can generate text with various models using the AI SDK:\\n\\n<PreviewSwitchProviders />\\n\\nTo effectively leverage the AI SDK, it helps to familiarize yourself with the following concepts:\\n\\n## Generative Artificial Intelligence\\n\\n**Generative artificial intelligence** refers to models that predict and generate various types of outputs (such as text, images, or audio) based on what’s statistically likely, pulling from patterns they’ve learned from their training data. For example:\\n\\n- Given a photo, a generative model can generate a caption.\\n- Given an audio file, a generative model can generate a transcription.\\n- Given a text description, a generative model can generate an image.\\n\\n## Large Language Models\\n\\nA **large language model (LLM)** is a subset of generative models focused primarily on **text**. An LLM takes a sequence of words as input and aims to predict the most likely sequence to follow. It assigns probabilities to potential next sequences and then selects one. The model continues to generate sequences until it meets a specified stopping criterion.\\n\\nLLMs learn by training on massive collections of written text, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well.\\n\\nHowever, it\\'s crucial to understand LLMs\\' limitations. When asked about less known or absent information, like the birthday of a personal relative, LLMs might \"hallucinate\" or make up information. It\\'s essential to consider how well-represented the information you need is in the model.\\n\\n## Embedding Models\\n\\nAn **embedding model** is used to convert complex data (like words or images) into a dense vector (a list of numbers) representation, known as an embedding. Unlike generative models, embedding models do not generate new text or data. Instead, they provide representations of semantic and syntactic relationships between entities that can be used as input for other models or other natural language processing tasks.\\n\\nIn the next section, you will learn about the difference between models providers and models, and which ones are available in the AI SDK.\\n', children=[]), DocItem(origPath=Path('02-foundations/02-providers-and-models.mdx'), name='02-providers-and-models.mdx', displayName='02-providers-and-models.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Providers and Models\\ndescription: Learn about the providers and models available in the AI SDK.\\n---\\n\\n# Providers and Models\\n\\nCompanies such as OpenAI and Anthropic (providers) offer access to a range of large language models (LLMs) with differing strengths and capabilities through their own APIs.\\n\\nEach provider typically has its own unique method for interfacing with their models, complicating the process of switching providers and increasing the risk of vendor lock-in.\\n\\nTo solve these challenges, AI SDK Core offers a standardized approach to interacting with LLMs through a [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v2) that abstracts differences between providers. This unified interface allows you to switch between providers with ease while using the same API for all providers.\\n\\nHere is an overview of the AI SDK Provider Architecture:\\n\\n<MDXImage\\n  srcLight=\"/images/ai-sdk-diagram.png\"\\n  srcDark=\"/images/ai-sdk-diagram-dark.png\"\\n  width={800}\\n  height={800}\\n/>\\n\\n## AI SDK Providers\\n\\nThe AI SDK comes with a wide range of providers that you can use to interact with different language models:\\n\\n- [xAI Grok Provider](/providers/ai-sdk-providers/xai) (`@ai-sdk/xai`)\\n- [OpenAI Provider](/providers/ai-sdk-providers/openai) (`@ai-sdk/openai`)\\n- [Azure OpenAI Provider](/providers/ai-sdk-providers/azure) (`@ai-sdk/azure`)\\n- [Anthropic Provider](/providers/ai-sdk-providers/anthropic) (`@ai-sdk/anthropic`)\\n- [Amazon Bedrock Provider](/providers/ai-sdk-providers/amazon-bedrock) (`@ai-sdk/amazon-bedrock`)\\n- [Google Generative AI Provider](/providers/ai-sdk-providers/google-generative-ai) (`@ai-sdk/google`)\\n- [Google Vertex Provider](/providers/ai-sdk-providers/google-vertex) (`@ai-sdk/google-vertex`)\\n- [Mistral Provider](/providers/ai-sdk-providers/mistral) (`@ai-sdk/mistral`)\\n- [Together.ai Provider](/providers/ai-sdk-providers/togetherai) (`@ai-sdk/togetherai`)\\n- [Cohere Provider](/providers/ai-sdk-providers/cohere) (`@ai-sdk/cohere`)\\n- [Fireworks Provider](/providers/ai-sdk-providers/fireworks) (`@ai-sdk/fireworks`)\\n- [DeepInfra Provider](/providers/ai-sdk-providers/deepinfra) (`@ai-sdk/deepinfra`)\\n- [DeepSeek Provider](/providers/ai-sdk-providers/deepseek) (`@ai-sdk/deepseek`)\\n- [Cerebras Provider](/providers/ai-sdk-providers/cerebras) (`@ai-sdk/cerebras`)\\n- [Groq Provider](/providers/ai-sdk-providers/groq) (`@ai-sdk/groq`)\\n- [Perplexity Provider](/providers/ai-sdk-providers/perplexity) (`@ai-sdk/perplexity`)\\n- [ElevenLabs Provider](/providers/ai-sdk-providers/elevenlabs) (`@ai-sdk/elevenlabs`)\\n- [LMNT Provider](/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)\\n- [Hume Provider](/providers/ai-sdk-providers/hume) (`@ai-sdk/hume`)\\n- [Rev.ai Provider](/providers/ai-sdk-providers/revai) (`@ai-sdk/revai`)\\n- [Deepgram Provider](/providers/ai-sdk-providers/deepgram) (`@ai-sdk/deepgram`)\\n- [Gladia Provider](/providers/ai-sdk-providers/gladia) (`@ai-sdk/gladia`)\\n- [LMNT Provider](/providers/ai-sdk-providers/lmnt) (`@ai-sdk/lmnt`)\\n- [AssemblyAI Provider](/providers/ai-sdk-providers/assemblyai) (`@ai-sdk/assemblyai`)\\n- [Baseten Provider](/providers/ai-sdk-providers/baseten) (`@ai-sdk/baseten`)\\n\\nYou can also use the [OpenAI Compatible provider](/providers/openai-compatible-providers) with OpenAI-compatible APIs:\\n\\n- [LM Studio](/providers/openai-compatible-providers/lmstudio)\\n- [Heroku](/providers/openai-compatible-providers/heroku)\\n\\nOur [language model specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v2) is published as an open-source package, which you can use to create [custom providers](/providers/community-providers/custom-providers).\\n\\nThe open-source community has created the following providers:\\n\\n- [Ollama Provider](/providers/community-providers/ollama) (`ollama-ai-provider`)\\n- [FriendliAI Provider](/providers/community-providers/friendliai) (`@friendliai/ai-provider`)\\n- [Portkey Provider](/providers/community-providers/portkey) (`@portkey-ai/vercel-provider`)\\n- [Cloudflare Workers AI Provider](/providers/community-providers/cloudflare-workers-ai) (`workers-ai-provider`)\\n- [OpenRouter Provider](/providers/community-providers/openrouter) (`@openrouter/ai-sdk-provider`)\\n- [Aihubmix Provider](/providers/community-providers/aihubmix) (`@aihubmix/ai-sdk-provider`)\\n- [Requesty Provider](/providers/community-providers/requesty) (`@requesty/ai-sdk`)\\n- [Crosshatch Provider](/providers/community-providers/crosshatch) (`@crosshatch/ai-provider`)\\n- [Mixedbread Provider](/providers/community-providers/mixedbread) (`mixedbread-ai-provider`)\\n- [Voyage AI Provider](/providers/community-providers/voyage-ai) (`voyage-ai-provider`)\\n- [Mem0 Provider](/providers/community-providers/mem0)(`@mem0/vercel-ai-provider`)\\n- [Letta Provider](/providers/community-providers/letta)(`@letta-ai/vercel-ai-sdk-provider`)\\n- [Supermemory Provider](/providers/community-providers/supermemory)(`@supermemory/tools`)\\n- [Spark Provider](/providers/community-providers/spark) (`spark-ai-provider`)\\n- [AnthropicVertex Provider](/providers/community-providers/anthropic-vertex-ai) (`anthropic-vertex-ai`)\\n- [LangDB Provider](/providers/community-providers/langdb) (`@langdb/vercel-provider`)\\n- [Dify Provider](/providers/community-providers/dify) (`dify-ai-provider`)\\n- [Sarvam Provider](/providers/community-providers/sarvam) (`sarvam-ai-provider`)\\n- [Claude Code Provider](/providers/community-providers/claude-code) (`ai-sdk-provider-claude-code`)\\n- [Built-in AI Provider](/providers/community-providers/built-in-ai) (`built-in-ai`)\\n- [Gemini CLI Provider](/providers/community-providers/gemini-cli) (`ai-sdk-provider-gemini-cli`)\\n- [A2A Provider](/providers/community-providers/a2a) (`a2a-ai-provider`)\\n- [SAP-AI Provider](/providers/community-providers/sap-ai) (`@mymediset/sap-ai-provider`)\\n- [AI/ML API Provider](/providers/community-providers/aimlapi) (`@ai-ml.api/aimlapi-vercel-ai`)\\n- [MCP Sampling Provider](/providers/community-providers/mcp-sampling) (`@mcpc-tech/mcp-sampling-ai-provider`)\\n- [ACP Provider](/providers/community-providers/acp) (`@mcpc-tech/acp-ai-provider`)\\n\\n## Self-Hosted Models\\n\\nYou can access self-hosted models with the following providers:\\n\\n- [Ollama Provider](/providers/community-providers/ollama)\\n- [LM Studio](/providers/openai-compatible-providers/lmstudio)\\n- [Baseten](/providers/ai-sdk-providers/baseten)\\n- [Built-in AI](/providers/community-providers/built-in-ai)\\n\\nAdditionally, any self-hosted provider that supports the OpenAI specification can be used with the [OpenAI Compatible Provider](/providers/openai-compatible-providers).\\n\\n## Model Capabilities\\n\\nThe AI providers support different language models with various capabilities.\\nHere are the capabilities of popular models:\\n\\n| Provider                                                                 | Model                                       | Image Input         | Object Generation   | Tool Usage          | Tool Streaming      |\\n| ------------------------------------------------------------------------ | ------------------------------------------- | ------------------- | ------------------- | ------------------- | ------------------- |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-4`                                    | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3`                                    | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3-fast`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3-mini`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-3-mini-fast`                          | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-2-1212`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-2-vision-1212`                        | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-beta`                                 | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [xAI Grok](/providers/ai-sdk-providers/xai)                              | `grok-vision-beta`                          | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |\\n| [Vercel](/providers/ai-sdk-providers/vercel)                             | `v0-1.0-md`                                 | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5`                                     | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-mini`                                | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-nano`                                | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1-chat-latest`                       | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1-codex-mini`                        | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1-codex`                             | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5.1`                                   | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-codex`                               | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [OpenAI](/providers/ai-sdk-providers/openai)                             | `gpt-5-chat-latest`                         | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-opus-4-5`                           | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-opus-4-1`                           | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-opus-4-0`                           | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-sonnet-4-0`                         | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-3-7-sonnet-latest`                  | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Anthropic](/providers/ai-sdk-providers/anthropic)                       | `claude-3-5-haiku-latest`                   | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `pixtral-large-latest`                      | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-large-latest`                      | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-medium-latest`                     | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-medium-2505`                       | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `mistral-small-latest`                      | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Mistral](/providers/ai-sdk-providers/mistral)                           | `pixtral-12b-2409`                          | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai) | `gemini-2.0-flash-exp`                      | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai) | `gemini-1.5-flash`                          | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai) | `gemini-1.5-pro`                            | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex)               | `gemini-2.0-flash-exp`                      | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex)               | `gemini-1.5-flash`                          | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex)               | `gemini-1.5-pro`                            | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [DeepSeek](/providers/ai-sdk-providers/deepseek)                         | `deepseek-chat`                             | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [DeepSeek](/providers/ai-sdk-providers/deepseek)                         | `deepseek-reasoner`                         | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> | <Cross size={18} /> |\\n| [Cerebras](/providers/ai-sdk-providers/cerebras)                         | `llama3.1-8b`                               | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Cerebras](/providers/ai-sdk-providers/cerebras)                         | `llama3.1-70b`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Cerebras](/providers/ai-sdk-providers/cerebras)                         | `llama3.3-70b`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `meta-llama/llama-4-scout-17b-16e-instruct` | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `llama-3.3-70b-versatile`                   | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `llama-3.1-8b-instant`                      | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `mixtral-8x7b-32768`                        | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [Groq](/providers/ai-sdk-providers/groq)                                 | `gemma2-9b-it`                              | <Cross size={18} /> | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n\\n<Note>\\n  This table is not exhaustive. Additional models can be found in the provider\\n  documentation pages and on the provider websites.\\n</Note>\\n', children=[]), DocItem(origPath=Path('02-foundations/03-prompts.mdx'), name='03-prompts.mdx', displayName='03-prompts.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Prompts\\ndescription: Learn about the Prompt structure used in the AI SDK.\\n---\\n\\n# Prompts\\n\\nPrompts are instructions that you give a [large language model (LLM)](/docs/foundations/overview#large-language-models) to tell it what to do.\\nIt\\'s like when you ask someone for directions; the clearer your question, the better the directions you\\'ll get.\\n\\nMany LLM providers offer complex interfaces for specifying prompts. They involve different roles and message types.\\nWhile these interfaces are powerful, they can be hard to use and understand.\\n\\nIn order to simplify prompting, the AI SDK supports text, message, and system prompts.\\n\\n## Text Prompts\\n\\nText prompts are strings.\\nThey are ideal for simple generation use cases,\\ne.g. repeatedly generating content for variants of the same prompt text.\\n\\nYou can set text prompts using the `prompt` property made available by AI SDK functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text) or [`generateObject`](/docs/reference/ai-sdk-core/generate-object).\\nYou can structure the text in any way and inject variables, e.g. using a template literal.\\n\\n```ts highlight=\"3\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\nYou can also use template literals to provide dynamic data to your prompt.\\n\\n```ts highlight=\"3-5\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt:\\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\\n    `Please suggest the best tourist activities for me to do.`,\\n});\\n```\\n\\n## System Prompts\\n\\nSystem prompts are the initial set of instructions given to models that help guide and constrain the models\\' behaviors and responses.\\nYou can set system prompts using the `system` property.\\nSystem prompts work with both the `prompt` and the `messages` properties.\\n\\n```ts highlight=\"3-6\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system:\\n    `You help planning travel itineraries. ` +\\n    `Respond to the users\\' request with a list ` +\\n    `of the best stops to make in their destination.`,\\n  prompt:\\n    `I am planning a trip to ${destination} for ${lengthOfStay} days. ` +\\n    `Please suggest the best tourist activities for me to do.`,\\n});\\n```\\n\\n<Note>\\n  When you use a message prompt, you can also use system messages instead of a\\n  system prompt.\\n</Note>\\n\\n## Message Prompts\\n\\nA message prompt is an array of user, assistant, and tool messages.\\nThey are great for chat interfaces and more complex, multi-modal prompts.\\nYou can use the `messages` property to set message prompts.\\n\\nEach message has a `role` and a `content` property. The content can either be text (for user and assistant messages), or an array of relevant parts (data) for that message type.\\n\\n```ts highlight=\"3-7\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Hi!\\' },\\n    { role: \\'assistant\\', content: \\'Hello, how can I help?\\' },\\n    { role: \\'user\\', content: \\'Where can I buy the best Currywurst in Berlin?\\' },\\n  ],\\n});\\n```\\n\\nInstead of sending a text in the `content` property, you can send an array of parts that includes a mix of text and other content parts.\\n\\n<Note type=\"warning\">\\n  Not all language models support all message and content types. For example,\\n  some models might not be capable of handling multi-modal inputs or tool\\n  messages. [Learn more about the capabilities of select\\n  models](./providers-and-models#model-capabilities).\\n</Note>\\n\\n### Provider Options\\n\\nYou can pass through additional provider-specific metadata to enable provider-specific functionality at 3 levels.\\n\\n#### Function Call Level\\n\\nFunctions like [`streamText`](/docs/reference/ai-sdk-core/stream-text#provider-options) or [`generateText`](/docs/reference/ai-sdk-core/generate-text#provider-options) accept a `providerOptions` property.\\n\\nAdding provider options at the function call level should be used when you do not need granular control over where the provider options are applied.\\n\\n```ts\\nconst { text } = await generateText({\\n  model: azure(\\'your-deployment-name\\'),\\n  providerOptions: {\\n    openai: {\\n      reasoningEffort: \\'low\\',\\n    },\\n  },\\n});\\n```\\n\\n#### Message Level\\n\\nFor granular control over applying provider options at the message level, you can pass `providerOptions` to the message object:\\n\\n```ts\\nimport { ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [\\n  {\\n    role: \\'system\\',\\n    content: \\'Cached system message\\',\\n    providerOptions: {\\n      // Sets a cache control breakpoint on the system message\\n      anthropic: { cacheControl: { type: \\'ephemeral\\' } },\\n    },\\n  },\\n];\\n```\\n\\n#### Message Part Level\\n\\nCertain provider-specific options require configuration at the message part level:\\n\\n```ts\\nimport { ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [\\n  {\\n    role: \\'user\\',\\n    content: [\\n      {\\n        type: \\'text\\',\\n        text: \\'Describe the image in detail.\\',\\n        providerOptions: {\\n          openai: { imageDetail: \\'low\\' },\\n        },\\n      },\\n      {\\n        type: \\'image\\',\\n        image:\\n          \\'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true\\',\\n        // Sets image detail configuration for image part:\\n        providerOptions: {\\n          openai: { imageDetail: \\'low\\' },\\n        },\\n      },\\n    ],\\n  },\\n];\\n```\\n\\n<Note type=\"warning\">\\n  AI SDK UI hooks like [`useChat`](/docs/reference/ai-sdk-ui/use-chat) return\\n  arrays of `UIMessage` objects, which do not support provider options. We\\n  recommend using the\\n  [`convertToModelMessages`](/docs/reference/ai-sdk-ui/convert-to-core-messages)\\n  function to convert `UIMessage` objects to\\n  [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) objects before\\n  applying or appending message(s) or message parts with `providerOptions`.\\n</Note>\\n\\n### User Messages\\n\\n#### Text Parts\\n\\nText content is the most common type of content. It is a string that is passed to the model.\\n\\nIf you only need to send text content in a message, the `content` property can be a string,\\nbut you can also use it to send multiple content parts.\\n\\n```ts highlight=\"7-10\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        {\\n          type: \\'text\\',\\n          text: \\'Where can I buy the best Currywurst in Berlin?\\',\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Image Parts\\n\\nUser messages can include image parts. An image can be one of the following:\\n\\n- base64-encoded image:\\n  - `string` with base-64 encoded content\\n  - data URL `string`, e.g. `data:image/png;base64,...`\\n- binary image:\\n  - `ArrayBuffer`\\n  - `Uint8Array`\\n  - `Buffer`\\n- URL:\\n  - http(s) URL `string`, e.g. `https://example.com/image.png`\\n  - `URL` object, e.g. `new URL(\\'https://example.com/image.png\\')`\\n\\n##### Example: Binary image (Buffer)\\n\\n```ts highlight=\"8-11\"\\nconst result = await generateText({\\n  model,\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'Describe the image in detail.\\' },\\n        {\\n          type: \\'image\\',\\n          image: fs.readFileSync(\\'./data/comic-cat.png\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n##### Example: Base-64 encoded image (string)\\n\\n```ts highlight=\"8-11\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'Describe the image in detail.\\' },\\n        {\\n          type: \\'image\\',\\n          image: fs.readFileSync(\\'./data/comic-cat.png\\').toString(\\'base64\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n##### Example: Image URL (string)\\n\\n```ts highlight=\"8-12\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'Describe the image in detail.\\' },\\n        {\\n          type: \\'image\\',\\n          image:\\n            \\'https://github.com/vercel/ai/blob/main/examples/ai-core/data/comic-cat.png?raw=true\\',\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### File Parts\\n\\n<Note type=\"warning\">\\n  Only a few providers and models currently support file parts: [Google\\n  Generative AI](/providers/ai-sdk-providers/google-generative-ai), [Google\\n  Vertex AI](/providers/ai-sdk-providers/google-vertex),\\n  [OpenAI](/providers/ai-sdk-providers/openai) (for `wav` and `mp3` audio with\\n  `gpt-4o-audio-preview`), [Anthropic](/providers/ai-sdk-providers/anthropic),\\n  [OpenAI](/providers/ai-sdk-providers/openai) (for `pdf`).\\n</Note>\\n\\nUser messages can include file parts. A file can be one of the following:\\n\\n- base64-encoded file:\\n  - `string` with base-64 encoded content\\n  - data URL `string`, e.g. `data:image/png;base64,...`\\n- binary data:\\n  - `ArrayBuffer`\\n  - `Uint8Array`\\n  - `Buffer`\\n- URL:\\n  - http(s) URL `string`, e.g. `https://example.com/some.pdf`\\n  - `URL` object, e.g. `new URL(\\'https://example.com/some.pdf\\')`\\n\\nYou need to specify the MIME type of the file you are sending.\\n\\n##### Example: PDF file from Buffer\\n\\n```ts highlight=\"12-15\"\\nimport { google } from \\'@ai-sdk/google\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: google(\\'gemini-1.5-flash\\'),\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'What is the file about?\\' },\\n        {\\n          type: \\'file\\',\\n          mediaType: \\'application/pdf\\',\\n          data: fs.readFileSync(\\'./data/example.pdf\\'),\\n          filename: \\'example.pdf\\', // optional, not used by all providers\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n##### Example: mp3 audio file from Buffer\\n\\n```ts highlight=\"12-14\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: openai(\\'gpt-4o-audio-preview\\'),\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        { type: \\'text\\', text: \\'What is the audio saying?\\' },\\n        {\\n          type: \\'file\\',\\n          mediaType: \\'audio/mpeg\\',\\n          data: fs.readFileSync(\\'./data/galileo.mp3\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Custom Download Function (Experimental)\\n\\nYou can use custom download functions to implement throttling, retries, authentication, caching, and more.\\n\\nThe default download implementation automatically downloads files in parallel when they are not supported by the model.\\n\\nCustom download function can be passed via the `experimental_download` property:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  experimental_download: async (\\n    requestedDownloads: Array<{\\n      url: URL;\\n      isUrlSupportedByModel: boolean;\\n    }>,\\n  ): PromiseLike<\\n    Array<{\\n      data: Uint8Array;\\n      mediaType: string | undefined;\\n    } | null>\\n  > => {\\n    // ... download the files and return an array with similar order\\n  },\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        {\\n          type: \\'file\\',\\n          data: new URL(\\'https://api.company.com/private/document.pdf\\'),\\n          mediaType: \\'application/pdf\\',\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n<Note>\\n  The `experimental_download` option is experimental and may change in future\\n  releases.\\n</Note>\\n\\n### Assistant Messages\\n\\nAssistant messages are messages that have a role of `assistant`.\\nThey are typically previous responses from the assistant\\nand can contain text, reasoning, and tool call parts.\\n\\n#### Example: Assistant message with text content\\n\\n```ts highlight=\"5\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Hi!\\' },\\n    { role: \\'assistant\\', content: \\'Hello, how can I help?\\' },\\n  ],\\n});\\n```\\n\\n#### Example: Assistant message with text content in array\\n\\n```ts highlight=\"7\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Hi!\\' },\\n    {\\n      role: \\'assistant\\',\\n      content: [{ type: \\'text\\', text: \\'Hello, how can I help?\\' }],\\n    },\\n  ],\\n});\\n```\\n\\n#### Example: Assistant message with tool call content\\n\\n```ts highlight=\"7-14\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'How many calories are in this block of cheese?\\' },\\n    {\\n      role: \\'assistant\\',\\n      content: [\\n        {\\n          type: \\'tool-call\\',\\n          toolCallId: \\'12345\\',\\n          toolName: \\'get-nutrition-data\\',\\n          input: { cheese: \\'Roquefort\\' },\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Example: Assistant message with file content\\n\\n<Note>\\n  This content part is for model-generated files. Only a few models support\\n  this, and only for file types that they can generate.\\n</Note>\\n\\n```ts highlight=\"9-11\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'user\\', content: \\'Generate an image of a roquefort cheese!\\' },\\n    {\\n      role: \\'assistant\\',\\n      content: [\\n        {\\n          type: \\'file\\',\\n          mediaType: \\'image/png\\',\\n          data: fs.readFileSync(\\'./data/roquefort.jpg\\'),\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n### Tool messages\\n\\n<Note>\\n  [Tools](/docs/foundations/tools) (also known as function calling) are programs\\n  that you can provide an LLM to extend its built-in functionality. This can be\\n  anything from calling an external API to calling functions within your UI.\\n  Learn more about Tools in [the next section](/docs/foundations/tools).\\n</Note>\\n\\nFor models that support [tool](/docs/foundations/tools) calls, assistant messages can contain tool call parts, and tool messages can contain tool output parts.\\nA single assistant message can call multiple tools, and a single tool message can contain multiple tool results.\\n\\n```ts highlight=\"14-42\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    {\\n      role: \\'user\\',\\n      content: [\\n        {\\n          type: \\'text\\',\\n          text: \\'How many calories are in this block of cheese?\\',\\n        },\\n        { type: \\'image\\', image: fs.readFileSync(\\'./data/roquefort.jpg\\') },\\n      ],\\n    },\\n    {\\n      role: \\'assistant\\',\\n      content: [\\n        {\\n          type: \\'tool-call\\',\\n          toolCallId: \\'12345\\',\\n          toolName: \\'get-nutrition-data\\',\\n          input: { cheese: \\'Roquefort\\' },\\n        },\\n        // there could be more tool calls here (parallel calling)\\n      ],\\n    },\\n    {\\n      role: \\'tool\\',\\n      content: [\\n        {\\n          type: \\'tool-result\\',\\n          toolCallId: \\'12345\\', // needs to match the tool call id\\n          toolName: \\'get-nutrition-data\\',\\n          output: {\\n            type: \\'json\\',\\n            value: {\\n              name: \\'Cheese, roquefort\\',\\n              calories: 369,\\n              fat: 31,\\n              protein: 22,\\n            },\\n          },\\n        },\\n        // there could be more tool results here (parallel calling)\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n#### Multi-modal Tool Results\\n\\n<Note type=\"warning\">\\n  Multi-part tool results are experimental and only supported by Anthropic.\\n</Note>\\n\\nTool results can be multi-part and multi-modal, e.g. a text and an image.\\nYou can use the `experimental_content` property on tool parts to specify multi-part tool results.\\n\\n```ts highlight=\"24-46\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    // ...\\n    {\\n      role: \\'tool\\',\\n      content: [\\n        {\\n          type: \\'tool-result\\',\\n          toolCallId: \\'12345\\', // needs to match the tool call id\\n          toolName: \\'get-nutrition-data\\',\\n          // for models that do not support multi-part tool results,\\n          // you can include a regular output part:\\n          output: {\\n            type: \\'json\\',\\n            value: {\\n              name: \\'Cheese, roquefort\\',\\n              calories: 369,\\n              fat: 31,\\n              protein: 22,\\n            },\\n          },\\n        },\\n        {\\n          type: \\'tool-result\\',\\n          toolCallId: \\'12345\\', // needs to match the tool call id\\n          toolName: \\'get-nutrition-data\\',\\n          // for models that support multi-part tool results,\\n          // you can include a multi-part content part:\\n          output: {\\n            type: \\'content\\',\\n            value: [\\n              {\\n                type: \\'text\\',\\n                text: \\'Here is an image of the nutrition data for the cheese:\\',\\n              },\\n              {\\n                type: \\'media\\',\\n                data: fs\\n                  .readFileSync(\\'./data/roquefort-nutrition-data.png\\')\\n                  .toString(\\'base64\\'),\\n                mediaType: \\'image/png\\',\\n              },\\n            ],\\n          },\\n        },\\n      ],\\n    },\\n  ],\\n});\\n```\\n\\n### System Messages\\n\\nSystem messages are messages that are sent to the model before the user messages to guide the assistant\\'s behavior.\\nYou can alternatively use the `system` property.\\n\\n```ts highlight=\"4\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  messages: [\\n    { role: \\'system\\', content: \\'You help planning travel itineraries.\\' },\\n    {\\n      role: \\'user\\',\\n      content:\\n        \\'I am planning a trip to Berlin for 3 days. Please suggest the best tourist activities for me to do.\\',\\n    },\\n  ],\\n});\\n```\\n', children=[]), DocItem(origPath=Path('02-foundations/04-tools.mdx'), name='04-tools.mdx', displayName='04-tools.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Tools\\ndescription: Learn about tools with the AI SDK.\\n---\\n\\n# Tools\\n\\nWhile [large language models (LLMs)](/docs/foundations/overview#large-language-models) have incredible generation capabilities,\\nthey struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather).\\n\\nTools are actions that an LLM can invoke.\\nThe results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, when you ask an LLM for the \"weather in London\", and there is a weather tool available, it could call a tool\\nwith London as the argument. The tool would then fetch the weather data and return it to the LLM. The LLM can then use this\\ninformation in its response.\\n\\n## What is a tool?\\n\\nA tool is an object that can be called by the model to perform a specific task.\\nYou can use tools with [`generateText`](/docs/reference/ai-sdk-core/generate-text)\\nand [`streamText`](/docs/reference/ai-sdk-core/stream-text) by passing one or more tools to the `tools` parameter.\\n\\nA tool consists of three properties:\\n\\n- **`description`**: An optional description of the tool that can influence when the tool is picked.\\n- **`inputSchema`**: A [Zod schema](/docs/foundations/tools#schema-specification-and-validation-with-zod) or a [JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the input required for the tool to run. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\\n- **`execute`**: An optional async function that is called with the arguments from the tool call.\\n\\n<Note>\\n  `streamUI` uses UI generator tools with a `generate` function that can return\\n  React components.\\n</Note>\\n\\nIf the LLM decides to use a tool, it will generate a tool call.\\nTools with an `execute` function are run automatically when these calls are generated.\\nThe output of the tool calls are returned using tool result objects.\\n\\nYou can automatically pass tool results back to the LLM\\nusing [multi-step calls](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls) with `streamText` and `generateText`.\\n\\n## Schemas\\n\\nSchemas are used to define the parameters for tools and to validate the [tool calls](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\nThe AI SDK supports both raw JSON schemas (using the [`jsonSchema` function](/docs/reference/ai-sdk-core/json-schema))\\nand [Zod](https://zod.dev/) schemas (either directly or using the [`zodSchema` function](/docs/reference/ai-sdk-core/zod-schema)).\\n\\n[Zod](https://zod.dev/) is a popular TypeScript schema validation library.\\nYou can install it with:\\n\\n<Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n  <Tab>\\n    <Snippet text=\"pnpm add zod\" dark />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"npm install zod\" dark />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"yarn add zod\" dark />\\n  </Tab>\\n\\n  <Tab>\\n    <Snippet text=\"bun add zod\" dark />\\n  </Tab>\\n</Tabs>\\n\\nYou can then specify a Zod schema, for example:\\n\\n```ts\\nimport z from \\'zod\\';\\n\\nconst recipeSchema = z.object({\\n  recipe: z.object({\\n    name: z.string(),\\n    ingredients: z.array(\\n      z.object({\\n        name: z.string(),\\n        amount: z.string(),\\n      }),\\n    ),\\n    steps: z.array(z.string()),\\n  }),\\n});\\n```\\n\\n<Note>\\n  You can also use schemas for structured output generation with\\n  [`generateObject`](/docs/reference/ai-sdk-core/generate-object) and\\n  [`streamObject`](/docs/reference/ai-sdk-core/stream-object).\\n</Note>\\n\\n## Tool Packages\\n\\nGiven tools are JavaScript objects, they can be packaged and distributed through npm like any other library. This makes it easy to share reusable tools across projects and with the community.\\n\\n### Using Ready-Made Tool Packages\\n\\nInstall a tool package and import the tools you need:\\n\\n```bash\\npnpm add some-tool-package\\n```\\n\\nThen pass them directly to `generateText`, `streamText`, or your agent definition:\\n\\n```ts highlight=\"2, 8\"\\nimport { generateText, stepCountIs } from \\'ai\\';\\nimport { searchTool } from \\'some-tool-package\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-haiku-4.5\\',\\n  prompt: \\'When was Vercel Ship AI?\\',\\n  tools: {\\n    webSearch: searchTool,\\n  },\\n  stopWhen: stepCountIs(10),\\n});\\n```\\n\\n### Publishing Your Own Tools\\n\\nYou can publish your own tool packages to npm for others to use. Simply export your tool objects from your package:\\n\\n```ts\\n// my-tools/index.ts\\nexport const myTool = {\\n  description: \\'A helpful tool\\',\\n  inputSchema: z.object({\\n    query: z.string(),\\n  }),\\n  execute: async ({ query }) => {\\n    // your tool logic\\n    return result;\\n  },\\n};\\n```\\n\\nAnyone can then install and use your tools by importing them.\\n\\nTo get started, you can use the [AI SDK Tool Package Template](https://github.com/vercel-labs/ai-sdk-tool-as-package-template) which provides a ready-to-use starting point for publishing your own tools.\\n\\n## Toolsets\\n\\nWhen you work with tools, you typically need a mix of application-specific tools and general-purpose tools. The community has created various toolsets and resources to help you build and use tools.\\n\\n### Ready-to-Use Tool Packages\\n\\nThese packages provide pre-built tools you can install and use immediately:\\n\\n- **[@exalabs/ai-sdk](https://www.npmjs.com/package/@exalabs/ai-sdk)** - Web search tool that lets AI search the web and get real-time information.\\n- **[@parallel-web/ai-sdk-tools](https://www.npmjs.com/package/@parallel-web/ai-sdk-tools)** - Web search and extract tools powered by Parallel Web API for real-time information and content extraction.\\n- **[Stripe agent tools](https://docs.stripe.com/agents?framework=vercel)** - Tools for interacting with Stripe.\\n- **[StackOne ToolSet](https://docs.stackone.com/agents/typescript/frameworks/vercel-ai-sdk)** - Agentic integrations for hundreds of [enterprise SaaS](https://www.stackone.com/integrations) platforms.\\n- **[agentic](https://docs.agentic.so/marketplace/ts-sdks/ai-sdk)** - A collection of 20+ tools that connect to external APIs such as [Exa](https://exa.ai/) or [E2B](https://e2b.dev/).\\n- **[AWS Bedrock AgentCore](https://github.com/aws/bedrock-agentcore-sdk-typescript)** - Fully managed AI agent services including [**Browser**](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html) (a fast and secure cloud-based browser runtime to enable agents to interact with web applications, fill forms, navigate websites, and extract information) and [**Code Interpreter**](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/built-in-tools.html) (an isolated sandbox environment for agents to execute code in Python, JavaScript, and TypeScript, enhancing accuracy and expanding ability to solve complex end-to-end tasks).\\n- **[Composio](https://docs.composio.dev/providers/vercel)** - 250+ tools like GitHub, Gmail, Salesforce and [more](https://composio.dev/tools).\\n- **[JigsawStack](http://www.jigsawstack.com/docs/integration/vercel)** - Over 30+ small custom fine-tuned models available for specific uses.\\n- **[AI Tools Registry](https://ai-tools-registry.vercel.app)** - A Shadcn-compatible tool definitions and components registry for the AI SDK.\\n- **[Toolhouse](https://docs.toolhouse.ai/toolhouse/toolhouse-sdk/using-vercel-ai)** - AI function-calling in 3 lines of code for over 25 different actions.\\n\\n### MCP Tools\\n\\nThese are pre-built tools available as MCP servers:\\n\\n- **[Smithery](https://smithery.ai/docs/integrations/vercel_ai_sdk)** - An open marketplace of 6,000+ MCPs, including [Browserbase](https://browserbase.com/) and [Exa](https://exa.ai/).\\n- **[Pipedream](https://pipedream.com/docs/connect/mcp/ai-frameworks/vercel-ai-sdk)** - Developer toolkit that lets you easily add 3,000+ integrations to your app or AI agent.\\n- **[Apify](https://docs.apify.com/platform/integrations/vercel-ai-sdk)** - Apify provides a [marketplace](https://apify.com/store) of thousands of tools for web scraping, data extraction, and browser automation.\\n\\n### Tool Building Tutorials\\n\\nThese tutorials and guides help you build your own tools that integrate with specific services:\\n\\n- **[browserbase](https://docs.browserbase.com/integrations/vercel/introduction#vercel-ai-integration)** - Tutorial for building browser tools that run a headless browser.\\n- **[browserless](https://docs.browserless.io/ai-integrations/vercel-ai-sdk)** - Guide for integrating browser automation (self-hosted or cloud-based).\\n- **[AI Tool Maker](https://github.com/nihaocami/ai-tool-maker)** - A CLI utility to generate AI SDK tools from OpenAPI specs.\\n- **[Interlify](https://www.interlify.com/docs/integrate-with-vercel-ai)** - Guide for converting APIs into tools.\\n- **[DeepAgent](https://deepagent.amardeep.space/docs/vercel-ai-sdk)** - A suite of 50+ AI tools and integrations, seamlessly connecting with APIs like Tavily, E2B, Airtable and [more](https://deepagent.amardeep.space/docs).\\n\\n<Note>\\n  Do you have open source tools or tool libraries that are compatible with the\\n  AI SDK? Please [file a pull request](https://github.com/vercel/ai/pulls) to\\n  add them to this list.\\n</Note>\\n\\n## Learn more\\n\\nThe AI SDK Core [Tool Calling](/docs/ai-sdk-core/tools-and-tool-calling)\\nand [Agents](/docs/foundations/agents) documentation has more information about tools and tool calling.\\n', children=[]), DocItem(origPath=Path('02-foundations/05-streaming.mdx'), name='05-streaming.mdx', displayName='05-streaming.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming\\ndescription: Why use streaming for AI applications?\\n---\\n\\n# Streaming\\n\\nStreaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.\\n\\n[Large language models (LLMs)](/docs/foundations/overview#large-language-models) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you\\'re likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by **displaying parts of the response as they become available**.\\n\\n<div className=\"grid lg:grid-cols-2 grid-cols-1 gap-4 mt-8\">\\n  <Card\\n    title=\"Blocking UI\"\\n    description=\"Blocking responses wait until the full response is available before displaying it.\"\\n  >\\n    <BrowserIllustration highlight blocking />\\n  </Card>\\n  <Card\\n    title=\"Streaming UI\"\\n    description=\"Streaming responses can transmit parts of the response as they become available.\"\\n  >\\n    <BrowserIllustration highlight />\\n  </Card>\\n</div>\\n\\n## Real-world Examples\\n\\nHere are 2 examples that illustrate how streaming UIs can improve user experiences in a real-world setting –\\xa0the first uses a blocking UI, while the second uses a streaming UI.\\n\\n### Blocking UI\\n\\n<InlinePrompt\\n  initialInput=\"Come up with the first 200 characters of the first book in the Harry Potter series.\"\\n  blocking\\n/>\\n\\n### Streaming UI\\n\\n<InlinePrompt initialInput=\"Come up with the first 200 characters of the first book in the Harry Potter series.\" />\\n\\nAs you can see, the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.\\n\\nWhile streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren\\'t always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.\\n\\nHowever, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible. In the example below, we stream text generation from OpenAI\\'s `gpt-4.1` in under 10 lines of code using the SDK\\'s [`streamText`](/docs/reference/ai-sdk-core/stream-text) function:\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\n\\nconst { textStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a poem about embedding models.\\',\\n});\\n\\nfor await (const textPart of textStream) {\\n  console.log(textPart);\\n}\\n```\\n\\nFor an introduction to streaming UIs and the AI SDK, check out our [Getting Started guides](/docs/getting-started).\\n', children=[]), DocItem(origPath=Path('02-foundations/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Foundations\\ndescription: A section that covers foundational knowledge around LLMs and concepts crucial to the AI SDK\\n---\\n\\n# Foundations\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description: 'Learn about foundational concepts around AI and LLMs.',\\n      href: '/docs/foundations/overview',\\n    },\\n    {\\n      title: 'Providers and Models',\\n      description:\\n        'Learn about the providers and models that you can use with the AI SDK.',\\n      href: '/docs/foundations/providers-and-models',\\n    },\\n    {\\n      title: 'Prompts',\\n      description:\\n        'Learn about how Prompts are used and defined in the AI SDK.',\\n      href: '/docs/foundations/prompts',\\n    },\\n    {\\n      title: 'Tools',\\n      description: 'Learn about tools in the AI SDK.',\\n      href: '/docs/foundations/tools',\\n    },\\n    {\\n      title: 'Streaming',\\n      description: 'Learn why streaming is used for AI applications.',\\n      href: '/docs/foundations/streaming',\\n    },\\n    {\\n      title: 'Agents',\\n      description: 'Learn how to build agents with the AI SDK.',\\n      href: '/docs/foundations/agents',\\n    },\\n  ]}\\n/>\\n\", children=[])]),\n",
       " DocItem(origPath=Path('02-getting-started'), name='02-getting-started', displayName='02-getting-started', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('02-getting-started/01-navigating-the-library.mdx'), name='01-navigating-the-library.mdx', displayName='01-navigating-the-library.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Navigating the Library\\ndescription: Learn how to navigate the AI SDK.\\n---\\n\\n# Navigating the Library\\n\\nThe AI SDK is a powerful toolkit for building AI applications. This page will help you pick the right tools for your requirements.\\n\\nLet’s start with a quick overview of the AI SDK, which is comprised of three parts:\\n\\n- **[AI SDK Core](/docs/ai-sdk-core/overview):**\\xa0A unified, provider agnostic API for generating text, structured objects, and tool calls with LLMs.\\n- **[AI SDK UI](/docs/ai-sdk-ui/overview):**\\xa0A set of framework-agnostic hooks for building chat and generative user interfaces.\\n- [AI SDK RSC](/docs/ai-sdk-rsc/overview):\\xa0Stream generative user interfaces with React Server Components (RSC). Development is currently experimental and we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview).\\n\\n## Choosing the Right Tool for Your Environment\\n\\nWhen deciding which part of the AI SDK to use, your first consideration should be the environment and existing stack you are working with. Different components of the SDK are tailored to specific frameworks and environments.\\n\\n| Library                                   | Purpose                                                                                                                                                                                                  | Environment Compatibility                                          |\\n| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |\\n| [AI SDK Core](/docs/ai-sdk-core/overview) | Call any LLM with unified API (e.g. [generateText](/docs/reference/ai-sdk-core/generate-text) and [generateObject](/docs/reference/ai-sdk-core/generate-object))                                         | Any JS environment (e.g. Node.js, Deno, Browser)                   |\\n| [AI SDK UI](/docs/ai-sdk-ui/overview)     | Build streaming chat and generative UIs (e.g. [useChat](/docs/reference/ai-sdk-ui/use-chat))                                                                                                             | React & Next.js, Vue & Nuxt, Svelte & SvelteKit                    |\\n| [AI SDK RSC](/docs/ai-sdk-rsc/overview)   | Stream generative UIs from Server to Client (e.g. [streamUI](/docs/reference/ai-sdk-rsc/stream-ui)). Development is currently experimental and we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview). | Any framework that supports React Server Components (e.g. Next.js) |\\n\\n## Environment Compatibility\\n\\nThese tools have been designed to work seamlessly with each other and it\\'s likely that you will be using them together. Let\\'s look at how you could decide which libraries to use based on your application environment, existing stack, and requirements.\\n\\nThe following table outlines AI SDK compatibility based on environment:\\n\\n| Environment           | [AI SDK Core](/docs/ai-sdk-core/overview) | [AI SDK UI](/docs/ai-sdk-ui/overview) | [AI SDK RSC](/docs/ai-sdk-rsc/overview) |\\n| --------------------- | ----------------------------------------- | ------------------------------------- | --------------------------------------- |\\n| None / Node.js / Deno | <Check size={18} />                       | <Cross size={18} />                   | <Cross size={18} />                     |\\n| Vue / Nuxt            | <Check size={18} />                       | <Check size={18} />                   | <Cross size={18} />                     |\\n| Svelte / SvelteKit    | <Check size={18} />                       | <Check size={18} />                   | <Cross size={18} />                     |\\n| Next.js Pages Router  | <Check size={18} />                       | <Check size={18} />                   | <Cross size={18} />                     |\\n| Next.js App Router    | <Check size={18} />                       | <Check size={18} />                   | <Check size={18} />                     |\\n\\n## When to use AI SDK UI\\n\\nAI SDK UI provides a set of framework-agnostic hooks for quickly building **production-ready AI-native applications**. It offers:\\n\\n- Full support for streaming chat and client-side generative UI\\n- Utilities for handling common AI interaction patterns (i.e. chat, completion, assistant)\\n- Production-tested reliability and performance\\n- Compatibility across popular frameworks\\n\\n## AI SDK UI Framework Compatibility\\n\\nAI SDK UI supports the following frameworks:\\xa0[React](https://react.dev/),\\xa0[Svelte](https://svelte.dev/),\\xa0and [Vue.js](https://vuejs.org/). Here is a comparison of the supported functions across these frameworks:\\n\\n| Function                                                   | React               | Svelte              | Vue.js              |\\n| ---------------------------------------------------------- | ------------------- | ------------------- | ------------------- |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)              | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat) tool calling | <Check size={18} /> | <Check size={18} /> | <Cross size={18} /> |\\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion)  | <Check size={18} /> | <Check size={18} /> | <Check size={18} /> |\\n| [useObject](/docs/reference/ai-sdk-ui/use-object)          | <Check size={18} /> | <Cross size={18} /> | <Cross size={18} /> |\\n\\n<Note>\\n  [Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\\n  welcome to implement missing features for non-React frameworks.\\n</Note>\\n\\n## When to use AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components)\\n(RSCs) provide a new approach to building React applications that allow components\\nto render on the server, fetch data directly, and stream the results to the client,\\nreducing bundle size and improving performance. They also introduce a new way to\\ncall server-side functions from anywhere in your application called [Server Actions](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations).\\n\\nAI SDK RSC provides a number of utilities that allow you to stream values and UI directly from the server to the client. However, **it\\'s important to be aware of current limitations**:\\n\\n- **Cancellation**: currently, it is not possible to abort a stream using Server Actions. This will be improved in future releases of React and Next.js.\\n- **Increased Data Transfer**: using [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) can lead to quadratic data transfer (quadratic to the length of generated text). You can avoid this using [ `createStreamableValue` ](/docs/reference/ai-sdk-rsc/create-streamable-value) instead, and rendering the component client-side.\\n- **Re-mounting Issue During Streaming**: when using `createStreamableUI`, components re-mount on `.done()`, causing [flickering](https://github.com/vercel/ai/issues/2232).\\n\\nGiven these limitations, **we recommend using [AI SDK UI](/docs/ai-sdk-ui/overview) for production applications**.\\n', children=[]), DocItem(origPath=Path('02-getting-started/02-nextjs-app-router.mdx'), name='02-nextjs-app-router.mdx', displayName='02-nextjs-app-router.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Next.js App Router\\ndescription: Learn how to build your first agent with the AI SDK and Next.js App Router.\\n---\\n\\n# Next.js App Router Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Create Your Application\\n\\nStart by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.\\n\\n<div className=\"mb-4\">\\n  <Note>\\n    Be sure to select yes when prompted to use the App Router and Tailwind CSS.\\n    If you are looking for the Next.js Pages Router quickstart guide, you can\\n    find it [here](/docs/getting-started/nextjs-pages-router).\\n  </Note>\\n</div>\\n\\n<Snippet text=\"pnpm create next-app@latest my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK\\'s React hooks. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable.\\n</Note>\\n\\n## Create a Route Handler\\n\\nCreate a route handler, `app/api/chat/route.ts` and add the following code:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n2. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n3. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\\n4. Finally, return the result to the client to stream the response.\\n\\nThis Route Handler creates a POST request endpoint at `/api/chat`.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have a Route Handler that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`app/page.tsx`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n<Note>\\n  Make sure you add the `\"use client\"` directive to the top of your file. This\\n  allows you to add interactivity with Javascript.\\n</Note>\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:3000. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your Route Handler\\n\\nModify your `app/api/chat/route.ts` file to include the new weather tool:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"2,13-27\"\\nimport { streamText, UIMessage, convertToModelMessages, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocation in your UI, update your `app/page.tsx` file:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"16-21\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your Route Handler\\n\\nModify your `app/api/chat/route.ts` file to include the `stopWhen` condition:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You set `stopWhen` to be when `stepCountIs` 5, allowing the model to use up to 5 \"steps\" for any given generation.\\n2. You add an `onStepFinish` callback to log any `toolResults` from each step of the interaction, helping you understand the model\\'s tool usage. This means we can also delete the `toolCall` and `toolResult` `console.log` statements from the previous example.\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\\n\\n### Add another tool\\n\\nUpdate your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"34-47\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Update Your Frontend\\n\\nupdate your `app/page.tsx` file to render the new temperature conversion tool:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"21\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n              case \\'tool-convertFahrenheitToCelsius\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/03-nextjs-pages-router.mdx'), name='03-nextjs-pages-router.mdx', displayName='03-nextjs-pages-router.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Next.js Pages Router\\ndescription: Learn how to build your first agent with the AI SDK and Next.js Pages Router.\\n---\\n\\n# Next.js Pages Router Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the AI SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Setup Your Application\\n\\nStart by creating a new Next.js application. This command will create a new directory named `my-ai-app` and set up a basic Next.js application inside it.\\n\\n<Note>\\n  Be sure to select no when prompted to use the App Router. If you are looking\\n  for the Next.js App Router quickstart guide, you can find it\\n  [here](/docs/getting-started/nextjs-app-router).\\n</Note>\\n\\n<Snippet text=\"pnpm create next-app@latest my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK\\'s React hooks. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/react@beta zod@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable.\\n</Note>\\n\\n## Create a Route Handler\\n\\n<Note>\\n  As long as you are on Next.js 13+, you can use Route Handlers (using the App\\n  Router) alongside the Pages Router. This is recommended to enable you to use\\n  the Web APIs interface/signature and to better support streaming.\\n</Note>\\n\\nCreate a Route Handler (`app/api/chat/route.ts`) and add the following code:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n2. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n3. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\\n4. Finally, return the result to the client to stream the response.\\n\\nThis Route Handler creates a POST request endpoint at `/api/chat`.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui) package abstract the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`pages/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```tsx filename=\"pages/index.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:3000. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Next.js.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\n### Update Your Route Handler\\n\\nLet\\'s start by giving your chatbot a weather tool. Update your Route Handler (`app/api/chat/route.ts`):\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { streamText, UIMessage, convertToModelMessages, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocations in your UI, update your `pages/index.tsx` file:\\n\\n```tsx filename=\"pages/index.tsx\" highlight=\"16-21\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your Route Handler\\n\\nModify your `app/api/chat/route.ts` file to include the `stopWhen` condition:\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\\n\\n### Add another tool\\n\\nUpdate your `app/api/chat/route.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"26-39\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Update Your Frontend\\n\\nUpdate your `pages/index.tsx` file to render the new temperature conversion tool:\\n\\n```tsx filename=\"pages/index.tsx\" highlight=\"21\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n              case \\'tool-weather\\':\\n              case \\'tool-convertFahrenheitToCelsius\\':\\n                return (\\n                  <pre key={`${message.id}-${i}`}>\\n                    {JSON.stringify(part, null, 2)}\\n                  </pre>\\n                );\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/04-svelte.mdx'), name='04-svelte.mdx', displayName='04-svelte.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Svelte\\ndescription: Learn how to build your first agent with the AI SDK and Svelte.\\n---\\n\\n# Svelte Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Set Up Your Application\\n\\nStart by creating a new SvelteKit application. This command will create a new directory named `my-ai-app` and set up a basic SvelteKit application inside it.\\n\\n<Snippet text=\"npx sv create my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install Dependencies\\n\\nInstall `ai` and `@ai-sdk/svelte`, the AI package and AI SDK\\'s Svelte bindings. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add -D ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install -D ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add -D ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"bun add -d ai@beta @ai-sdk/svelte@beta zod\" dark />\\n    </Tab>\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable. Vite does not automatically load\\n  environment variables onto `process.env`, so you\\'ll need to import\\n  `AI_GATEWAY_API_KEY` from `$env/static/private` in your code (see below).\\n</Note>\\n\\n## Create an API route\\n\\nCreate a SvelteKit Endpoint, `src/routes/api/chat/+server.ts` and add the following code:\\n\\n```tsx filename=\"src/routes/api/chat/+server.ts\"\\nimport {\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  createGateway,\\n} from \\'ai\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n<Note>\\n  If you see type errors with `AI_GATEWAY_API_KEY` or your `POST` function, run\\n  the dev server.\\n</Note>\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Create a gateway provider instance with the `createGateway` function from the `ai` package.\\n2. Define a `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n3. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n4. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-data-stream-response) function which converts the result to a streamed response object.\\n5. Return the result to the client to stream the response.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to set up your frontend. The AI SDK\\'s [UI](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one class, `Chat`.\\nIts properties and API are largely the same as React\\'s [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`src/routes/+page.svelte`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```svelte filename=\"src/routes/+page.svelte\"\\n<script lang=\"ts\">\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let input = \\'\\';\\n  const chat = new Chat({});\\n\\n  function handleSubmit(event: SubmitEvent) {\\n    event.preventDefault();\\n    chat.sendMessage({ text: input });\\n    input = \\'\\';\\n  }\\n</script>\\n\\n<main>\\n  <ul>\\n    {#each chat.messages as message, messageIndex (messageIndex)}\\n      <li>\\n        <div>{message.role}</div>\\n        <div>\\n          {#each message.parts as part, partIndex (partIndex)}\\n            {#if part.type === \\'text\\'}\\n              <div>{part.text}</div>\\n            {/if}\\n          {/each}\\n        </div>\\n      </li>\\n    {/each}\\n  </ul>\\n  <form onsubmit={handleSubmit}>\\n    <input bind:value={input} />\\n    <button type=\"submit\">Send</button>\\n  </form>\\n</main>\\n```\\n\\nThis page utilizes the `Chat` class, which will, by default, use the `POST` route handler you created earlier. The class provides functions and state for handling user input and form submission. The `Chat` class provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:5173. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Svelte.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your API Route\\n\\nModify your `src/routes/api/chat/+server.ts` file to include the new weather tool:\\n\\n```tsx filename=\"src/routes/api/chat/+server.ts\" highlight=\"2,3,17-31\"\\nimport {\\n  createGateway,\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocation in your UI, update your `src/routes/+page.svelte` file:\\n\\n```svelte filename=\"src/routes/+page.svelte\"\\n<script lang=\"ts\">\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let input = \\'\\';\\n  const chat = new Chat({});\\n\\n  function handleSubmit(event: SubmitEvent) {\\n    event.preventDefault();\\n    chat.sendMessage({ text: input });\\n    input = \\'\\';\\n  }\\n</script>\\n\\n<main>\\n  <ul>\\n    {#each chat.messages as message, messageIndex (messageIndex)}\\n      <li>\\n        <div>{message.role}</div>\\n        <div>\\n          {#each message.parts as part, partIndex (partIndex)}\\n            {#if part.type === \\'text\\'}\\n              <div>{part.text}</div>\\n            {:else if part.type === \\'tool-weather\\'}\\n              <pre>{JSON.stringify(part, null, 2)}</pre>\\n            {/if}\\n          {/each}\\n        </div>\\n      </li>\\n    {/each}\\n  </ul>\\n  <form onsubmit={handleSubmit}>\\n    <input bind:value={input} />\\n    <button type=\"submit\">Send</button>\\n  </form>\\n</main>\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your API Route\\n\\nModify your `src/routes/api/chat/+server.ts` file to include the `stopWhen` condition:\\n\\n```ts filename=\"src/routes/api/chat/+server.ts\" highlight=\"15\"\\nimport {\\n  createGateway,\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\\n\\n### Add another tool\\n\\nUpdate your `src/routes/api/chat/+server.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"src/routes/api/chat/+server.ts\" highlight=\"32-45\"\\nimport {\\n  createGateway,\\n  streamText,\\n  type UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nimport { AI_GATEWAY_API_KEY } from \\'$env/static/private\\';\\n\\nconst gateway = createGateway({\\n  apiKey: AI_GATEWAY_API_KEY,\\n});\\n\\nexport async function POST({ request }) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Update Your Frontend\\n\\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\\n\\n```svelte filename=\"src/routes/+page.svelte\" highlight=\"17\"\\n<script lang=\"ts\">\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let input = \\'\\';\\n  const chat = new Chat({});\\n\\n  function handleSubmit(event: SubmitEvent) {\\n    event.preventDefault();\\n    chat.sendMessage({ text: input });\\n    input = \\'\\';\\n  }\\n</script>\\n\\n<main>\\n  <ul>\\n    {#each chat.messages as message, messageIndex (messageIndex)}\\n      <li>\\n        <div>{message.role}</div>\\n        <div>\\n          {#each message.parts as part, partIndex (partIndex)}\\n            {#if part.type === \\'text\\'}\\n              <div>{part.text}</div>\\n            {:else if part.type === \\'tool-weather\\' || part.type === \\'tool-convertFahrenheitToCelsius\\'}\\n              <pre>{JSON.stringify(part, null, 2)}</pre>\\n            {/if}\\n          {/each}\\n        </div>\\n      </li>\\n    {/each}\\n  </ul>\\n  <form onsubmit={handleSubmit}>\\n    <input bind:value={input} />\\n    <button type=\"submit\">Send</button>\\n  </form>\\n</main>\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## How does `@ai-sdk/svelte` differ from `@ai-sdk/react`?\\n\\nThe surface-level difference is that Svelte uses classes to manage state, whereas React uses hooks, so `useChat` in React is `Chat` in Svelte. Other than that, there are a few things to keep in mind:\\n\\n### 1. Arguments to classes aren\\'t reactive by default\\n\\nUnlike in React, where hooks are rerun any time their containing component is invalidated, code in the `script` block of a Svelte component is only run once when the component is created.\\nThis means that, if you want arguments to your class to be reactive, you need to make sure you pass a _reference_ into the class, rather than a value:\\n\\n```svelte\\n<script>\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  let { id } = $props();\\n\\n  // won\\'t work; the class instance will be created once, `id` will be copied by value, and won\\'t update when $props.id changes\\n  let chat = new Chat({ id });\\n\\n  // will work; passes `id` by reference, so `Chat` always has the latest value\\n  let chat = new Chat({\\n    get id() {\\n      return id;\\n    },\\n  });\\n</script>\\n```\\n\\nKeep in mind that this normally doesn\\'t matter; most parameters you\\'ll pass into the Chat class are static (for example, you typically wouldn\\'t expect your `onError` handler to change).\\n\\n### 2. You can\\'t destructure class properties\\n\\nIn vanilla JavaScript, destructuring class properties copies them by value and \"disconnects\" them from their class instance:\\n\\n```js\\nconst classInstance = new Whatever();\\nclassInstance.foo = \\'bar\\';\\nconst { foo } = classInstance;\\nclassInstance.foo = \\'baz\\';\\n\\nconsole.log(foo); // \\'bar\\'\\n```\\n\\nThe same is true of classes in Svelte:\\n\\n```svelte\\n<script>\\n  import { Chat } from \\'@ai-sdk/svelte\\';\\n\\n  const chat = new Chat({});\\n  let { messages } = chat;\\n\\n  chat.append({ content: \\'Hello, world!\\', role: \\'user\\' }).then(() => {\\n    console.log(messages); // []\\n    console.log(chat.messages); // [{ content: \\'Hello, world!\\', role: \\'user\\' }] (plus some other stuff)\\n  });\\n</script>\\n```\\n\\n### 3. Instance synchronization requires context\\n\\nIn React, hook instances with the same `id` are synchronized -- so two instances of `useChat` will have the same `messages`, `status`, etc. if they have the same `id`.\\nFor most use cases, you probably don\\'t need this behavior -- but if you do, you can create a context in your root layout file using `createAIContext`:\\n\\n```svelte\\n<script>\\n  import { createAIContext } from \\'@ai-sdk/svelte\\';\\n\\n  let { children } = $props();\\n\\n  createAIContext();\\n  // all hooks created after this or in components that are children of this component\\n  // will have synchronized state\\n</script>\\n\\n{@render children()}\\n```\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n- To learn more about Svelte, check out the [official documentation](https://svelte.dev/docs/svelte).\\n', children=[]), DocItem(origPath=Path('02-getting-started/05-nuxt.mdx'), name='05-nuxt.mdx', displayName='05-nuxt.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Vue.js (Nuxt)\\ndescription: Learn how to build your first agent with the AI SDK and Vue.js (Nuxt).\\n---\\n\\n# Vue.js (Nuxt) Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Setup Your Application\\n\\nStart by creating a new Nuxt application. This command will create a new directory named `my-ai-app` and set up a basic Nuxt application inside it.\\n\\n<Snippet text=\"pnpm create nuxt my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/vue`. The Vercel AI Gateway provider ships with the `ai` package.\\n\\n<Note>\\n  The AI SDK is designed to be a unified interface to interact with any large\\n  language model. This means that you can change model and providers with just\\n  one line of code! Learn more about [available providers](/providers) and\\n  [building custom providers](/providers/community-providers/custom-providers)\\n  in the [providers](/providers) section.\\n</Note>\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/vue@beta zod\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n### Configure Vercel AI Gateway API key\\n\\nCreate a `.env` file in your project root and add your Vercel AI Gateway API Key. This key is used to authenticate your application with the Vercel AI Gateway service.\\n\\n<Snippet text=\"touch .env\" />\\n\\nEdit the `.env` file:\\n\\n```env filename=\".env\"\\nNUXT_AI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key and configure the environment variable in `nuxt.config.ts`:\\n\\n```ts filename=\"nuxt.config.ts\"\\nexport default defineNuxtConfig({\\n  // rest of your nuxt config\\n  runtimeConfig: {\\n    aiGatewayApiKey: \\'\\',\\n  },\\n});\\n```\\n\\n<Note className=\"mb-4\">\\n  This guide uses Nuxt\\'s runtime config to manage the API key. The `NUXT_`\\n  prefix in the environment variable allows Nuxt to automatically load it into\\n  the runtime config. While the AI Gateway Provider also supports a default\\n  `AI_GATEWAY_API_KEY` environment variable, this approach provides better\\n  integration with Nuxt\\'s configuration system.\\n</Note>\\n\\n## Create an API route\\n\\nCreate an API route, `server/api/chat.ts` and add the following code:\\n\\n```typescript filename=\"server/api/chat.ts\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  createGateway,\\n} from \\'ai\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Create a gateway provider instance with the `createGateway` function from the `ai` package.\\n2. Define an Event Handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation. The `messages` are of UIMessage type, which are designed for use in application UI - they contain the entire message history and associated metadata like timestamps.\\n3. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (defined in step 1) and `messages` (defined in step 2). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour. The `messages` key expects a `ModelMessage[]` array. This type is different from `UIMessage` in that it does not include metadata, such as timestamps or sender information. To convert between these types, we use the `convertToModelMessages` function, which strips the UI-specific metadata and transforms the `UIMessage[]` array into the `ModelMessage[]` format that the model expects.\\n4. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\\n5. Return the result to the client to stream the response.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui/overview) package abstract the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`pages/index.vue`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```typescript filename=\"pages/index.vue\"\\n<script setup lang=\"ts\">\\nimport { Chat } from \"@ai-sdk/vue\";\\nimport { ref } from \"vue\";\\n\\nconst input = ref(\"\");\\nconst chat = new Chat({});\\n\\nconst handleSubmit = (e: Event) => {\\n    e.preventDefault();\\n    chat.sendMessage({ text: input.value });\\n    input.value = \"\";\\n};\\n</script>\\n\\n<template>\\n    <div>\\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\\n            <div\\n                v-for=\"(part, index) in m.parts\"\\n                :key=\"`${m.id}-${part.type}-${index}`\"\\n            >\\n                <div v-if=\"part.type === \\'text\\'\">{{ part.text }}</div>\\n            </div>\\n        </div>\\n\\n        <form @submit=\"handleSubmit\">\\n            <input v-model=\"input\" placeholder=\"Say something...\" />\\n        </form>\\n    </div>\\n</template>\\n```\\n\\n<Note>\\n  If your project has `app.vue` instead of `pages/index.vue`, delete the\\n  `app.vue` file and create a new `pages/index.vue` file with the code above.\\n</Note>\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`ref`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm run dev\" />\\n\\nHead to your browser and open http://localhost:3000. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Nuxt.\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your API Route\\n\\nModify your `server/api/chat.ts` file to include the new weather tool:\\n\\n```typescript filename=\"server/api/chat.ts\" highlight=\"1,16-32\"\\nimport {\\n  createGateway,\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the tool invocation in your UI, update your `pages/index.vue` file:\\n\\n```typescript filename=\"pages/index.vue\" highlight=\"16-18\"\\n<script setup lang=\"ts\">\\nimport { Chat } from \"@ai-sdk/vue\";\\nimport { ref } from \"vue\";\\n\\nconst input = ref(\"\");\\nconst chat = new Chat({});\\n\\nconst handleSubmit = (e: Event) => {\\n    e.preventDefault();\\n    chat.sendMessage({ text: input.value });\\n    input.value = \"\";\\n};\\n</script>\\n\\n<template>\\n    <div>\\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\\n            <div\\n                v-for=\"(part, index) in m.parts\"\\n                :key=\"`${m.id}-${part.type}-${index}`\"\\n            >\\n                <div v-if=\"part.type === \\'text\\'\">{{ part.text }}</div>\\n                <pre v-if=\"part.type === \\'tool-weather\\'\">{{ JSON.stringify(part, null, 2) }}</pre>\\n            </div>\\n        </div>\\n\\n        <form @submit=\"handleSubmit\">\\n            <input v-model=\"input\" placeholder=\"Say something...\" />\\n        </form>\\n    </div>\\n</template>\\n```\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool is now visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your API Route\\n\\nModify your `server/api/chat.ts` file to include the `stopWhen` condition:\\n\\n```typescript filename=\"server/api/chat.ts\" highlight=\"22\"\\nimport {\\n  createGateway,\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n      stopWhen: stepCountIs(5),\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\nHead back to the browser and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\\n\\n### Add another tool\\n\\nUpdate your `server/api/chat.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```typescript filename=\"server/api/chat.ts\" highlight=\"32-45\"\\nimport {\\n  createGateway,\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport default defineLazyEventHandler(async () => {\\n  const apiKey = useRuntimeConfig().aiGatewayApiKey;\\n  if (!apiKey) throw new Error(\\'Missing AI Gateway API key\\');\\n  const gateway = createGateway({\\n    apiKey: apiKey,\\n  });\\n\\n  return defineEventHandler(async (event: any) => {\\n    const { messages }: { messages: UIMessage[] } = await readBody(event);\\n\\n    const result = streamText({\\n      model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n      messages: convertToModelMessages(messages),\\n      stopWhen: stepCountIs(5),\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n        convertFahrenheitToCelsius: tool({\\n          description: \\'Convert a temperature in fahrenheit to celsius\\',\\n          inputSchema: z.object({\\n            temperature: z\\n              .number()\\n              .describe(\\'The temperature in fahrenheit to convert\\'),\\n          }),\\n          execute: async ({ temperature }) => {\\n            const celsius = Math.round((temperature - 32) * (5 / 9));\\n            return {\\n              celsius,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    return result.toUIMessageStreamResponse();\\n  });\\n});\\n```\\n\\n### Update Your Frontend\\n\\nUpdate your UI to handle the new temperature conversion tool by modifying the tool part handling:\\n\\n```typescript filename=\"pages/index.vue\" highlight=\"24\"\\n<script setup lang=\"ts\">\\nimport { Chat } from \"@ai-sdk/vue\";\\nimport { ref } from \"vue\";\\n\\nconst input = ref(\"\");\\nconst chat = new Chat({});\\n\\nconst handleSubmit = (e: Event) => {\\n    e.preventDefault();\\n    chat.sendMessage({ text: input.value });\\n    input.value = \"\";\\n};\\n</script>\\n\\n<template>\\n    <div>\\n        <div v-for=\"(m, index) in chat.messages\" :key=\"m.id ? m.id : index\">\\n            {{ m.role === \"user\" ? \"User: \" : \"AI: \" }}\\n            <div\\n                v-for=\"(part, index) in m.parts\"\\n                :key=\"`${m.id}-${part.type}-${index}`\"\\n            >\\n                <div v-if=\"part.type === \\'text\\'\">{{ part.text }}</div>\\n                <pre\\n                    v-if=\"\\n                        part.type === \\'tool-weather\\' ||\\n                        part.type === \\'tool-convertFahrenheitToCelsius\\'\\n                    \"\\n                    >{{ JSON.stringify(part, null, 2) }}</pre\\n                >\\n            </div>\\n        </div>\\n\\n        <form @submit=\"handleSubmit\">\\n            <input v-model=\"input\" placeholder=\"Say something...\" />\\n        </form>\\n    </div>\\n</template>\\n```\\n\\nThis update handles the new `tool-convertFahrenheitToCelsius` part type, displaying the temperature conversion tool calls and results in the UI.\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool output displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/06-nodejs.mdx'), name='06-nodejs.mdx', displayName='06-nodejs.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Node.js\\ndescription: Learn how to build your first agent with the AI SDK and Node.js.\\n---\\n\\n# Node.js Quickstart\\n\\nThe AI SDK is a powerful Typescript library designed to help developers build AI-powered applications.\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface. Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Setup Your Application\\n\\nStart by creating a new directory using the `mkdir` command. Change into your new directory and then run the `pnpm init` command. This will create a `package.json` in your new directory.\\n\\n```bash\\nmkdir my-ai-app\\ncd my-ai-app\\npnpm init\\n```\\n\\n### Install Dependencies\\n\\nInstall `ai`, the AI SDK, along with other necessary dependencies.\\n\\n<Note>\\n  The AI SDK is designed to be a unified interface to interact with any large\\n  language model. This means that you can change model and providers with just\\n  one line of code! Learn more about [available providers](/providers) and\\n  [building custom providers](/providers/community-providers/custom-providers)\\n  in the [providers](/providers) section.\\n</Note>\\n\\n```bash\\npnpm add ai@beta zod dotenv\\npnpm add -D @types/node tsx typescript\\n```\\n\\nThe `ai` package contains the AI SDK. You will use `zod` to define type-safe schemas that you will pass to the large language model (LLM). You will use `dotenv` to access environment variables (your Vercel AI Gateway key) within your application. There are also three development dependencies, installed with the `-D` flag, that are necessary to run your Typescript code.\\n\\n### Configure Vercel AI Gateway API key\\n\\nCreate a `.env` file in your project\\'s root directory and add your Vercel AI Gateway API Key. This key is used to authenticate your application with the Vercel AI Gateway service.\\n\\n<Snippet text=\"touch .env\" />\\n\\nEdit the `.env` file:\\n\\n```env filename=\".env\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK will use the `AI_GATEWAY_API_KEY` environment variable to\\n  authenticate with Vercel AI Gateway.\\n</Note>\\n\\n## Create Your Application\\n\\nCreate an `index.ts` file in the root of your project and add the following code:\\n\\n```ts filename=\"index.ts\"\\nimport { ModelMessage, streamText } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Set up a readline interface to take input from the terminal, enabling interactive sessions directly from the command line.\\n2. Initialize an array called `messages` to store the history of your conversation. This history allows the agent to maintain context in ongoing dialogues.\\n3. In the `main` function:\\n\\n- Prompt for and capture user input, storing it in `userInput`.\\n- Add user input to the `messages` array as a user message.\\n- Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider and `messages`.\\n- Iterate over the text stream returned by the `streamText` function (`result.textStream`) and print the contents of the stream to the terminal.\\n- Add the assistant\\'s response to the `messages` array.\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your agent! To start your application, use the command:\\n\\n<Snippet text=\"pnpm tsx index.ts\" />\\n\\nYou should see a prompt in your terminal. Test it out by entering a message and see the AI agent respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Node.js.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n## Enhance Your Agent with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the agent would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your agent by adding a simple weather tool.\\n\\n### Update Your Application\\n\\nModify your `index.ts` file to include the new weather tool:\\n\\n```ts filename=\"index.ts\" highlight=\"2,4,24-37\"\\nimport { ModelMessage, streamText, tool } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the agent understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The agent will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your agent can \"fetch\" weather information for any location the user asks about. When the agent determines it needs to use the weather tool, it will generate a tool call with the necessary parameters. The `execute` function will then be automatically run, and the results will be used by the agent to generate its response.\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the agent uses the new tool.\\n\\nNotice the blank \"assistant\" response? This is because instead of generating a text response, the agent generated a tool call. You can access the tool call and subsequent tool result in the `toolCall` and `toolResult` keys of the result object.\\n\\n```typescript highlight=\"46-47\"\\nimport { ModelMessage, streamText, tool } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    console.log(await result.toolCalls);\\n    console.log(await result.toolResults);\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool results are visible in the chat interface, the agent isn\\'t using this information to answer your original query. This is because once the agent generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. This feature will automatically send tool results back to the agent to trigger an additional generation until the stopping condition you define is met. In this case, you want the agent to answer your question using the results from the weather tool.\\n\\n### Update Your Application\\n\\nModify your `index.ts` file to configure stopping conditions with `stopWhen`:\\n\\n```ts filename=\"index.ts\" highlight=\"38-41\"\\nimport { ModelMessage, streamText, tool, stepCountIs } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n      },\\n      stopWhen: stepCountIs(5),\\n      onStepFinish: async ({ toolResults }) => {\\n        if (toolResults.length) {\\n          console.log(JSON.stringify(toolResults, null, 2));\\n        }\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nIn this updated code:\\n\\n1. You set `stopWhen` to be when `stepCountIs` 5, allowing the agent to use up to 5 \"steps\" for any given generation.\\n2. You add an `onStepFinish` callback to log any `toolResults` from each step of the interaction, helping you understand the agent\\'s tool usage. This means we can also delete the `toolCall` and `toolResult` `console.log` statements from the previous example.\\n\\nNow, when you ask about the weather in a location, you should see the agent using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the agent to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the agent to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Celsius to Fahrenheit.\\n\\n### Adding a second tool\\n\\nUpdate your `index.ts` file to add a new tool to convert the temperature from Celsius to Fahrenheit:\\n\\n```ts filename=\"index.ts\" highlight=\"37-48\"\\nimport { ModelMessage, streamText, tool, stepCountIs } from \\'ai\\';\\nimport \\'dotenv/config\\';\\nimport { z } from \\'zod\\';\\nimport * as readline from \\'node:readline/promises\\';\\n\\nconst terminal = readline.createInterface({\\n  input: process.stdin,\\n  output: process.stdout,\\n});\\n\\nconst messages: ModelMessage[] = [];\\n\\nasync function main() {\\n  while (true) {\\n    const userInput = await terminal.question(\\'You: \\');\\n\\n    messages.push({ role: \\'user\\', content: userInput });\\n\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      messages,\\n      tools: {\\n        weather: tool({\\n          description: \\'Get the weather in a location (fahrenheit)\\',\\n          inputSchema: z.object({\\n            location: z\\n              .string()\\n              .describe(\\'The location to get the weather for\\'),\\n          }),\\n          execute: async ({ location }) => {\\n            const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n            return {\\n              location,\\n              temperature,\\n            };\\n          },\\n        }),\\n        convertFahrenheitToCelsius: tool({\\n          description: \\'Convert a temperature in fahrenheit to celsius\\',\\n          inputSchema: z.object({\\n            temperature: z\\n              .number()\\n              .describe(\\'The temperature in fahrenheit to convert\\'),\\n          }),\\n          execute: async ({ temperature }) => {\\n            const celsius = Math.round((temperature - 32) * (5 / 9));\\n            return {\\n              celsius,\\n            };\\n          },\\n        }),\\n      },\\n      stopWhen: stepCountIs(5),\\n      onStepFinish: async ({ toolResults }) => {\\n        if (toolResults.length) {\\n          console.log(JSON.stringify(toolResults, null, 2));\\n        }\\n      },\\n    });\\n\\n    let fullResponse = \\'\\';\\n    process.stdout.write(\\'\\\\nAssistant: \\');\\n    for await (const delta of result.textStream) {\\n      fullResponse += delta;\\n      process.stdout.write(delta);\\n    }\\n    process.stdout.write(\\'\\\\n\\\\n\\');\\n\\n    messages.push({ role: \\'assistant\\', content: fullResponse });\\n  }\\n}\\n\\nmain().catch(console.error);\\n```\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The agent will call the weather tool for New York.\\n2. You\\'ll see the tool result logged.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The agent will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the agent to gather information and use it to provide more accurate and contextual responses, making your agent considerably more useful.\\n\\nThis example demonstrates how tools can expand your agent\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the agent to access and process real-world data in real-time and perform actions that interact with the outside world. Tools bridge the gap between the agent\\'s knowledge cutoff and current information, while also enabling it to take meaningful actions beyond just generating text responses.\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI agent using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/07-expo.mdx'), name='07-expo.mdx', displayName='07-expo.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Expo\\ndescription: Learn how to build your first agent with the AI SDK and Expo.\\n---\\n\\n# Expo Quickstart\\n\\nIn this quickstart tutorial, you\\'ll build a simple agent with a streaming chat user interface with [Expo](https://expo.dev/). Along the way, you\\'ll learn key concepts and techniques that are fundamental to using the SDK in your own projects.\\n\\nIf you are unfamiliar with the concepts of [Prompt Engineering](/docs/advanced/prompt-engineering) and [HTTP Streaming](/docs/advanced/why-streaming), you can optionally read these documents first.\\n\\n## Prerequisites\\n\\nTo follow this quickstart, you\\'ll need:\\n\\n- Node.js 18+ and pnpm installed on your local development machine.\\n- A [ Vercel AI Gateway ](https://vercel.com/ai-gateway) API key.\\n\\nIf you haven\\'t obtained your Vercel AI Gateway API key, you can do so by [signing up](https://vercel.com/d?to=%2F%5Bteam%5D%2F%7E%2Fai&title=Go+to+AI+Gateway) on the Vercel website.\\n\\n## Create Your Application\\n\\nStart by creating a new Expo application. This command will create a new directory named `my-ai-app` and set up a basic Expo application inside it.\\n\\n<Snippet text=\"pnpm create expo-app@latest my-ai-app\" />\\n\\nNavigate to the newly created directory:\\n\\n<Snippet text=\"cd my-ai-app\" />\\n\\n<Note>This guide requires Expo 52 or higher.</Note>\\n\\n### Install dependencies\\n\\nInstall `ai` and `@ai-sdk/react`, the AI package and AI SDK\\'s React hooks. The AI SDK\\'s [ Vercel AI Gateway provider ](/providers/ai-sdk-providers/ai-gateway) ships with the `ai` package. You\\'ll also install `zod`, a schema validation library used for defining tool inputs.\\n\\n<Note>\\n  This guide uses the Vercel AI Gateway provider so you can access hundreds of\\n  models from different providers with one API key, but you can switch to any\\n  provider or model by installing its package. Check out available [AI SDK\\n  providers](/providers/ai-sdk-providers) for more information.\\n</Note>\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"bun add ai@beta @ai-sdk/react@beta zod\" dark />\\n    </Tab>\\n  </Tabs>\\n</div>\\n\\n### Configure your AI Gateway API key\\n\\nCreate a `.env.local` file in your project root and add your AI Gateway API key. This key authenticates your application with the Vercel AI Gateway.\\n\\n<Snippet text=\"touch .env.local\" />\\n\\nEdit the `.env.local` file:\\n\\n```env filename=\".env.local\"\\nAI_GATEWAY_API_KEY=xxxxxxxxx\\n```\\n\\nReplace `xxxxxxxxx` with your actual Vercel AI Gateway API key.\\n\\n<Note className=\"mb-4\">\\n  The AI SDK\\'s Vercel AI Gateway Provider will default to using the\\n  `AI_GATEWAY_API_KEY` environment variable.\\n</Note>\\n\\n## Create an API Route\\n\\nCreate a route handler, `app/api/chat+api.ts` and add the following code:\\n\\n```tsx filename=\"app/api/chat+api.ts\"\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\nLet\\'s take a look at what is happening in this code:\\n\\n1. Define an asynchronous `POST` request handler and extract `messages` from the body of the request. The `messages` variable contains a history of the conversation between you and the chatbot and provides the chatbot with the necessary context to make the next generation.\\n2. Call [`streamText`](/docs/reference/ai-sdk-core/stream-text), which is imported from the `ai` package. This function accepts a configuration object that contains a `model` provider (imported from `ai`) and `messages` (defined in step 1). You can pass additional [settings](/docs/ai-sdk-core/settings) to further customise the model\\'s behaviour.\\n3. The `streamText` function returns a [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text#result-object). This result object contains the [ `toUIMessageStreamResponse` ](/docs/reference/ai-sdk-core/stream-text#to-ui-message-stream-response) function which converts the result to a streamed response object.\\n4. Finally, return the result to the client to stream the response.\\n\\nThis API route creates a POST request endpoint at `/api/chat`.\\n\\n## Choosing a Provider\\n\\nThe AI SDK supports dozens of model providers through [first-party](/providers/ai-sdk-providers), [OpenAI-compatible](/providers/openai-compatible-providers), and [ community ](/providers/community-providers) packages.\\n\\nThis quickstart uses the [Vercel AI Gateway](https://vercel.com/ai-gateway) provider, which is the default [global provider](/docs/ai-sdk-core/provider-management#global-provider-configuration). This means you can access models using a simple string in the model configuration:\\n\\n```ts\\nmodel: \\'anthropic/claude-sonnet-4.5\\';\\n```\\n\\nYou can also explicitly import and use the gateway provider in two other equivalent ways:\\n\\n```ts\\n// Option 1: Import from \\'ai\\' package (included by default)\\nimport { gateway } from \\'ai\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n\\n// Option 2: Install and import from \\'@ai-sdk/gateway\\' package\\nimport { gateway } from \\'@ai-sdk/gateway\\';\\nmodel: gateway(\\'anthropic/claude-sonnet-4.5\\');\\n```\\n\\n### Using other providers\\n\\nTo use a different provider, install its package and create a provider instance. For example, to use OpenAI directly:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet text=\"pnpm add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"npm install @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n    <Tab>\\n      <Snippet text=\"yarn add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n    <Tab>\\n      <Snippet text=\"bun add @ai-sdk/openai@beta\" dark />\\n    </Tab>\\n\\n  </Tabs>\\n</div>\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nmodel: openai(\\'gpt-5.1\\');\\n```\\n\\n#### Updating the global provider\\n\\nYou can change the default global provider so string model references use your preferred provider everywhere in your application. Learn more about [provider management](/docs/ai-sdk-core/provider-management#global-provider-configuration).\\n\\nPick the approach that best matches how you want to manage providers across your application.\\n\\n## Wire up the UI\\n\\nNow that you have an API route that can query an LLM, it\\'s time to setup your frontend. The AI SDK\\'s [ UI ](/docs/ai-sdk-ui) package abstracts the complexity of a chat interface into one hook, [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\n\\nUpdate your root page (`app/(tabs)/index.tsx`) with the following code to show a list of chat messages and provide a user message input:\\n\\n```tsx filename=\"app/(tabs)/index.tsx\"\\nimport { generateAPIUrl } from \\'@/utils\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { fetch as expoFetch } from \\'expo/fetch\\';\\nimport { useState } from \\'react\\';\\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from \\'react-native\\';\\n\\nexport default function App() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, error, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\\n      api: generateAPIUrl(\\'/api/chat\\'),\\n    }),\\n    onError: error => console.error(error, \\'ERROR\\'),\\n  });\\n\\n  if (error) return <Text>{error.message}</Text>;\\n\\n  return (\\n    <SafeAreaView style={{ height: \\'100%\\' }}>\\n      <View\\n        style={{\\n          height: \\'95%\\',\\n          display: \\'flex\\',\\n          flexDirection: \\'column\\',\\n          paddingHorizontal: 8,\\n        }}\\n      >\\n        <ScrollView style={{ flex: 1 }}>\\n          {messages.map(m => (\\n            <View key={m.id} style={{ marginVertical: 8 }}>\\n              <View>\\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\\n                {m.parts.map((part, i) => {\\n                  switch (part.type) {\\n                    case \\'text\\':\\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\\n                  }\\n                })}\\n              </View>\\n            </View>\\n          ))}\\n        </ScrollView>\\n\\n        <View style={{ marginTop: 8 }}>\\n          <TextInput\\n            style={{ backgroundColor: \\'white\\', padding: 8 }}\\n            placeholder=\"Say something...\"\\n            value={input}\\n            onChange={e => setInput(e.nativeEvent.text)}\\n            onSubmitEditing={e => {\\n              e.preventDefault();\\n              sendMessage({ text: input });\\n              setInput(\\'\\');\\n            }}\\n            autoFocus={true}\\n          />\\n        </View>\\n      </View>\\n    </SafeAreaView>\\n  );\\n}\\n```\\n\\nThis page utilizes the `useChat` hook, which will, by default, use the `POST` API route you created earlier (`/api/chat`). The hook provides functions and state for handling user input and form submission. The `useChat` hook provides multiple utility functions and state variables:\\n\\n- `messages` - the current chat messages (an array of objects with `id`, `role`, and `parts` properties).\\n- `sendMessage` - a function to send a message to the chat API.\\n\\nThe component uses local state (`useState`) to manage the input field value, and handles form submission by calling `sendMessage` with the input text and then clearing the input field.\\n\\nThe LLM\\'s response is accessed through the message `parts` array. Each message contains an ordered array of `parts` that represents everything the model generated in its response. These parts can include plain text, reasoning tokens, and more that you will see later. The `parts` array preserves the sequence of the model\\'s outputs, allowing you to display or process each component in the order it was generated.\\n\\n<Note>\\n  You use the expo/fetch function instead of the native node fetch to enable\\n  streaming of chat responses. This requires Expo 52 or higher.\\n</Note>\\n\\n### Create the API URL Generator\\n\\nBecause you\\'re using expo/fetch for streaming responses instead of the native fetch function, you\\'ll need an API URL generator to ensure you are using the correct base url and format depending on the client environment (e.g. web or mobile). Create a new file called `utils.ts` in the root of your project and add the following code:\\n\\n```ts filename=\"utils.ts\"\\nimport Constants from \\'expo-constants\\';\\n\\nexport const generateAPIUrl = (relativePath: string) => {\\n  const origin = Constants.experienceUrl.replace(\\'exp://\\', \\'http://\\');\\n\\n  const path = relativePath.startsWith(\\'/\\') ? relativePath : `/${relativePath}`;\\n\\n  if (process.env.NODE_ENV === \\'development\\') {\\n    return origin.concat(path);\\n  }\\n\\n  if (!process.env.EXPO_PUBLIC_API_BASE_URL) {\\n    throw new Error(\\n      \\'EXPO_PUBLIC_API_BASE_URL environment variable is not defined\\',\\n    );\\n  }\\n\\n  return process.env.EXPO_PUBLIC_API_BASE_URL.concat(path);\\n};\\n```\\n\\nThis utility function handles URL generation for both development and production environments, ensuring your API calls work correctly across different devices and configurations.\\n\\n<Note>\\n  Before deploying to production, you must set the `EXPO_PUBLIC_API_BASE_URL`\\n  environment variable in your production environment. This variable should\\n  point to the base URL of your API server.\\n</Note>\\n\\n## Running Your Application\\n\\nWith that, you have built everything you need for your chatbot! To start your application, use the command:\\n\\n<Snippet text=\"pnpm expo\" />\\n\\nHead to your browser and open http://localhost:8081. You should see an input field. Test it out by entering a message and see the AI chatbot respond in real-time! The AI SDK makes it fast and easy to build AI chat interfaces with Expo.\\n\\n<Note>\\n  If you experience \"Property `structuredClone` doesn\\'t exist\" errors on mobile,\\n  add the [polyfills described below](#polyfills).\\n</Note>\\n\\n## Enhance Your Chatbot with Tools\\n\\nWhile large language models (LLMs) have incredible generation capabilities, they struggle with discrete tasks (e.g. mathematics) and interacting with the outside world (e.g. getting the weather). This is where [tools](/docs/ai-sdk-core/tools-and-tool-calling) come in.\\n\\nTools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.\\n\\nFor example, if a user asks about the current weather, without tools, the model would only be able to provide general information based on its training data. But with a weather tool, it can fetch and provide up-to-date, location-specific weather information.\\n\\nLet\\'s enhance your chatbot by adding a simple weather tool.\\n\\n### Update Your API route\\n\\nModify your `app/api/chat+api.ts` file to include the new weather tool:\\n\\n```tsx filename=\"app/api/chat+api.ts\" highlight=\"2,11-25\"\\nimport { streamText, UIMessage, convertToModelMessages, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\nIn this updated code:\\n\\n1. You import the `tool` function from the `ai` package and `z` from `zod` for schema validation.\\n2. You define a `tools` object with a `weather` tool. This tool:\\n\\n   - Has a description that helps the model understand when to use it.\\n   - Defines `inputSchema` using a Zod schema, specifying that it requires a `location` string to execute this tool. The model will attempt to extract this input from the context of the conversation. If it can\\'t, it will ask the user for the missing information.\\n   - Defines an `execute` function that simulates getting weather data (in this case, it returns a random temperature). This is an asynchronous function running on the server so you can fetch real data from an external API.\\n\\nNow your chatbot can \"fetch\" weather information for any location the user asks about. When the model determines it needs to use the weather tool, it will generate a tool call with the necessary input. The `execute` function will then be automatically run, and the tool output will be added to the `messages` as a `tool` message.\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nTry asking something like \"What\\'s the weather in New York?\" and see how the model uses the new tool.\\n\\nNotice the blank response in the UI? This is because instead of generating a text response, the model generated a tool call. You can access the tool call and subsequent tool result on the client via the `tool-weather` part of the `message.parts` array.\\n\\n<Note>\\n  Tool parts are always named `tool-{toolName}`, where `{toolName}` is the key\\n  you used when defining the tool. In this case, since we defined the tool as\\n  `weather`, the part type is `tool-weather`.\\n</Note>\\n\\n### Update the UI\\n\\nTo display the weather tool invocation in your UI, update your `app/(tabs)/index.tsx` file:\\n\\n```tsx filename=\"app/(tabs)/index.tsx\" highlight=\"31-35\"\\nimport { generateAPIUrl } from \\'@/utils\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { fetch as expoFetch } from \\'expo/fetch\\';\\nimport { useState } from \\'react\\';\\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from \\'react-native\\';\\n\\nexport default function App() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, error, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\\n      api: generateAPIUrl(\\'/api/chat\\'),\\n    }),\\n    onError: error => console.error(error, \\'ERROR\\'),\\n  });\\n\\n  if (error) return <Text>{error.message}</Text>;\\n\\n  return (\\n    <SafeAreaView style={{ height: \\'100%\\' }}>\\n      <View\\n        style={{\\n          height: \\'95%\\',\\n          display: \\'flex\\',\\n          flexDirection: \\'column\\',\\n          paddingHorizontal: 8,\\n        }}\\n      >\\n        <ScrollView style={{ flex: 1 }}>\\n          {messages.map(m => (\\n            <View key={m.id} style={{ marginVertical: 8 }}>\\n              <View>\\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\\n                {m.parts.map((part, i) => {\\n                  switch (part.type) {\\n                    case \\'text\\':\\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\\n                    case \\'tool-weather\\':\\n                      return (\\n                        <Text key={`${m.id}-${i}`}>\\n                          {JSON.stringify(part, null, 2)}\\n                        </Text>\\n                      );\\n                  }\\n                })}\\n              </View>\\n            </View>\\n          ))}\\n        </ScrollView>\\n\\n        <View style={{ marginTop: 8 }}>\\n          <TextInput\\n            style={{ backgroundColor: \\'white\\', padding: 8 }}\\n            placeholder=\"Say something...\"\\n            value={input}\\n            onChange={e => setInput(e.nativeEvent.text)}\\n            onSubmitEditing={e => {\\n              e.preventDefault();\\n              sendMessage({ text: input });\\n              setInput(\\'\\');\\n            }}\\n            autoFocus={true}\\n          />\\n        </View>\\n      </View>\\n    </SafeAreaView>\\n  );\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nWith this change, you\\'re updating the UI to handle different message parts. For text parts, you display the text content as before. For weather tool invocations, you display a JSON representation of the tool call and its result.\\n\\nNow, when you ask about the weather, you\\'ll see the tool call and its result displayed in your chat interface.\\n\\n## Enabling Multi-Step Tool Calls\\n\\nYou may have noticed that while the tool results are visible in the chat interface, the model isn\\'t using this information to answer your original query. This is because once the model generates a tool call, it has technically completed its generation.\\n\\nTo solve this, you can enable multi-step tool calls using `stopWhen`. By default, `stopWhen` is set to `stepCountIs(1)`, which means generation stops after the first step when there are tool results. By changing this condition, you can allow the model to automatically send tool results back to itself to trigger additional generations until your specified stopping condition is met. In this case, you want the model to continue generating so it can use the weather tool results to answer your original question.\\n\\n### Update Your API Route\\n\\nModify your `app/api/chat+api.ts` file to include the `stopWhen` condition:\\n\\n```tsx filename=\"app/api/chat+api.ts\" highlight=\"10\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nHead back to the Expo app and ask about the weather in a location. You should now see the model using the weather tool results to answer your question.\\n\\nBy setting `stopWhen: stepCountIs(5)`, you\\'re allowing the model to use up to 5 \"steps\" for any given generation. This enables more complex interactions and allows the model to gather and process information over several steps if needed. You can see this in action by adding another tool to convert the temperature from Fahrenheit to Celsius.\\n\\n### Add More Tools\\n\\nUpdate your `app/api/chat+api.ts` file to add a new tool to convert the temperature from Fahrenheit to Celsius:\\n\\n```tsx filename=\"app/api/chat+api.ts\" highlight=\"28-41\"\\nimport {\\n  streamText,\\n  UIMessage,\\n  convertToModelMessages,\\n  tool,\\n  stepCountIs,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools: {\\n      weather: tool({\\n        description: \\'Get the weather in a location (fahrenheit)\\',\\n        inputSchema: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }) => {\\n          const temperature = Math.round(Math.random() * (90 - 32) + 32);\\n          return {\\n            location,\\n            temperature,\\n          };\\n        },\\n      }),\\n      convertFahrenheitToCelsius: tool({\\n        description: \\'Convert a temperature in fahrenheit to celsius\\',\\n        inputSchema: z.object({\\n          temperature: z\\n            .number()\\n            .describe(\\'The temperature in fahrenheit to convert\\'),\\n        }),\\n        execute: async ({ temperature }) => {\\n          const celsius = Math.round((temperature - 32) * (5 / 9));\\n          return {\\n            celsius,\\n          };\\n        },\\n      }),\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      \\'Content-Type\\': \\'application/octet-stream\\',\\n      \\'Content-Encoding\\': \\'none\\',\\n    },\\n  });\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\n### Update the UI for the new tool\\n\\nTo display the temperature conversion tool invocation in your UI, update your `app/(tabs)/index.tsx` file to handle the new tool part:\\n\\n```tsx filename=\"app/(tabs)/index.tsx\" highlight=\"37-42\"\\nimport { generateAPIUrl } from \\'@/utils\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { fetch as expoFetch } from \\'expo/fetch\\';\\nimport { useState } from \\'react\\';\\nimport { View, TextInput, ScrollView, Text, SafeAreaView } from \\'react-native\\';\\n\\nexport default function App() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, error, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      fetch: expoFetch as unknown as typeof globalThis.fetch,\\n      api: generateAPIUrl(\\'/api/chat\\'),\\n    }),\\n    onError: error => console.error(error, \\'ERROR\\'),\\n  });\\n\\n  if (error) return <Text>{error.message}</Text>;\\n\\n  return (\\n    <SafeAreaView style={{ height: \\'100%\\' }}>\\n      <View\\n        style={{\\n          height: \\'95%\\',\\n          display: \\'flex\\',\\n          flexDirection: \\'column\\',\\n          paddingHorizontal: 8,\\n        }}\\n      >\\n        <ScrollView style={{ flex: 1 }}>\\n          {messages.map(m => (\\n            <View key={m.id} style={{ marginVertical: 8 }}>\\n              <View>\\n                <Text style={{ fontWeight: 700 }}>{m.role}</Text>\\n                {m.parts.map((part, i) => {\\n                  switch (part.type) {\\n                    case \\'text\\':\\n                      return <Text key={`${m.id}-${i}`}>{part.text}</Text>;\\n                    case \\'tool-weather\\':\\n                    case \\'tool-convertFahrenheitToCelsius\\':\\n                      return (\\n                        <Text key={`${m.id}-${i}`}>\\n                          {JSON.stringify(part, null, 2)}\\n                        </Text>\\n                      );\\n                  }\\n                })}\\n              </View>\\n            </View>\\n          ))}\\n        </ScrollView>\\n\\n        <View style={{ marginTop: 8 }}>\\n          <TextInput\\n            style={{ backgroundColor: \\'white\\', padding: 8 }}\\n            placeholder=\"Say something...\"\\n            value={input}\\n            onChange={e => setInput(e.nativeEvent.text)}\\n            onSubmitEditing={e => {\\n              e.preventDefault();\\n              sendMessage({ text: input });\\n              setInput(\\'\\');\\n            }}\\n            autoFocus={true}\\n          />\\n        </View>\\n      </View>\\n    </SafeAreaView>\\n  );\\n}\\n```\\n\\n<Note>\\n  You may need to restart your development server for the changes to take\\n  effect.\\n</Note>\\n\\nNow, when you ask \"What\\'s the weather in New York in celsius?\", you should see a more complete interaction:\\n\\n1. The model will call the weather tool for New York.\\n2. You\\'ll see the tool result displayed.\\n3. It will then call the temperature conversion tool to convert the temperature from Fahrenheit to Celsius.\\n4. The model will then use that information to provide a natural language response about the weather in New York.\\n\\nThis multi-step approach allows the model to gather information and use it to provide more accurate and contextual responses, making your chatbot considerably more useful.\\n\\nThis simple example demonstrates how tools can expand your model\\'s capabilities. You can create more complex tools to integrate with real APIs, databases, or any other external systems, allowing the model to access and process real-world data in real-time. Tools bridge the gap between the model\\'s knowledge cutoff and current information.\\n\\n## Polyfills\\n\\nSeveral functions that are internally used by the AI SDK might not available in the Expo runtime depending on your configuration and the target platform.\\n\\nFirst, install the following packages:\\n\\n<div className=\"my-4\">\\n  <Tabs items={[\\'pnpm\\', \\'npm\\', \\'yarn\\', \\'bun\\']}>\\n    <Tab>\\n      <Snippet\\n        text=\"pnpm add @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n    <Tab>\\n      <Snippet\\n        text=\"npm install @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n    <Tab>\\n      <Snippet\\n        text=\"yarn add @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n    <Tab>\\n      <Snippet\\n        text=\"bun add @ungap/structured-clone @stardazed/streams-text-encoding\"\\n        dark\\n      />\\n    </Tab>\\n  </Tabs>\\n</div>\\n\\nThen create a new file in the root of your project with the following polyfills:\\n\\n```ts filename=\"polyfills.js\"\\nimport { Platform } from \\'react-native\\';\\nimport structuredClone from \\'@ungap/structured-clone\\';\\n\\nif (Platform.OS !== \\'web\\') {\\n  const setupPolyfills = async () => {\\n    const { polyfillGlobal } = await import(\\n      \\'react-native/Libraries/Utilities/PolyfillFunctions\\'\\n    );\\n\\n    const { TextEncoderStream, TextDecoderStream } = await import(\\n      \\'@stardazed/streams-text-encoding\\'\\n    );\\n\\n    if (!(\\'structuredClone\\' in global)) {\\n      polyfillGlobal(\\'structuredClone\\', () => structuredClone);\\n    }\\n\\n    polyfillGlobal(\\'TextEncoderStream\\', () => TextEncoderStream);\\n    polyfillGlobal(\\'TextDecoderStream\\', () => TextDecoderStream);\\n  };\\n\\n  setupPolyfills();\\n}\\n\\nexport {};\\n```\\n\\nFinally, import the polyfills in your root `_layout.tsx`:\\n\\n```ts filename=\"_layout.tsx\"\\nimport \\'@/polyfills\\';\\n```\\n\\n## Where to Next?\\n\\nYou\\'ve built an AI chatbot using the AI SDK! From here, you have several paths to explore:\\n\\n- To learn more about the AI SDK, read through the [documentation](/docs).\\n- If you\\'re interested in diving deeper with guides, check out the [RAG (retrieval-augmented generation)](/docs/guides/rag-chatbot) and [multi-modal chatbot](/docs/guides/multi-modal-chatbot) guides.\\n- To jumpstart your first AI project, explore available [templates](https://vercel.com/templates?type=ai).\\n', children=[]), DocItem(origPath=Path('02-getting-started/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Getting Started\\ndescription: Welcome to the AI SDK documentation!\\n---\\n\\n# Getting Started\\n\\nThe following guides are intended to provide you with an introduction to some of the core features provided by the AI SDK.\\n\\n<QuickstartFrameworkCards />\\n\\n## Backend Framework Examples\\n\\nYou can also use [AI SDK Core](/docs/ai-sdk-core/overview) and [AI SDK UI](/docs/ai-sdk-ui/overview) with the following backend frameworks:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Node.js HTTP Server',\\n      description: 'Send AI responses from a Node.js HTTP server.',\\n      href: '/examples/api-servers/node-js-http-server',\\n    },\\n    {\\n      title: 'Express',\\n      description: 'Send AI responses from an Express server.',\\n      href: '/examples/api-servers/express',\\n    },\\n    {\\n      title: 'Hono',\\n      description: 'Send AI responses from a Hono server.',\\n      href: '/examples/api-servers/hono',\\n    },\\n    {\\n      title: 'Fastify',\\n      description: 'Send AI responses from a Fastify server.',\\n      href: '/examples/api-servers/fastify',\\n    },\\n    {\\n      title: 'Nest.js',\\n      description: 'Send AI responses from a Nest.js server.',\\n      href: '/examples/api-servers/nest',\\n    },\\n  ]}\\n/>\\n\", children=[])]),\n",
       " DocItem(origPath=Path('03-agents'), name='03-agents', displayName='03-agents', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('03-agents/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Overview\\ndescription: Learn how to build agents with the AI SDK.\\n---\\n\\n# Agents\\n\\nAgents are **large language models (LLMs)** that use **tools** in a **loop** to accomplish tasks.\\n\\nThese components work together:\\n\\n- **LLMs** process input and decide the next action\\n- **Tools** extend capabilities beyond text generation (reading files, calling APIs, writing to databases)\\n- **Loop** orchestrates execution through:\\n  - **Context management** - Maintaining conversation history and deciding what the model sees (input) at each step\\n  - **Stopping conditions** - Determining when the loop (task) is complete\\n\\n## ToolLoopAgent Class\\n\\nThe ToolLoopAgent class handles these three components. Here's an agent that uses multiple tools in a loop to accomplish a task:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs, tool } from 'ai';\\nimport { z } from 'zod';\\n\\nconst weatherAgent = new ToolLoopAgent({\\n  model: 'anthropic/claude-sonnet-4.5',\\n  tools: {\\n    weather: tool({\\n      description: 'Get the weather in a location (in Fahrenheit)',\\n      inputSchema: z.object({\\n        location: z.string().describe('The location to get the weather for'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n    convertFahrenheitToCelsius: tool({\\n      description: 'Convert temperature from Fahrenheit to Celsius',\\n      inputSchema: z.object({\\n        temperature: z.number().describe('Temperature in Fahrenheit'),\\n      }),\\n      execute: async ({ temperature }) => {\\n        const celsius = Math.round((temperature - 32) * (5 / 9));\\n        return { celsius };\\n      },\\n    }),\\n  },\\n  // Agent's default behavior is to stop after a maximum of 20 steps\\n  // stopWhen: stepCountIs(20),\\n});\\n\\nconst result = await weatherAgent.generate({\\n  prompt: 'What is the weather in San Francisco in celsius?',\\n});\\n\\nconsole.log(result.text); // agent's final answer\\nconsole.log(result.steps); // steps taken by the agent\\n```\\n\\nThe agent automatically:\\n\\n1. Calls the `weather` tool to get the temperature in Fahrenheit\\n2. Calls `convertFahrenheitToCelsius` to convert it\\n3. Generates a final text response with the result\\n\\nThe Agent class handles the loop, context management, and stopping conditions.\\n\\n## Why Use the Agent Class?\\n\\nThe Agent class is the recommended approach for building agents with the AI SDK because it:\\n\\n- **Reduces boilerplate** - Manages loops and message arrays\\n- **Improves reusability** - Define once, use throughout your application\\n- **Simplifies maintenance** - Single place to update agent configuration\\n\\nFor most use cases, start with the Agent class. Use core functions (`generateText`, `streamText`) when you need explicit control over each step for complex structured workflows.\\n\\n## Structured Workflows\\n\\nAgents are flexible and powerful, but non-deterministic. When you need reliable, repeatable outcomes with explicit control flow, use core functions with structured workflow patterns combining:\\n\\n- Conditional statements for explicit branching\\n- Standard functions for reusable logic\\n- Error handling for robustness\\n- Explicit control flow for predictability\\n\\n[Explore workflow patterns](/docs/agents/workflows) to learn more about building structured, reliable systems.\\n\\n## Next Steps\\n\\n- **[Building Agents](/docs/agents/building-agents)** - Guide to creating agents with the Agent class\\n- **[Workflow Patterns](/docs/agents/workflows)** - Structured patterns using core functions for complex workflows\\n- **[Loop Control](/docs/agents/loop-control)** - Execution control with stopWhen and prepareStep\\n\", children=[]), DocItem(origPath=Path('03-agents/02-building-agents.mdx'), name='02-building-agents.mdx', displayName='02-building-agents.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Building Agents\\ndescription: Complete guide to creating agents with the Agent class.\\n---\\n\\n# Building Agents\\n\\nThe Agent class provides a structured way to encapsulate LLM configuration, tools, and behavior into reusable components. It handles the agent loop for you, allowing the LLM to call tools multiple times in sequence to accomplish complex tasks. Define agents once and use them across your application.\\n\\n## Why Use the ToolLoopAgent Class?\\n\\nWhen building AI applications, you often need to:\\n\\n- **Reuse configurations** - Same model settings, tools, and prompts across different parts of your application\\n- **Maintain consistency** - Ensure the same behavior and capabilities throughout your codebase\\n- **Simplify API routes** - Reduce boilerplate in your endpoints\\n- **Type safety** - Get full TypeScript support for your agent\\'s tools and outputs\\n\\nThe ToolLoopAgent class provides a single place to define your agent\\'s behavior.\\n\\n## Creating an Agent\\n\\nDefine an agent by instantiating the ToolLoopAgent class with your desired configuration:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst myAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: {\\n    // Your tools here\\n  },\\n});\\n```\\n\\n## Configuration Options\\n\\nThe Agent class accepts all the same settings as `generateText` and `streamText`. Configure:\\n\\n### Model and System Instructions\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are an expert software engineer.\\',\\n});\\n```\\n\\n### Tools\\n\\nProvide tools that the agent can use to accomplish tasks:\\n\\n```ts\\nimport { ToolLoopAgent, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst codeAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    runCode: tool({\\n      description: \\'Execute Python code\\',\\n      inputSchema: z.object({\\n        code: z.string(),\\n      }),\\n      execute: async ({ code }) => {\\n        // Execute code and return result\\n        return { output: \\'Code executed successfully\\' };\\n      },\\n    }),\\n  },\\n});\\n```\\n\\n### Loop Control\\n\\nBy default, agents run for 20 steps (`stopWhen: stepCountIs(20)`). In each step, the model either generates text or calls a tool. If it generates text, the agent completes. If it calls a tool, the AI SDK executes that tool.\\n\\nTo let agents call multiple tools in sequence, configure `stopWhen` to allow more steps. After each tool execution, the agent triggers a new generation where the model can call another tool or generate text:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  stopWhen: stepCountIs(20), // Allow up to 20 steps\\n});\\n```\\n\\nEach step represents one generation (which results in either text or a tool call). The loop continues until:\\n\\n- A finish reasoning other than tool-calls is returned, or\\n- A tool that is invoked does not have an execute function, or\\n- A tool call needs approval, or\\n- A stop condition is met\\n\\nYou can combine multiple conditions:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  stopWhen: [\\n    stepCountIs(20), // Maximum 20 steps\\n    yourCustomCondition(), // Custom logic for when to stop\\n  ],\\n});\\n```\\n\\nLearn more about [loop control and stop conditions](/docs/agents/loop-control).\\n\\n### Tool Choice\\n\\nControl how the agent uses tools:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools here\\n  },\\n  toolChoice: \\'required\\', // Force tool use\\n  // or toolChoice: \\'none\\' to disable tools\\n  // or toolChoice: \\'auto\\' (default) to let the model decide\\n});\\n```\\n\\nYou can also force the use of a specific tool:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: weatherTool,\\n    cityAttractions: attractionsTool,\\n  },\\n  toolChoice: {\\n    type: \\'tool\\',\\n    toolName: \\'weather\\', // Force the weather tool to be used\\n  },\\n});\\n```\\n\\n### Structured Output\\n\\nDefine structured output schemas:\\n\\n```ts\\nimport { ToolLoopAgent, Output, stepCountIs } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst analysisAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: Output.object({\\n    schema: z.object({\\n      sentiment: z.enum([\\'positive\\', \\'neutral\\', \\'negative\\']),\\n      summary: z.string(),\\n      keyPoints: z.array(z.string()),\\n    }),\\n  }),\\n  stopWhen: stepCountIs(10),\\n});\\n\\nconst { output } = await analysisAgent.generate({\\n  prompt: \\'Analyze customer feedback from the last quarter\\',\\n});\\n```\\n\\n## Define Agent Behavior with System Instructions\\n\\nSystem instructions define your agent\\'s behavior, personality, and constraints. They set the context for all interactions and guide how the agent responds to user queries and uses tools.\\n\\n### Basic System Instructions\\n\\nSet the agent\\'s role and expertise:\\n\\n```ts\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions:\\n    \\'You are an expert data analyst. You provide clear insights from complex data.\\',\\n});\\n```\\n\\n### Detailed Behavioral Instructions\\n\\nProvide specific guidelines for agent behavior:\\n\\n```ts\\nconst codeReviewAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a senior software engineer conducting code reviews.\\n\\n  Your approach:\\n  - Focus on security vulnerabilities first\\n  - Identify performance bottlenecks\\n  - Suggest improvements for readability and maintainability\\n  - Be constructive and educational in your feedback\\n  - Always explain why something is an issue and how to fix it`,\\n});\\n```\\n\\n### Constrain Agent Behavior\\n\\nSet boundaries and ensure consistent behavior:\\n\\n```ts\\nconst customerSupportAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a customer support specialist for an e-commerce platform.\\n\\n  Rules:\\n  - Never make promises about refunds without checking the policy\\n  - Always be empathetic and professional\\n  - If you don\\'t know something, say so and offer to escalate\\n  - Keep responses concise and actionable\\n  - Never share internal company information`,\\n  tools: {\\n    checkOrderStatus,\\n    lookupPolicy,\\n    createTicket,\\n  },\\n});\\n```\\n\\n### Tool Usage Instructions\\n\\nGuide how the agent should use available tools:\\n\\n```ts\\nconst researchAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a research assistant with access to search and document tools.\\n\\n  When researching:\\n  1. Always start with a broad search to understand the topic\\n  2. Use document analysis for detailed information\\n  3. Cross-reference multiple sources before drawing conclusions\\n  4. Cite your sources when presenting information\\n  5. If information conflicts, present both viewpoints`,\\n  tools: {\\n    webSearch,\\n    analyzeDocument,\\n    extractQuotes,\\n  },\\n});\\n```\\n\\n### Format and Style Instructions\\n\\nControl the output format and communication style:\\n\\n```ts\\nconst technicalWriterAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: `You are a technical documentation writer.\\n\\n  Writing style:\\n  - Use clear, simple language\\n  - Avoid jargon unless necessary\\n  - Structure information with headers and bullet points\\n  - Include code examples where relevant\\n  - Write in second person (\"you\" instead of \"the user\")\\n\\n  Always format responses in Markdown.`,\\n});\\n```\\n\\n## Using an Agent\\n\\nOnce defined, you can use your agent in three ways:\\n\\n### Generate Text\\n\\nUse `generate()` for one-time text generation:\\n\\n```ts\\nconst result = await myAgent.generate({\\n  prompt: \\'What is the weather like?\\',\\n});\\n\\nconsole.log(result.text);\\n```\\n\\n### Stream Text\\n\\nUse `stream()` for streaming responses:\\n\\n```ts\\nconst stream = myAgent.stream({\\n  prompt: \\'Tell me a story\\',\\n});\\n\\nfor await (const chunk of stream.textStream) {\\n  console.log(chunk);\\n}\\n```\\n\\n### Respond to UI Messages\\n\\nUse `createAgentUIStreamResponse()` to create API responses for client applications:\\n\\n```ts\\n// In your API route (e.g., app/api/chat/route.ts)\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: myAgent,\\n    messages,\\n  });\\n}\\n```\\n\\n## End-to-end Type Safety\\n\\nYou can infer types for your agent\\'s `UIMessage`s:\\n\\n```ts\\nimport { ToolLoopAgent, InferAgentUIMessage } from \\'ai\\';\\n\\nconst myAgent = new ToolLoopAgent({\\n  // ... configuration\\n});\\n\\n// Infer the UIMessage type for UI components or persistence\\nexport type MyAgentUIMessage = InferAgentUIMessage<typeof myAgent>;\\n```\\n\\nUse this type in your client components with `useChat`:\\n\\n```tsx filename=\"components/chat.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyAgentUIMessage } from \\'@/agent/my-agent\\';\\n\\nexport function Chat() {\\n  const { messages } = useChat<MyAgentUIMessage>();\\n  // Full type safety for your messages and tools\\n}\\n```\\n\\n## Next Steps\\n\\nNow that you understand building agents, you can:\\n\\n- Explore [workflow patterns](/docs/agents/workflows) for structured patterns using core functions\\n- Learn about [loop control](/docs/agents/loop-control) for advanced execution control\\n- See [manual loop examples](/cookbook/node/manual-agent-loop) for custom workflow implementations\\n', children=[]), DocItem(origPath=Path('03-agents/03-workflows.mdx'), name='03-workflows.mdx', displayName='03-workflows.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Workflow Patterns\\ndescription: Learn workflow patterns for building reliable agents with the AI SDK.\\n---\\n\\n# Workflow Patterns\\n\\nCombine the building blocks from the [overview](/docs/agents/overview) with these patterns to add structure and reliability to your agents:\\n\\n- [Sequential Processing](#sequential-processing-chains) - Steps executed in order\\n- [Parallel Processing](#parallel-processing) - Independent tasks run simultaneously\\n- [Evaluation/Feedback Loops](#evaluator-optimizer) - Results checked and improved iteratively\\n- [Orchestration](#orchestrator-worker) - Coordinating multiple components\\n- [Routing](#routing) - Directing work based on context\\n\\n## Choose Your Approach\\n\\nConsider these key factors:\\n\\n- **Flexibility vs Control** - How much freedom does the LLM need vs how tightly you must constrain its actions?\\n- **Error Tolerance** - What are the consequences of mistakes in your use case?\\n- **Cost Considerations** - More complex systems typically mean more LLM calls and higher costs\\n- **Maintenance** - Simpler architectures are easier to debug and modify\\n\\n**Start with the simplest approach that meets your needs**. Add complexity only when required by:\\n\\n1. Breaking down tasks into clear steps\\n2. Adding tools for specific capabilities\\n3. Implementing feedback loops for quality control\\n4. Introducing multiple agents for complex workflows\\n\\nLet's look at examples of these patterns in action.\\n\\n## Patterns with Examples\\n\\nThese patterns, adapted from [Anthropic's guide on building effective agents](https://www.anthropic.com/research/building-effective-agents), serve as building blocks you can combine to create comprehensive workflows. Each pattern addresses specific aspects of task execution. Combine them thoughtfully to build reliable solutions for complex problems.\\n\\n## Sequential Processing (Chains)\\n\\nThe simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. Use this pattern for tasks with well-defined sequences, like content generation pipelines or data transformation processes.\\n\\n```ts\\nimport { generateText, generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function generateMarketingCopy(input: string) {\\n  const model = 'openai/gpt-4o';\\n\\n  // First step: Generate marketing copy\\n  const { text: copy } = await generateText({\\n    model,\\n    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,\\n  });\\n\\n  // Perform quality check on copy\\n  const { object: qualityMetrics } = await generateObject({\\n    model,\\n    schema: z.object({\\n      hasCallToAction: z.boolean(),\\n      emotionalAppeal: z.number().min(1).max(10),\\n      clarity: z.number().min(1).max(10),\\n    }),\\n    prompt: `Evaluate this marketing copy for:\\n    1. Presence of call to action (true/false)\\n    2. Emotional appeal (1-10)\\n    3. Clarity (1-10)\\n\\n    Copy to evaluate: ${copy}`,\\n  });\\n\\n  // If quality check fails, regenerate with more specific instructions\\n  if (\\n    !qualityMetrics.hasCallToAction ||\\n    qualityMetrics.emotionalAppeal < 7 ||\\n    qualityMetrics.clarity < 7\\n  ) {\\n    const { text: improvedCopy } = await generateText({\\n      model,\\n      prompt: `Rewrite this marketing copy with:\\n      ${!qualityMetrics.hasCallToAction ? '- A clear call to action' : ''}\\n      ${qualityMetrics.emotionalAppeal < 7 ? '- Stronger emotional appeal' : ''}\\n      ${qualityMetrics.clarity < 7 ? '- Improved clarity and directness' : ''}\\n\\n      Original copy: ${copy}`,\\n    });\\n    return { copy: improvedCopy, qualityMetrics };\\n  }\\n\\n  return { copy, qualityMetrics };\\n}\\n```\\n\\n## Routing\\n\\nThis pattern lets the model decide which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. Use this when handling varied inputs that require different processing approaches. In the example below, the first LLM call's results determine the second call's model size and system prompt.\\n\\n```ts\\nimport { generateObject, generateText } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function handleCustomerQuery(query: string) {\\n  const model = 'openai/gpt-4o';\\n\\n  // First step: Classify the query type\\n  const { object: classification } = await generateObject({\\n    model,\\n    schema: z.object({\\n      reasoning: z.string(),\\n      type: z.enum(['general', 'refund', 'technical']),\\n      complexity: z.enum(['simple', 'complex']),\\n    }),\\n    prompt: `Classify this customer query:\\n    ${query}\\n\\n    Determine:\\n    1. Query type (general, refund, or technical)\\n    2. Complexity (simple or complex)\\n    3. Brief reasoning for classification`,\\n  });\\n\\n  // Route based on classification\\n  // Set model and system prompt based on query type and complexity\\n  const { text: response } = await generateText({\\n    model:\\n      classification.complexity === 'simple'\\n        ? 'openai/gpt-4o-mini'\\n        : 'openai/o4-mini',\\n    system: {\\n      general:\\n        'You are an expert customer service agent handling general inquiries.',\\n      refund:\\n        'You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.',\\n      technical:\\n        'You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.',\\n    }[classification.type],\\n    prompt: query,\\n  });\\n\\n  return { response, classification };\\n}\\n```\\n\\n## Parallel Processing\\n\\nBreak down tasks into independent subtasks that execute simultaneously. This pattern uses parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyze multiple documents or process different aspects of a single input concurrently (like code review).\\n\\n```ts\\nimport { generateText, generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\n// Example: Parallel code review with multiple specialized reviewers\\nasync function parallelCodeReview(code: string) {\\n  const model = 'openai/gpt-4o';\\n\\n  // Run parallel reviews\\n  const [securityReview, performanceReview, maintainabilityReview] =\\n    await Promise.all([\\n      generateObject({\\n        model,\\n        system:\\n          'You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.',\\n        schema: z.object({\\n          vulnerabilities: z.array(z.string()),\\n          riskLevel: z.enum(['low', 'medium', 'high']),\\n          suggestions: z.array(z.string()),\\n        }),\\n        prompt: `Review this code:\\n      ${code}`,\\n      }),\\n\\n      generateObject({\\n        model,\\n        system:\\n          'You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.',\\n        schema: z.object({\\n          issues: z.array(z.string()),\\n          impact: z.enum(['low', 'medium', 'high']),\\n          optimizations: z.array(z.string()),\\n        }),\\n        prompt: `Review this code:\\n      ${code}`,\\n      }),\\n\\n      generateObject({\\n        model,\\n        system:\\n          'You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.',\\n        schema: z.object({\\n          concerns: z.array(z.string()),\\n          qualityScore: z.number().min(1).max(10),\\n          recommendations: z.array(z.string()),\\n        }),\\n        prompt: `Review this code:\\n      ${code}`,\\n      }),\\n    ]);\\n\\n  const reviews = [\\n    { ...securityReview.object, type: 'security' },\\n    { ...performanceReview.object, type: 'performance' },\\n    { ...maintainabilityReview.object, type: 'maintainability' },\\n  ];\\n\\n  // Aggregate results using another model instance\\n  const { text: summary } = await generateText({\\n    model,\\n    system: 'You are a technical lead summarizing multiple code reviews.',\\n    prompt: `Synthesize these code review results into a concise summary with key actions:\\n    ${JSON.stringify(reviews, null, 2)}`,\\n  });\\n\\n  return { reviews, summary };\\n}\\n```\\n\\n## Orchestrator-Worker\\n\\nA primary model (orchestrator) coordinates the execution of specialized workers. Each worker optimizes for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.\\n\\n```ts\\nimport { generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function implementFeature(featureRequest: string) {\\n  // Orchestrator: Plan the implementation\\n  const { object: implementationPlan } = await generateObject({\\n    model: 'openai/o4-mini',\\n    schema: z.object({\\n      files: z.array(\\n        z.object({\\n          purpose: z.string(),\\n          filePath: z.string(),\\n          changeType: z.enum(['create', 'modify', 'delete']),\\n        }),\\n      ),\\n      estimatedComplexity: z.enum(['low', 'medium', 'high']),\\n    }),\\n    system:\\n      'You are a senior software architect planning feature implementations.',\\n    prompt: `Analyze this feature request and create an implementation plan:\\n    ${featureRequest}`,\\n  });\\n\\n  // Workers: Execute the planned changes\\n  const fileChanges = await Promise.all(\\n    implementationPlan.files.map(async file => {\\n      // Each worker is specialized for the type of change\\n      const workerSystemPrompt = {\\n        create:\\n          'You are an expert at implementing new files following best practices and project patterns.',\\n        modify:\\n          'You are an expert at modifying existing code while maintaining consistency and avoiding regressions.',\\n        delete:\\n          'You are an expert at safely removing code while ensuring no breaking changes.',\\n      }[file.changeType];\\n\\n      const { object: change } = await generateObject({\\n        model: 'anthropic/claude-sonnet-4.5',\\n        schema: z.object({\\n          explanation: z.string(),\\n          code: z.string(),\\n        }),\\n        system: workerSystemPrompt,\\n        prompt: `Implement the changes for ${file.filePath} to support:\\n        ${file.purpose}\\n\\n        Consider the overall feature context:\\n        ${featureRequest}`,\\n      });\\n\\n      return {\\n        file,\\n        implementation: change,\\n      };\\n    }),\\n  );\\n\\n  return {\\n    plan: implementationPlan,\\n    changes: fileChanges,\\n  };\\n}\\n```\\n\\n## Evaluator-Optimizer\\n\\nAdd quality control to workflows with dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow proceeds, retries with adjusted parameters, or takes corrective action. This creates robust workflows capable of self-improvement and error recovery.\\n\\n```ts\\nimport { generateText, generateObject } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function translateWithFeedback(text: string, targetLanguage: string) {\\n  let currentTranslation = '';\\n  let iterations = 0;\\n  const MAX_ITERATIONS = 3;\\n\\n  // Initial translation\\n  const { text: translation } = await generateText({\\n    model: 'openai/gpt-4o-mini', // use small model for first attempt\\n    system: 'You are an expert literary translator.',\\n    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:\\n    ${text}`,\\n  });\\n\\n  currentTranslation = translation;\\n\\n  // Evaluation-optimization loop\\n  while (iterations < MAX_ITERATIONS) {\\n    // Evaluate current translation\\n    const { object: evaluation } = await generateObject({\\n      model: 'anthropic/claude-sonnet-4.5', // use a larger model to evaluate\\n      schema: z.object({\\n        qualityScore: z.number().min(1).max(10),\\n        preservesTone: z.boolean(),\\n        preservesNuance: z.boolean(),\\n        culturallyAccurate: z.boolean(),\\n        specificIssues: z.array(z.string()),\\n        improvementSuggestions: z.array(z.string()),\\n      }),\\n      system: 'You are an expert in evaluating literary translations.',\\n      prompt: `Evaluate this translation:\\n\\n      Original: ${text}\\n      Translation: ${currentTranslation}\\n\\n      Consider:\\n      1. Overall quality\\n      2. Preservation of tone\\n      3. Preservation of nuance\\n      4. Cultural accuracy`,\\n    });\\n\\n    // Check if quality meets threshold\\n    if (\\n      evaluation.qualityScore >= 8 &&\\n      evaluation.preservesTone &&\\n      evaluation.preservesNuance &&\\n      evaluation.culturallyAccurate\\n    ) {\\n      break;\\n    }\\n\\n    // Generate improved translation based on feedback\\n    const { text: improvedTranslation } = await generateText({\\n      model: 'anthropic/claude-sonnet-4.5', // use a larger model\\n      system: 'You are an expert literary translator.',\\n      prompt: `Improve this translation based on the following feedback:\\n      ${evaluation.specificIssues.join('\\\\n')}\\n      ${evaluation.improvementSuggestions.join('\\\\n')}\\n\\n      Original: ${text}\\n      Current Translation: ${currentTranslation}`,\\n    });\\n\\n    currentTranslation = improvedTranslation;\\n    iterations++;\\n  }\\n\\n  return {\\n    finalTranslation: currentTranslation,\\n    iterationsRequired: iterations,\\n  };\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('03-agents/04-loop-control.mdx'), name='04-loop-control.mdx', displayName='04-loop-control.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Loop Control\\ndescription: Control agent execution with built-in loop management using stopWhen and prepareStep\\n---\\n\\n# Loop Control\\n\\nYou can control both the execution flow and the settings at each step of the agent loop. The loop continues until:\\n\\n- A finish reasoning other than tool-calls is returned, or\\n- A tool that is invoked does not have an execute function, or\\n- A tool call needs approval, or\\n- A stop condition is met\\n\\nThe AI SDK provides built-in loop control through two parameters: `stopWhen` for defining stopping conditions and `prepareStep` for modifying settings (model, tools, messages, and more) between steps.\\n\\n## Stop Conditions\\n\\nThe `stopWhen` parameter controls when to stop execution when there are tool results in the last step. By default, agents stop after 20 steps using `stepCountIs(20)`.\\n\\nWhen you provide `stopWhen`, the agent continues executing after tool calls until a stopping condition is met. When the condition is an array, execution stops when any of the conditions are met.\\n\\n### Use Built-in Conditions\\n\\nThe AI SDK provides several built-in stopping conditions:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  stopWhen: stepCountIs(20), // Default state: stop after 20 steps maximum\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Analyze this dataset and create a summary report\\',\\n});\\n```\\n\\n### Combine Multiple Conditions\\n\\nCombine multiple stopping conditions. The loop stops when it meets any condition:\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs, hasToolCall } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  stopWhen: [\\n    stepCountIs(20), // Maximum 20 steps\\n    hasToolCall(\\'someTool\\'), // Stop after calling \\'someTool\\'\\n  ],\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Research and analyze the topic\\',\\n});\\n```\\n\\n### Create Custom Conditions\\n\\nBuild custom stopping conditions for specific requirements:\\n\\n```ts\\nimport { ToolLoopAgent, StopCondition, ToolSet } from \\'ai\\';\\n\\nconst tools = {\\n  // your tools\\n} satisfies ToolSet;\\n\\nconst hasAnswer: StopCondition<typeof tools> = ({ steps }) => {\\n  // Stop when the model generates text containing \"ANSWER:\"\\n  return steps.some(step => step.text?.includes(\\'ANSWER:\\')) ?? false;\\n};\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools,\\n  stopWhen: hasAnswer,\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Find the answer and respond with \"ANSWER: [your answer]\"\\',\\n});\\n```\\n\\nCustom conditions receive step information across all steps:\\n\\n```ts\\nconst budgetExceeded: StopCondition<typeof tools> = ({ steps }) => {\\n  const totalUsage = steps.reduce(\\n    (acc, step) => ({\\n      inputTokens: acc.inputTokens + (step.usage?.inputTokens ?? 0),\\n      outputTokens: acc.outputTokens + (step.usage?.outputTokens ?? 0),\\n    }),\\n    { inputTokens: 0, outputTokens: 0 },\\n  );\\n\\n  const costEstimate =\\n    (totalUsage.inputTokens * 0.01 + totalUsage.outputTokens * 0.03) / 1000;\\n  return costEstimate > 0.5; // Stop if cost exceeds $0.50\\n};\\n```\\n\\n## Prepare Step\\n\\nThe `prepareStep` callback runs before each step in the loop and defaults to the initial settings if you don\\'t return any changes. Use it to modify settings, manage context, or implement dynamic behavior based on execution history.\\n\\n### Dynamic Model Selection\\n\\nSwitch models based on step requirements:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/gpt-4o-mini\\', // Default model\\n  tools: {\\n    // your tools\\n  },\\n  prepareStep: async ({ stepNumber, messages }) => {\\n    // Use a stronger model for complex reasoning after initial steps\\n    if (stepNumber > 2 && messages.length > 10) {\\n      return {\\n        model: \\'anthropic/claude-sonnet-4.5\\',\\n      };\\n    }\\n    // Continue with default settings\\n    return {};\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\n### Context Management\\n\\nManage growing conversation history in long-running loops:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  prepareStep: async ({ messages }) => {\\n    // Keep only recent messages to stay within context limits\\n    if (messages.length > 20) {\\n      return {\\n        messages: [\\n          messages[0], // Keep system instructions\\n          ...messages.slice(-10), // Keep last 10 messages\\n        ],\\n      };\\n    }\\n    return {};\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\n### Tool Selection\\n\\nControl which tools are available at each step:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    search: searchTool,\\n    analyze: analyzeTool,\\n    summarize: summarizeTool,\\n  },\\n  prepareStep: async ({ stepNumber, steps }) => {\\n    // Search phase (steps 0-2)\\n    if (stepNumber <= 2) {\\n      return {\\n        activeTools: [\\'search\\'],\\n        toolChoice: \\'required\\',\\n      };\\n    }\\n\\n    // Analysis phase (steps 3-5)\\n    if (stepNumber <= 5) {\\n      return {\\n        activeTools: [\\'analyze\\'],\\n      };\\n    }\\n\\n    // Summary phase (step 6+)\\n    return {\\n      activeTools: [\\'summarize\\'],\\n      toolChoice: \\'required\\',\\n    };\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\nYou can also force a specific tool to be used:\\n\\n```ts\\nprepareStep: async ({ stepNumber }) => {\\n  if (stepNumber === 0) {\\n    // Force the search tool to be used first\\n    return {\\n      toolChoice: { type: \\'tool\\', toolName: \\'search\\' },\\n    };\\n  }\\n\\n  if (stepNumber === 5) {\\n    // Force the summarize tool after analysis\\n    return {\\n      toolChoice: { type: \\'tool\\', toolName: \\'summarize\\' },\\n    };\\n  }\\n\\n  return {};\\n};\\n```\\n\\n### Message Modification\\n\\nTransform messages before sending them to the model:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  prepareStep: async ({ messages, stepNumber }) => {\\n    // Summarize tool results to reduce token usage\\n    const processedMessages = messages.map(msg => {\\n      if (msg.role === \\'tool\\' && msg.content.length > 1000) {\\n        return {\\n          ...msg,\\n          content: summarizeToolResult(msg.content),\\n        };\\n      }\\n      return msg;\\n    });\\n\\n    return { messages: processedMessages };\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'...\\',\\n});\\n```\\n\\n## Access Step Information\\n\\nBoth `stopWhen` and `prepareStep` receive detailed information about the current execution:\\n\\n```ts\\nprepareStep: async ({\\n  model, // Current model configuration\\n  stepNumber, // Current step number (0-indexed)\\n  steps, // All previous steps with their results\\n  messages, // Messages to be sent to the model\\n}) => {\\n  // Access previous tool calls and results\\n  const previousToolCalls = steps.flatMap(step => step.toolCalls);\\n  const previousResults = steps.flatMap(step => step.toolResults);\\n\\n  // Make decisions based on execution history\\n  if (previousToolCalls.some(call => call.toolName === \\'dataAnalysis\\')) {\\n    return {\\n      toolChoice: { type: \\'tool\\', toolName: \\'reportGenerator\\' },\\n    };\\n  }\\n\\n  return {};\\n},\\n```\\n\\n## Manual Loop Control\\n\\nFor scenarios requiring complete control over the agent loop, you can use AI SDK Core functions (`generateText` and `streamText`) to implement your own loop management instead of using `stopWhen` and `prepareStep`. This approach provides maximum flexibility for complex workflows.\\n\\n### Implementing a Manual Loop\\n\\nBuild your own agent loop when you need full control over execution:\\n\\n```ts\\nimport { generateText, ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [{ role: \\'user\\', content: \\'...\\' }];\\n\\nlet step = 0;\\nconst maxSteps = 10;\\n\\nwhile (step < maxSteps) {\\n  const result = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages,\\n    tools: {\\n      // your tools here\\n    },\\n  });\\n\\n  messages.push(...result.response.messages);\\n\\n  if (result.text) {\\n    break; // Stop when model generates text\\n  }\\n\\n  step++;\\n}\\n```\\n\\nThis manual approach gives you complete control over:\\n\\n- Message history management\\n- Step-by-step decision making\\n- Custom stopping conditions\\n- Dynamic tool and model selection\\n- Error handling and recovery\\n\\n[Learn more about manual agent loops in the cookbook](/cookbook/node/manual-agent-loop).\\n', children=[]), DocItem(origPath=Path('03-agents/05-configuring-call-options.mdx'), name='05-configuring-call-options.mdx', displayName='05-configuring-call-options.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Configuring Call Options\\ndescription: Pass type-safe runtime inputs to dynamically configure agent behavior.\\n---\\n\\n# Configuring Call Options\\n\\nCall options allow you to pass type-safe structured inputs to your agent. Use them to dynamically modify any agent setting based on the specific request.\\n\\n## Why Use Call Options?\\n\\nWhen you need agent behavior to change based on runtime context:\\n\\n- **Add dynamic context** - Inject retrieved documents, user preferences, or session data into prompts\\n- **Select models dynamically** - Choose faster or more capable models based on request complexity\\n- **Configure tools per request** - Pass user location to search tools or adjust tool behavior\\n- **Customize provider options** - Set reasoning effort, temperature, or other provider-specific settings\\n\\nWithout call options, you\\'d need to create multiple agents or handle configuration logic outside the agent.\\n\\n## How It Works\\n\\nDefine call options in three steps:\\n\\n1. **Define the schema** - Specify what inputs you accept using `callOptionsSchema`\\n2. **Configure with `prepareCall`** - Use those inputs to modify agent settings\\n3. **Pass options at runtime** - Provide the options when calling `generate()` or `stream()`\\n\\n## Basic Example\\n\\nAdd user context to your agent\\'s prompt at runtime:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst supportAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    userId: z.string(),\\n    accountType: z.enum([\\'free\\', \\'pro\\', \\'enterprise\\']),\\n  }),\\n  instructions: \\'You are a helpful customer support agent.\\',\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    instructions:\\n      settings.instructions +\\n      `\\\\nUser context:\\n- Account type: ${options.accountType}\\n- User ID: ${options.userId}\\n\\nAdjust your response based on the user\\'s account level.`,\\n  }),\\n});\\n\\n// Call the agent with specific user context\\nconst result = await supportAgent.generate({\\n  prompt: \\'How do I upgrade my account?\\',\\n  options: {\\n    userId: \\'user_123\\',\\n    accountType: \\'free\\',\\n  },\\n});\\n```\\n\\nThe `options` parameter is now required and type-checked. If you don\\'t provide it or pass incorrect types, TypeScript will error.\\n\\n## Modifying Agent Settings\\n\\nUse `prepareCall` to modify any agent setting. Return only the settings you want to change.\\n\\n### Dynamic Model Selection\\n\\nChoose models based on request characteristics:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/gpt-4o-mini\\', // Default model\\n  callOptionsSchema: z.object({\\n    complexity: z.enum([\\'simple\\', \\'complex\\']),\\n  }),\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    model:\\n      options.complexity === \\'simple\\' ? \\'openai/gpt-4o-mini\\' : \\'openai/o1-mini\\',\\n  }),\\n});\\n\\n// Use faster model for simple queries\\nawait agent.generate({\\n  prompt: \\'What is 2+2?\\',\\n  options: { complexity: \\'simple\\' },\\n});\\n\\n// Use more capable model for complex reasoning\\nawait agent.generate({\\n  prompt: \\'Explain quantum entanglement\\',\\n  options: { complexity: \\'complex\\' },\\n});\\n```\\n\\n### Dynamic Tool Configuration\\n\\nConfigure tools based on runtime context:\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst newsAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    userCity: z.string().optional(),\\n    userRegion: z.string().optional(),\\n  }),\\n  tools: {\\n    web_search: openai.tools.webSearch(),\\n  },\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    tools: {\\n      web_search: openai.tools.webSearch({\\n        searchContextSize: \\'low\\',\\n        userLocation: {\\n          type: \\'approximate\\',\\n          city: options.userCity,\\n          region: options.userRegion,\\n          country: \\'US\\',\\n        },\\n      }),\\n    },\\n  }),\\n});\\n\\nawait newsAgent.generate({\\n  prompt: \\'What are the top local news stories?\\',\\n  options: {\\n    userCity: \\'San Francisco\\',\\n    userRegion: \\'California\\',\\n  },\\n});\\n```\\n\\n### Provider-Specific Options\\n\\nConfigure provider settings dynamically:\\n\\n```ts\\nimport { openai, OpenAIProviderOptions } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/o1-mini\\',\\n  callOptionsSchema: z.object({\\n    taskDifficulty: z.enum([\\'low\\', \\'medium\\', \\'high\\']),\\n  }),\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    providerOptions: {\\n      openai: {\\n        reasoningEffort: options.taskDifficulty,\\n      } satisfies OpenAIProviderOptions,\\n    },\\n  }),\\n});\\n\\nawait agent.generate({\\n  prompt: \\'Analyze this complex scenario...\\',\\n  options: { taskDifficulty: \\'high\\' },\\n});\\n```\\n\\n## Advanced Patterns\\n\\n### Retrieval Augmented Generation (RAG)\\n\\nFetch relevant context and inject it into your prompt:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst ragAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  callOptionsSchema: z.object({\\n    query: z.string(),\\n  }),\\n  prepareCall: async ({ options, ...settings }) => {\\n    // Fetch relevant documents (this can be async)\\n    const documents = await vectorSearch(options.query);\\n\\n    return {\\n      ...settings,\\n      instructions: `Answer questions using the following context:\\n\\n${documents.map(doc => doc.content).join(\\'\\\\n\\\\n\\')}`,\\n    };\\n  },\\n});\\n\\nawait ragAgent.generate({\\n  prompt: \\'What is our refund policy?\\',\\n  options: { query: \\'refund policy\\' },\\n});\\n```\\n\\nThe `prepareCall` function can be async, enabling you to fetch data before configuring the agent.\\n\\n### Combining Multiple Modifications\\n\\nModify multiple settings together:\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'openai/gpt-5-nano\\',\\n  callOptionsSchema: z.object({\\n    userRole: z.enum([\\'admin\\', \\'user\\']),\\n    urgency: z.enum([\\'low\\', \\'high\\']),\\n  }),\\n  tools: {\\n    readDatabase: readDatabaseTool,\\n    writeDatabase: writeDatabaseTool,\\n  },\\n  prepareCall: ({ options, ...settings }) => ({\\n    ...settings,\\n    // Upgrade model for urgent requests\\n    model:\\n      options.urgency === \\'high\\'\\n        ? \\'anthropic/claude-sonnet-4.5\\'\\n        : settings.model,\\n    // Limit tools based on user role\\n    activeTools:\\n      options.userRole === \\'admin\\'\\n        ? [\\'readDatabase\\', \\'writeDatabase\\']\\n        : [\\'readDatabase\\'],\\n    // Adjust instructions\\n    instructions: `You are a ${options.userRole} assistant.\\n${options.userRole === \\'admin\\' ? \\'You have full database access.\\' : \\'You have read-only access.\\'}`,\\n  }),\\n});\\n\\nawait agent.generate({\\n  prompt: \\'Update the user record\\',\\n  options: {\\n    userRole: \\'admin\\',\\n    urgency: \\'high\\',\\n  },\\n});\\n```\\n\\n## Using with createAgentUIStreamResponse\\n\\nPass call options through API routes to your agent:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\nimport { myAgent } from \\'@/ai/agents/my-agent\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages, userId, accountType } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: myAgent,\\n    messages,\\n    options: {\\n      userId,\\n      accountType,\\n    },\\n  });\\n}\\n```\\n\\n## Next Steps\\n\\n- Learn about [loop control](/docs/agents/loop-control) for execution management\\n- Explore [workflow patterns](/docs/agents/workflows) for complex multi-step processes\\n', children=[]), DocItem(origPath=Path('03-agents/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Agents\\ndescription: An overview of building agents with the AI SDK.\\n---\\n\\n# Agents\\n\\nThe following section show you how to build agents with the AI SDK - systems where large language models (LLMs) use tools in a loop to accomplish tasks.\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description: 'Learn what agents are and why to use the Agent class.',\\n      href: '/docs/agents/overview',\\n    },\\n    {\\n      title: 'Building Agents',\\n      description: 'Complete guide to creating agents with the Agent class.',\\n      href: '/docs/agents/building-agents',\\n    },\\n    {\\n      title: 'Workflow Patterns',\\n      description:\\n        'Structured patterns using core functions for complex workflows.',\\n      href: '/docs/agents/workflows',\\n    },\\n    {\\n      title: 'Loop Control',\\n      description: 'Advanced execution control with stopWhen and prepareStep.',\\n      href: '/docs/agents/loop-control',\\n    },\\n    {\\n      title: 'Configuring Call Options',\\n      description:\\n        'Pass type-safe runtime inputs to dynamically configure agent behavior.',\\n      href: '/docs/agents/configuring-call-options',\\n    },\\n  ]}\\n/>\\n\", children=[])]),\n",
       " DocItem(origPath=Path('03-ai-sdk-core'), name='03-ai-sdk-core', displayName='03-ai-sdk-core', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('03-ai-sdk-core/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of AI SDK Core.\\n---\\n\\n# AI SDK Core\\n\\nLarge Language Models (LLMs) are advanced programs that can understand, create, and engage with human language on a large scale.\\nThey are trained on vast amounts of written material to recognize patterns in language and predict what might come next in a given piece of text.\\n\\nAI SDK Core **simplifies working with LLMs by offering a standardized way of integrating them into your app** - so you can focus on building great AI applications for your users, not waste time on technical details.\\n\\nFor example, here’s how you can generate text with various models using the AI SDK:\\n\\n<PreviewSwitchProviders />\\n\\n## AI SDK Core Functions\\n\\nAI SDK Core has various functions designed for [text generation](./generating-text), [structured data generation](./generating-structured-data), and [tool usage](./tools-and-tool-calling).\\nThese functions take a standardized approach to setting up [prompts](./prompts) and [settings](./settings), making it easier to work with different models.\\n\\n- [`generateText`](/docs/ai-sdk-core/generating-text): Generates text and [tool calls](./tools-and-tool-calling).\\n  This function is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\\n- [`streamText`](/docs/ai-sdk-core/generating-text): Stream text and tool calls.\\n  You can use the `streamText` function for interactive use cases such as [chat bots](/docs/ai-sdk-ui/chatbot) and [content streaming](/docs/ai-sdk-ui/completion).\\n- [`generateObject`](/docs/ai-sdk-core/generating-structured-data): Generates a typed, structured object that matches a [Zod](https://zod.dev/) schema.\\n  You can use this function to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\\n- [`streamObject`](/docs/ai-sdk-core/generating-structured-data): Stream a structured object that matches a Zod schema.\\n  You can use this function to [stream generated UIs](/docs/ai-sdk-ui/object-generation).\\n\\n## API Reference\\n\\nPlease check out the [AI SDK Core API Reference](/docs/reference/ai-sdk-core) for more details on each function.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/05-generating-text.mdx'), name='05-generating-text.mdx', displayName='05-generating-text.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Generating Text\\ndescription: Learn how to generate text with the AI SDK.\\n---\\n\\n# Generating and Streaming Text\\n\\nLarge language models (LLMs) can generate text in response to a prompt, which can contain instructions and information to process.\\nFor example, you can ask a model to come up with a recipe, draft an email, or summarize a document.\\n\\nThe AI SDK Core provides two functions to generate text and stream it from LLMs:\\n\\n- [`generateText`](#generatetext): Generates text for a given prompt and model.\\n- [`streamText`](#streamtext): Streams text from a given prompt and model.\\n\\nAdvanced LLM features such as [tool calling](./tools-and-tool-calling) and [structured data generation](./generating-structured-data) are built on top of text generation.\\n\\n## `generateText`\\n\\nYou can generate text using the [`generateText`](/docs/reference/ai-sdk-core/generate-text) function. This function is ideal for non-interactive use cases where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\\n\\n```tsx\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n});\\n```\\n\\nYou can use more [advanced prompts](./prompts) to generate text with more complex instructions and content:\\n\\n```tsx\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system:\\n    \\'You are a professional writer. \\' +\\n    \\'You write simple, clear, and concise content.\\',\\n  prompt: `Summarize the following article in 3-5 sentences: ${article}`,\\n});\\n```\\n\\nThe result object of `generateText` contains several promises that resolve when all required data is available:\\n\\n- `result.content`: The content that was generated in the last step.\\n- `result.text`: The generated text.\\n- `result.reasoning`: The full reasoning that the model has generated in the last step.\\n- `result.reasoningText`: The reasoning text of the model (only available for some models).\\n- `result.files`: The files that were generated in the last step.\\n- `result.sources`: Sources that have been used as references in the last step (only available for some models).\\n- `result.toolCalls`: The tool calls that were made in the last step.\\n- `result.toolResults`: The results of the tool calls from the last step.\\n- `result.finishReason`: The reason the model finished generating text.\\n- `result.usage`: The usage of the model during the final step of text generation.\\n- `result.totalUsage`: The total usage across all steps (for multi-step generations).\\n- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).\\n- `result.request`: Additional request information.\\n- `result.response`: Additional response information, including response messages and body.\\n- `result.providerMetadata`: Additional provider-specific metadata.\\n- `result.steps`: Details for all steps, useful for getting information about intermediate steps.\\n- `result.output`: The generated structured output using the `output` specification.\\n\\n### Accessing response headers & body\\n\\nSometimes you need access to the full response from the model provider,\\ne.g. to access some provider-specific headers or body content.\\n\\nYou can access the raw response headers and body using the `response` property:\\n\\n```ts\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n});\\n\\nconsole.log(JSON.stringify(result.response.headers, null, 2));\\nconsole.log(JSON.stringify(result.response.body, null, 2));\\n```\\n\\n### `onFinish` callback\\n\\nWhen using `generateText`, you can provide an `onFinish` callback that is triggered after the last step is finished (\\n[API Reference](/docs/reference/ai-sdk-core/generate-text#on-finish)\\n).\\nIt contains the text, usage information, finish reason, messages, steps, total usage, and more:\\n\\n```tsx highlight=\"6-8\"\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\\n    // your own logic, e.g. for saving the chat history or recording usage\\n\\n    const messages = response.messages; // messages that were generated\\n  },\\n});\\n```\\n\\n## `streamText`\\n\\nDepending on your model and prompt, it can take a large language model (LLM) up to a minute to finish generating its response. This delay can be unacceptable for interactive use cases such as chatbots or real-time applications, where users expect immediate responses.\\n\\nAI SDK Core provides the [`streamText`](/docs/reference/ai-sdk-core/stream-text) function which simplifies streaming text from LLMs:\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n\\n// example: use textStream as an async iterable\\nfor await (const textPart of result.textStream) {\\n  console.log(textPart);\\n}\\n```\\n\\n<Note>\\n  `result.textStream` is both a `ReadableStream` and an `AsyncIterable`.\\n</Note>\\n\\n<Note type=\"warning\">\\n  `streamText` immediately starts streaming and suppresses errors to prevent\\n  server crashes. Use the `onError` callback to log errors.\\n</Note>\\n\\nYou can use `streamText` on its own or in combination with [AI SDK\\nUI](/examples/next-pages/basics/streaming-text-generation) and [AI SDK\\nRSC](/examples/next-app/basics/streaming-text-generation).\\nThe result object contains several helper functions to make the integration into [AI SDK UI](/docs/ai-sdk-ui) easier:\\n\\n- `result.toUIMessageStreamResponse()`: Creates a UI Message stream HTTP response (with tool calls etc.) that can be used in a Next.js App Router API route.\\n- `result.pipeUIMessageStreamToResponse()`: Writes UI Message stream delta output to a Node.js response-like object.\\n- `result.toTextStreamResponse()`: Creates a simple text stream HTTP response.\\n- `result.pipeTextStreamToResponse()`: Writes text delta output to a Node.js response-like object.\\n\\n<Note>\\n  `streamText` is using backpressure and only generates tokens as they are\\n  requested. You need to consume the stream in order for it to finish.\\n</Note>\\n\\nIt also provides several promises that resolve when the stream is finished:\\n\\n- `result.content`: The content that was generated in the last step.\\n- `result.text`: The generated text.\\n- `result.reasoning`: The full reasoning that the model has generated.\\n- `result.reasoningText`: The reasoning text of the model (only available for some models).\\n- `result.files`: Files that have been generated by the model in the last step.\\n- `result.sources`: Sources that have been used as references in the last step (only available for some models).\\n- `result.toolCalls`: The tool calls that have been executed in the last step.\\n- `result.toolResults`: The tool results that have been generated in the last step.\\n- `result.finishReason`: The reason the model finished generating text.\\n- `result.usage`: The usage of the model during the final step of text generation.\\n- `result.totalUsage`: The total usage across all steps (for multi-step generations).\\n- `result.warnings`: Warnings from the model provider (e.g. unsupported settings).\\n- `result.steps`: Details for all steps, useful for getting information about intermediate steps.\\n- `result.request`: Additional request information from the last step.\\n- `result.response`: Additional response information from the last step.\\n- `result.providerMetadata`: Additional provider-specific metadata from the last step.\\n\\n### `onError` callback\\n\\n`streamText` immediately starts streaming to enable sending data without waiting for the model.\\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\\n\\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\\n\\n```tsx highlight=\"6-8\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onError({ error }) {\\n    console.error(error); // your error logging logic here\\n  },\\n});\\n```\\n\\n### `onChunk` callback\\n\\nWhen using `streamText`, you can provide an `onChunk` callback that is triggered for each chunk of the stream.\\n\\nIt receives the following chunk types:\\n\\n- `text`\\n- `reasoning`\\n- `source`\\n- `tool-call`\\n- `tool-input-start`\\n- `tool-input-delta`\\n- `tool-result`\\n- `raw`\\n\\n```tsx highlight=\"6-11\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onChunk({ chunk }) {\\n    // implement your own logic here, e.g.:\\n    if (chunk.type === \\'text\\') {\\n      console.log(chunk.text);\\n    }\\n  },\\n});\\n```\\n\\n### `onFinish` callback\\n\\nWhen using `streamText`, you can provide an `onFinish` callback that is triggered when the stream is finished (\\n[API Reference](/docs/reference/ai-sdk-core/stream-text#on-finish)\\n).\\nIt contains the text, usage information, finish reason, messages, steps, total usage, and more:\\n\\n```tsx highlight=\"6-8\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onFinish({ text, finishReason, usage, response, steps, totalUsage }) {\\n    // your own logic, e.g. for saving the chat history or recording usage\\n\\n    const messages = response.messages; // messages that were generated\\n  },\\n});\\n```\\n\\n### `fullStream` property\\n\\nYou can read a stream with all events using the `fullStream` property.\\nThis can be useful if you want to implement your own UI or handle the stream in a different way.\\nHere is an example of how to use the `fullStream` property:\\n\\n```tsx\\nimport { streamText } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    cityAttractions: {\\n      inputSchema: z.object({ city: z.string() }),\\n      execute: async ({ city }) => ({\\n        attractions: [\\'attraction1\\', \\'attraction2\\', \\'attraction3\\'],\\n      }),\\n    },\\n  },\\n  prompt: \\'What are some San Francisco tourist attractions?\\',\\n});\\n\\nfor await (const part of result.fullStream) {\\n  switch (part.type) {\\n    case \\'start\\': {\\n      // handle start of stream\\n      break;\\n    }\\n    case \\'start-step\\': {\\n      // handle start of step\\n      break;\\n    }\\n    case \\'text-start\\': {\\n      // handle text start\\n      break;\\n    }\\n    case \\'text-delta\\': {\\n      // handle text delta here\\n      break;\\n    }\\n    case \\'text-end\\': {\\n      // handle text end\\n      break;\\n    }\\n    case \\'reasoning-start\\': {\\n      // handle reasoning start\\n      break;\\n    }\\n    case \\'reasoning-delta\\': {\\n      // handle reasoning delta here\\n      break;\\n    }\\n    case \\'reasoning-end\\': {\\n      // handle reasoning end\\n      break;\\n    }\\n    case \\'source\\': {\\n      // handle source here\\n      break;\\n    }\\n    case \\'file\\': {\\n      // handle file here\\n      break;\\n    }\\n    case \\'tool-call\\': {\\n      switch (part.toolName) {\\n        case \\'cityAttractions\\': {\\n          // handle tool call here\\n          break;\\n        }\\n      }\\n      break;\\n    }\\n    case \\'tool-input-start\\': {\\n      // handle tool input start\\n      break;\\n    }\\n    case \\'tool-input-delta\\': {\\n      // handle tool input delta\\n      break;\\n    }\\n    case \\'tool-input-end\\': {\\n      // handle tool input end\\n      break;\\n    }\\n    case \\'tool-result\\': {\\n      switch (part.toolName) {\\n        case \\'cityAttractions\\': {\\n          // handle tool result here\\n          break;\\n        }\\n      }\\n      break;\\n    }\\n    case \\'tool-error\\': {\\n      // handle tool error\\n      break;\\n    }\\n    case \\'finish-step\\': {\\n      // handle finish step\\n      break;\\n    }\\n    case \\'finish\\': {\\n      // handle finish here\\n      break;\\n    }\\n    case \\'error\\': {\\n      // handle error here\\n      break;\\n    }\\n    case \\'raw\\': {\\n      // handle raw value\\n      break;\\n    }\\n  }\\n}\\n```\\n\\n### Stream transformation\\n\\nYou can use the `experimental_transform` option to transform the stream.\\nThis is useful for e.g. filtering, changing, or smoothing the text stream.\\n\\nThe transformations are applied before the callbacks are invoked and the promises are resolved.\\nIf you e.g. have a transformation that changes all text to uppercase, the `onFinish` callback will receive the transformed text.\\n\\n#### Smoothing streams\\n\\nThe AI SDK Core provides a [`smoothStream` function](/docs/reference/ai-sdk-core/smooth-stream) that\\ncan be used to smooth out text streaming.\\n\\n```tsx highlight=\"6\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: smoothStream(),\\n});\\n```\\n\\n#### Custom transformations\\n\\nYou can also implement your own custom transformations.\\nThe transformation function receives the tools that are available to the model,\\nand returns a function that is used to transform the stream.\\nTools can either be generic or limited to the tools that you are using.\\n\\nHere is an example of how to implement a custom transformation that converts\\nall text to uppercase:\\n\\n```ts\\nconst upperCaseTransform =\\n  <TOOLS extends ToolSet>() =>\\n  (options: { tools: TOOLS; stopStream: () => void }) =>\\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\\n      transform(chunk, controller) {\\n        controller.enqueue(\\n          // for text chunks, convert the text to uppercase:\\n          chunk.type === \\'text\\'\\n            ? { ...chunk, text: chunk.text.toUpperCase() }\\n            : chunk,\\n        );\\n      },\\n    });\\n```\\n\\nYou can also stop the stream using the `stopStream` function.\\nThis is e.g. useful if you want to stop the stream when model guardrails are violated, e.g. by generating inappropriate content.\\n\\nWhen you invoke `stopStream`, it is important to simulate the `step-finish` and `finish` events to guarantee that a well-formed stream is returned\\nand all callbacks are invoked.\\n\\n```ts\\nconst stopWordTransform =\\n  <TOOLS extends ToolSet>() =>\\n  ({ stopStream }: { stopStream: () => void }) =>\\n    new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\\n      // note: this is a simplified transformation for testing;\\n      // in a real-world version more there would need to be\\n      // stream buffering and scanning to correctly emit prior text\\n      // and to detect all STOP occurrences.\\n      transform(chunk, controller) {\\n        if (chunk.type !== \\'text\\') {\\n          controller.enqueue(chunk);\\n          return;\\n        }\\n\\n        if (chunk.text.includes(\\'STOP\\')) {\\n          // stop the stream\\n          stopStream();\\n\\n          // simulate the finish-step event\\n          controller.enqueue({\\n            type: \\'finish-step\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: {\\n              completionTokens: NaN,\\n              promptTokens: NaN,\\n              totalTokens: NaN,\\n            },\\n            request: {},\\n            response: {\\n              id: \\'response-id\\',\\n              modelId: \\'mock-model-id\\',\\n              timestamp: new Date(0),\\n            },\\n            warnings: [],\\n            isContinued: false,\\n          });\\n\\n          // simulate the finish event\\n          controller.enqueue({\\n            type: \\'finish\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: {\\n              completionTokens: NaN,\\n              promptTokens: NaN,\\n              totalTokens: NaN,\\n            },\\n            response: {\\n              id: \\'response-id\\',\\n              modelId: \\'mock-model-id\\',\\n              timestamp: new Date(0),\\n            },\\n          });\\n\\n          return;\\n        }\\n\\n        controller.enqueue(chunk);\\n      },\\n    });\\n```\\n\\n#### Multiple transformations\\n\\nYou can also provide multiple transformations. They are applied in the order they are provided.\\n\\n```tsx highlight=\"4\"\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: [firstTransform, secondTransform],\\n});\\n```\\n\\n## Sources\\n\\nSome providers such as [Perplexity](/providers/ai-sdk-providers/perplexity#sources) and\\n[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.\\n\\nCurrently sources are limited to web pages that ground the response.\\nYou can access them using the `sources` property of the result.\\n\\nEach `url` source contains the following properties:\\n\\n- `id`: The ID of the source.\\n- `url`: The URL of the source.\\n- `title`: The optional title of the source.\\n- `providerMetadata`: Provider metadata for the source.\\n\\nWhen you use `generateText`, you can access the sources using the `sources` property:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'google/gemini-2.5-flash\\',\\n  tools: {\\n    google_search: google.tools.googleSearch({}),\\n  },\\n  prompt: \\'List the top 5 San Francisco news from the past week.\\',\\n});\\n\\nfor (const source of result.sources) {\\n  if (source.sourceType === \\'url\\') {\\n    console.log(\\'ID:\\', source.id);\\n    console.log(\\'Title:\\', source.title);\\n    console.log(\\'URL:\\', source.url);\\n    console.log(\\'Provider metadata:\\', source.providerMetadata);\\n    console.log();\\n  }\\n}\\n```\\n\\nWhen you use `streamText`, you can access the sources using the `fullStream` property:\\n\\n```tsx\\nconst result = streamText({\\n  model: \\'google/gemini-2.5-flash\\',\\n  tools: {\\n    google_search: google.tools.googleSearch({}),\\n  },\\n  prompt: \\'List the top 5 San Francisco news from the past week.\\',\\n});\\n\\nfor await (const part of result.fullStream) {\\n  if (part.type === \\'source\\' && part.sourceType === \\'url\\') {\\n    console.log(\\'ID:\\', part.id);\\n    console.log(\\'Title:\\', part.title);\\n    console.log(\\'URL:\\', part.url);\\n    console.log(\\'Provider metadata:\\', part.providerMetadata);\\n    console.log();\\n  }\\n}\\n```\\n\\nThe sources are also available in the `result.sources` promise.\\n\\n## Examples\\n\\nYou can see `generateText` and `streamText` in action using various frameworks in the following examples:\\n\\n### `generateText`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to generate text in Node.js\\',\\n      link: \\'/examples/node/generating-text/generate-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate text in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/generating-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate text in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/generating-text\\',\\n    },\\n  ]}\\n/>\\n\\n### `streamText`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to stream text in Node.js\\',\\n      link: \\'/examples/node/generating-text/stream-text\\',\\n    },\\n    {\\n      title: \\'Learn to stream text in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/streaming-text-generation\\',\\n    },\\n    {\\n      title: \\'Learn to stream text in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/streaming-text-generation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/10-generating-structured-data.mdx'), name='10-generating-structured-data.mdx', displayName='10-generating-structured-data.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Generating Structured Data\\ndescription: Learn how to generate structured data with the AI SDK.\\n---\\n\\n# Generating Structured Data\\n\\nWhile text generation can be useful, your use case will likely call for generating structured data.\\nFor example, you might want to extract information from text, classify data, or generate synthetic data.\\n\\nMany language models are capable of generating structured data, often defined as using \"JSON modes\" or \"tools\".\\nHowever, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.\\n\\nThe AI SDK standardises structured object generation across model providers\\nwith the [`generateObject`](/docs/reference/ai-sdk-core/generate-object)\\nand [`streamObject`](/docs/reference/ai-sdk-core/stream-object) functions.\\nYou can use both functions with different output strategies, e.g. `array`, `object`, `enum`, or `no-schema`,\\nand with different generation modes, e.g. `auto`, `tool`, or `json`.\\nYou can use [Zod schemas](/docs/reference/ai-sdk-core/zod-schema), [Valibot](/docs/reference/ai-sdk-core/valibot-schema), or [JSON schemas](/docs/reference/ai-sdk-core/json-schema) to specify the shape of the data that you want,\\nand the AI model will generate data that conforms to that structure.\\n\\n<Note>\\n  You can pass Zod objects directly to the AI SDK functions or use the\\n  `zodSchema` helper function.\\n</Note>\\n\\n## Generate Object\\n\\nThe `generateObject` generates structured data from a prompt.\\nThe schema is also used to validate the generated data, ensuring type safety and correctness.\\n\\n```ts\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\n<Note>\\n  See `generateObject` in action with [these examples](#more-examples)\\n</Note>\\n\\n### Accessing response headers & body\\n\\nSometimes you need access to the full response from the model provider,\\ne.g. to access some provider-specific headers or body content.\\n\\nYou can access the raw response headers and body using the `response` property:\\n\\n```ts\\nimport { generateObject } from \\'ai\\';\\n\\nconst result = await generateObject({\\n  // ...\\n});\\n\\nconsole.log(JSON.stringify(result.response.headers, null, 2));\\nconsole.log(JSON.stringify(result.response.body, null, 2));\\n```\\n\\n## Stream Object\\n\\nGiven the added complexity of returning structured data, model response time can be unacceptable for your interactive use case.\\nWith the [`streamObject`](/docs/reference/ai-sdk-core/stream-object) function, you can stream the model\\'s response as it is generated.\\n\\n```ts\\nimport { streamObject } from \\'ai\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  // ...\\n});\\n\\n// use partialObjectStream as an async iterable\\nfor await (const partialObject of partialObjectStream) {\\n  console.log(partialObject);\\n}\\n```\\n\\nYou can use `streamObject` to stream generated UIs in combination with React Server Components (see [Generative UI](../ai-sdk-rsc))) or the [`useObject`](/docs/reference/ai-sdk-ui/use-object) hook.\\n\\n<Note>See `streamObject` in action with [these examples](#more-examples)</Note>\\n\\n### `onError` callback\\n\\n`streamObject` immediately starts streaming.\\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\\n\\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\\n\\n```tsx highlight=\"5-7\"\\nimport { streamObject } from \\'ai\\';\\n\\nconst result = streamObject({\\n  // ...\\n  onError({ error }) {\\n    console.error(error); // your error logging logic here\\n  },\\n});\\n```\\n\\n## Output Strategy\\n\\nYou can use both functions with different output strategies, e.g. `array`, `object`, `enum`, or `no-schema`.\\n\\n### Object\\n\\nThe default output strategy is `object`, which returns the generated data as an object.\\nYou don\\'t need to specify the output strategy if you want to use the default.\\n\\n### Array\\n\\nIf you want to generate an array of objects, you can set the output strategy to `array`.\\nWhen you use the `array` output strategy, the schema specifies the shape of an array element.\\nWith `streamObject`, you can also stream the generated array elements using `elementStream`.\\n\\n```ts highlight=\"7,18\"\\nimport { streamObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { elementStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'array\\',\\n  schema: z.object({\\n    name: z.string(),\\n    class: z\\n      .string()\\n      .describe(\\'Character class, e.g. warrior, mage, or thief.\\'),\\n    description: z.string(),\\n  }),\\n  prompt: \\'Generate 3 hero descriptions for a fantasy role playing game.\\',\\n});\\n\\nfor await (const hero of elementStream) {\\n  console.log(hero);\\n}\\n```\\n\\n### Enum\\n\\nIf you want to generate a specific enum value, e.g. for classification tasks,\\nyou can set the output strategy to `enum`\\nand provide a list of possible values in the `enum` parameter.\\n\\n<Note>Enum output is only available with `generateObject`.</Note>\\n\\n```ts highlight=\"5-6\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'enum\\',\\n  enum: [\\'action\\', \\'comedy\\', \\'drama\\', \\'horror\\', \\'sci-fi\\'],\\n  prompt:\\n    \\'Classify the genre of this movie plot: \\' +\\n    \\'\"A group of astronauts travel through a wormhole in search of a \\' +\\n    \\'new habitable planet for humanity.\"\\',\\n});\\n```\\n\\n### No Schema\\n\\nIn some cases, you might not want to use a schema,\\nfor example when the data is a dynamic user request.\\nYou can use the `output` setting to set the output format to `no-schema` in those cases\\nand omit the schema parameter.\\n\\n```ts highlight=\"6\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'no-schema\\',\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\n## Schema Name and Description\\n\\nYou can optionally specify a name and description for the schema. These are used by some providers for additional LLM guidance, e.g. via tool or schema name.\\n\\n```ts highlight=\"6-7\"\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schemaName: \\'Recipe\\',\\n  schemaDescription: \\'A recipe for a dish.\\',\\n  schema: z.object({\\n    name: z.string(),\\n    ingredients: z.array(z.object({ name: z.string(), amount: z.string() })),\\n    steps: z.array(z.string()),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\n## Accessing Reasoning\\n\\nYou can access the reasoning used by the language model to generate the object via the `reasoning` property on the result. This property contains a string with the model\\'s thought process, if available.\\n\\n```ts\\nimport { OpenAIResponsesProviderOptions } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = await generateObject({\\n  model: \\'openai/gpt-5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(\\n        z.object({\\n          name: z.string(),\\n          amount: z.string(),\\n        }),\\n      ),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n  providerOptions: {\\n    openai: {\\n      strictJsonSchema: true,\\n      reasoningSummary: \\'detailed\\',\\n    } satisfies OpenAIResponsesProviderOptions,\\n  },\\n});\\n\\nconsole.log(result.reasoning);\\n```\\n\\n## Error Handling\\n\\nWhen `generateObject` cannot generate a valid object, it throws a [`AI_NoObjectGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-object-generated-error).\\n\\nThis error occurs when the AI provider fails to generate a parsable object that conforms to the schema.\\nIt can arise due to the following reasons:\\n\\n- The model failed to generate a response.\\n- The model generated a response that could not be parsed.\\n- The model generated a response that could not be validated against the schema.\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.\\n- `response`: Metadata about the language model response, including response id, timestamp, and model.\\n- `usage`: Request token usage.\\n- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.\\n\\n```ts\\nimport { generateObject, NoObjectGeneratedError } from \\'ai\\';\\n\\ntry {\\n  await generateObject({ model, schema, prompt });\\n} catch (error) {\\n  if (NoObjectGeneratedError.isInstance(error)) {\\n    console.log(\\'NoObjectGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Text:\\', error.text);\\n    console.log(\\'Response:\\', error.response);\\n    console.log(\\'Usage:\\', error.usage);\\n  }\\n}\\n```\\n\\n## Repairing Invalid or Malformed JSON\\n\\n<Note type=\"warning\">\\n  The `repairText` function is experimental and may change in the future.\\n</Note>\\n\\nSometimes the model will generate invalid or malformed JSON.\\nYou can use the `repairText` function to attempt to repair the JSON.\\n\\nIt receives the error, either a `JSONParseError` or a `TypeValidationError`,\\nand the text that was generated by the model.\\nYou can then attempt to repair the text and return the repaired text.\\n\\n```ts highlight=\"7-10\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model,\\n  schema,\\n  prompt,\\n  experimental_repairText: async ({ text, error }) => {\\n    // example: add a closing brace to the text\\n    return text + \\'}\\';\\n  },\\n});\\n```\\n\\n## Structured outputs with `generateText` and `streamText`\\n\\nYou can generate structured data with `generateText` and `streamText` by using the `output` setting.\\n\\n<Note>\\n  Some models, e.g. those by OpenAI, support structured outputs and tool calling\\n  at the same time. This is only possible with `generateText` and `streamText`.\\n</Note>\\n\\n### `generateText`\\n\\n```ts highlight=\"2,4-18\"\\n// output is a structured object that matches the schema:\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number().nullable().describe(\\'Age of the person.\\'),\\n      contact: z.object({\\n        type: z.literal(\\'email\\'),\\n        value: z.string(),\\n      }),\\n      occupation: z.object({\\n        type: z.literal(\\'employed\\'),\\n        company: z.string(),\\n        position: z.string(),\\n      }),\\n    }),\\n  }),\\n  prompt: \\'Generate an example person for testing.\\',\\n});\\n```\\n\\n### `streamText`\\n\\n```ts highlight=\"2,4-18\"\\n// partialOutputStream contains generated partial objects:\\nconst { partialOutputStream } = await streamText({\\n  // ...\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number().nullable().describe(\\'Age of the person.\\'),\\n      contact: z.object({\\n        type: z.literal(\\'email\\'),\\n        value: z.string(),\\n      }),\\n      occupation: z.object({\\n        type: z.literal(\\'employed\\'),\\n        company: z.string(),\\n        position: z.string(),\\n      }),\\n    }),\\n  }),\\n  prompt: \\'Generate an example person for testing.\\',\\n});\\n```\\n\\n### Output Types\\n\\nThe AI SDK supports multiple ways of specifying the expected structure of generated data via the `Output` object. You can select from various strategies for structured/text generation and validation.\\n\\n#### `Output.text()`\\n\\nUse `Output.text()` to generate plain text from a model. This option doesn\\'t enforce any schema on the result: you simply receive the model\\'s text as a string.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.text(),\\n  prompt: \\'Tell me a joke.\\',\\n});\\n// output will be a string (the joke)\\n```\\n\\n#### `Output.object()`\\n\\nUse `Output.object({ schema })` to generate a structured object based on a schema (for example, a Zod schema). The output is type-validated to ensure the returned result matches the schema.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.object({\\n    schema: z.object({\\n      name: z.string(),\\n      age: z.number().nullable(),\\n      labels: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate information for a test user.\\',\\n});\\n// output will be an object matching the schema above\\n```\\n\\nPartial outputs (e.g. with `streamText`) are also validated against your provided schema, as much as possible.\\n\\n#### `Output.array()`\\n\\nUse `Output.array({ element })` to specify that you expect an array of typed objects from the model, where each element should conform to a schema.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.array({\\n    element: z.object({\\n      location: z.string(),\\n      temperature: z.number(),\\n      condition: z.string(),\\n    }),\\n  }),\\n  prompt: \\'List the weather for San Francisco and Paris.\\',\\n});\\n// output will be an array of objects like:\\n// [\\n//   { location: \\'San Francisco\\', temperature: 70, condition: \\'Sunny\\' },\\n//   { location: \\'Paris\\', temperature: 65, condition: \\'Cloudy\\' },\\n// ]\\n```\\n\\nWith `Output.array`, the model is guided to return an object with an `elements` property that is an array; the SDK then extracts and validates this array for you.\\n\\nFor more advanced validation or different structures, see [the Output API reference](/docs/reference/ai-sdk-core/output).\\n\\n#### `Output.choice()`\\n\\nUse `Output.choice({ options })` when you expect the model to choose from a specific set of string options, such as for classification or fixed-enum answers.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.choice({\\n    options: [\\'sunny\\', \\'rainy\\', \\'snowy\\'],\\n  }),\\n  prompt: \\'Is the weather sunny, rainy, or snowy today?\\',\\n});\\n// output will be one of: \\'sunny\\', \\'rainy\\', or \\'snowy\\'\\n```\\n\\nYou can provide any set of string options, and the output will always be a single string value that matches one of the specified options. The SDK validates that the result matches one of your options, and will throw if the model returns something invalid.\\n\\nThis is especially useful for making classification-style generations or forcing valid values for API compatibility.\\n\\n#### `Output.json()`\\n\\nUse `Output.json()` when you want to generate and parse unstructured JSON values from the model, without enforcing a specific schema. This is useful if you want to capture arbitrary objects, flexible structures, or when you want to rely on the model\\'s natural output rather than rigid validation.\\n\\n```ts\\nconst { output } = await generateText({\\n  // ...\\n  output: Output.json(),\\n  prompt:\\n    \\'For each city, return the current temperature and weather condition as a JSON object.\\',\\n});\\n\\n// output could be any valid JSON, for example:\\n// {\\n//   \"San Francisco\": { \"temperature\": 70, \"condition\": \"Sunny\" },\\n//   \"Paris\": { \"temperature\": 65, \"condition\": \"Cloudy\" }\\n// }\\n```\\n\\nWith `Output.json`, the SDK only checks that the response is valid JSON; it doesn\\'t validate the structure or types of the values. If you need schema validation, use the `.object` or `.array` outputs instead.\\n\\nThis makes `Output.json()` ideal for custom or dynamic data structures, prototyping, working with unpredictable model output, or extracting information when you don\\'t have a strict schema.\\n\\n## More Examples\\n\\nYou can see `generateObject` and `streamObject` in action using various frameworks in the following examples:\\n\\n### `generateObject`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to generate objects in Node.js\\',\\n      link: \\'/examples/node/generating-structured-data/generate-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate objects in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/generating-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate objects in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/generating-object\\',\\n    },\\n  ]}\\n/>\\n\\n### `streamObject`\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to stream objects in Node.js\\',\\n      link: \\'/examples/node/streaming-structured-data/stream-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream objects in Next.js with Route Handlers (AI SDK UI)\\',\\n      link: \\'/examples/next-pages/basics/streaming-object-generation\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream objects in Next.js with Server Actions (AI SDK RSC)\\',\\n      link: \\'/examples/next-app/basics/streaming-object-generation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/15-tools-and-tool-calling.mdx'), name='15-tools-and-tool-calling.mdx', displayName='15-tools-and-tool-calling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Tool Calling\\ndescription: Learn about tool calling and multi-step calls (using stopWhen) with AI SDK Core.\\n---\\n\\n# Tool Calling\\n\\nAs covered under Foundations, [tools](/docs/foundations/tools) are objects that can be called by the model to perform a specific task.\\nAI SDK Core tools contain three elements:\\n\\n- **`description`**: An optional description of the tool that can influence when the tool is picked.\\n- **`inputSchema`**: A [Zod schema](/docs/foundations/tools#schemas) or a [JSON schema](/docs/reference/ai-sdk-core/json-schema) that defines the input parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.\\n- **`execute`**: An optional async function that is called with the inputs from the tool call. It produces a value of type `RESULT` (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.\\n\\n<Note className=\"mb-2\">\\n  You can use the [`tool`](/docs/reference/ai-sdk-core/tool) helper function to\\n  infer the types of the `execute` parameters.\\n</Note>\\n\\nThe `tools` parameter of `generateText` and `streamText` is an object that has the tool names as keys and the tools as values:\\n\\n```ts highlight=\"6-17\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n  },\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n<Note>\\n  When a model uses a tool, it is called a \"tool call\" and the output of the\\n  tool is called a \"tool result\".\\n</Note>\\n\\nTool calling is not restricted to only text generation.\\nYou can also use it to render user interfaces (Generative UI).\\n\\n## Multi-Step Calls (using stopWhen)\\n\\nWith the `stopWhen` setting, you can enable multi-step calls in `generateText` and `streamText`. When `stopWhen` is set and the model generates a tool call, the AI SDK will trigger a new generation passing in the tool result until there are no further tool calls or the stopping condition is met.\\n\\n<Note>\\n  The `stopWhen` conditions are only evaluated when the last step contains tool\\n  results.\\n</Note>\\n\\nBy default, when you use `generateText` or `streamText`, it triggers a single generation. This works well for many use cases where you can rely on the model\\'s training data to generate a response. However, when you provide tools, the model now has the choice to either generate a normal text response, or generate a tool call. If the model generates a tool call, its generation is complete and that step is finished.\\n\\nYou may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.\\n\\nYou can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model\\'s training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.\\n\\n### Example\\n\\nIn the following example, there are two steps:\\n\\n1. **Step 1**\\n   1. The prompt `\\'What is the weather in San Francisco?\\'` is sent to the model.\\n   1. The model generates a tool call.\\n   1. The tool call is executed.\\n1. **Step 2**\\n   1. The tool result is sent to the model.\\n   1. The model generates a response considering the tool result.\\n\\n```ts highlight=\"18-19\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool, stepCountIs } from \\'ai\\';\\n\\nconst { text, steps } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n  },\\n  stopWhen: stepCountIs(5), // stop after a maximum of 5 steps if tools were called\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n<Note>You can use `streamText` in a similar way.</Note>\\n\\n### Steps\\n\\nTo access intermediate tool calls and results, you can use the `steps` property in the result object\\nor the `streamText` `onFinish` callback.\\nIt contains all the text, tool calls, tool results, and more from each step.\\n\\n#### Example: Extract tool results from all steps\\n\\n```ts highlight=\"3,9-10\"\\nimport { generateText } from \\'ai\\';\\n\\nconst { steps } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  stopWhen: stepCountIs(10),\\n  // ...\\n});\\n\\n// extract all tool calls from the steps:\\nconst allToolCalls = steps.flatMap(step => step.toolCalls);\\n```\\n\\n### `onStepFinish` callback\\n\\nWhen using `generateText` or `streamText`, you can provide an `onStepFinish` callback that\\nis triggered when a step is finished,\\ni.e. all text deltas, tool calls, and tool results for the step are available.\\nWhen you have multiple steps, the callback is triggered for each step.\\n\\n```tsx highlight=\"5-7\"\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n  onStepFinish({ text, toolCalls, toolResults, finishReason, usage }) {\\n    // your own logic, e.g. for saving the chat history or recording usage\\n  },\\n});\\n```\\n\\n### `prepareStep` callback\\n\\nThe `prepareStep` callback is called before a step is started.\\n\\nIt is called with the following parameters:\\n\\n- `model`: The model that was passed into `generateText`.\\n- `stopWhen`: The stopping condition that was passed into `generateText`.\\n- `stepNumber`: The number of the step that is being executed.\\n- `steps`: The steps that have been executed so far.\\n- `messages`: The messages that will be sent to the model for the current step.\\n\\nYou can use it to provide different settings for a step, including modifying the input messages.\\n\\n```tsx highlight=\"5-7\"\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n  prepareStep: async ({ model, stepNumber, steps, messages }) => {\\n    if (stepNumber === 0) {\\n      return {\\n        // use a different model for this step:\\n        model: modelForThisParticularStep,\\n        // force a tool choice for this step:\\n        toolChoice: { type: \\'tool\\', toolName: \\'tool1\\' },\\n        // limit the tools that are available for this step:\\n        activeTools: [\\'tool1\\'],\\n      };\\n    }\\n\\n    // when nothing is returned, the default settings are used\\n  },\\n});\\n```\\n\\n#### Message Modification for Longer Agentic Loops\\n\\nIn longer agentic loops, you can use the `messages` parameter to modify the input messages for each step. This is particularly useful for prompt compression:\\n\\n```tsx\\nprepareStep: async ({ stepNumber, steps, messages }) => {\\n  // Compress conversation history for longer loops\\n  if (messages.length > 20) {\\n    return {\\n      messages: messages.slice(-10),\\n    };\\n  }\\n\\n  return {};\\n},\\n```\\n\\n## Response Messages\\n\\nAdding the generated assistant and tool messages to your conversation history is a common task,\\nespecially if you are using multi-step tool calls.\\n\\nBoth `generateText` and `streamText` have a `response.messages` property that you can use to\\nadd the assistant and tool messages to your conversation history.\\nIt is also available in the `onFinish` callback of `streamText`.\\n\\nThe `response.messages` property contains an array of `ModelMessage` objects that you can add to your conversation history:\\n\\n```ts\\nimport { generateText, ModelMessage } from \\'ai\\';\\n\\nconst messages: ModelMessage[] = [\\n  // ...\\n];\\n\\nconst { response } = await generateText({\\n  // ...\\n  messages,\\n});\\n\\n// add the response messages to your conversation history:\\nmessages.push(...response.messages); // streamText: ...((await response).messages)\\n```\\n\\n## Dynamic Tools\\n\\nAI SDK Core supports dynamic tools for scenarios where tool schemas are not known at compile time. This is useful for:\\n\\n- MCP (Model Context Protocol) tools without schemas\\n- User-defined functions at runtime\\n- Tools loaded from external sources\\n\\n### Using dynamicTool\\n\\nThe `dynamicTool` helper creates tools with unknown input/output types:\\n\\n```ts\\nimport { dynamicTool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst customTool = dynamicTool({\\n  description: \\'Execute a custom function\\',\\n  inputSchema: z.object({}),\\n  execute: async input => {\\n    // input is typed as \\'unknown\\'\\n    // You need to validate/cast it at runtime\\n    const { action, parameters } = input as any;\\n\\n    // Execute your dynamic logic\\n    return { result: `Executed ${action}` };\\n  },\\n});\\n```\\n\\n### Type-Safe Handling\\n\\nWhen using both static and dynamic tools, use the `dynamic` flag for type narrowing:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // Static tool with known types\\n    weather: weatherTool,\\n    // Dynamic tool\\n    custom: dynamicTool({\\n      /* ... */\\n    }),\\n  },\\n  onStepFinish: ({ toolCalls, toolResults }) => {\\n    // Type-safe iteration\\n    for (const toolCall of toolCalls) {\\n      if (toolCall.dynamic) {\\n        // Dynamic tool: input is \\'unknown\\'\\n        console.log(\\'Dynamic:\\', toolCall.toolName, toolCall.input);\\n        continue;\\n      }\\n\\n      // Static tool: full type inference\\n      switch (toolCall.toolName) {\\n        case \\'weather\\':\\n          console.log(toolCall.input.location); // typed as string\\n          break;\\n      }\\n    }\\n  },\\n});\\n```\\n\\n## Preliminary Tool Results\\n\\nYou can return an `AsyncIterable` over multiple results.\\nIn this case, the last value from the iterable is the final tool result.\\n\\nThis can be used in combination with generator functions to e.g. stream status information\\nduring the tool execution:\\n\\n```ts\\ntool({\\n  description: \\'Get the current weather.\\',\\n  inputSchema: z.object({\\n    location: z.string(),\\n  }),\\n  async *execute({ location }) {\\n    yield {\\n      status: \\'loading\\' as const,\\n      text: `Getting weather for ${location}`,\\n      weather: undefined,\\n    };\\n\\n    await new Promise(resolve => setTimeout(resolve, 3000));\\n\\n    const temperature = 72 + Math.floor(Math.random() * 21) - 10;\\n\\n    yield {\\n      status: \\'success\\' as const,\\n      text: `The weather in ${location} is ${temperature}°F`,\\n      temperature,\\n    };\\n  },\\n});\\n```\\n\\n## Tool Choice\\n\\nYou can use the `toolChoice` setting to influence when a tool is selected.\\nIt supports the following settings:\\n\\n- `auto` (default): the model can choose whether and which tools to call.\\n- `required`: the model must call a tool. It can choose which tool to call.\\n- `none`: the model must not call tools\\n- `{ type: \\'tool\\', toolName: string (typed) }`: the model must call the specified tool\\n\\n```ts highlight=\"18\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        location,\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n    }),\\n  },\\n  toolChoice: \\'required\\', // force the model to call a tool\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n## Tool Execution Options\\n\\nWhen tools are called, they receive additional options as a second parameter.\\n\\n### Tool Call ID\\n\\nThe ID of the tool call is forwarded to the tool execution.\\nYou can use it e.g. when sending tool-call related information with stream data.\\n\\n```ts highlight=\"14-20\"\\nimport {\\n  streamText,\\n  tool,\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n} from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const stream = createUIMessageStream({\\n    execute: ({ writer }) => {\\n      const result = streamText({\\n        // ...\\n        messages,\\n        tools: {\\n          myTool: tool({\\n            // ...\\n            execute: async (args, { toolCallId }) => {\\n              // return e.g. custom status for tool call\\n              writer.write({\\n                type: \\'data-tool-status\\',\\n                id: toolCallId,\\n                data: {\\n                  name: \\'myTool\\',\\n                  status: \\'in-progress\\',\\n                },\\n              });\\n              // ...\\n            },\\n          }),\\n        },\\n      });\\n\\n      writer.merge(result.toUIMessageStream());\\n    },\\n  });\\n\\n  return createUIMessageStreamResponse({ stream });\\n}\\n```\\n\\n### Messages\\n\\nThe messages that were sent to the language model to initiate the response that contained the tool call are forwarded to the tool execution.\\nYou can access them in the second parameter of the `execute` function.\\nIn multi-step calls, the messages contain the text, tool calls, and tool results from all previous steps.\\n\\n```ts highlight=\"8-9\"\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  // ...\\n  tools: {\\n    myTool: tool({\\n      // ...\\n      execute: async (args, { messages }) => {\\n        // use the message history in e.g. calls to other language models\\n        return { ... };\\n      },\\n    }),\\n  },\\n});\\n```\\n\\n### Abort Signals\\n\\nThe abort signals from `generateText` and `streamText` are forwarded to the tool execution.\\nYou can access them in the second parameter of the `execute` function and e.g. abort long-running computations or forward them to fetch calls inside tools.\\n\\n```ts highlight=\"6,11,14\"\\nimport { z } from \\'zod\\';\\nimport { generateText, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  abortSignal: myAbortSignal, // signal that will be forwarded to tools\\n  tools: {\\n    weather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({ location: z.string() }),\\n      execute: async ({ location }, { abortSignal }) => {\\n        return fetch(\\n          `https://api.weatherapi.com/v1/current.json?q=${location}`,\\n          { signal: abortSignal }, // forward the abort signal to fetch\\n        );\\n      },\\n    }),\\n  },\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n### Context (experimental)\\n\\nYou can pass in arbitrary context from `generateText` or `streamText` via the `experimental_context` setting.\\nThis context is available in the `experimental_context` tool execution option.\\n\\n```ts\\nconst result = await generateText({\\n  // ...\\n  tools: {\\n    someTool: tool({\\n      // ...\\n      execute: async (input, { experimental_context: context }) => {\\n        const typedContext = context as { example: string }; // or use type validation library\\n        // ...\\n      },\\n    }),\\n  },\\n  experimental_context: { example: \\'123\\' },\\n});\\n```\\n\\n## Tool Input Lifecycle Hooks\\n\\nThe following tool input lifecycle hooks are available:\\n\\n- **`onInputStart`**: Called when the model starts generating the input (arguments) for the tool call\\n- **`onInputDelta`**: Called for each chunk of text as the input is streamed\\n- **`onInputAvailable`**: Called when the complete input is available and validated\\n\\n`onInputStart` and `onInputDelta` are only called in streaming contexts (when using `streamText`). They are not called when using `generateText`.\\n\\n### Example\\n\\n```ts highlight=\"15-23\"\\nimport { streamText, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    getWeather: tool({\\n      description: \\'Get the weather in a location\\',\\n      inputSchema: z.object({\\n        location: z.string().describe(\\'The location to get the weather for\\'),\\n      }),\\n      execute: async ({ location }) => ({\\n        temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n      }),\\n      onInputStart: () => {\\n        console.log(\\'Tool call starting\\');\\n      },\\n      onInputDelta: ({ inputTextDelta }) => {\\n        console.log(\\'Received input chunk:\\', inputTextDelta);\\n      },\\n      onInputAvailable: ({ input }) => {\\n        console.log(\\'Complete input:\\', input);\\n      },\\n    }),\\n  },\\n  prompt: \\'What is the weather in San Francisco?\\',\\n});\\n```\\n\\n## Types\\n\\nModularizing your code often requires defining types to ensure type safety and reusability.\\nTo enable this, the AI SDK provides several helper types for tools, tool calls, and tool results.\\n\\nYou can use them to strongly type your variables, function parameters, and return types\\nin parts of the code that are not directly related to `streamText` or `generateText`.\\n\\nEach tool call is typed with `ToolCall<NAME extends string, ARGS>`, depending\\non the tool that has been invoked.\\nSimilarly, the tool results are typed with `ToolResult<NAME extends string, ARGS, RESULT>`.\\n\\nThe tools in `streamText` and `generateText` are defined as a `ToolSet`.\\nThe type inference helpers `TypedToolCall<TOOLS extends ToolSet>`\\nand `TypedToolResult<TOOLS extends ToolSet>` can be used to\\nextract the tool call and tool result types from the tools.\\n\\n```ts highlight=\"18-19,23-24\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { TypedToolCall, TypedToolResult, generateText, tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst myToolSet = {\\n  firstTool: tool({\\n    description: \\'Greets the user\\',\\n    inputSchema: z.object({ name: z.string() }),\\n    execute: async ({ name }) => `Hello, ${name}!`,\\n  }),\\n  secondTool: tool({\\n    description: \\'Tells the user their age\\',\\n    inputSchema: z.object({ age: z.number() }),\\n    execute: async ({ age }) => `You are ${age} years old!`,\\n  }),\\n};\\n\\ntype MyToolCall = TypedToolCall<typeof myToolSet>;\\ntype MyToolResult = TypedToolResult<typeof myToolSet>;\\n\\nasync function generateSomething(prompt: string): Promise<{\\n  text: string;\\n  toolCalls: Array<MyToolCall>; // typed tool calls\\n  toolResults: Array<MyToolResult>; // typed tool results\\n}> {\\n  return generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    tools: myToolSet,\\n    prompt,\\n  });\\n}\\n```\\n\\n## Handling Errors\\n\\nThe AI SDK has three tool-call related errors:\\n\\n- [`NoSuchToolError`](/docs/reference/ai-sdk-errors/ai-no-such-tool-error): the model tries to call a tool that is not defined in the tools object\\n- [`InvalidToolInputError`](/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error): the model calls a tool with inputs that do not match the tool\\'s input schema\\n- [`ToolCallRepairError`](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error): an error that occurred during tool call repair\\n\\nWhen tool execution fails (errors thrown by your tool\\'s `execute` function), the AI SDK adds them as `tool-error` content parts to enable automated LLM roundtrips in multi-step scenarios.\\n\\n### `generateText`\\n\\n`generateText` throws errors for tool schema validation issues and other errors, and can be handled using a `try`/`catch` block. Tool execution errors appear as `tool-error` parts in the result steps:\\n\\n```ts\\ntry {\\n  const result = await generateText({\\n    //...\\n  });\\n} catch (error) {\\n  if (NoSuchToolError.isInstance(error)) {\\n    // handle the no such tool error\\n  } else if (InvalidToolInputError.isInstance(error)) {\\n    // handle the invalid tool inputs error\\n  } else {\\n    // handle other errors\\n  }\\n}\\n```\\n\\nTool execution errors are available in the result steps:\\n\\n```ts\\nconst { steps } = await generateText({\\n  // ...\\n});\\n\\n// check for tool errors in the steps\\nconst toolErrors = steps.flatMap(step =>\\n  step.content.filter(part => part.type === \\'tool-error\\'),\\n);\\n\\ntoolErrors.forEach(toolError => {\\n  console.log(\\'Tool error:\\', toolError.error);\\n  console.log(\\'Tool name:\\', toolError.toolName);\\n  console.log(\\'Tool input:\\', toolError.input);\\n});\\n```\\n\\n### `streamText`\\n\\n`streamText` sends errors as part of the full stream. Tool execution errors appear as `tool-error` parts, while other errors appear as `error` parts.\\n\\nWhen using `toUIMessageStreamResponse`, you can pass an `onError` function to extract the error message from the error part and forward it as part of the stream response:\\n\\n```ts\\nconst result = streamText({\\n  // ...\\n});\\n\\nreturn result.toUIMessageStreamResponse({\\n  onError: error => {\\n    if (NoSuchToolError.isInstance(error)) {\\n      return \\'The model tried to call a unknown tool.\\';\\n    } else if (InvalidToolInputError.isInstance(error)) {\\n      return \\'The model called a tool with invalid inputs.\\';\\n    } else {\\n      return \\'An unknown error occurred.\\';\\n    }\\n  },\\n});\\n```\\n\\n## Tool Call Repair\\n\\n<Note type=\"warning\">\\n  The tool call repair feature is experimental and may change in the future.\\n</Note>\\n\\nLanguage models sometimes fail to generate valid tool calls,\\nespecially when the input schema is complex or the model is smaller.\\n\\nIf you use multiple steps, those failed tool calls will be sent back to the LLM\\nin the next step to give it an opportunity to fix it.\\nHowever, you may want to control how invalid tool calls are repaired without requiring\\nadditional steps that pollute the message history.\\n\\nYou can use the `experimental_repairToolCall` function to attempt to repair the tool call\\nwith a custom function.\\n\\nYou can use different strategies to repair the tool call:\\n\\n- Use a model with structured outputs to generate the inputs.\\n- Send the messages, system prompt, and tool schema to a stronger model to generate the inputs.\\n- Provide more specific repair instructions based on which tool was called.\\n\\n### Example: Use a model with structured outputs for repair\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject, generateText, NoSuchToolError, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model,\\n  tools,\\n  prompt,\\n\\n  experimental_repairToolCall: async ({\\n    toolCall,\\n    tools,\\n    inputSchema,\\n    error,\\n  }) => {\\n    if (NoSuchToolError.isInstance(error)) {\\n      return null; // do not attempt to fix invalid tool names\\n    }\\n\\n    const tool = tools[toolCall.toolName as keyof typeof tools];\\n\\n    const { object: repairedArgs } = await generateObject({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      schema: tool.inputSchema,\\n      prompt: [\\n        `The model tried to call the tool \"${toolCall.toolName}\"` +\\n          ` with the following inputs:`,\\n        JSON.stringify(toolCall.input),\\n        `The tool accepts the following schema:`,\\n        JSON.stringify(inputSchema(toolCall)),\\n        \\'Please fix the inputs.\\',\\n      ].join(\\'\\\\n\\'),\\n    });\\n\\n    return { ...toolCall, input: JSON.stringify(repairedArgs) };\\n  },\\n});\\n```\\n\\n### Example: Use the re-ask strategy for repair\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject, generateText, NoSuchToolError, tool } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model,\\n  tools,\\n  prompt,\\n\\n  experimental_repairToolCall: async ({\\n    toolCall,\\n    tools,\\n    error,\\n    messages,\\n    system,\\n  }) => {\\n    const result = await generateText({\\n      model,\\n      system,\\n      messages: [\\n        ...messages,\\n        {\\n          role: \\'assistant\\',\\n          content: [\\n            {\\n              type: \\'tool-call\\',\\n              toolCallId: toolCall.toolCallId,\\n              toolName: toolCall.toolName,\\n              input: toolCall.input,\\n            },\\n          ],\\n        },\\n        {\\n          role: \\'tool\\' as const,\\n          content: [\\n            {\\n              type: \\'tool-result\\',\\n              toolCallId: toolCall.toolCallId,\\n              toolName: toolCall.toolName,\\n              output: error.message,\\n            },\\n          ],\\n        },\\n      ],\\n      tools,\\n    });\\n\\n    const newToolCall = result.toolCalls.find(\\n      newToolCall => newToolCall.toolName === toolCall.toolName,\\n    );\\n\\n    return newToolCall != null\\n      ? {\\n          toolCallType: \\'function\\' as const,\\n          toolCallId: toolCall.toolCallId,\\n          toolName: toolCall.toolName,\\n          input: JSON.stringify(newToolCall.input),\\n        }\\n      : null;\\n  },\\n});\\n```\\n\\n## Active Tools\\n\\nLanguage models can only handle a limited number of tools at a time, depending on the model.\\nTo allow for static typing using a large number of tools and limiting the available tools to the model at the same time,\\nthe AI SDK provides the `activeTools` property.\\n\\nIt is an array of tool names that are currently active.\\nBy default, the value is `undefined` and all tools are active.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: myToolSet,\\n  activeTools: [\\'firstTool\\'],\\n});\\n```\\n\\n## Multi-modal Tool Results\\n\\n<Note type=\"warning\">\\n  Multi-modal tool results are experimental and only supported by Anthropic and\\n  OpenAI.\\n</Note>\\n\\nIn order to send multi-modal tool results, e.g. screenshots, back to the model,\\nthey need to be converted into a specific format.\\n\\nAI SDK Core tools have an optional `toModelOutput` function\\nthat converts the tool result into a content part.\\n\\nHere is an example for converting a screenshot into a content part:\\n\\n```ts highlight=\"22-27\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    computer: anthropic.tools.computer_20241022({\\n      // ...\\n      async execute({ action, coordinate, text }) {\\n        switch (action) {\\n          case \\'screenshot\\': {\\n            return {\\n              type: \\'image\\',\\n              data: fs\\n                .readFileSync(\\'./data/screenshot-editor.png\\')\\n                .toString(\\'base64\\'),\\n            };\\n          }\\n          default: {\\n            return `executed ${action}`;\\n          }\\n        }\\n      },\\n\\n      // map to tool result content for LLM consumption:\\n      toModelOutput(result) {\\n        return {\\n          type: \\'content\\',\\n          value:\\n            typeof result === \\'string\\'\\n              ? [{ type: \\'text\\', text: result }]\\n              : [{ type: \\'media\\', data: result.data, mediaType: \\'image/png\\' }],\\n        };\\n      },\\n    }),\\n  },\\n  // ...\\n});\\n```\\n\\n## Extracting Tools\\n\\nOnce you start having many tools, you might want to extract them into separate files.\\nThe `tool` helper function is crucial for this, because it ensures correct type inference.\\n\\nHere is an example of an extracted tool:\\n\\n```ts filename=\"tools/weather-tool.ts\" highlight=\"1,4-5\"\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// the `tool` helper function ensures correct type inference:\\nexport const weatherTool = tool({\\n  description: \\'Get the weather in a location\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The location to get the weather for\\'),\\n  }),\\n  execute: async ({ location }) => ({\\n    location,\\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n  }),\\n});\\n```\\n\\n## MCP Tools\\n\\nThe AI SDK supports connecting to Model Context Protocol (MCP) servers to access their tools.\\nMCP enables your AI applications to discover and use tools across various services through a standardized interface.\\n\\nFor detailed information about MCP tools, including initialization, transport options, and usage patterns, see the [MCP Tools documentation](/docs/ai-sdk-core/mcp-tools).\\n\\n### AI SDK Tools vs MCP Tools\\n\\nIn most cases, you should define your own AI SDK tools for production applications. They provide full control, type safety, and optimal performance. MCP tools are best suited for rapid development iteration and scenarios where users bring their own tools.\\n\\n| Aspect                 | AI SDK Tools                                              | MCP Tools                                             |\\n| ---------------------- | --------------------------------------------------------- | ----------------------------------------------------- |\\n| **Type Safety**        | Full static typing end-to-end                             | Dynamic discovery at runtime                          |\\n| **Execution**          | Same process as your request (low latency)                | Separate server (network overhead)                    |\\n| **Prompt Control**     | Full control over descriptions and schemas                | Controlled by MCP server owner                        |\\n| **Schema Control**     | You define and optimize for your model                    | Controlled by MCP server owner                        |\\n| **Version Management** | Full visibility over updates                              | Can update independently (version skew risk)          |\\n| **Authentication**     | Same process, no additional auth required                 | Separate server introduces additional auth complexity |\\n| **Best For**           | Production applications requiring control and performance | Development iteration, user-provided tools            |\\n\\n## Examples\\n\\nYou can see tools in action using various frameworks in the following examples:\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to use tools in Node.js\\',\\n      link: \\'/cookbook/node/call-tools\\',\\n    },\\n    {\\n      title: \\'Learn to use tools in Next.js with Route Handlers\\',\\n      link: \\'/cookbook/next/call-tools\\',\\n    },\\n    {\\n      title: \\'Learn to use MCP tools in Node.js\\',\\n      link: \\'/cookbook/node/mcp-tools\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/16-mcp-tools.mdx'), name='16-mcp-tools.mdx', displayName='16-mcp-tools.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Model Context Protocol (MCP)\\ndescription: Learn how to connect to Model Context Protocol (MCP) servers and use their tools with AI SDK Core.\\n---\\n\\n# Model Context Protocol (MCP)\\n\\n<Note type=\"warning\">\\n  The MCP tools feature is experimental and may change in the future.\\n</Note>\\n\\nThe AI SDK supports connecting to [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) servers to access their tools, resources, and prompts.\\nThis enables your AI applications to discover and use capabilities across various services through a standardized interface.\\n\\n<Note>\\n  If you\\'re using OpenAI\\'s Responses API, you can also use the built-in\\n  `openai.tools.mcp` tool, which provides direct MCP server integration without\\n  needing to convert tools. See the [OpenAI provider\\n  documentation](/providers/ai-sdk-providers/openai#mcp-tool) for details.\\n</Note>\\n\\n## Initializing an MCP Client\\n\\nWe recommend using HTTP transport (like `StreamableHTTPClientTransport`) for production deployments. The stdio transport should only be used for connecting to local servers as it cannot be deployed to production environments.\\n\\nCreate an MCP client using one of the following transport options:\\n\\n- **HTTP transport (Recommended)**: Either configure HTTP directly via the client using `transport: { type: \\'http\\', ... }`, or use MCP\\'s official TypeScript SDK `StreamableHTTPClientTransport`\\n- SSE (Server-Sent Events): An alternative HTTP-based transport\\n- `stdio`: For local development only. Uses standard input/output streams for local MCP servers\\n\\n### HTTP Transport (Recommended)\\n\\nFor production deployments, we recommend using the HTTP transport. You can configure it directly on the client:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\n\\nconst mcpClient = await createMCPClient({\\n  transport: {\\n    type: \\'http\\',\\n    url: \\'https://your-server.com/mcp\\',\\n\\n    // optional: configure HTTP headers\\n    headers: { Authorization: \\'Bearer my-api-key\\' },\\n\\n    // optional: provide an OAuth client provider for automatic authorization\\n    authProvider: myOAuthClientProvider,\\n  },\\n});\\n```\\n\\nAlternatively, you can use `StreamableHTTPClientTransport` from MCP\\'s official TypeScript SDK:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\nimport { StreamableHTTPClientTransport } from \\'@modelcontextprotocol/sdk/client/streamableHttp.js\\';\\n\\nconst url = new URL(\\'https://your-server.com/mcp\\');\\nconst mcpClient = await createMCPClient({\\n  transport: new StreamableHTTPClientTransport(url, {\\n    sessionId: \\'session_123\\',\\n  }),\\n});\\n```\\n\\n### SSE Transport\\n\\nSSE provides an alternative HTTP-based transport option. Configure it with a `type` and `url` property. You can also provide an `authProvider` for OAuth:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\n\\nconst mcpClient = await createMCPClient({\\n  transport: {\\n    type: \\'sse\\',\\n    url: \\'https://my-server.com/sse\\',\\n\\n    // optional: configure HTTP headers\\n    headers: { Authorization: \\'Bearer my-api-key\\' },\\n\\n    // optional: provide an OAuth client provider for automatic authorization\\n    authProvider: myOAuthClientProvider,\\n  },\\n});\\n```\\n\\n### Stdio Transport (Local Servers)\\n\\n<Note type=\"warning\">\\n  The stdio transport should only be used for local servers.\\n</Note>\\n\\nThe Stdio transport can be imported from either the MCP SDK or the AI SDK:\\n\\n```typescript\\nimport { experimental_createMCPClient as createMCPClient } from \\'@ai-sdk/mcp\\';\\nimport { StdioClientTransport } from \\'@modelcontextprotocol/sdk/client/stdio.js\\';\\n// Or use the AI SDK\\'s stdio transport:\\n// import { Experimental_StdioMCPTransport as StdioClientTransport } from \\'@ai-sdk/mcp/mcp-stdio\\';\\n\\nconst mcpClient = await createMCPClient({\\n  transport: new StdioClientTransport({\\n    command: \\'node\\',\\n    args: [\\'src/stdio/dist/server.js\\'],\\n  }),\\n});\\n```\\n\\n### Custom Transport\\n\\nYou can also bring your own transport by implementing the `MCPTransport` interface for specific requirements not covered by the standard transports.\\n\\n<Note>\\n  The client returned by the `experimental_createMCPClient` function is a\\n  lightweight client intended for use in tool conversion. It currently does not\\n  support all features of the full MCP client, such as: session\\n  management, resumable streams, and receiving notifications.\\n\\nAuthorization via OAuth is supported when using the AI SDK MCP HTTP or SSE\\ntransports by providing an `authProvider`.\\n\\n</Note>\\n\\n### Closing the MCP Client\\n\\nAfter initialization, you should close the MCP client based on your usage pattern:\\n\\n- For short-lived usage (e.g., single requests), close the client when the response is finished\\n- For long-running clients (e.g., command line apps), keep the client open but ensure it\\'s closed when the application terminates\\n\\nWhen streaming responses, you can close the client when the LLM response has finished. For example, when using `streamText`, you should use the `onFinish` callback:\\n\\n```typescript\\nconst mcpClient = await experimental_createMCPClient({\\n  // ...\\n});\\n\\nconst tools = await mcpClient.tools();\\n\\nconst result = await streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools,\\n  prompt: \\'What is the weather in Brooklyn, New York?\\',\\n  onFinish: async () => {\\n    await mcpClient.close();\\n  },\\n});\\n```\\n\\nWhen generating responses without streaming, you can use try/finally or cleanup functions in your framework:\\n\\n```typescript\\nlet mcpClient: MCPClient | undefined;\\n\\ntry {\\n  mcpClient = await experimental_createMCPClient({\\n    // ...\\n  });\\n} finally {\\n  await mcpClient?.close();\\n}\\n```\\n\\n## Using MCP Tools\\n\\nThe client\\'s `tools` method acts as an adapter between MCP tools and AI SDK tools. It supports two approaches for working with tool schemas:\\n\\n### Schema Discovery\\n\\nWith schema discovery, all tools offered by the server are automatically listed, and input parameter types are inferred based on the schemas provided by the server:\\n\\n```typescript\\nconst tools = await mcpClient.tools();\\n```\\n\\nThis approach is simpler to implement and automatically stays in sync with server changes. However, you won\\'t have TypeScript type safety during development, and all tools from the server will be loaded\\n\\n### Schema Definition\\n\\nFor better type safety and control, you can define the tools and their input schemas explicitly in your client code:\\n\\n```typescript\\nimport { z } from \\'zod\\';\\n\\nconst tools = await mcpClient.tools({\\n  schemas: {\\n    \\'get-data\\': {\\n      inputSchema: z.object({\\n        query: z.string().describe(\\'The data query\\'),\\n        format: z.enum([\\'json\\', \\'text\\']).optional(),\\n      }),\\n    },\\n    // For tools with zero inputs, you should use an empty object:\\n    \\'tool-with-no-args\\': {\\n      inputSchema: z.object({}),\\n    },\\n  },\\n});\\n```\\n\\nThis approach provides full TypeScript type safety and IDE autocompletion, letting you catch parameter mismatches during development. When you define `schemas`, the client only pulls the explicitly defined tools, keeping your application focused on the tools it needs\\n\\n## Using MCP Resources\\n\\nAccording to the [MCP specification](https://modelcontextprotocol.io/docs/learn/server-concepts#resources), resources are **application-driven** data sources that provide context to the model. Unlike tools (which are model-controlled), your application decides when to fetch and pass resources as context.\\n\\nThe MCP client provides three methods for working with resources:\\n\\n### Listing Resources\\n\\nList all available resources from the MCP server:\\n\\n```typescript\\nconst resources = await mcpClient.listResources();\\n```\\n\\n### Reading Resource Contents\\n\\nRead the contents of a specific resource by its URI:\\n\\n```typescript\\nconst resourceData = await mcpClient.readResource({\\n  uri: \\'file:///example/document.txt\\',\\n});\\n```\\n\\n### Listing Resource Templates\\n\\nResource templates are dynamic URI patterns that allow flexible queries. List all available templates:\\n\\n```typescript\\nconst templates = await mcpClient.listResourceTemplates();\\n```\\n\\n## Using MCP Prompts\\n\\nAccording to the MCP specification, prompts are user-controlled templates that servers expose for clients to list and retrieve with optional arguments.\\n\\n### Listing Prompts\\n\\n```typescript\\nconst prompts = await mcpClient.listPrompts();\\n```\\n\\n### Getting a Prompt\\n\\nRetrieve prompt messages, optionally passing arguments defined by the server:\\n\\n```typescript\\nconst prompt = await mcpClient.getPrompt({\\n  name: \\'code_review\\',\\n  arguments: { code: \\'function add(a, b) { return a + b; }\\' },\\n});\\n```\\n\\n## Handling Elicitation Requests\\n\\nElicitation is a mechanism where MCP servers can request additional information from the client during tool execution. For example, a server might need user input to complete a registration form or confirmation for a sensitive operation.\\n\\n<Note type=\"warning\">\\n  It is up to the client application to handle elicitation requests properly.\\n  The MCP client simply surfaces these requests from the server to your\\n  application code.\\n</Note>\\n\\n### Enabling Elicitation Support\\n\\nTo enable elicitation, you need to advertise the capability when creating the MCP client:\\n\\n```typescript\\nconst mcpClient = await experimental_createMCPClient({\\n  transport: {\\n    type: \\'sse\\',\\n    url: \\'https://your-server.com/sse\\',\\n  },\\n  capabilities: {\\n    elicitation: {},\\n  },\\n});\\n```\\n\\n### Registering an Elicitation Handler\\n\\nUse the `onElicitationRequest` method to register a handler that will be called when the server requests input:\\n\\n```typescript\\nimport { ElicitationRequestSchema } from \\'@ai-sdk/mcp\\';\\n\\nmcpClient.onElicitationRequest(ElicitationRequestSchema, async request => {\\n  // request.params.message: A message describing what input is needed\\n  // request.params.requestedSchema: JSON schema defining the expected input structure\\n\\n  // Get input from the user (implement according to your application\\'s needs)\\n  const userInput = await getInputFromUser(\\n    request.params.message,\\n    request.params.requestedSchema,\\n  );\\n\\n  // Return the result with one of three actions:\\n  return {\\n    action: \\'accept\\', // or \\'decline\\' or \\'cancel\\'\\n    content: userInput, // only required when action is \\'accept\\'\\n  };\\n});\\n```\\n\\n### Elicitation Response Actions\\n\\nYour handler must return an object with an `action` field that can be one of:\\n\\n- `\\'accept\\'`: User provided the requested information. Must include `content` with the data.\\n- `\\'decline\\'`: User chose not to provide the information.\\n- `\\'cancel\\'`: User cancelled the operation entirely.\\n\\n## Examples\\n\\nYou can see MCP in action in the following examples:\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to use MCP tools in Node.js\\',\\n      link: \\'/cookbook/node/mcp-tools\\',\\n    },\\n    {\\n      title: \\'Learn to handle MCP elicitation requests in Node.js\\',\\n      link: \\'/cookbook/node/mcp-elicitation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/20-prompt-engineering.mdx'), name='20-prompt-engineering.mdx', displayName='20-prompt-engineering.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Prompt Engineering\\ndescription: Learn how to develop prompts with AI SDK Core.\\n---\\n\\n# Prompt Engineering\\n\\n## Tips\\n\\n### Prompts for Tools\\n\\nWhen you create prompts that include tools, getting good results can be tricky as the number and complexity of your tools increases.\\n\\nHere are a few tips to help you get the best results:\\n\\n1. Use a model that is strong at tool calling, such as `gpt-5` or `gpt-4.1`. Weaker models will often struggle to call tools effectively and flawlessly.\\n1. Keep the number of tools low, e.g. to 5 or less.\\n1. Keep the complexity of the tool parameters low. Complex Zod schemas with many nested and optional elements, unions, etc. can be challenging for the model to work with.\\n1. Use semantically meaningful names for your tools, parameters, parameter properties, etc. The more information you pass to the model, the better it can understand what you want.\\n1. Add `.describe(\"...\")` to your Zod schema properties to give the model hints about what a particular property is for.\\n1. When the output of a tool might be unclear to the model and there are dependencies between tools, use the `description` field of a tool to provide information about the output of the tool execution.\\n1. You can include example input/outputs of tool calls in your prompt to help the model understand how to use the tools. Keep in mind that the tools work with JSON objects, so the examples should use JSON.\\n\\nIn general, the goal should be to give the model all information it needs in a clear way.\\n\\n### Tool & Structured Data Schemas\\n\\nThe mapping from Zod schemas to LLM inputs (typically JSON schema) is not always straightforward, since the mapping is not one-to-one.\\n\\n#### Zod Dates\\n\\nZod expects JavaScript Date objects, but models return dates as strings.\\nYou can specify and validate the date format using `z.string().datetime()` or `z.string().date()`,\\nand then use a Zod transformer to convert the string to a Date object.\\n\\n```ts highlight=\"7-10\"\\nconst result = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    events: z.array(\\n      z.object({\\n        event: z.string(),\\n        date: z\\n          .string()\\n          .date()\\n          .transform(value => new Date(value)),\\n      }),\\n    ),\\n  }),\\n  prompt: \\'List 5 important events from the year 2000.\\',\\n});\\n```\\n\\n#### Optional Parameters\\n\\nWhen working with tools that have optional parameters, you may encounter compatibility issues with certain providers that use strict schema validation.\\n\\n<Note>\\n  This is particularly relevant for OpenAI models with structured outputs\\n  (strict mode).\\n</Note>\\n\\nFor maximum compatibility, optional parameters should use `.nullable()` instead of `.optional()`:\\n\\n```ts highlight=\"6,7,16,17\"\\n// This may fail with strict schema validation\\nconst failingTool = tool({\\n  description: \\'Execute a command\\',\\n  inputSchema: z.object({\\n    command: z.string(),\\n    workdir: z.string().optional(), // This can cause errors\\n    timeout: z.string().optional(),\\n  }),\\n});\\n\\n// This works with strict schema validation\\nconst workingTool = tool({\\n  description: \\'Execute a command\\',\\n  inputSchema: z.object({\\n    command: z.string(),\\n    workdir: z.string().nullable(), // Use nullable instead\\n    timeout: z.string().nullable(),\\n  }),\\n});\\n```\\n\\n#### Temperature Settings\\n\\nFor tool calls and object generation, it\\'s recommended to use `temperature: 0` to ensure deterministic and consistent results:\\n\\n```ts highlight=\"3\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  temperature: 0, // Recommended for tool calls\\n  tools: {\\n    myTool: tool({\\n      description: \\'Execute a command\\',\\n      inputSchema: z.object({\\n        command: z.string(),\\n      }),\\n    }),\\n  },\\n  prompt: \\'Execute the ls command\\',\\n});\\n```\\n\\nLower temperature values reduce randomness in model outputs, which is particularly important when the model needs to:\\n\\n- Generate structured data with specific formats\\n- Make precise tool calls with correct parameters\\n- Follow strict schemas consistently\\n\\n## Debugging\\n\\n### Inspecting Warnings\\n\\nNot all providers support all AI SDK features.\\nProviders either throw exceptions or return warnings when they do not support a feature.\\nTo check if your prompt, tools, and settings are handled correctly by the provider, you can check the call warnings:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Hello, world!\\',\\n});\\n\\nconsole.log(result.warnings);\\n```\\n\\n### HTTP Request Bodies\\n\\nYou can inspect the raw HTTP request bodies for models that expose them, e.g. [OpenAI](/providers/ai-sdk-providers/openai).\\nThis allows you to inspect the exact payload that is sent to the model provider in the provider-specific way.\\n\\nRequest bodies are available via the `request.body` property of the response:\\n\\n```ts highlight=\"6\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Hello, world!\\',\\n});\\n\\nconsole.log(result.request.body);\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/25-settings.mdx'), name='25-settings.mdx', displayName='25-settings.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Settings\\ndescription: Learn how to configure the AI SDK.\\n---\\n\\n# Settings\\n\\nLarge language models (LLMs) typically provide settings to augment their output.\\n\\nAll AI SDK functions support the following common settings in addition to the model, the [prompt](./prompts), and additional provider-specific settings:\\n\\n```ts highlight=\"3-5\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  maxOutputTokens: 512,\\n  temperature: 0.3,\\n  maxRetries: 5,\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\n<Note>\\n  Some providers do not support all common settings. If you use a setting with a\\n  provider that does not support it, a warning will be generated. You can check\\n  the `warnings` property in the result object to see if any warnings were\\n  generated.\\n</Note>\\n\\n### `maxOutputTokens`\\n\\nMaximum number of tokens to generate.\\n\\n### `temperature`\\n\\nTemperature setting.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, `0` means almost deterministic results, and higher values mean more randomness.\\n\\nIt is recommended to set either `temperature` or `topP`, but not both.\\n\\n<Note>In AI SDK 5.0, temperature is no longer set to `0` by default.</Note>\\n\\n### `topP`\\n\\nNucleus sampling.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, nucleus sampling is a number between 0 and 1.\\nE.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.\\n\\nIt is recommended to set either `temperature` or `topP`, but not both.\\n\\n### `topK`\\n\\nOnly sample from the top K options for each subsequent token.\\n\\nUsed to remove \"long tail\" low probability responses.\\nRecommended for advanced use cases only. You usually only need to use `temperature`.\\n\\n### `presencePenalty`\\n\\nThe presence penalty affects the likelihood of the model to repeat information that is already in the prompt.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, `0` means no penalty.\\n\\n### `frequencyPenalty`\\n\\nThe frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.\\n\\nThe value is passed through to the provider. The range depends on the provider and model.\\nFor most providers, `0` means no penalty.\\n\\n### `stopSequences`\\n\\nThe stop sequences to use for stopping the text generation.\\n\\nIf set, the model will stop generating text when one of the stop sequences is generated.\\nProviders may have limits on the number of stop sequences.\\n\\n### `seed`\\n\\nIt is the seed (integer) to use for random sampling.\\nIf set and supported by the model, calls will generate deterministic results.\\n\\n### `maxRetries`\\n\\nMaximum number of retries. Set to 0 to disable retries. Default: `2`.\\n\\n### `abortSignal`\\n\\nAn optional abort signal that can be used to cancel the call.\\n\\nThe abort signal can e.g. be forwarded from a user interface to cancel the call,\\nor to define a timeout.\\n\\n#### Example: Timeout\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  abortSignal: AbortSignal.timeout(5000), // 5 seconds\\n});\\n```\\n\\n### `headers`\\n\\nAdditional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\n\\nYou can use the request headers to provide additional information to the provider,\\ndepending on what the provider supports. For example, some observability providers support\\nheaders such as `Prompt-Id`.\\n\\n```ts\\nimport { generateText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  headers: {\\n    \\'Prompt-Id\\': \\'my-prompt-id\\',\\n  },\\n});\\n```\\n\\n<Note>\\n  The `headers` setting is for request-specific headers. You can also set\\n  `headers` in the provider configuration. These headers will be sent with every\\n  request made by the provider.\\n</Note>\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/30-embeddings.mdx'), name='30-embeddings.mdx', displayName='30-embeddings.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Embeddings\\ndescription: Learn how to embed values with the AI SDK.\\n---\\n\\n# Embeddings\\n\\nEmbeddings are a way to represent words, phrases, or images as vectors in a high-dimensional space.\\nIn this space, similar words are close to each other, and the distance between words can be used to measure their similarity.\\n\\n## Embedding a Single Value\\n\\nThe AI SDK provides the [`embed`](/docs/reference/ai-sdk-core/embed) function to embed single values, which is useful for tasks such as finding similar words\\nor phrases or clustering text.\\nYou can use it with embeddings models, e.g. `openai.embeddingModel(\\'text-embedding-3-large\\')` or `mistral.embeddingModel(\\'mistral-embed\\')`.\\n\\n```tsx\\nimport { embed } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// \\'embedding\\' is a single embedding object (number[])\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n## Embedding Many Values\\n\\nWhen loading data, e.g. when preparing a data store for retrieval-augmented generation (RAG),\\nit is often useful to embed many values at once (batch embedding).\\n\\nThe AI SDK provides the [`embedMany`](/docs/reference/ai-sdk-core/embed-many) function for this purpose.\\nSimilar to `embed`, you can use it with embeddings models,\\ne.g. `openai.embeddingModel(\\'text-embedding-3-large\\')` or `mistral.embeddingModel(\\'mistral-embed\\')`.\\n\\n```tsx\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embedMany } from \\'ai\\';\\n\\n// \\'embeddings\\' is an array of embedding objects (number[][]).\\n// It is sorted in the same order as the input values.\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n});\\n```\\n\\n## Embedding Similarity\\n\\nAfter embedding values, you can calculate the similarity between them using the [`cosineSimilarity`](/docs/reference/ai-sdk-core/cosine-similarity) function.\\nThis is useful to e.g. find similar words or phrases in a dataset.\\nYou can also rank and filter related items based on their similarity.\\n\\n```ts highlight={\"2,10\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { cosineSimilarity, embedMany } from \\'ai\\';\\n\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n});\\n\\nconsole.log(\\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\\n);\\n```\\n\\n## Token Usage\\n\\nMany providers charge based on the number of tokens used to generate embeddings.\\nBoth `embed` and `embedMany` provide token usage information in the `usage` property of the result object:\\n\\n```ts highlight={\"4,9\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding, usage } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n\\nconsole.log(usage); // { tokens: 10 }\\n```\\n\\n## Settings\\n\\n### Provider Options\\n\\nEmbedding model settings can be configured using `providerOptions` for provider-specific parameters:\\n\\n```ts highlight={\"5-9\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  providerOptions: {\\n    openai: {\\n      dimensions: 512, // Reduce embedding dimensions\\n    },\\n  },\\n});\\n```\\n\\n### Parallel Requests\\n\\nThe `embedMany` function now supports parallel processing with configurable `maxParallelCalls` to optimize performance:\\n\\n```ts highlight={\"4\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embedMany } from \\'ai\\';\\n\\nconst { embeddings, usage } = await embedMany({\\n  maxParallelCalls: 2, // Limit parallel requests\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n});\\n```\\n\\n### Retries\\n\\nBoth `embed` and `embedMany` accept an optional `maxRetries` parameter of type `number`\\nthat you can use to set the maximum number of retries for the embedding process.\\nIt defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  maxRetries: 0, // Disable retries\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\nBoth `embed` and `embedMany` accept an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the embedding process or set a timeout.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\nBoth `embed` and `embedMany` accept an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the embedding request.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n## Response Information\\n\\nBoth `embed` and `embedMany` return response information that includes the raw provider response:\\n\\n```ts highlight={\"4,9\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding, response } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n\\nconsole.log(response); // Raw provider response\\n```\\n\\n## Embedding Middleware\\n\\nYou can enhance embedding models, e.g. to set default values, using\\n`wrapEmbeddingModel` and `EmbeddingModelV3Middleware`.\\n\\nHere is an example that uses the built-in `defaultEmbeddingSettingsMiddleware`:\\n\\n```ts\\nimport {\\n  customProvider,\\n  defaultEmbeddingSettingsMiddleware,\\n  embed,\\n  wrapEmbeddingModel,\\n  gateway,\\n} from \\'ai\\';\\n\\nconst embeddingModelWithDefaults = wrapEmbeddingModel({\\n  model: gateway.embeddingModel(\\'google/gemini-embedding-001\\'),\\n  middleware: defaultEmbeddingSettingsMiddleware({\\n    settings: {\\n      providerOptions: {\\n        google: {\\n          outputDimensionality: 256,\\n          taskType: \\'CLASSIFICATION\\',\\n        },\\n      },\\n    },\\n  }),\\n});\\n```\\n\\n## Embedding Providers & Models\\n\\nSeveral providers offer embedding models:\\n\\n| Provider                                                                                  | Model                           | Embedding Dimensions |\\n| ----------------------------------------------------------------------------------------- | ------------------------------- | -------------------- |\\n| [OpenAI](/providers/ai-sdk-providers/openai#embedding-models)                             | `text-embedding-3-large`        | 3072                 |\\n| [OpenAI](/providers/ai-sdk-providers/openai#embedding-models)                             | `text-embedding-3-small`        | 1536                 |\\n| [OpenAI](/providers/ai-sdk-providers/openai#embedding-models)                             | `text-embedding-ada-002`        | 1536                 |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `gemini-embedding-001`          | 3072                 |\\n| [Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#embedding-models) | `text-embedding-004`            | 768                  |\\n| [Mistral](/providers/ai-sdk-providers/mistral#embedding-models)                           | `mistral-embed`                 | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-v3.0`            | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-multilingual-v3.0`       | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-light-v3.0`      | 384                  |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-multilingual-light-v3.0` | 384                  |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-v2.0`            | 4096                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-english-light-v2.0`      | 1024                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#embedding-models)                             | `embed-multilingual-v2.0`       | 768                  |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models)             | `amazon.titan-embed-text-v1`    | 1536                 |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#embedding-models)             | `amazon.titan-embed-text-v2:0`  | 1024                 |\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/31-reranking.mdx'), name='31-reranking.mdx', displayName='31-reranking.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Reranking\\ndescription: Learn how to rerank documents with the AI SDK.\\n---\\n\\n# Reranking\\n\\nReranking is a technique used to improve search relevance by reordering a set of documents based on their relevance to a query.\\nUnlike embedding-based similarity search, reranking models are specifically trained to understand the relationship between queries and documents,\\noften producing more accurate relevance scores.\\n\\n## Reranking Documents\\n\\nThe AI SDK provides the [`rerank`](/docs/reference/ai-sdk-core/rerank) function to rerank documents based on their relevance to a query.\\nYou can use it with reranking models, e.g. `cohere.reranking(\\'rerank-v3.5\\')` or `bedrock.reranking(\\'cohere.rerank-v3-5:0\\')`.\\n\\n```tsx\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  \\'sunny day at the beach\\',\\n  \\'rainy afternoon in the city\\',\\n  \\'snowy night in the mountains\\',\\n];\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'talk about rain\\',\\n  topN: 2, // Return top 2 most relevant documents\\n});\\n\\nconsole.log(ranking);\\n// [\\n//   { originalIndex: 1, score: 0.9, document: \\'rainy afternoon in the city\\' },\\n//   { originalIndex: 0, score: 0.3, document: \\'sunny day at the beach\\' }\\n// ]\\n```\\n\\n## Working with Object Documents\\n\\nReranking also supports structured documents (JSON objects), making it ideal for searching through databases, emails, or other structured content:\\n\\n```tsx\\nimport { rerank } from \\'ai\\';\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\n\\nconst documents = [\\n  {\\n    from: \\'Paul Doe\\',\\n    subject: \\'Follow-up\\',\\n    text: \\'We are happy to give you a discount of 20% on your next order.\\',\\n  },\\n  {\\n    from: \\'John McGill\\',\\n    subject: \\'Missing Info\\',\\n    text: \\'Sorry, but here is the pricing information from Oracle: $5000/month\\',\\n  },\\n];\\n\\nconst { ranking, rerankedDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'Which pricing did we get from Oracle?\\',\\n  topN: 1,\\n});\\n\\nconsole.log(rerankedDocuments[0]);\\n// { from: \\'John McGill\\', subject: \\'Missing Info\\', text: \\'...\\' }\\n```\\n\\n## Understanding the Results\\n\\nThe `rerank` function returns a comprehensive result object:\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking, rerankedDocuments, originalDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n});\\n\\n// ranking: sorted array of { originalIndex, score, document }\\n// rerankedDocuments: documents sorted by relevance (convenience property)\\n// originalDocuments: original documents array\\n```\\n\\nEach item in the `ranking` array contains:\\n\\n- `originalIndex`: Position in the original documents array\\n- `score`: Relevance score (typically 0-1, where higher is more relevant)\\n- `document`: The original document\\n\\n## Settings\\n\\n### Top-N Results\\n\\nUse `topN` to limit the number of results returned. This is useful for retrieving only the most relevant documents:\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'doc1\\', \\'doc2\\', \\'doc3\\', \\'doc4\\', \\'doc5\\'],\\n  query: \\'relevant information\\',\\n  topN: 3, // Return only top 3 most relevant documents\\n});\\n```\\n\\n### Provider Options\\n\\nReranking model settings can be configured using `providerOptions` for provider-specific parameters:\\n\\n```ts highlight={\"8-12\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  providerOptions: {\\n    cohere: {\\n      maxTokensPerDoc: 1000, // Limit tokens per document\\n    },\\n  },\\n});\\n```\\n\\n### Retries\\n\\nThe `rerank` function accepts an optional `maxRetries` parameter of type `number`\\nthat you can use to set the maximum number of retries for the reranking process.\\nIt defaults to `2` retries (3 attempts in total). You can set it to `0` to disable retries.\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  maxRetries: 0, // Disable retries\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\nThe `rerank` function accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the reranking process or set a timeout.\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  abortSignal: AbortSignal.timeout(5000), // Abort after 5 seconds\\n});\\n```\\n\\n### Custom Headers\\n\\nThe `rerank` function accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the reranking request.\\n\\n```ts highlight={\"7\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n## Response Information\\n\\nThe `rerank` function returns response information that includes the raw provider response:\\n\\n```ts highlight={\"4,10\"}\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking, response } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n});\\n\\nconsole.log(response); // { id, timestamp, modelId, headers, body }\\n```\\n\\n## Reranking Providers & Models\\n\\nSeveral providers offer reranking models:\\n\\n| Provider                                                                      | Model                                 |\\n| ----------------------------------------------------------------------------- | ------------------------------------- |\\n| [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)                 | `rerank-v3.5`                         |\\n| [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)                 | `rerank-english-v3.0`                 |\\n| [Cohere](/providers/ai-sdk-providers/cohere#reranking-models)                 | `rerank-multilingual-v3.0`            |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `amazon.rerank-v1:0`                  |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#reranking-models) | `cohere.rerank-v3-5:0`                |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models)        | `Salesforce/Llama-Rank-v1`            |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#reranking-models)        | `mixedbread-ai/Mxbai-Rerank-Large-V2` |\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/35-image-generation.mdx'), name='35-image-generation.mdx', displayName='35-image-generation.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Image Generation\\ndescription: Learn how to generate images with the AI SDK.\\n---\\n\\n# Image Generation\\n\\n<Note type=\"warning\">Image generation is an experimental feature.</Note>\\n\\nThe AI SDK provides the [`generateImage`](/docs/reference/ai-sdk-core/generate-image)\\nfunction to generate images based on a given prompt using an image model.\\n\\n```tsx\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n});\\n```\\n\\nYou can access the image data using the `base64` or `uint8Array` properties:\\n\\n```tsx\\nconst base64 = image.base64; // base64 image data\\nconst uint8Array = image.uint8Array; // Uint8Array image data\\n```\\n\\n## Settings\\n\\n### Size and Aspect Ratio\\n\\nDepending on the model, you can either specify the size or the aspect ratio.\\n\\n##### Size\\n\\nThe size is specified as a string in the format `{width}x{height}`.\\nModels only support a few sizes, and the supported sizes are different for each model and provider.\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  size: \\'1024x1024\\',\\n});\\n```\\n\\n##### Aspect Ratio\\n\\nThe aspect ratio is specified as a string in the format `{width}:{height}`.\\nModels only support a few aspect ratios, and the supported aspect ratios are different for each model and provider.\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { vertex } from \\'@ai-sdk/google-vertex\\';\\n\\nconst { image } = await generateImage({\\n  model: vertex.image(\\'imagen-3.0-generate-002\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  aspectRatio: \\'16:9\\',\\n});\\n```\\n\\n### Generating Multiple Images\\n\\n`generateImage` also supports generating multiple images at once:\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { images } = await generateImage({\\n  model: openai.image(\\'dall-e-2\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  n: 4, // number of images to generate\\n});\\n```\\n\\n<Note>\\n  `generateImage` will automatically call the model as often as needed (in\\n  parallel) to generate the requested number of images.\\n</Note>\\n\\nEach image model has an internal limit on how many images it can generate in a single API call. The AI SDK manages this automatically by batching requests appropriately when you request multiple images using the `n` parameter. By default, the SDK uses provider-documented limits (for example, DALL-E 3 can only generate 1 image per call, while DALL-E 2 supports up to 10).\\n\\nIf needed, you can override this behavior using the `maxImagesPerCall` setting when generating your image. This is particularly useful when working with new or custom models where the default batch size might not be optimal:\\n\\n```tsx\\nconst { images } = await generateImage({\\n  model: openai.image(\\'dall-e-2\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  maxImagesPerCall: 5, // Override the default batch size\\n  n: 10, // Will make 2 calls of 5 images each\\n});\\n```\\n\\n### Providing a Seed\\n\\nYou can provide a seed to the `generateImage` function to control the output of the image generation process.\\nIf supported by the model, the same seed will always produce the same image.\\n\\n```tsx highlight={\"7\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  seed: 1234567890,\\n});\\n```\\n\\n### Provider-specific Settings\\n\\nImage models often have provider- or even model-specific settings.\\nYou can pass such settings to the `generateImage` function\\nusing the `providerOptions` parameter. The options for the provider\\n(`openai` in the example below) become request body properties.\\n\\n```tsx highlight={\"9\"}\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  size: \\'1024x1024\\',\\n  providerOptions: {\\n    openai: { style: \\'vivid\\', quality: \\'hd\\' },\\n  },\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\n`generateImage` accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the image generation process or set a timeout.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\n`generateImage` accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the image generation request.\\n\\n```ts highlight={\"7\"}\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\n\\nconst { image } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n### Warnings\\n\\nIf the model returns warnings, e.g. for unsupported parameters, they will be available in the `warnings` property of the response.\\n\\n```tsx\\nconst { image, warnings } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'Santa Claus driving a Cadillac\\',\\n});\\n```\\n\\n### Additional provider-specific meta data\\n\\nSome providers expose additional meta data for the result overall or per image.\\n\\n```tsx\\nconst prompt = \\'Santa Claus driving a Cadillac\\';\\n\\nconst { image, providerMetadata } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt,\\n});\\n\\nconst revisedPrompt = providerMetadata.openai.images[0]?.revisedPrompt;\\n\\nconsole.log({\\n  prompt,\\n  revisedPrompt,\\n});\\n```\\n\\nThe outer key of the returned `providerMetadata` is the provider name. The inner values are the metadata. An `images` key is always present in the metadata and is an array with the same length as the top level `images` key.\\n\\n### Error Handling\\n\\nWhen `generateImage` cannot generate a valid image, it throws a [`AI_NoImageGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-image-generated-error).\\n\\nThis error occurs when the AI provider fails to generate an image. It can arise due to the following reasons:\\n\\n- The model failed to generate a response\\n- The model generated a response that could not be parsed\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `responses`: Metadata about the image model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling\\n\\n```ts\\nimport { generateImage, NoImageGeneratedError } from \\'ai\\';\\n\\ntry {\\n  await generateImage({ model, prompt });\\n} catch (error) {\\n  if (NoImageGeneratedError.isInstance(error)) {\\n    console.log(\\'NoImageGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Responses:\\', error.responses);\\n  }\\n}\\n```\\n\\n## Generating Images with Language Models\\n\\nSome language models such as Google `gemini-2.5-flash-image-preview` support multi-modal outputs including images.\\nWith such models, you can access the generated images using the `files` property of the response.\\n\\n```ts\\nimport { google } from \\'@ai-sdk/google\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: google(\\'gemini-2.5-flash-image-preview\\'),\\n  prompt: \\'Generate an image of a comic cat\\',\\n});\\n\\nfor (const file of result.files) {\\n  if (file.mediaType.startsWith(\\'image/\\')) {\\n    // The file object provides multiple data formats:\\n    // Access images as base64 string, Uint8Array binary data, or check type\\n    // - file.base64: string (data URL format)\\n    // - file.uint8Array: Uint8Array (binary data)\\n    // - file.mediaType: string (e.g. \"image/png\")\\n  }\\n}\\n```\\n\\n## Image Models\\n\\n| Provider                                                                        | Model                                                        | Support sizes (`width x height`) or aspect ratios (`width : height`)                                                                                                |\\n| ------------------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [xAI Grok](/providers/ai-sdk-providers/xai#image-models)                        | `grok-2-image`                                               | 1024x768 (default)                                                                                                                                                  |\\n| [OpenAI](/providers/ai-sdk-providers/openai#image-models)                       | `gpt-image-1`                                                | 1024x1024, 1536x1024, 1024x1536                                                                                                                                     |\\n| [OpenAI](/providers/ai-sdk-providers/openai#image-models)                       | `dall-e-3`                                                   | 1024x1024, 1792x1024, 1024x1792                                                                                                                                     |\\n| [OpenAI](/providers/ai-sdk-providers/openai#image-models)                       | `dall-e-2`                                                   | 256x256, 512x512, 1024x1024                                                                                                                                         |\\n| [Amazon Bedrock](/providers/ai-sdk-providers/amazon-bedrock#image-models)       | `amazon.nova-canvas-v1:0`                                    | 320-4096 (multiples of 16), 1:4 to 4:1, max 4.2M pixels                                                                                                             |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/flux/dev`                                            | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/flux-lora`                                           | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/fast-sdxl`                                           | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/flux-pro/v1.1-ultra`                                 | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/ideogram/v2`                                         | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/recraft-v3`                                          | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/stable-diffusion-3.5-large`                          | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Fal](/providers/ai-sdk-providers/fal#image-models)                             | `fal-ai/hyper-sdxl`                                          | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `stabilityai/sd3.5`                                          | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21                                                                                                                      |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-1.1-pro`                             | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-1-schnell`                           | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-1-dev`                               | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `black-forest-labs/FLUX-pro`                                 | 256-1440 (multiples of 32)                                                                                                                                          |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `stabilityai/sd3.5-medium`                                   | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21                                                                                                                      |\\n| [DeepInfra](/providers/ai-sdk-providers/deepinfra#image-models)                 | `stabilityai/sdxl-turbo`                                     | 1:1, 16:9, 1:9, 3:2, 2:3, 4:5, 5:4, 9:16, 9:21                                                                                                                      |\\n| [Replicate](/providers/ai-sdk-providers/replicate)                              | `black-forest-labs/flux-schnell`                             | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9                                                                                                                     |\\n| [Replicate](/providers/ai-sdk-providers/replicate)                              | `recraft-ai/recraft-v3`                                      | 1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024 |\\n| [Google](/providers/ai-sdk-providers/google#image-models)                       | `imagen-3.0-generate-002`                                    | 1:1, 3:4, 4:3, 9:16, 16:9                                                                                                                                           |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models)         | `imagen-3.0-generate-002`                                    | 1:1, 3:4, 4:3, 9:16, 16:9                                                                                                                                           |\\n| [Google Vertex](/providers/ai-sdk-providers/google-vertex#image-models)         | `imagen-3.0-fast-generate-001`                               | 1:1, 3:4, 4:3, 9:16, 16:9                                                                                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/flux-1-dev-fp8`                   | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9                                                                                                                     |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/flux-1-schnell-fp8`               | 1:1, 2:3, 3:2, 4:5, 5:4, 16:9, 9:16, 9:21, 21:9                                                                                                                     |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/playground-v2-5-1024px-aesthetic` | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/japanese-stable-diffusion-xl`     | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/playground-v2-1024px-aesthetic`   | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/SSD-1B`                           | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Fireworks](/providers/ai-sdk-providers/fireworks#image-models)                 | `accounts/fireworks/models/stable-diffusion-xl-1024-v1-0`    | 640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640                                                                           |\\n| [Luma](/providers/ai-sdk-providers/luma#image-models)                           | `photon-1`                                                   | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Luma](/providers/ai-sdk-providers/luma#image-models)                           | `photon-flash-1`                                             | 1:1, 3:4, 4:3, 9:16, 16:9, 9:21, 21:9                                                                                                                               |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `stabilityai/stable-diffusion-xl-base-1.0`                   | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-dev`                               | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-dev-lora`                          | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-schnell`                           | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-canny`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-depth`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-redux`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1.1-pro`                             | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-pro`                               | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Together.ai](/providers/ai-sdk-providers/togetherai#image-models)              | `black-forest-labs/FLUX.1-schnell-Free`                      | 512x512, 768x768, 1024x1024                                                                                                                                         |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-pro`                                           | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-kontext-max`                                           | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1-ultra`                                         | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.1`                                               | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n| [Black Forest Labs](/providers/ai-sdk-providers/black-forest-labs#image-models) | `flux-pro-1.0-fill`                                          | From 3:7 (portrait) to 7:3 (landscape)                                                                                                                              |\\n\\nAbove are a small subset of the image models supported by the AI SDK providers. For more, see the respective provider documentation.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/36-transcription.mdx'), name='36-transcription.mdx', displayName='36-transcription.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Transcription\\ndescription: Learn how to transcribe audio with the AI SDK.\\n---\\n\\n# Transcription\\n\\n<Note type=\"warning\">Transcription is an experimental feature.</Note>\\n\\nThe AI SDK provides the [`transcribe`](/docs/reference/ai-sdk-core/transcribe)\\nfunction to transcribe audio using a transcription model.\\n\\n```ts\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n});\\n```\\n\\nThe `audio` property can be a `Uint8Array`, `ArrayBuffer`, `Buffer`, `string` (base64 encoded audio data), or a `URL`.\\n\\nTo access the generated transcript:\\n\\n```ts\\nconst text = transcript.text; // transcript text e.g. \"Hello, world!\"\\nconst segments = transcript.segments; // array of segments with start and end times, if available\\nconst language = transcript.language; // language of the transcript e.g. \"en\", if available\\nconst durationInSeconds = transcript.durationInSeconds; // duration of the transcript in seconds, if available\\n```\\n\\n## Settings\\n\\n### Provider-Specific settings\\n\\nTranscription models often have provider or model-specific settings which you can set using the `providerOptions` parameter.\\n\\n```ts highlight=\"8-12\"\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n  providerOptions: {\\n    openai: {\\n      timestampGranularities: [\\'word\\'],\\n    },\\n  },\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\n`transcribe` accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the transcription process or set a timeout.\\n\\n```ts highlight=\"8\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\n`transcribe` accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the transcription request.\\n\\n```ts highlight=\"8\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n### Warnings\\n\\nWarnings (e.g. unsupported parameters) are available on the `warnings` property.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst transcript = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n});\\n\\nconst warnings = transcript.warnings;\\n```\\n\\n### Error Handling\\n\\nWhen `transcribe` cannot generate a valid transcript, it throws a [`AI_NoTranscriptGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error).\\n\\nThis error can arise for any the following reasons:\\n\\n- The model failed to generate a response\\n- The model generated a response that could not be parsed\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `responses`: Metadata about the transcription model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling.\\n\\n```ts\\nimport {\\n  experimental_transcribe as transcribe,\\n  NoTranscriptGeneratedError,\\n} from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\ntry {\\n  await transcribe({\\n    model: openai.transcription(\\'whisper-1\\'),\\n    audio: await readFile(\\'audio.mp3\\'),\\n  });\\n} catch (error) {\\n  if (NoTranscriptGeneratedError.isInstance(error)) {\\n    console.log(\\'NoTranscriptGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Responses:\\', error.responses);\\n  }\\n}\\n```\\n\\n## Transcription Models\\n\\n| Provider                                                                  | Model                        |\\n| ------------------------------------------------------------------------- | ---------------------------- |\\n| [OpenAI](/providers/ai-sdk-providers/openai#transcription-models)         | `whisper-1`                  |\\n| [OpenAI](/providers/ai-sdk-providers/openai#transcription-models)         | `gpt-4o-transcribe`          |\\n| [OpenAI](/providers/ai-sdk-providers/openai#transcription-models)         | `gpt-4o-mini-transcribe`     |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe_v1`                  |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#transcription-models) | `scribe_v1_experimental`     |\\n| [Groq](/providers/ai-sdk-providers/groq#transcription-models)             | `whisper-large-v3-turbo`     |\\n| [Groq](/providers/ai-sdk-providers/groq#transcription-models)             | `distil-whisper-large-v3-en` |\\n| [Groq](/providers/ai-sdk-providers/groq#transcription-models)             | `whisper-large-v3`           |\\n| [Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)    | `whisper-1`                  |\\n| [Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)    | `gpt-4o-transcribe`          |\\n| [Azure OpenAI](/providers/ai-sdk-providers/azure#transcription-models)    | `gpt-4o-mini-transcribe`     |\\n| [Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)          | `machine`                    |\\n| [Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)          | `low_cost`                   |\\n| [Rev.ai](/providers/ai-sdk-providers/revai#transcription-models)          | `fusion`                     |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `base` (+ variants)          |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `enhanced` (+ variants)      |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `nova` (+ variants)          |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `nova-2` (+ variants)        |\\n| [Deepgram](/providers/ai-sdk-providers/deepgram#transcription-models)     | `nova-3` (+ variants)        |\\n| [Gladia](/providers/ai-sdk-providers/gladia#transcription-models)         | `default`                    |\\n| [AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models) | `best`                       |\\n| [AssemblyAI](/providers/ai-sdk-providers/assemblyai#transcription-models) | `nano`                       |\\n| [Fal](/providers/ai-sdk-providers/fal#transcription-models)               | `whisper`                    |\\n| [Fal](/providers/ai-sdk-providers/fal#transcription-models)               | `wizper`                     |\\n\\nAbove are a small subset of the transcription models supported by the AI SDK providers. For more, see the respective provider documentation.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/37-speech.mdx'), name='37-speech.mdx', displayName='37-speech.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Speech\\ndescription: Learn how to generate speech from text with the AI SDK.\\n---\\n\\n# Speech\\n\\n<Note type=\"warning\">Speech is an experimental feature.</Note>\\n\\nThe AI SDK provides the [`generateSpeech`](/docs/reference/ai-sdk-core/generate-speech)\\nfunction to generate speech from text using a speech model.\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  voice: \\'alloy\\',\\n});\\n```\\n\\n### Language Setting\\n\\nYou can specify the language for speech generation (provider support varies):\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { lmnt } from \\'@ai-sdk/lmnt\\';\\n\\nconst audio = await generateSpeech({\\n  model: lmnt.speech(\\'aurora\\'),\\n  text: \\'Hola, mundo!\\',\\n  language: \\'es\\', // Spanish\\n});\\n```\\n\\nTo access the generated audio:\\n\\n```ts\\nconst audio = audio.audioData; // audio data e.g. Uint8Array\\n```\\n\\n## Settings\\n\\n### Provider-Specific settings\\n\\nYou can set model-specific settings with the `providerOptions` parameter.\\n\\n```ts highlight=\"7-11\"\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  providerOptions: {\\n    openai: {\\n      // ...\\n    },\\n  },\\n});\\n```\\n\\n### Abort Signals and Timeouts\\n\\n`generateSpeech` accepts an optional `abortSignal` parameter of\\ntype [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal)\\nthat you can use to abort the speech generation process or set a timeout.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  abortSignal: AbortSignal.timeout(1000), // Abort after 1 second\\n});\\n```\\n\\n### Custom Headers\\n\\n`generateSpeech` accepts an optional `headers` parameter of type `Record<string, string>`\\nthat you can use to add custom headers to the speech generation request.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n  headers: { \\'X-Custom-Header\\': \\'custom-value\\' },\\n});\\n```\\n\\n### Warnings\\n\\nWarnings (e.g. unsupported parameters) are available on the `warnings` property.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\n\\nconst audio = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello, world!\\',\\n});\\n\\nconst warnings = audio.warnings;\\n```\\n\\n### Error Handling\\n\\nWhen `generateSpeech` cannot generate a valid audio, it throws a [`AI_NoSpeechGeneratedError`](/docs/reference/ai-sdk-errors/ai-no-speech-generated-error).\\n\\nThis error can arise for any the following reasons:\\n\\n- The model failed to generate a response\\n- The model generated a response that could not be parsed\\n\\nThe error preserves the following information to help you log the issue:\\n\\n- `responses`: Metadata about the speech model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling.\\n\\n```ts\\nimport {\\n  experimental_generateSpeech as generateSpeech,\\n  NoSpeechGeneratedError,\\n} from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\ntry {\\n  await generateSpeech({\\n    model: openai.speech(\\'tts-1\\'),\\n    text: \\'Hello, world!\\',\\n  });\\n} catch (error) {\\n  if (NoSpeechGeneratedError.isInstance(error)) {\\n    console.log(\\'AI_NoSpeechGeneratedError\\');\\n    console.log(\\'Cause:\\', error.cause);\\n    console.log(\\'Responses:\\', error.responses);\\n  }\\n}\\n```\\n\\n## Speech Models\\n\\n| Provider                                                           | Model                    |\\n| ------------------------------------------------------------------ | ------------------------ |\\n| [OpenAI](/providers/ai-sdk-providers/openai#speech-models)         | `tts-1`                  |\\n| [OpenAI](/providers/ai-sdk-providers/openai#speech-models)         | `tts-1-hd`               |\\n| [OpenAI](/providers/ai-sdk-providers/openai#speech-models)         | `gpt-4o-mini-tts`        |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_v3`              |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_multilingual_v2` |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_flash_v2_5`      |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_flash_v2`        |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_turbo_v2_5`      |\\n| [ElevenLabs](/providers/ai-sdk-providers/elevenlabs#speech-models) | `eleven_turbo_v2`        |\\n| [LMNT](/providers/ai-sdk-providers/lmnt#speech-models)             | `aurora`                 |\\n| [LMNT](/providers/ai-sdk-providers/lmnt#speech-models)             | `blizzard`               |\\n| [Hume](/providers/ai-sdk-providers/hume#speech-models)             | `default`                |\\n\\nAbove are a small subset of the speech models supported by the AI SDK providers. For more, see the respective provider documentation.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/40-middleware.mdx'), name='40-middleware.mdx', displayName='40-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Language Model Middleware\\ndescription: Learn how to use middleware to enhance the behavior of language models\\n---\\n\\n# Language Model Middleware\\n\\nLanguage model middleware is a way to enhance the behavior of language models\\nby intercepting and modifying the calls to the language model.\\n\\nIt can be used to add features like guardrails, RAG, caching, and logging\\nin a language model agnostic way. Such middleware can be developed and\\ndistributed independently from the language models that they are applied to.\\n\\n## Using Language Model Middleware\\n\\nYou can use language model middleware with the `wrapLanguageModel` function.\\nIt takes a language model and a language model middleware and returns a new\\nlanguage model that incorporates the middleware.\\n\\n```ts\\nimport { wrapLanguageModel } from \\'ai\\';\\n\\nconst wrappedLanguageModel = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: yourLanguageModelMiddleware,\\n});\\n```\\n\\nThe wrapped language model can be used just like any other language model, e.g. in `streamText`:\\n\\n```ts highlight=\"2\"\\nconst result = streamText({\\n  model: wrappedLanguageModel,\\n  prompt: \\'What cities are in the United States?\\',\\n});\\n```\\n\\n## Multiple middlewares\\n\\nYou can provide multiple middlewares to the `wrapLanguageModel` function.\\nThe middlewares will be applied in the order they are provided.\\n\\n```ts\\nconst wrappedLanguageModel = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: [firstMiddleware, secondMiddleware],\\n});\\n\\n// applied as: firstMiddleware(secondMiddleware(yourModel))\\n```\\n\\n## Built-in Middleware\\n\\nThe AI SDK comes with several built-in middlewares that you can use to configure language models:\\n\\n- `extractReasoningMiddleware`: Extracts reasoning information from the generated text and exposes it as a `reasoning` property on the result.\\n- `simulateStreamingMiddleware`: Simulates streaming behavior with responses from non-streaming language models.\\n- `defaultSettingsMiddleware`: Applies default settings to a language model.\\n\\n### Extract Reasoning\\n\\nSome providers and models expose reasoning information in the generated text using special tags,\\ne.g. &lt;think&gt; and &lt;/think&gt;.\\n\\nThe `extractReasoningMiddleware` function can be used to extract this reasoning information and expose it as a `reasoning` property on the result.\\n\\n```ts\\nimport { wrapLanguageModel, extractReasoningMiddleware } from \\'ai\\';\\n\\nconst model = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: extractReasoningMiddleware({ tagName: \\'think\\' }),\\n});\\n```\\n\\nYou can then use that enhanced model in functions like `generateText` and `streamText`.\\n\\nThe `extractReasoningMiddleware` function also includes a `startWithReasoning` option.\\nWhen set to `true`, the reasoning tag will be prepended to the generated text.\\nThis is useful for models that do not include the reasoning tag at the beginning of the response.\\nFor more details, see the [DeepSeek R1 guide](/docs/guides/r1#deepseek-r1-middleware).\\n\\n### Simulate Streaming\\n\\nThe `simulateStreamingMiddleware` function can be used to simulate streaming behavior with responses from non-streaming language models.\\nThis is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\\n\\n```ts\\nimport { wrapLanguageModel, simulateStreamingMiddleware } from \\'ai\\';\\n\\nconst model = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: simulateStreamingMiddleware(),\\n});\\n```\\n\\n### Default Settings\\n\\nThe `defaultSettingsMiddleware` function can be used to apply default settings to a language model.\\n\\n```ts\\nimport { wrapLanguageModel, defaultSettingsMiddleware } from \\'ai\\';\\n\\nconst model = wrapLanguageModel({\\n  model: yourModel,\\n  middleware: defaultSettingsMiddleware({\\n    settings: {\\n      temperature: 0.5,\\n      maxOutputTokens: 800,\\n      providerOptions: { openai: { store: false } },\\n    },\\n  }),\\n});\\n```\\n\\n## Community Middleware\\n\\nThe AI SDK provides a Language Model Middleware specification. Community members can develop middleware that adheres to this specification, making it compatible with the AI SDK ecosystem.\\n\\nHere are some community middlewares that you can explore:\\n\\n### Custom tool call parser\\n\\nThe [Custom tool call parser](https://github.com/minpeter/ai-sdk-tool-call-middleware) middleware extends tool call capabilities to models that don\\'t natively support the OpenAI-style `tools` parameter. This includes many self-hosted and third-party models that lack native function calling features.\\n\\n<Note>\\n  Using this middleware on models that support native function calls may result\\n  in unintended performance degradation, so check whether your model supports\\n  native function calls before deciding to use it.\\n</Note>\\n\\nThis middleware enables function calling capabilities by converting function schemas into prompt instructions and parsing the model\\'s responses into structured function calls. It works by transforming the JSON function definitions into natural language instructions the model can understand, then analyzing the generated text to extract function call attempts. This approach allows developers to use the same function calling API across different model providers, even with models that don\\'t natively support the OpenAI-style function calling format, providing a consistent function calling experience regardless of the underlying model implementation.\\n\\nThe `@ai-sdk-tool/parser` package offers three middleware variants:\\n\\n- `createToolMiddleware`: A flexible function for creating custom tool call middleware tailored to specific models\\n- `hermesToolMiddleware`: Ready-to-use middleware for Hermes & Qwen format function calls\\n- `gemmaToolMiddleware`: Pre-configured middleware for Gemma 3 model series function call format\\n\\nHere\\'s how you can enable function calls with Gemma models that don\\'t support them natively:\\n\\n```ts\\nimport { wrapLanguageModel } from \\'ai\\';\\nimport { gemmaToolMiddleware } from \\'@ai-sdk-tool/parser\\';\\n\\nconst model = wrapLanguageModel({\\n  model: openrouter(\\'google/gemma-3-27b-it\\'),\\n  middleware: gemmaToolMiddleware,\\n});\\n```\\n\\nFind more examples at this [link](https://github.com/minpeter/ai-sdk-tool-call-middleware/tree/main/examples/core/src).\\n\\n## Implementing Language Model Middleware\\n\\n<Note>\\n  Implementing language model middleware is advanced functionality and requires\\n  a solid understanding of the [language model\\n  specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\\n</Note>\\n\\nYou can implement any of the following three function to modify the behavior of the language model:\\n\\n1. `transformParams`: Transforms the parameters before they are passed to the language model, for both `doGenerate` and `doStream`.\\n2. `wrapGenerate`: Wraps the `doGenerate` method of the [language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\\n   You can modify the parameters, call the language model, and modify the result.\\n3. `wrapStream`: Wraps the `doStream` method of the [language model](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v2/language-model-v2.ts).\\n   You can modify the parameters, call the language model, and modify the result.\\n\\nHere are some examples of how to implement language model middleware:\\n\\n## Examples\\n\\n<Note>\\n  These examples are not meant to be used in production. They are just to show\\n  how you can use middleware to enhance the behavior of language models.\\n</Note>\\n\\n### Logging\\n\\nThis example shows how to log the parameters and generated text of a language model call.\\n\\n```ts\\nimport type {\\n  LanguageModelV3Middleware,\\n  LanguageModelV3StreamPart,\\n} from \\'@ai-sdk/provider\\';\\n\\nexport const yourLogMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    console.log(\\'doGenerate called\\');\\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\\n\\n    const result = await doGenerate();\\n\\n    console.log(\\'doGenerate finished\\');\\n    console.log(`generated text: ${result.text}`);\\n\\n    return result;\\n  },\\n\\n  wrapStream: async ({ doStream, params }) => {\\n    console.log(\\'doStream called\\');\\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\\n\\n    const { stream, ...rest } = await doStream();\\n\\n    let generatedText = \\'\\';\\n    const textBlocks = new Map<string, string>();\\n\\n    const transformStream = new TransformStream<\\n      LanguageModelV3StreamPart,\\n      LanguageModelV3StreamPart\\n    >({\\n      transform(chunk, controller) {\\n        switch (chunk.type) {\\n          case \\'text-start\\': {\\n            textBlocks.set(chunk.id, \\'\\');\\n            break;\\n          }\\n          case \\'text-delta\\': {\\n            const existing = textBlocks.get(chunk.id) || \\'\\';\\n            textBlocks.set(chunk.id, existing + chunk.delta);\\n            generatedText += chunk.delta;\\n            break;\\n          }\\n          case \\'text-end\\': {\\n            console.log(\\n              `Text block ${chunk.id} completed:`,\\n              textBlocks.get(chunk.id),\\n            );\\n            break;\\n          }\\n        }\\n\\n        controller.enqueue(chunk);\\n      },\\n\\n      flush() {\\n        console.log(\\'doStream finished\\');\\n        console.log(`generated text: ${generatedText}`);\\n      },\\n    });\\n\\n    return {\\n      stream: stream.pipeThrough(transformStream),\\n      ...rest,\\n    };\\n  },\\n};\\n```\\n\\n### Caching\\n\\nThis example shows how to build a simple cache for the generated text of a language model call.\\n\\n```ts\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nconst cache = new Map<string, any>();\\n\\nexport const yourCacheMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    const cacheKey = JSON.stringify(params);\\n\\n    if (cache.has(cacheKey)) {\\n      return cache.get(cacheKey);\\n    }\\n\\n    const result = await doGenerate();\\n\\n    cache.set(cacheKey, result);\\n\\n    return result;\\n  },\\n\\n  // here you would implement the caching logic for streaming\\n};\\n```\\n\\n### Retrieval Augmented Generation (RAG)\\n\\nThis example shows how to use RAG as middleware.\\n\\n<Note>\\n  Helper functions like `getLastUserMessageText` and `findSources` are not part\\n  of the AI SDK. They are just used in this example to illustrate the concept of\\n  RAG.\\n</Note>\\n\\n```ts\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nexport const yourRagMiddleware: LanguageModelV3Middleware = {\\n  transformParams: async ({ params }) => {\\n    const lastUserMessageText = getLastUserMessageText({\\n      prompt: params.prompt,\\n    });\\n\\n    if (lastUserMessageText == null) {\\n      return params; // do not use RAG (send unmodified parameters)\\n    }\\n\\n    const instruction =\\n      \\'Use the following information to answer the question:\\\\n\\' +\\n      findSources({ text: lastUserMessageText })\\n        .map(chunk => JSON.stringify(chunk))\\n        .join(\\'\\\\n\\');\\n\\n    return addToLastUserMessage({ params, text: instruction });\\n  },\\n};\\n```\\n\\n### Guardrails\\n\\nGuard rails are a way to ensure that the generated text of a language model call\\nis safe and appropriate. This example shows how to use guardrails as middleware.\\n\\n```ts\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nexport const yourGuardrailMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate }) => {\\n    const { text, ...rest } = await doGenerate();\\n\\n    // filtering approach, e.g. for PII or other sensitive information:\\n    const cleanedText = text?.replace(/badword/g, \\'<REDACTED>\\');\\n\\n    return { text: cleanedText, ...rest };\\n  },\\n\\n  // here you would implement the guardrail logic for streaming\\n  // Note: streaming guardrails are difficult to implement, because\\n  // you do not know the full content of the stream until it\\'s finished.\\n};\\n```\\n\\n## Configuring Per Request Custom Metadata\\n\\nTo send and access custom metadata in Middleware, you can use `providerOptions`. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText, wrapLanguageModel } from \\'ai\\';\\nimport type { LanguageModelV3Middleware } from \\'@ai-sdk/provider\\';\\n\\nexport const yourLogMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    console.log(\\'METADATA\\', params?.providerMetadata?.yourLogMiddleware);\\n    const result = await doGenerate();\\n    return result;\\n  },\\n};\\n\\nconst { text } = await generateText({\\n  model: wrapLanguageModel({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    middleware: yourLogMiddleware,\\n  }),\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  providerOptions: {\\n    yourLogMiddleware: {\\n      hello: \\'world\\',\\n    },\\n  },\\n});\\n\\nconsole.log(text);\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/45-provider-management.mdx'), name='45-provider-management.mdx', displayName='45-provider-management.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Provider & Model Management\\ndescription: Learn how to work with multiple providers and models\\n---\\n\\n# Provider & Model Management\\n\\nWhen you work with multiple providers and models, it is often desirable to manage them in a central place\\nand access the models through simple string ids.\\n\\nThe AI SDK offers [custom providers](/docs/reference/ai-sdk-core/custom-provider) and\\na [provider registry](/docs/reference/ai-sdk-core/provider-registry) for this purpose:\\n\\n- With **custom providers**, you can pre-configure model settings, provide model name aliases,\\n  and limit the available models.\\n- The **provider registry** lets you mix multiple providers and access them through simple string ids.\\n\\nYou can mix and match custom providers, the provider registry, and [middleware](/docs/ai-sdk-core/middleware) in your application.\\n\\n## Custom Providers\\n\\nYou can create a [custom provider](/docs/reference/ai-sdk-core/custom-provider) using `customProvider`.\\n\\n### Example: custom model settings\\n\\nYou might want to override the default model settings for a provider or provide model name aliases\\nwith pre-configured settings.\\n\\n```ts\\nimport {\\n  gateway,\\n  customProvider,\\n  defaultSettingsMiddleware,\\n  wrapLanguageModel,\\n} from \\'ai\\';\\n\\n// custom provider with different provider options:\\nexport const openai = customProvider({\\n  languageModels: {\\n    // replacement model with custom provider options:\\n    \\'gpt-5.1\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n    // alias model with custom provider options:\\n    \\'gpt-5.1-high-reasoning\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n  },\\n  fallbackProvider: gateway,\\n});\\n```\\n\\n### Example: model name alias\\n\\nYou can also provide model name aliases, so you can update the model version in one place in the future:\\n\\n```ts\\nimport { customProvider, gateway } from \\'ai\\';\\n\\n// custom provider with alias names:\\nexport const anthropic = customProvider({\\n  languageModels: {\\n    opus: gateway(\\'anthropic/claude-opus-4.1\\'),\\n    sonnet: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n    haiku: gateway(\\'anthropic/claude-haiku-4.5\\'),\\n  },\\n  fallbackProvider: gateway,\\n});\\n```\\n\\n### Example: limit available models\\n\\nYou can limit the available models in the system, even if you have multiple providers.\\n\\n```ts\\nimport {\\n  customProvider,\\n  defaultSettingsMiddleware,\\n  wrapLanguageModel,\\n  gateway,\\n} from \\'ai\\';\\n\\nexport const myProvider = customProvider({\\n  languageModels: {\\n    \\'text-medium\\': gateway(\\'anthropic/claude-3-5-sonnet-20240620\\'),\\n    \\'text-small\\': gateway(\\'openai/gpt-5-mini\\'),\\n    \\'reasoning-medium\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n    \\'reasoning-fast\\': wrapLanguageModel({\\n      model: gateway(\\'openai/gpt-5.1\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'low\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n  },\\n  embeddingModels: {\\n    embedding: gateway.embeddingModel(\\'openai/text-embedding-3-small\\'),\\n  },\\n  // no fallback provider\\n});\\n```\\n\\n## Provider Registry\\n\\nYou can create a [provider registry](/docs/reference/ai-sdk-core/provider-registry) with multiple providers and models using `createProviderRegistry`.\\n\\n### Setup\\n\\n```ts filename={\"registry.ts\"}\\nimport { anthropic } from \\'@ai-sdk/anthropic\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createProviderRegistry, gateway } from \\'ai\\';\\n\\nexport const registry = createProviderRegistry({\\n  // register provider with prefix and default setup using gateway:\\n  gateway,\\n\\n  // register provider with prefix and direct provider import:\\n  anthropic,\\n  openai,\\n});\\n```\\n\\n### Setup with Custom Separator\\n\\nBy default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator:\\n\\n```ts filename={\"registry.ts\"}\\nimport { anthropic } from \\'@ai-sdk/anthropic\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createProviderRegistry, gateway } from \\'ai\\';\\n\\nexport const customSeparatorRegistry = createProviderRegistry(\\n  {\\n    gateway,\\n    anthropic,\\n    openai,\\n  },\\n  { separator: \\' > \\' },\\n);\\n```\\n\\n### Example: Use language models\\n\\nYou can access language models by using the `languageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateText } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { text } = await generateText({\\n  model: registry.languageModel(\\'openai:gpt-5.1\\'), // default separator\\n  // or with custom separator:\\n  // model: customSeparatorRegistry.languageModel(\\'openai > gpt-5.1\\'),\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\n### Example: Use text embedding models\\n\\nYou can access text embedding models by using the `.embeddingModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { embed } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { embedding } = await embed({\\n  model: registry.embeddingModel(\\'openai:text-embedding-3-small\\'),\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n### Example: Use image models\\n\\nYou can access image models by using the `imageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateImage } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { image } = await generateImage({\\n  model: registry.imageModel(\\'openai:dall-e-3\\'),\\n  prompt: \\'A beautiful sunset over a calm ocean\\',\\n});\\n```\\n\\n## Combining Custom Providers, Provider Registry, and Middleware\\n\\nThe central idea of provider management is to set up a file that contains all the providers and models you want to use.\\nYou may want to pre-configure model settings, provide model name aliases, limit the available models, and more.\\n\\nHere is an example that implements the following concepts:\\n\\n- pass through gateway with a namespace prefix (here: `gateway > *`)\\n- pass through a full provider with a namespace prefix (here: `xai > *`)\\n- setup an OpenAI-compatible provider with custom api key and base URL (here: `custom > *`)\\n- setup model name aliases (here: `anthropic > fast`, `anthropic > writing`, `anthropic > reasoning`)\\n- pre-configure model settings (here: `anthropic > reasoning`)\\n- validate the provider-specific options (here: `AnthropicProviderOptions`)\\n- use a fallback provider (here: `anthropic > *`)\\n- limit a provider to certain models without a fallback (here: `groq > gemma2-9b-it`, `groq > qwen-qwq-32b`)\\n- define a custom separator for the provider registry (here: `>`)\\n\\n```ts\\nimport { anthropic, AnthropicProviderOptions } from \\'@ai-sdk/anthropic\\';\\nimport { createOpenAICompatible } from \\'@ai-sdk/openai-compatible\\';\\nimport { xai } from \\'@ai-sdk/xai\\';\\nimport { groq } from \\'@ai-sdk/groq\\';\\nimport {\\n  createProviderRegistry,\\n  customProvider,\\n  defaultSettingsMiddleware,\\n  gateway,\\n  wrapLanguageModel,\\n} from \\'ai\\';\\n\\nexport const registry = createProviderRegistry(\\n  {\\n    // pass through gateway with a namespace prefix\\n    gateway,\\n\\n    // pass through full providers with namespace prefixes\\n    xai,\\n\\n    // access an OpenAI-compatible provider with custom setup\\n    custom: createOpenAICompatible({\\n      name: \\'provider-name\\',\\n      apiKey: process.env.CUSTOM_API_KEY,\\n      baseURL: \\'https://api.custom.com/v1\\',\\n    }),\\n\\n    // setup model name aliases\\n    anthropic: customProvider({\\n      languageModels: {\\n        fast: anthropic(\\'claude-haiku-4-5\\'),\\n\\n        // simple model\\n        writing: anthropic(\\'claude-sonnet-4-5\\'),\\n\\n        // extended reasoning model configuration:\\n        reasoning: wrapLanguageModel({\\n          model: anthropic(\\'claude-sonnet-4-5\\'),\\n          middleware: defaultSettingsMiddleware({\\n            settings: {\\n              maxOutputTokens: 100000, // example default setting\\n              providerOptions: {\\n                anthropic: {\\n                  thinking: {\\n                    type: \\'enabled\\',\\n                    budgetTokens: 32000,\\n                  },\\n                } satisfies AnthropicProviderOptions,\\n              },\\n            },\\n          }),\\n        }),\\n      },\\n      fallbackProvider: anthropic,\\n    }),\\n\\n    // limit a provider to certain models without a fallback\\n    groq: customProvider({\\n      languageModels: {\\n        \\'gemma2-9b-it\\': groq(\\'gemma2-9b-it\\'),\\n        \\'qwen-qwq-32b\\': groq(\\'qwen-qwq-32b\\'),\\n      },\\n    }),\\n  },\\n  { separator: \\' > \\' },\\n);\\n\\n// usage:\\nconst model = registry.languageModel(\\'anthropic > reasoning\\');\\n```\\n\\n## Global Provider Configuration\\n\\nThe AI SDK 5 includes a global provider feature that allows you to specify a model using just a plain model ID string:\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\n\\nconst result = await streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\', // Uses the global provider (defaults to gateway)\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\nBy default, the global provider is set to the Vercel AI Gateway.\\n\\n### Customizing the Global Provider\\n\\nYou can set your own preferred global provider:\\n\\n```ts filename=\"setup.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// Initialize once during startup:\\nglobalThis.AI_SDK_DEFAULT_PROVIDER = openai;\\n```\\n\\n```ts filename=\"app.ts\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = await streamText({\\n  model: \\'gpt-5.1\\', // Uses OpenAI provider without prefix\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\nThis simplifies provider usage and makes it easier to switch between providers without changing your model references throughout your codebase.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/50-error-handling.mdx'), name='50-error-handling.mdx', displayName='50-error-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Error Handling\\ndescription: Learn how to handle errors in the AI SDK Core\\n---\\n\\n# Error Handling\\n\\n## Handling regular errors\\n\\nRegular errors are thrown and can be handled using the `try/catch` block.\\n\\n```ts highlight=\"3,8-10\"\\nimport { generateText } from \\'ai\\';\\n\\ntry {\\n  const { text } = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  });\\n} catch (error) {\\n  // handle error\\n}\\n```\\n\\nSee [Error Types](/docs/reference/ai-sdk-errors) for more information on the different types of errors that may be thrown.\\n\\n## Handling streaming errors (simple streams)\\n\\nWhen errors occur during streams that do not support error chunks,\\nthe error is thrown as a regular error.\\nYou can handle these errors using the `try/catch` block.\\n\\n```ts highlight=\"3,12-14\"\\nimport { streamText } from \\'ai\\';\\n\\ntry {\\n  const { textStream } = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  });\\n\\n  for await (const textPart of textStream) {\\n    process.stdout.write(textPart);\\n  }\\n} catch (error) {\\n  // handle error\\n}\\n```\\n\\n## Handling streaming errors (streaming with `error` support)\\n\\nFull streams support error parts.\\nYou can handle those parts similar to other parts.\\nIt is recommended to also add a try-catch block for errors that\\nhappen outside of the streaming.\\n\\n```ts highlight=\"13-21\"\\nimport { streamText } from \\'ai\\';\\n\\ntry {\\n  const { fullStream } = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  });\\n\\n  for await (const part of fullStream) {\\n    switch (part.type) {\\n      // ... handle other part types\\n\\n      case \\'error\\': {\\n        const error = part.error;\\n        // handle error\\n        break;\\n      }\\n\\n      case \\'abort\\': {\\n        // handle stream abort\\n        break;\\n      }\\n\\n      case \\'tool-error\\': {\\n        const error = part.error;\\n        // handle error\\n        break;\\n      }\\n    }\\n  }\\n} catch (error) {\\n  // handle error\\n}\\n```\\n\\n## Handling stream aborts\\n\\nWhen streams are aborted (e.g., via chat stop button), you may want to perform cleanup operations like updating stored messages in your UI. Use the `onAbort` callback to handle these cases.\\n\\nThe `onAbort` callback is called when a stream is aborted via `AbortSignal`, but `onFinish` is not called. This ensures you can still update your UI state appropriately.\\n\\n```ts highlight=\"5-9\"\\nimport { streamText } from \\'ai\\';\\n\\nconst { textStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n  onAbort: ({ steps }) => {\\n    // Update stored messages or perform cleanup\\n    console.log(\\'Stream aborted after\\', steps.length, \\'steps\\');\\n  },\\n  onFinish: ({ steps, totalUsage }) => {\\n    // This is called on normal completion\\n    console.log(\\'Stream completed normally\\');\\n  },\\n});\\n\\nfor await (const textPart of textStream) {\\n  process.stdout.write(textPart);\\n}\\n```\\n\\nThe `onAbort` callback receives:\\n\\n- `steps`: An array of all completed steps before the abort\\n\\nYou can also handle abort events directly in the stream:\\n\\n```ts highlight=\"10-13\"\\nimport { streamText } from \\'ai\\';\\n\\nconst { fullStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a vegetarian lasagna recipe for 4 people.\\',\\n});\\n\\nfor await (const chunk of fullStream) {\\n  switch (chunk.type) {\\n    case \\'abort\\': {\\n      // Handle abort directly in stream\\n      console.log(\\'Stream was aborted\\');\\n      break;\\n    }\\n    // ... handle other part types\\n  }\\n}\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/55-testing.mdx'), name='55-testing.mdx', displayName='55-testing.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Testing\\ndescription: Learn how to use AI SDK Core mock providers for testing.\\n---\\n\\n# Testing\\n\\nTesting language models can be challenging, because they are non-deterministic\\nand calling them is slow and expensive.\\n\\nTo enable you to unit test your code that uses the AI SDK, the AI SDK Core\\nincludes mock providers and test helpers. You can import the following helpers from `ai/test`:\\n\\n- `MockEmbeddingModelV3`: A mock embedding model using the [embedding model v3 specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/embedding-model/v3/embedding-model-v3.ts).\\n- `MockLanguageModelV3`: A mock language model using the [language model v3 specification](https://github.com/vercel/ai/blob/v5/packages/provider/src/language-model/v3/language-model-v3.ts).\\n- `mockId`: Provides an incrementing integer ID.\\n- `mockValues`: Iterates over an array of values with each call. Returns the last value when the array is exhausted.\\n- [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream): Simulates a readable stream with delays.\\n\\nWith mock providers and test helpers, you can control the output of the AI SDK\\nand test your code in a repeatable and deterministic way without actually calling\\na language model provider.\\n\\n## Examples\\n\\nYou can use the test helpers with the AI Core functions in your unit tests:\\n\\n### generateText\\n\\n```ts\\nimport { generateText } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\n\\nconst result = await generateText({\\n  model: new MockLanguageModelV3({\\n    doGenerate: async () => ({\\n      finishReason: \\'stop\\',\\n      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\\n      content: [{ type: \\'text\\', text: `Hello, world!` }],\\n      warnings: [],\\n    }),\\n  }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### streamText\\n\\n```ts\\nimport { streamText, simulateReadableStream } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\n\\nconst result = streamText({\\n  model: new MockLanguageModelV3({\\n    doStream: async () => ({\\n      stream: simulateReadableStream({\\n        chunks: [\\n          { type: \\'text-start\\', id: \\'text-1\\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'Hello\\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\', \\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'world!\\' },\\n          { type: \\'text-end\\', id: \\'text-1\\' },\\n          {\\n            type: \\'finish\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\\n          },\\n        ],\\n      }),\\n    }),\\n  }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### generateObject\\n\\n```ts\\nimport { generateObject } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = await generateObject({\\n  model: new MockLanguageModelV3({\\n    doGenerate: async () => ({\\n      finishReason: \\'stop\\',\\n      usage: { inputTokens: 10, outputTokens: 20, totalTokens: 30 },\\n      content: [{ type: \\'text\\', text: `{\"content\":\"Hello, world!\"}` }],\\n      warnings: [],\\n    }),\\n  }),\\n  schema: z.object({ content: z.string() }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### streamObject\\n\\n```ts\\nimport { streamObject, simulateReadableStream } from \\'ai\\';\\nimport { MockLanguageModelV3 } from \\'ai/test\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = streamObject({\\n  model: new MockLanguageModelV3({\\n    doStream: async () => ({\\n      stream: simulateReadableStream({\\n        chunks: [\\n          { type: \\'text-start\\', id: \\'text-1\\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'{ \\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\'\"content\": \\' },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: `\"Hello, ` },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: `world` },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: `!\"` },\\n          { type: \\'text-delta\\', id: \\'text-1\\', delta: \\' }\\' },\\n          { type: \\'text-end\\', id: \\'text-1\\' },\\n          {\\n            type: \\'finish\\',\\n            finishReason: \\'stop\\',\\n            logprobs: undefined,\\n            usage: { inputTokens: 3, outputTokens: 10, totalTokens: 13 },\\n          },\\n        ],\\n      }),\\n    }),\\n  }),\\n  schema: z.object({ content: z.string() }),\\n  prompt: \\'Hello, test!\\',\\n});\\n```\\n\\n### Simulate UI Message Stream Responses\\n\\nYou can also simulate [UI Message Stream](/docs/ai-sdk-ui/stream-protocol#ui-message-stream) responses for testing,\\ndebugging, or demonstration purposes.\\n\\nHere is a Next example:\\n\\n```ts filename=\"route.ts\"\\nimport { simulateReadableStream } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  return new Response(\\n    simulateReadableStream({\\n      initialDelayInMs: 1000, // Delay before the first chunk\\n      chunkDelayInMs: 300, // Delay between chunks\\n      chunks: [\\n        `data: {\"type\":\"start\",\"messageId\":\"msg-123\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-start\",\"id\":\"text-1\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\"This\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" is an\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-delta\",\"id\":\"text-1\",\"delta\":\" example.\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"text-end\",\"id\":\"text-1\"}\\\\n\\\\n`,\\n        `data: {\"type\":\"finish\"}\\\\n\\\\n`,\\n        `data: [DONE]\\\\n\\\\n`,\\n      ],\\n    }).pipeThrough(new TextEncoderStream()),\\n    {\\n      status: 200,\\n      headers: {\\n        \\'Content-Type\\': \\'text/event-stream\\',\\n        \\'Cache-Control\\': \\'no-cache\\',\\n        Connection: \\'keep-alive\\',\\n        \\'x-vercel-ai-ui-message-stream\\': \\'v1\\',\\n      },\\n    },\\n  );\\n}\\n```\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/60-telemetry.mdx'), name='60-telemetry.mdx', displayName='60-telemetry.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Telemetry\\ndescription: Using OpenTelemetry with AI SDK Core\\n---\\n\\n# Telemetry\\n\\n<Note type=\"warning\">\\n  AI SDK Telemetry is experimental and may change in the future.\\n</Note>\\n\\nThe AI SDK uses [OpenTelemetry](https://opentelemetry.io/) to collect telemetry data.\\nOpenTelemetry is an open-source observability framework designed to provide\\nstandardized instrumentation for collecting telemetry data.\\n\\nCheck out the [AI SDK Observability Integrations](/providers/observability)\\nto see providers that offer monitoring and tracing for AI SDK applications.\\n\\n## Enabling telemetry\\n\\nFor Next.js applications, please follow the [Next.js OpenTelemetry guide](https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry) to enable telemetry first.\\n\\nYou can then use the `experimental_telemetry` option to enable telemetry on specific function calls while the feature is experimental:\\n\\n```ts highlight=\"4\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a short story about a cat.\\',\\n  experimental_telemetry: { isEnabled: true },\\n});\\n```\\n\\nWhen telemetry is enabled, you can also control if you want to record the input values and the output values for the function.\\nBy default, both are enabled. You can disable them by setting the `recordInputs` and `recordOutputs` options to `false`.\\n\\nDisabling the recording of inputs and outputs can be useful for privacy, data transfer, and performance reasons.\\nYou might for example want to disable recording inputs if they contain sensitive information.\\n\\n## Telemetry Metadata\\n\\nYou can provide a `functionId` to identify the function that the telemetry data is for,\\nand `metadata` to include additional information in the telemetry data.\\n\\n```ts highlight=\"6-10\"\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a short story about a cat.\\',\\n  experimental_telemetry: {\\n    isEnabled: true,\\n    functionId: \\'my-awesome-function\\',\\n    metadata: {\\n      something: \\'custom\\',\\n      someOtherThing: \\'other-value\\',\\n    },\\n  },\\n});\\n```\\n\\n## Custom Tracer\\n\\nYou may provide a `tracer` which must return an OpenTelemetry `Tracer`. This is useful in situations where\\nyou want your traces to use a `TracerProvider` other than the one provided by the `@opentelemetry/api` singleton.\\n\\n```ts highlight=\"7\"\\nconst tracerProvider = new NodeTracerProvider();\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a short story about a cat.\\',\\n  experimental_telemetry: {\\n    isEnabled: true,\\n    tracer: tracerProvider.getTracer(\\'ai\\'),\\n  },\\n});\\n```\\n\\n## Collected Data\\n\\n### generateText function\\n\\n`generateText` records 3 types of spans:\\n\\n- `ai.generateText` (span): the full length of the generateText call. It contains 1 or more `ai.generateText.doGenerate` spans.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateText` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateText\"`\\n  - `ai.prompt`: the prompt that was used when calling `generateText`\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n  - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set\\n\\n- `ai.generateText.doGenerate` (span): a provider doGenerate call. It can contain `ai.toolCall` spans.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateText.doGenerate` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateText.doGenerate\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.\\n    Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).\\n    Provider-defined-client tools have a `name`, `id`, and `input` (Record).\\n  - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property\\n    (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.\\n\\n### streamText function\\n\\n`streamText` records 3 types of spans and 2 types of events:\\n\\n- `ai.streamText` (span): the full length of the streamText call. It contains a `ai.streamText.doStream` span.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamText` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamText\"`\\n  - `ai.prompt`: the prompt that was used when calling `streamText`\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n  - `ai.settings.maxOutputTokens`: the maximum number of output tokens that were set\\n\\n- `ai.streamText.doStream` (span): a provider doStream call.\\n  This span contains an `ai.stream.firstChunk` event and `ai.toolCall` spans.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamText.doStream` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamText.doStream\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.prompt.tools`: array of stringified tool definitions. The tools can be of type `function` or `provider-defined-client`.\\n    Function tools have a `name`, `description` (optional), and `inputSchema` (JSON schema).\\n    Provider-defined-client tools have a `name`, `id`, and `input` (Record).\\n  - `ai.prompt.toolChoice`: the stringified tool choice setting (JSON). It has a `type` property\\n    (`auto`, `none`, `required`, `tool`), and if the type is `tool`, a `toolName` property with the specific tool.\\n  - `ai.response.text`: the text that was generated\\n  - `ai.response.toolCalls`: the tool calls that were made as part of the generation (stringified JSON)\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk in milliseconds\\n  - `ai.response.msToFinish`: the time it took to receive the finish part of the LLM stream in milliseconds\\n  - `ai.response.avgCompletionTokensPerSecond`: the average number of completion tokens per second\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n- `ai.toolCall` (span): a tool call that is made as part of the generateText call. See [Tool call spans](#tool-call-spans) for more details.\\n\\n- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.\\n\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\\n\\n- `ai.stream.finish` (event): an event that is emitted when the finish part of the LLM stream is received.\\n\\nIt also records a `ai.stream.firstChunk` event when the first chunk of the stream is received.\\n\\n### generateObject function\\n\\n`generateObject` records 2 types of spans:\\n\\n- `ai.generateObject` (span): the full length of the generateObject call. It contains 1 or more `ai.generateObject.doGenerate` spans.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateObject` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateObject\"`\\n  - `ai.prompt`: the prompt that was used when calling `generateObject`\\n  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `generateObject` function\\n  - `ai.schema.name`: the name of the schema that was passed into the `generateObject` function\\n  - `ai.schema.description`: the description of the schema that was passed into the `generateObject` function\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`\\n\\n- `ai.generateObject.doGenerate` (span): a provider doGenerate call.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.generateObject.doGenerate` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.generateObject.doGenerate\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n### streamObject function\\n\\n`streamObject` records 2 types of spans and 1 type of event:\\n\\n- `ai.streamObject` (span): the full length of the streamObject call. It contains 1 or more `ai.streamObject.doStream` spans.\\n  It contains the [basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamObject` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamObject\"`\\n  - `ai.prompt`: the prompt that was used when calling `streamObject`\\n  - `ai.schema`: Stringified JSON schema version of the schema that was passed into the `streamObject` function\\n  - `ai.schema.name`: the name of the schema that was passed into the `streamObject` function\\n  - `ai.schema.description`: the description of the schema that was passed into the `streamObject` function\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.settings.output`: the output type that was used, e.g. `object` or `no-schema`\\n\\n- `ai.streamObject.doStream` (span): a provider doStream call.\\n  This span contains an `ai.stream.firstChunk` event.\\n  It contains the [call LLM span information](#call-llm-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.streamObject.doStream` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.streamObject.doStream\"`\\n  - `ai.prompt.messages`: the messages that were passed into the provider\\n  - `ai.response.object`: the object that was generated (stringified JSON)\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\\n  - `ai.response.finishReason`: the reason why the generation finished\\n\\n- `ai.stream.firstChunk` (event): an event that is emitted when the first chunk of the stream is received.\\n  - `ai.response.msToFirstChunk`: the time it took to receive the first chunk\\n\\n### embed function\\n\\n`embed` records 2 types of spans:\\n\\n- `ai.embed` (span): the full length of the embed call. It contains 1 `ai.embed.doEmbed` spans.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embed` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embed\"`\\n  - `ai.value`: the value that was passed into the `embed` function\\n  - `ai.embedding`: a JSON-stringified embedding\\n\\n- `ai.embed.doEmbed` (span): a provider doEmbed call.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embed.doEmbed` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embed.doEmbed\"`\\n  - `ai.values`: the values that were passed into the provider (array)\\n  - `ai.embeddings`: an array of JSON-stringified embeddings\\n\\n### embedMany function\\n\\n`embedMany` records 2 types of spans:\\n\\n- `ai.embedMany` (span): the full length of the embedMany call. It contains 1 or more `ai.embedMany.doEmbed` spans.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embedMany` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embedMany\"`\\n  - `ai.values`: the values that were passed into the `embedMany` function\\n  - `ai.embeddings`: an array of JSON-stringified embedding\\n\\n- `ai.embedMany.doEmbed` (span): a provider doEmbed call.\\n  It contains the [basic embedding span information](#basic-embedding-span-information) and the following attributes:\\n\\n  - `operation.name`: `ai.embedMany.doEmbed` and the functionId that was set through `telemetry.functionId`\\n  - `ai.operationId`: `\"ai.embedMany.doEmbed\"`\\n  - `ai.values`: the values that were sent to the provider\\n  - `ai.embeddings`: an array of JSON-stringified embeddings for each value\\n\\n## Span Details\\n\\n### Basic LLM span information\\n\\nMany spans that use LLMs (`ai.generateText`, `ai.generateText.doGenerate`, `ai.streamText`, `ai.streamText.doStream`,\\n`ai.generateObject`, `ai.generateObject.doGenerate`, `ai.streamObject`, `ai.streamObject.doStream`) contain the following attributes:\\n\\n- `resource.name`: the functionId that was set through `telemetry.functionId`\\n- `ai.model.id`: the id of the model\\n- `ai.model.provider`: the provider of the model\\n- `ai.request.headers.*`: the request headers that were passed in through `headers`\\n- `ai.response.providerMetadata`: provider specific metadata returned with the generation response\\n- `ai.settings.maxRetries`: the maximum number of retries that were set\\n- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`\\n- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`\\n- `ai.usage.completionTokens`: the number of completion tokens that were used\\n- `ai.usage.promptTokens`: the number of prompt tokens that were used\\n\\n### Call LLM span information\\n\\nSpans that correspond to individual LLM calls (`ai.generateText.doGenerate`, `ai.streamText.doStream`, `ai.generateObject.doGenerate`, `ai.streamObject.doStream`) contain\\n[basic LLM span information](#basic-llm-span-information) and the following attributes:\\n\\n- `ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\\n- `ai.response.id`: the id of the response. Uses the ID from the provider when available.\\n- `ai.response.timestamp`: the timestamp of the response. Uses the timestamp from the provider when available.\\n- [Semantic Conventions for GenAI operations](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/)\\n  - `gen_ai.system`: the provider that was used\\n  - `gen_ai.request.model`: the model that was requested\\n  - `gen_ai.request.temperature`: the temperature that was set\\n  - `gen_ai.request.max_tokens`: the maximum number of tokens that were set\\n  - `gen_ai.request.frequency_penalty`: the frequency penalty that was set\\n  - `gen_ai.request.presence_penalty`: the presence penalty that was set\\n  - `gen_ai.request.top_k`: the topK parameter value that was set\\n  - `gen_ai.request.top_p`: the topP parameter value that was set\\n  - `gen_ai.request.stop_sequences`: the stop sequences\\n  - `gen_ai.response.finish_reasons`: the finish reasons that were returned by the provider\\n  - `gen_ai.response.model`: the model that was used to generate the response. This can be different from the model that was requested if the provider supports aliases.\\n  - `gen_ai.response.id`: the id of the response. Uses the ID from the provider when available.\\n  - `gen_ai.usage.input_tokens`: the number of prompt tokens that were used\\n  - `gen_ai.usage.output_tokens`: the number of completion tokens that were used\\n\\n### Basic embedding span information\\n\\nMany spans that use embedding models (`ai.embed`, `ai.embed.doEmbed`, `ai.embedMany`, `ai.embedMany.doEmbed`) contain the following attributes:\\n\\n- `ai.model.id`: the id of the model\\n- `ai.model.provider`: the provider of the model\\n- `ai.request.headers.*`: the request headers that were passed in through `headers`\\n- `ai.settings.maxRetries`: the maximum number of retries that were set\\n- `ai.telemetry.functionId`: the functionId that was set through `telemetry.functionId`\\n- `ai.telemetry.metadata.*`: the metadata that was passed in through `telemetry.metadata`\\n- `ai.usage.tokens`: the number of tokens that were used\\n- `resource.name`: the functionId that was set through `telemetry.functionId`\\n\\n### Tool call spans\\n\\nTool call spans (`ai.toolCall`) contain the following attributes:\\n\\n- `operation.name`: `\"ai.toolCall\"`\\n- `ai.operationId`: `\"ai.toolCall\"`\\n- `ai.toolCall.name`: the name of the tool\\n- `ai.toolCall.id`: the id of the tool call\\n- `ai.toolCall.args`: the input parameters of the tool call\\n- `ai.toolCall.result`: the output result of the tool call. Only available if the tool call is successful and the result is serializable.\\n', children=[]), DocItem(origPath=Path('03-ai-sdk-core/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK Core\\ndescription: Learn about AI SDK Core.\\n---\\n\\n# AI SDK Core\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description:\\n        'Learn about AI SDK Core and how to work with Large Language Models (LLMs).',\\n      href: '/docs/ai-sdk-core/overview',\\n    },\\n    {\\n      title: 'Generating Text',\\n      description: 'Learn how to generate text.',\\n      href: '/docs/ai-sdk-core/generating-text',\\n    },\\n    {\\n      title: 'Generating Structured Data',\\n      description: 'Learn how to generate structured data.',\\n      href: '/docs/ai-sdk-core/generating-structured-data',\\n    },\\n    {\\n      title: 'Tool Calling',\\n      description: 'Learn how to do tool calling with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/tools-and-tool-calling',\\n    },\\n    {\\n      title: 'Prompt Engineering',\\n      description: 'Learn how to write prompts with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/prompt-engineering',\\n    },\\n    {\\n      title: 'Settings',\\n      description:\\n        'Learn how to set up settings for language models generations.',\\n      href: '/docs/ai-sdk-core/settings',\\n    },\\n    {\\n      title: 'Embeddings',\\n      description: 'Learn how to use embeddings with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/embeddings',\\n    },\\n    {\\n      title: 'Image Generation',\\n      description: 'Learn how to generate images with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/image-generation',\\n    },\\n    {\\n      title: 'Transcription',\\n      description: 'Learn how to transcribe audio with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/transcription',\\n    },\\n    {\\n      title: 'Speech',\\n      description: 'Learn how to generate speech with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/speech',\\n    },\\n    {\\n      title: 'Provider Management',\\n      description: 'Learn how to work with multiple providers.',\\n      href: '/docs/ai-sdk-core/provider-management',\\n    },\\n    {\\n      title: 'Middleware',\\n      description: 'Learn how to use middleware with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/middleware',\\n    },\\n    {\\n      title: 'Error Handling',\\n      description: 'Learn how to handle errors with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/error-handling',\\n    },\\n    {\\n      title: 'Testing',\\n      description: 'Learn how to test with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/testing',\\n    },\\n    {\\n      title: 'Telemetry',\\n      description: 'Learn how to use telemetry with AI SDK Core.',\\n      href: '/docs/ai-sdk-core/telemetry',\\n    },\\n  ]}\\n/>\\n\", children=[])]),\n",
       " DocItem(origPath=Path('04-ai-sdk-ui'), name='04-ai-sdk-ui', displayName='04-ai-sdk-ui', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('04-ai-sdk-ui/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of AI SDK UI.\\n---\\n\\n# AI SDK UI\\n\\nAI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a **framework-agnostic toolkit**, streamlining the integration of advanced AI functionalities into your applications.\\n\\nAI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With three main hooks — **`useChat`**, **`useCompletion`**, and **`useObject`** — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.\\n\\n- **[`useChat`](/docs/ai-sdk-ui/chatbot)** offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.\\n- **[`useCompletion`](/docs/ai-sdk-ui/completion)** enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.\\n- **[`useObject`](/docs/ai-sdk-ui/object-generation)** is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.\\n\\nThese hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.\\n\\n## UI Framework Support\\n\\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), [Vue.js](https://vuejs.org/), and [Angular](https://angular.dev/).\\nHere is a comparison of the supported functions across these frameworks:\\n\\n| Function                                                  | React               | Svelte                               | Vue.js              | Angular                              |\\n| --------------------------------------------------------- | ------------------- | ------------------------------------ | ------------------- | ------------------------------------ |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)             | <Check size={18} /> | <Check size={18} /> Chat             | <Check size={18} /> | <Check size={18} /> Chat             |\\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion) | <Check size={18} /> | <Check size={18} /> Completion       | <Check size={18} /> | <Check size={18} /> Completion       |\\n| [useObject](/docs/reference/ai-sdk-ui/use-object)         | <Check size={18} /> | <Check size={18} /> StructuredObject | <Cross size={18} /> | <Check size={18} /> StructuredObject |\\n\\n<Note>\\n  [Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\\n  welcome to implement missing features for non-React frameworks.\\n</Note>\\n\\n## Framework Examples\\n\\nExplore these example implementations for different frameworks:\\n\\n- [**Next.js**](https://github.com/vercel/ai/tree/main/examples/next-openai)\\n- [**Nuxt**](https://github.com/vercel/ai/tree/main/examples/nuxt-openai)\\n- [**SvelteKit**](https://github.com/vercel/ai/tree/main/examples/sveltekit-openai)\\n- [**Angular**](https://github.com/vercel/ai/tree/main/examples/angular)\\n\\n## API Reference\\n\\nPlease check out the [AI SDK UI API Reference](/docs/reference/ai-sdk-ui) for more details on each function.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/02-chatbot.mdx'), name='02-chatbot.mdx', displayName='02-chatbot.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot\\ndescription: Learn how to use the useChat hook.\\n---\\n\\n# Chatbot\\n\\nThe `useChat` hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.\\n\\nTo summarize, the `useChat` hook provides the following features:\\n\\n- **Message Streaming**: All the messages from the AI provider are streamed to the chat UI in real-time.\\n- **Managed States**: The hook manages the states for input, messages, status, error and more for you.\\n- **Seamless Integration**: Easily integrate your chat AI into any design or layout with minimal effort.\\n\\nIn this guide, you will learn how to use the `useChat` hook to create a chatbot application with real-time message streaming.\\nCheck out our [chatbot with tools guide](/docs/ai-sdk-ui/chatbot-with-tool-calling) to learn how to use tools in your chatbot.\\nLet\\'s start with the following example first.\\n\\n## Example\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n          placeholder=\"Say something...\"\\n        />\\n        <button type=\"submit\" disabled={status !== \\'ready\\'}>\\n          Submit\\n        </button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/chat/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'You are a helpful assistant.\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n<Note>\\n  The UI messages have a new `parts` property that contains the message parts.\\n  We recommend rendering the messages using the `parts` property instead of the\\n  `content` property. The parts property supports different message types,\\n  including text, tool invocation, and tool result, and allows for more flexible\\n  and complex chat UIs.\\n</Note>\\n\\nIn the `Page` component, the `useChat` hook will request to your AI provider endpoint whenever the user sends a message using `sendMessage`.\\nThe messages are then streamed back in real-time and displayed in the chat UI.\\n\\nThis enables a seamless chat experience where the user can see the AI response as soon as it is available,\\nwithout having to wait for the entire response to be received.\\n\\n## Customized UI\\n\\n`useChat` also provides ways to manage the chat message states via code, show status, and update messages without being triggered by user interactions.\\n\\n### Status\\n\\nThe `useChat` hook returns a `status`. It has the following possible values:\\n\\n- `submitted`: The message has been sent to the API and we\\'re awaiting the start of the response stream.\\n- `streaming`: The response is actively streaming in from the API, receiving chunks of data.\\n- `ready`: The full response has been received and processed; a new user message can be submitted.\\n- `error`: An error occurred during the API request, preventing successful completion.\\n\\nYou can use `status` for e.g. the following purposes:\\n\\n- To show a loading spinner while the chatbot is processing the user\\'s message.\\n- To show a \"Stop\" button to abort the current message.\\n- To disable the submit button.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"6,22-29,36\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status, stop } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      {(status === \\'submitted\\' || status === \\'streaming\\') && (\\n        <div>\\n          {status === \\'submitted\\' && <Spinner />}\\n          <button type=\"button\" onClick={() => stop()}>\\n            Stop\\n          </button>\\n        </div>\\n      )}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n          placeholder=\"Say something...\"\\n        />\\n        <button type=\"submit\" disabled={status !== \\'ready\\'}>\\n          Submit\\n        </button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n### Error State\\n\\nSimilarly, the `error` state reflects the error object thrown during the fetch request.\\nIt can be used to display an error message, disable the submit button, or show a retry button:\\n\\n<Note>\\n  We recommend showing a generic error message to the user, such as \"Something\\n  went wrong.\" This is a good practice to avoid leaking information from the\\n  server.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"6,20-27,33\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, error, reload } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      {error && (\\n        <>\\n          <div>An error occurred.</div>\\n          <button type=\"button\" onClick={() => reload()}>\\n            Retry\\n          </button>\\n        </>\\n      )}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={error != null}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nPlease also see the [error handling](/docs/ai-sdk-ui/error-handling) guide for more information.\\n\\n### Modify messages\\n\\nSometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.\\n\\nThe `setMessages` function can help you achieve these tasks:\\n\\n```tsx\\nconst { messages, setMessages } = useChat()\\n\\nconst handleDelete = (id) => {\\n  setMessages(messages.filter(message => message.id !== id))\\n}\\n\\nreturn <>\\n  {messages.map(message => (\\n    <div key={message.id}>\\n      {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n      {message.parts.map((part, index) => (\\n        part.type === \\'text\\' ? (\\n          <span key={index}>{part.text}</span>\\n        ) : null\\n      ))}\\n      <button onClick={() => handleDelete(message.id)}>Delete</button>\\n    </div>\\n  ))}\\n  ...\\n```\\n\\nYou can think of `messages` and `setMessages` as a pair of `state` and `setState` in React.\\n\\n### Cancellation and regeneration\\n\\nIt\\'s also a common use case to abort the response message while it\\'s still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useChat` hook.\\n\\n```tsx\\nconst { stop, status } = useChat()\\n\\nreturn <>\\n  <button onClick={stop} disabled={!(status === \\'streaming\\' || status === \\'submitted\\')}>Stop</button>\\n  ...\\n```\\n\\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.\\n\\nSimilarly, you can also request the AI provider to reprocess the last message by calling the `regenerate` function returned by the `useChat` hook:\\n\\n```tsx\\nconst { regenerate, status } = useChat();\\n\\nreturn (\\n  <>\\n    <button\\n      onClick={regenerate}\\n      disabled={!(status === \\'ready\\' || status === \\'error\\')}\\n    >\\n      Regenerate\\n    </button>\\n    ...\\n  </>\\n);\\n```\\n\\nWhen the user clicks the \"Regenerate\" button, the AI provider will regenerate the last message and replace the current one correspondingly.\\n\\n### Throttling UI Updates\\n\\n<Note>This feature is currently only available for React.</Note>\\n\\nBy default, the `useChat` hook will trigger a render every time a new chunk is received.\\nYou can throttle the UI updates with the `experimental_throttle` option.\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { messages, ... } = useChat({\\n  // Throttle the messages and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n\\n## Event Callbacks\\n\\n`useChat` provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:\\n\\n- `onFinish`: Called when the assistant response is completed. The event includes the response message, all messages, and flags for abort, disconnect, and errors.\\n- `onError`: Called when an error occurs during the fetch request.\\n- `onData`: Called whenever a data part is received.\\n\\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\\n\\n```tsx\\nimport { UIMessage } from \\'ai\\';\\n\\nconst {\\n  /* ... */\\n} = useChat({\\n  onFinish: ({ message, messages, isAbort, isDisconnect, isError }) => {\\n    // use information to e.g. update other UI states\\n  },\\n  onError: error => {\\n    console.error(\\'An error occurred:\\', error);\\n  },\\n  onData: data => {\\n    console.log(\\'Received data part from server:\\', data);\\n  },\\n});\\n```\\n\\nIt\\'s worth noting that you can abort the processing by throwing an error in the `onData` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\\n\\n## Request Configuration\\n\\n### Custom headers, body, and credentials\\n\\nBy default, the `useChat` hook sends a HTTP POST request to the `/api/chat` endpoint with the message list as the request body. You can customize the request in two ways:\\n\\n#### Hook-Level Configuration (Applied to all requests)\\n\\nYou can configure transport-level options that will be applied to all requests made by the hook:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: \\'/api/custom-chat\\',\\n    headers: {\\n      Authorization: \\'your_token\\',\\n    },\\n    body: {\\n      user_id: \\'123\\',\\n    },\\n    credentials: \\'same-origin\\',\\n  }),\\n});\\n```\\n\\n#### Dynamic Hook-Level Configuration\\n\\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: \\'/api/custom-chat\\',\\n    headers: () => ({\\n      Authorization: `Bearer ${getAuthToken()}`,\\n      \\'X-User-ID\\': getCurrentUserId(),\\n    }),\\n    body: () => ({\\n      sessionId: getCurrentSessionId(),\\n      preferences: getUserPreferences(),\\n    }),\\n    credentials: () => \\'include\\',\\n  }),\\n});\\n```\\n\\n<Note>\\n  For component state that changes over time, use `useRef` to store the current\\n  value and reference `ref.current` in your configuration function, or prefer\\n  request-level options (see next section) for better reliability.\\n</Note>\\n\\n#### Request-Level Configuration (Recommended)\\n\\n<Note>\\n  **Recommended**: Use request-level options for better flexibility and control.\\n  Request-level options take precedence over hook-level options and allow you to\\n  customize each request individually.\\n</Note>\\n\\n```tsx\\n// Pass options as the second parameter to sendMessage\\nsendMessage(\\n  { text: input },\\n  {\\n    headers: {\\n      Authorization: \\'Bearer token123\\',\\n      \\'X-Custom-Header\\': \\'custom-value\\',\\n    },\\n    body: {\\n      temperature: 0.7,\\n      max_tokens: 100,\\n      user_id: \\'123\\',\\n    },\\n    metadata: {\\n      userId: \\'user123\\',\\n      sessionId: \\'session456\\',\\n    },\\n  },\\n);\\n```\\n\\nThe request-level options are merged with hook-level options, with request-level options taking precedence. On your server side, you can handle the request with this additional information.\\n\\n### Setting custom body fields per request\\n\\nYou can configure custom `body` fields on a per-request basis using the second parameter of the `sendMessage` function.\\nThis is useful if you want to pass in additional information to your backend that is not part of the message list.\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"20-25\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage } = useChat();\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            sendMessage(\\n              { text: input },\\n              {\\n                body: {\\n                  customKey: \\'customValue\\',\\n                },\\n              },\\n            );\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nYou can retrieve these custom fields on your server side by destructuring the request body:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"3,4\"\\nexport async function POST(req: Request) {\\n  // Extract additional information (\"customKey\") from the body of the request:\\n  const { messages, customKey }: { messages: UIMessage[]; customKey: string } =\\n    await req.json();\\n  //...\\n}\\n```\\n\\n## Message Metadata\\n\\nYou can attach custom metadata to messages for tracking information like timestamps, model details, and token usage.\\n\\n```ts\\n// Server: Send metadata about the message\\nreturn result.toUIMessageStreamResponse({\\n  messageMetadata: ({ part }) => {\\n    if (part.type === \\'start\\') {\\n      return {\\n        createdAt: Date.now(),\\n        model: \\'gpt-5.1\\',\\n      };\\n    }\\n\\n    if (part.type === \\'finish\\') {\\n      return {\\n        totalTokens: part.totalUsage.totalTokens,\\n      };\\n    }\\n  },\\n});\\n```\\n\\n```tsx\\n// Client: Access metadata via message.metadata\\n{\\n  messages.map(message => (\\n    <div key={message.id}>\\n      {message.role}:{\\' \\'}\\n      {message.metadata?.createdAt &&\\n        new Date(message.metadata.createdAt).toLocaleTimeString()}\\n      {/* Render message content */}\\n      {message.parts.map((part, index) =>\\n        part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n      )}\\n      {/* Show token count if available */}\\n      {message.metadata?.totalTokens && (\\n        <span>{message.metadata.totalTokens} tokens</span>\\n      )}\\n    </div>\\n  ));\\n}\\n```\\n\\nFor complete examples with type safety and advanced use cases, see the [Message Metadata documentation](/docs/ai-sdk-ui/message-metadata).\\n\\n## Transport Configuration\\n\\nYou can configure custom transport behavior using the `transport` option to customize how messages are sent to your API:\\n\\n```tsx filename=\"app/page.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage } = useChat({\\n    id: \\'my-chat\\',\\n    transport: new DefaultChatTransport({\\n      prepareSendMessagesRequest: ({ id, messages }) => {\\n        return {\\n          body: {\\n            id,\\n            message: messages[messages.length - 1],\\n          },\\n        };\\n      },\\n    }),\\n  });\\n\\n  // ... rest of your component\\n}\\n```\\n\\nThe corresponding API route receives the custom request format:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nexport async function POST(req: Request) {\\n  const { id, message } = await req.json();\\n\\n  // Load existing messages and add the new one\\n  const messages = await loadMessages(id);\\n  messages.push(message);\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Advanced: Trigger-based routing\\n\\nFor more complex scenarios like message regeneration, you can use trigger-based routing:\\n\\n```tsx filename=\"app/page.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, regenerate } = useChat({\\n    id: \\'my-chat\\',\\n    transport: new DefaultChatTransport({\\n      prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\\n        if (trigger === \\'submit-user-message\\') {\\n          return {\\n            body: {\\n              trigger: \\'submit-user-message\\',\\n              id,\\n              message: messages[messages.length - 1],\\n              messageId,\\n            },\\n          };\\n        } else if (trigger === \\'regenerate-assistant-message\\') {\\n          return {\\n            body: {\\n              trigger: \\'regenerate-assistant-message\\',\\n              id,\\n              messageId,\\n            },\\n          };\\n        }\\n        throw new Error(`Unsupported trigger: ${trigger}`);\\n      },\\n    }),\\n  });\\n\\n  // ... rest of your component\\n}\\n```\\n\\nThe corresponding API route would handle different triggers:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nexport async function POST(req: Request) {\\n  const { trigger, id, message, messageId } = await req.json();\\n\\n  const chat = await readChat(id);\\n  let messages = chat.messages;\\n\\n  if (trigger === \\'submit-user-message\\') {\\n    // Handle new user message\\n    messages = [...messages, message];\\n  } else if (trigger === \\'regenerate-assistant-message\\') {\\n    // Handle message regeneration - remove messages after messageId\\n    const messageIndex = messages.findIndex(m => m.id === messageId);\\n    if (messageIndex !== -1) {\\n      messages = messages.slice(0, messageIndex);\\n    }\\n  }\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nTo learn more about building custom transports, refer to the [Transport API documentation](/docs/ai-sdk-ui/transport).\\n\\n## Controlling the response stream\\n\\nWith `streamText`, you can control how error messages and usage information are sent back to the client.\\n\\n### Error Messages\\n\\nBy default, the error message is masked for security reasons.\\nThe default error message is \"An error occurred.\"\\nYou can forward error messages or send your own error message by providing a `getErrorMessage` function:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"13-27\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onError: error => {\\n      if (error == null) {\\n        return \\'unknown error\\';\\n      }\\n\\n      if (typeof error === \\'string\\') {\\n        return error;\\n      }\\n\\n      if (error instanceof Error) {\\n        return error.message;\\n      }\\n\\n      return JSON.stringify(error);\\n    },\\n  });\\n}\\n```\\n\\n### Usage Information\\n\\nTrack token consumption and resource usage with [message metadata](/docs/ai-sdk-ui/message-metadata):\\n\\n1. Define a custom metadata type with usage fields (optional, for type safety)\\n2. Attach usage data using `messageMetadata` in your response\\n3. Display usage metrics in your UI components\\n\\nUsage data is attached as metadata to messages and becomes available once the model completes its response generation.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport {\\n  convertToModelMessages,\\n  streamText,\\n  UIMessage,\\n  type LanguageModelUsage,\\n} from \\'ai\\';\\n\\n// Create a new metadata type (optional for type-safety)\\ntype MyMetadata = {\\n  totalUsage: LanguageModelUsage;\\n};\\n\\n// Create a new custom message type with your own metadata\\nexport type MyUIMessage = UIMessage<MyMetadata>;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    messageMetadata: ({ part }) => {\\n      // Send total usage when generation is finished\\n      if (part.type === \\'finish\\') {\\n        return { totalUsage: part.totalUsage };\\n      }\\n    },\\n  });\\n}\\n```\\n\\nThen, on the client, you can access the message-level metadata.\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyUIMessage } from \\'./api/chat/route\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  // Use custom message type defined on the server (optional for type-safety)\\n  const { messages } = useChat<MyUIMessage>({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(m => (\\n        <div key={m.id} className=\"whitespace-pre-wrap\">\\n          {m.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {m.parts.map(part => {\\n            if (part.type === \\'text\\') {\\n              return part.text;\\n            }\\n          })}\\n          {/* Render usage via metadata */}\\n          {m.metadata?.totalUsage && (\\n            <div>Total usage: {m.metadata?.totalUsage.totalTokens} tokens</div>\\n          )}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\nYou can also access your metadata from the `onFinish` callback of `useChat`:\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyUIMessage } from \\'./api/chat/route\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  // Use custom message type defined on the server (optional for type-safety)\\n  const { messages } = useChat<MyUIMessage>({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n    onFinish: ({ message }) => {\\n      // Access message metadata via onFinish callback\\n      console.log(message.metadata?.totalUsage);\\n    },\\n  });\\n}\\n```\\n\\n### Text Streams\\n\\n`useChat` can handle plain text streams by setting the `streamProtocol` option to `text`:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"7\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { TextStreamChatTransport } from \\'ai\\';\\n\\nexport default function Chat() {\\n  const { messages } = useChat({\\n    transport: new TextStreamChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return <>...</>;\\n}\\n```\\n\\nThis configuration also works with other backend servers that stream plain text.\\nCheck out the [stream protocol guide](/docs/ai-sdk-ui/stream-protocol) for more information.\\n\\n<Note>\\n  When using `TextStreamChatTransport`, tool calls, usage information and finish\\n  reasons are not available.\\n</Note>\\n\\n## Reasoning\\n\\nSome models such as as DeepSeek `deepseek-r1`\\nand Anthropic `claude-3-7-sonnet-20250219` support reasoning tokens.\\nThese tokens are typically sent before the message content.\\nYou can forward them to the client with the `sendReasoning` option:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"13\"\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'deepseek/deepseek-r1\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    sendReasoning: true,\\n  });\\n}\\n```\\n\\nOn the client side, you can access the reasoning parts of the message object.\\n\\nReasoning parts have a `text` property that contains the reasoning content.\\n\\n```tsx filename=\"app/page.tsx\"\\nmessages.map(message => (\\n  <div key={message.id}>\\n    {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n    {message.parts.map((part, index) => {\\n      // text parts:\\n      if (part.type === \\'text\\') {\\n        return <div key={index}>{part.text}</div>;\\n      }\\n\\n      // reasoning parts:\\n      if (part.type === \\'reasoning\\') {\\n        return <pre key={index}>{part.text}</pre>;\\n      }\\n    })}\\n  </div>\\n));\\n```\\n\\n## Sources\\n\\nSome providers such as [Perplexity](/providers/ai-sdk-providers/perplexity#sources) and\\n[Google Generative AI](/providers/ai-sdk-providers/google-generative-ai#sources) include sources in the response.\\n\\nCurrently sources are limited to web pages that ground the response.\\nYou can forward them to the client with the `sendSources` option:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"13\"\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'perplexity/sonar-pro\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    sendSources: true,\\n  });\\n}\\n```\\n\\nOn the client side, you can access source parts of the message object.\\nThere are two types of sources: `source-url` for web pages and `source-document` for documents.\\nHere is an example that renders both types of sources:\\n\\n```tsx filename=\"app/page.tsx\"\\nmessages.map(message => (\\n  <div key={message.id}>\\n    {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n\\n    {/* Render URL sources */}\\n    {message.parts\\n      .filter(part => part.type === \\'source-url\\')\\n      .map(part => (\\n        <span key={`source-${part.id}`}>\\n          [\\n          <a href={part.url} target=\"_blank\">\\n            {part.title ?? new URL(part.url).hostname}\\n          </a>\\n          ]\\n        </span>\\n      ))}\\n\\n    {/* Render document sources */}\\n    {message.parts\\n      .filter(part => part.type === \\'source-document\\')\\n      .map(part => (\\n        <span key={`source-${part.id}`}>\\n          [<span>{part.title ?? `Document ${part.id}`}</span>]\\n        </span>\\n      ))}\\n  </div>\\n));\\n```\\n\\n## Image Generation\\n\\nSome models such as Google `gemini-2.5-flash-image-preview` support image generation.\\nWhen images are generated, they are exposed as files to the client.\\nOn the client side, you can access file parts of the message object\\nand render them as images.\\n\\n```tsx filename=\"app/page.tsx\"\\nmessages.map(message => (\\n  <div key={message.id}>\\n    {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n    {message.parts.map((part, index) => {\\n      if (part.type === \\'text\\') {\\n        return <div key={index}>{part.text}</div>;\\n      } else if (part.type === \\'file\\' && part.mediaType.startsWith(\\'image/\\')) {\\n        return <img key={index} src={part.url} alt=\"Generated image\" />;\\n      }\\n    })}\\n  </div>\\n));\\n```\\n\\n## Attachments\\n\\nThe `useChat` hook supports sending file attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.\\n\\nThere are two ways to send files with a message: using a `FileList` object from file inputs or using an array of file objects.\\n\\n### FileList\\n\\nBy using `FileList`, you can send multiple files as attachments along with a message using the file input element. The `useChat` hook will automatically convert them into data URLs and send them to the AI provider.\\n\\n<Note>\\n  Currently, only `image/*` and `text/*` content types get automatically\\n  converted into [multi-modal content\\n  parts](/docs/foundations/prompts#multi-modal-messages). You will need to\\n  handle other content types manually.\\n</Note>\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useRef, useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status } = useChat();\\n\\n  const [input, setInput] = useState(\\'\\');\\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\\n  const fileInputRef = useRef<HTMLInputElement>(null);\\n\\n  return (\\n    <div>\\n      <div>\\n        {messages.map(message => (\\n          <div key={message.id}>\\n            <div>{`${message.role}: `}</div>\\n\\n            <div>\\n              {message.parts.map((part, index) => {\\n                if (part.type === \\'text\\') {\\n                  return <span key={index}>{part.text}</span>;\\n                }\\n\\n                if (\\n                  part.type === \\'file\\' &&\\n                  part.mediaType?.startsWith(\\'image/\\')\\n                ) {\\n                  return <img key={index} src={part.url} alt={part.filename} />;\\n                }\\n\\n                return null;\\n              })}\\n            </div>\\n          </div>\\n        ))}\\n      </div>\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({\\n              text: input,\\n              files,\\n            });\\n            setInput(\\'\\');\\n            setFiles(undefined);\\n\\n            if (fileInputRef.current) {\\n              fileInputRef.current.value = \\'\\';\\n            }\\n          }\\n        }}\\n      >\\n        <input\\n          type=\"file\"\\n          onChange={event => {\\n            if (event.target.files) {\\n              setFiles(event.target.files);\\n            }\\n          }}\\n          multiple\\n          ref={fileInputRef}\\n        />\\n        <input\\n          value={input}\\n          placeholder=\"Send message...\"\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### File Objects\\n\\nYou can also send files as objects along with a message. This can be useful for sending pre-uploaded files or data URLs.\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport { FileUIPart } from \\'ai\\';\\n\\nexport default function Page() {\\n  const { messages, sendMessage, status } = useChat();\\n\\n  const [input, setInput] = useState(\\'\\');\\n  const [files] = useState<FileUIPart[]>([\\n    {\\n      type: \\'file\\',\\n      filename: \\'earth.png\\',\\n      mediaType: \\'image/png\\',\\n      url: \\'https://example.com/earth.png\\',\\n    },\\n    {\\n      type: \\'file\\',\\n      filename: \\'moon.png\\',\\n      mediaType: \\'image/png\\',\\n      url: \\'data:image/png;base64,iVBORw0KGgo...\\',\\n    },\\n  ]);\\n\\n  return (\\n    <div>\\n      <div>\\n        {messages.map(message => (\\n          <div key={message.id}>\\n            <div>{`${message.role}: `}</div>\\n\\n            <div>\\n              {message.parts.map((part, index) => {\\n                if (part.type === \\'text\\') {\\n                  return <span key={index}>{part.text}</span>;\\n                }\\n\\n                if (\\n                  part.type === \\'file\\' &&\\n                  part.mediaType?.startsWith(\\'image/\\')\\n                ) {\\n                  return <img key={index} src={part.url} alt={part.filename} />;\\n                }\\n\\n                return null;\\n              })}\\n            </div>\\n          </div>\\n        ))}\\n      </div>\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({\\n              text: input,\\n              files,\\n            });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input\\n          value={input}\\n          placeholder=\"Send message...\"\\n          onChange={e => setInput(e.target.value)}\\n          disabled={status !== \\'ready\\'}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n## Type Inference for Tools\\n\\nWhen working with tools in TypeScript, AI SDK UI provides type inference helpers to ensure type safety for your tool inputs and outputs.\\n\\n### InferUITool\\n\\nThe `InferUITool` type helper infers the input and output types of a single tool for use in UI messages:\\n\\n```tsx\\nimport { InferUITool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst weatherTool = {\\n  description: \\'Get the current weather\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The city and state\\'),\\n  }),\\n  execute: async ({ location }) => {\\n    return `The weather in ${location} is sunny.`;\\n  },\\n};\\n\\n// Infer the types from the tool\\ntype WeatherUITool = InferUITool<typeof weatherTool>;\\n// This creates a type with:\\n// {\\n//   input: { location: string };\\n//   output: string;\\n// }\\n```\\n\\n### InferUITools\\n\\nThe `InferUITools` type helper infers the input and output types of a `ToolSet`:\\n\\n```tsx\\nimport { InferUITools, ToolSet } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst tools = {\\n  weather: {\\n    description: \\'Get the current weather\\',\\n    inputSchema: z.object({\\n      location: z.string().describe(\\'The city and state\\'),\\n    }),\\n    execute: async ({ location }) => {\\n      return `The weather in ${location} is sunny.`;\\n    },\\n  },\\n  calculator: {\\n    description: \\'Perform basic arithmetic\\',\\n    inputSchema: z.object({\\n      operation: z.enum([\\'add\\', \\'subtract\\', \\'multiply\\', \\'divide\\']),\\n      a: z.number(),\\n      b: z.number(),\\n    }),\\n    execute: async ({ operation, a, b }) => {\\n      switch (operation) {\\n        case \\'add\\':\\n          return a + b;\\n        case \\'subtract\\':\\n          return a - b;\\n        case \\'multiply\\':\\n          return a * b;\\n        case \\'divide\\':\\n          return a / b;\\n      }\\n    },\\n  },\\n} satisfies ToolSet;\\n\\n// Infer the types from the tool set\\ntype MyUITools = InferUITools<typeof tools>;\\n// This creates a type with:\\n// {\\n//   weather: { input: { location: string }; output: string };\\n//   calculator: { input: { operation: \\'add\\' | \\'subtract\\' | \\'multiply\\' | \\'divide\\'; a: number; b: number }; output: number };\\n// }\\n```\\n\\n### Using Inferred Types\\n\\nYou can use these inferred types to create a custom UIMessage type and pass it to various AI SDK UI functions:\\n\\n```tsx\\nimport { InferUITools, UIMessage, UIDataTypes } from \\'ai\\';\\n\\ntype MyUITools = InferUITools<typeof tools>;\\ntype MyUIMessage = UIMessage<never, UIDataTypes, MyUITools>;\\n```\\n\\nPass the custom type to `useChat` or `createUIMessageStream`:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { createUIMessageStream } from \\'ai\\';\\nimport type { MyUIMessage } from \\'./types\\';\\n\\n// With useChat\\nconst { messages } = useChat<MyUIMessage>();\\n\\n// With createUIMessageStream\\nconst stream = createUIMessageStream<MyUIMessage>(/* ... */);\\n```\\n\\nThis provides full type safety for tool inputs and outputs on the client and server.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/03-chatbot-message-persistence.mdx'), name='03-chatbot-message-persistence.mdx', displayName='03-chatbot-message-persistence.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot Message Persistence\\ndescription: Learn how to store and load chat messages in a chatbot.\\n---\\n\\n# Chatbot Message Persistence\\n\\nBeing able to store and load chat messages is crucial for most AI chatbots.\\nIn this guide, we\\'ll show how to implement message persistence with `useChat` and `streamText`.\\n\\n<Note>\\n  This guide does not cover authorization, error handling, or other real-world\\n  considerations. It is intended to be a simple example of how to implement\\n  message persistence.\\n</Note>\\n\\n## Starting a new chat\\n\\nWhen the user navigates to the chat page without providing a chat ID,\\nwe need to create a new chat and redirect to the chat page with the new chat ID.\\n\\n```tsx filename=\"app/chat/page.tsx\"\\nimport { redirect } from \\'next/navigation\\';\\nimport { createChat } from \\'@util/chat-store\\';\\n\\nexport default async function Page() {\\n  const id = await createChat(); // create a new chat\\n  redirect(`/chat/${id}`); // redirect to chat page, see below\\n}\\n```\\n\\nOur example chat store implementation uses files to store the chat messages.\\nIn a real-world application, you would use a database or a cloud storage service,\\nand get the chat ID from the database.\\nThat being said, the function interfaces are designed to be easily replaced with other implementations.\\n\\n```tsx filename=\"util/chat-store.ts\"\\nimport { generateId } from \\'ai\\';\\nimport { existsSync, mkdirSync } from \\'fs\\';\\nimport { writeFile } from \\'fs/promises\\';\\nimport path from \\'path\\';\\n\\nexport async function createChat(): Promise<string> {\\n  const id = generateId(); // generate a unique chat ID\\n  await writeFile(getChatFile(id), \\'[]\\'); // create an empty chat file\\n  return id;\\n}\\n\\nfunction getChatFile(id: string): string {\\n  const chatDir = path.join(process.cwd(), \\'.chats\\');\\n  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });\\n  return path.join(chatDir, `${id}.json`);\\n}\\n```\\n\\n## Loading an existing chat\\n\\nWhen the user navigates to the chat page with a chat ID, we need to load the chat messages from storage.\\n\\nThe `loadChat` function in our file-based chat store is implemented as follows:\\n\\n```tsx filename=\"util/chat-store.ts\"\\nimport { UIMessage } from \\'ai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nexport async function loadChat(id: string): Promise<UIMessage[]> {\\n  return JSON.parse(await readFile(getChatFile(id), \\'utf8\\'));\\n}\\n\\n// ... rest of the file\\n```\\n\\n## Validating messages on the server\\n\\nWhen processing messages on the server that contain tool calls, custom metadata, or data parts, you should validate them using `validateUIMessages` before sending them to the model.\\n\\n### Validation with tools\\n\\nWhen your messages include tool calls, validate them against your tool definitions:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"7-25,32-37\"\\nimport {\\n  convertToModelMessages,\\n  streamText,\\n  UIMessage,\\n  validateUIMessages,\\n  tool,\\n} from \\'ai\\';\\nimport { z } from \\'zod\\';\\nimport { loadChat, saveChat } from \\'@util/chat-store\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { dataPartsSchema, metadataSchema } from \\'@util/schemas\\';\\n\\n// Define your tools\\nconst tools = {\\n  weather: tool({\\n    description: \\'Get weather information\\',\\n    parameters: z.object({\\n      location: z.string(),\\n      units: z.enum([\\'celsius\\', \\'fahrenheit\\']),\\n    }),\\n    execute: async ({ location, units }) => {\\n      /* tool implementation */\\n    },\\n  }),\\n  // other tools\\n};\\n\\nexport async function POST(req: Request) {\\n  const { message, id } = await req.json();\\n\\n  // Load previous messages from database\\n  const previousMessages = await loadChat(id);\\n\\n  // Append new message to previousMessages messages\\n  const messages = [...previousMessages, message];\\n\\n  // Validate loaded messages against\\n  // tools, data parts schema, and metadata schema\\n  const validatedMessages = await validateUIMessages({\\n    messages,\\n    tools, // Ensures tool calls in messages match current schemas\\n    dataPartsSchema,\\n    metadataSchema,\\n  });\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(validatedMessages),\\n    tools,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId: id, messages });\\n    },\\n  });\\n}\\n```\\n\\n### Handling validation errors\\n\\nHandle validation errors gracefully when messages from the database don\\'t match current schemas:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"3,10-24\"\\nimport {\\n  convertToModelMessages,\\n  streamText,\\n  validateUIMessages,\\n  TypeValidationError,\\n} from \\'ai\\';\\nimport { type MyUIMessage } from \\'@/types\\';\\n\\nexport async function POST(req: Request) {\\n  const { message, id } = await req.json();\\n\\n  // Load and validate messages from database\\n  let validatedMessages: MyUIMessage[];\\n\\n  try {\\n    const previousMessages = await loadMessagesFromDB(id);\\n    validatedMessages = await validateUIMessages({\\n      // append the new message to the previous messages:\\n      messages: [...previousMessages, message],\\n      tools,\\n      metadataSchema,\\n    });\\n  } catch (error) {\\n    if (error instanceof TypeValidationError) {\\n      // Log validation error for monitoring\\n      console.error(\\'Database messages validation failed:\\', error);\\n      // Could implement message migration or filtering here\\n      // For now, start with empty history\\n      validatedMessages = [];\\n    } else {\\n      throw error;\\n    }\\n  }\\n\\n  // Continue with validated messages...\\n}\\n```\\n\\n## Displaying the chat\\n\\nOnce messages are loaded from storage, you can display them in your chat UI. Here\\'s how to set up the page component and the chat display:\\n\\n```tsx filename=\"app/chat/[id]/page.tsx\"\\nimport { loadChat } from \\'@util/chat-store\\';\\nimport Chat from \\'@ui/chat\\';\\n\\nexport default async function Page(props: { params: Promise<{ id: string }> }) {\\n  const { id } = await props.params;\\n  const messages = await loadChat(id);\\n  return <Chat id={id} initialMessages={messages} />;\\n}\\n```\\n\\nThe chat component uses the `useChat` hook to manage the conversation:\\n\\n```tsx filename=\"ui/chat.tsx\" highlight=\"10-16\"\\n\\'use client\\';\\n\\nimport { UIMessage, useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat({\\n  id,\\n  initialMessages,\\n}: { id?: string | undefined; initialMessages?: UIMessage[] } = {}) {\\n  const [input, setInput] = useState(\\'\\');\\n  const { sendMessage, messages } = useChat({\\n    id, // use the provided chat ID\\n    messages: initialMessages, // load initial messages\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    if (input.trim()) {\\n      sendMessage({ text: input });\\n      setInput(\\'\\');\\n    }\\n  };\\n\\n  // simplified rendering code, extend as needed:\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {m.parts\\n            .map(part => (part.type === \\'text\\' ? part.text : \\'\\'))\\n            .join(\\'\\')}\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Type a message...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n## Storing messages\\n\\n`useChat` sends the chat id and the messages to the backend.\\n\\n<Note>\\n  The `useChat` message format is different from the `ModelMessage` format. The\\n  `useChat` message format is designed for frontend display, and contains\\n  additional fields such as `id` and `createdAt`. We recommend storing the\\n  messages in the `useChat` message format.\\n\\nWhen loading messages from storage that contain tools, metadata, or custom data\\nparts, validate them using `validateUIMessages` before processing (see the\\n[validation section](#validating-messages-from-database) above).\\n\\n</Note>\\n\\nStoring messages is done in the `onFinish` callback of the `toUIMessageStreamResponse` function.\\n`onFinish` receives the complete messages including the new AI response as `UIMessage[]`.\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"6,11-17\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { saveChat } from \\'@util/chat-store\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\\n    await req.json();\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId, messages });\\n    },\\n  });\\n}\\n```\\n\\nThe actual storage of the messages is done in the `saveChat` function, which in\\nour file-based chat store is implemented as follows:\\n\\n```tsx filename=\"util/chat-store.ts\"\\nimport { UIMessage } from \\'ai\\';\\nimport { writeFile } from \\'fs/promises\\';\\n\\nexport async function saveChat({\\n  chatId,\\n  messages,\\n}: {\\n  chatId: string;\\n  messages: UIMessage[];\\n}): Promise<void> {\\n  const content = JSON.stringify(messages, null, 2);\\n  await writeFile(getChatFile(chatId), content);\\n}\\n\\n// ... rest of the file\\n```\\n\\n## Message IDs\\n\\nIn addition to a chat ID, each message has an ID.\\nYou can use this message ID to e.g. manipulate individual messages.\\n\\n### Client-side vs Server-side ID Generation\\n\\nBy default, message IDs are generated client-side:\\n\\n- User message IDs are generated by the `useChat` hook on the client\\n- AI response message IDs are generated by `streamText` on the server\\n\\nFor applications without persistence, client-side ID generation works perfectly.\\nHowever, **for persistence, you need server-side generated IDs** to ensure consistency across sessions and prevent ID conflicts when messages are stored and retrieved.\\n\\n### Setting Up Server-side ID Generation\\n\\nWhen implementing persistence, you have two options for generating server-side IDs:\\n\\n1. **Using `generateMessageId` in `toUIMessageStreamResponse`**\\n2. **Setting IDs in your start message part with `createUIMessageStream`**\\n\\n#### Option 1: Using `generateMessageId` in `toUIMessageStreamResponse`\\n\\nYou can control the ID format by providing ID generators using [`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator):\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"7-11\"\\nimport { createIdGenerator, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  // ...\\n  const result = streamText({\\n    // ...\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    // Generate consistent server-side IDs for persistence:\\n    generateMessageId: createIdGenerator({\\n      prefix: \\'msg\\',\\n      size: 16,\\n    }),\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId, messages });\\n    },\\n  });\\n}\\n```\\n\\n#### Option 2: Setting IDs with `createUIMessageStream`\\n\\nAlternatively, you can use `createUIMessageStream` to control the message ID by writing a start message part:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"8-18\"\\nimport {\\n  generateId,\\n  streamText,\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n} from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages, chatId } = await req.json();\\n\\n  const stream = createUIMessageStream({\\n    execute: ({ writer }) => {\\n      // Write start message part with custom ID\\n      writer.write({\\n        type: \\'start\\',\\n        messageId: generateId(), // Generate server-side ID for persistence\\n      });\\n\\n      const result = streamText({\\n        model: \\'openai/gpt-5-mini\\',\\n        messages: convertToModelMessages(messages),\\n      });\\n\\n      writer.merge(result.toUIMessageStream({ sendStart: false })); // omit start message part\\n    },\\n    originalMessages: messages,\\n    onFinish: ({ responseMessage }) => {\\n      // save your chat here\\n    },\\n  });\\n\\n  return createUIMessageStreamResponse({ stream });\\n}\\n```\\n\\n<Note>\\n  For client-side applications that don\\'t require persistence, you can still customize client-side ID generation:\\n\\n```tsx filename=\"ui/chat.tsx\"\\nimport { createIdGenerator } from \\'ai\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nconst { ... } = useChat({\\n  generateId: createIdGenerator({\\n    prefix: \\'msgc\\',\\n    size: 16,\\n  }),\\n  // ...\\n});\\n```\\n\\n</Note>\\n\\n## Sending only the last message\\n\\nOnce you have implemented message persistence, you might want to send only the last message to the server.\\nThis reduces the amount of data sent to the server on each request and can improve performance.\\n\\nTo achieve this, you can provide a `prepareSendMessagesRequest` function to the transport.\\nThis function receives the messages and the chat ID, and returns the request body to be sent to the server.\\n\\n```tsx filename=\"ui/chat.tsx\" highlight=\"7-12\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nconst {\\n  // ...\\n} = useChat({\\n  // ...\\n  transport: new DefaultChatTransport({\\n    api: \\'/api/chat\\',\\n    // only send the last message to the server:\\n    prepareSendMessagesRequest({ messages, id }) {\\n      return { body: { message: messages[messages.length - 1], id } };\\n    },\\n  }),\\n});\\n```\\n\\nOn the server, you can then load the previous messages and append the new message to the previous messages. If your messages contain tools, metadata, or custom data parts, you should validate them:\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"2-11,14-18\"\\nimport { convertToModelMessages, UIMessage, validateUIMessages } from \\'ai\\';\\n// import your tools and schemas\\n\\nexport async function POST(req: Request) {\\n  // get the last message from the client:\\n  const { message, id } = await req.json();\\n\\n  // load the previous messages from the server:\\n  const previousMessages = await loadChat(id);\\n\\n  // validate messages if they contain tools, metadata, or data parts:\\n  const validatedMessages = await validateUIMessages({\\n    // append the new message to the previous messages:\\n    messages: [...previousMessages, message],\\n    tools, // if using tools\\n    metadataSchema, // if using custom metadata\\n    dataSchemas, // if using custom data parts\\n  });\\n\\n  const result = streamText({\\n    // ...\\n    messages: convertToModelMessages(validatedMessages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: validatedMessages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId: id, messages });\\n    },\\n  });\\n}\\n```\\n\\n## Handling client disconnects\\n\\nBy default, the AI SDK `streamText` function uses backpressure to the language model provider to prevent\\nthe consumption of tokens that are not yet requested.\\n\\nHowever, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue,\\nthe stream from the LLM will be aborted and the conversation may end up in a broken state.\\n\\nAssuming that you have a [storage solution](#storing-messages) in place, you can use the `consumeStream` method to consume the stream on the backend,\\nand then save the result as usual.\\n`consumeStream` effectively removes the backpressure,\\nmeaning that the result is stored even when the client has already disconnected.\\n\\n```tsx filename=\"app/api/chat/route.ts\" highlight=\"19-21\"\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\nimport { saveChat } from \\'@util/chat-store\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\\n    await req.json();\\n\\n  const result = streamText({\\n    model,\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  // consume the stream to ensure it runs to completion & triggers onFinish\\n  // even when the client response is aborted:\\n  result.consumeStream(); // no await\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    onFinish: ({ messages }) => {\\n      saveChat({ chatId, messages });\\n    },\\n  });\\n}\\n```\\n\\nWhen the client reloads the page after a disconnect, the chat will be restored from the storage solution.\\n\\n<Note>\\n  In production applications, you would also track the state of the request (in\\n  progress, complete) in your stored messages and use it on the client to cover\\n  the case where the client reloads the page after a disconnection, but the\\n  streaming is not yet complete.\\n</Note>\\n\\nFor more robust handling of disconnects, you may want to add resumability on disconnects. Check out the [Chatbot Resume Streams](/docs/ai-sdk-ui/chatbot-resume-streams) documentation to learn more.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/03-chatbot-resume-streams.mdx'), name='03-chatbot-resume-streams.mdx', displayName='03-chatbot-resume-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot Resume Streams\\ndescription: Learn how to resume chatbot streams after client disconnects.\\n---\\n\\n# Chatbot Resume Streams\\n\\n`useChat` supports resuming ongoing streams after page reloads. Use this feature to build applications with long-running generations.\\n\\n<Note type=\"warning\">\\n  Stream resumption is not compatible with abort functionality. Closing a tab or\\n  refreshing the page triggers an abort signal that will break the resumption\\n  mechanism. Do not use `resume: true` if you need abort functionality in your\\n  application. See\\n  [troubleshooting](/docs/troubleshooting/abort-breaks-resumable-streams) for\\n  more details.\\n</Note>\\n\\n## How stream resumption works\\n\\nStream resumption requires persistence for messages and active streams in your application. The AI SDK provides tools to connect to storage, but you need to set up the storage yourself.\\n\\n**The AI SDK provides:**\\n\\n- A `resume` option in `useChat` that automatically reconnects to active streams\\n- Access to the outgoing stream through the `consumeSseStream` callback\\n- Automatic HTTP requests to your resume endpoints\\n\\n**You build:**\\n\\n- Storage to track which stream belongs to each chat\\n- Redis to store the UIMessage stream\\n- Two API endpoints: POST to create streams, GET to resume them\\n- Integration with [`resumable-stream`](https://www.npmjs.com/package/resumable-stream) to manage Redis storage\\n\\n## Prerequisites\\n\\nTo implement resumable streams in your chat application, you need:\\n\\n1. **The `resumable-stream` package** - Handles the publisher/subscriber mechanism for streams\\n2. **A Redis instance** - Stores stream data (e.g. [Redis through Vercel](https://vercel.com/marketplace/redis))\\n3. **A persistence layer** - Tracks which stream ID is active for each chat (e.g. database)\\n\\n## Implementation\\n\\n### 1. Client-side: Enable stream resumption\\n\\nUse the `resume` option in the `useChat` hook to enable stream resumption. When `resume` is true, the hook automatically attempts to reconnect to any active stream for the chat on mount:\\n\\n```tsx filename=\"app/chat/[chatId]/chat.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport, type UIMessage } from \\'ai\\';\\n\\nexport function Chat({\\n  chatData,\\n  resume = false,\\n}: {\\n  chatData: { id: string; messages: UIMessage[] };\\n  resume?: boolean;\\n}) {\\n  const { messages, sendMessage, status } = useChat({\\n    id: chatData.id,\\n    messages: chatData.messages,\\n    resume, // Enable automatic stream resumption\\n    transport: new DefaultChatTransport({\\n      // You must send the id of the chat\\n      prepareSendMessagesRequest: ({ id, messages }) => {\\n        return {\\n          body: {\\n            id,\\n            message: messages[messages.length - 1],\\n          },\\n        };\\n      },\\n    }),\\n  });\\n\\n  return <div>{/* Your chat UI */}</div>;\\n}\\n```\\n\\n<Note>\\n  You must send the chat ID with each request (see\\n  `prepareSendMessagesRequest`).\\n</Note>\\n\\nWhen you enable `resume`, the `useChat` hook makes a `GET` request to `/api/chat/[id]/stream` on mount to check for and resume any active streams.\\n\\nLet\\'s start by creating the POST handler to create the resumable stream.\\n\\n### 2. Create the POST handler\\n\\nThe POST handler creates resumable streams using the `consumeSseStream` callback:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readChat, saveChat } from \\'@util/chat-store\\';\\nimport {\\n  convertToModelMessages,\\n  generateId,\\n  streamText,\\n  type UIMessage,\\n} from \\'ai\\';\\nimport { after } from \\'next/server\\';\\nimport { createResumableStreamContext } from \\'resumable-stream\\';\\n\\nexport async function POST(req: Request) {\\n  const {\\n    message,\\n    id,\\n  }: {\\n    message: UIMessage | undefined;\\n    id: string;\\n  } = await req.json();\\n\\n  const chat = await readChat(id);\\n  let messages = chat.messages;\\n\\n  messages = [...messages, message!];\\n\\n  // Clear any previous active stream and save the user message\\n  saveChat({ id, messages, activeStreamId: null });\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages,\\n    generateMessageId: generateId,\\n    onFinish: ({ messages }) => {\\n      // Clear the active stream when finished\\n      saveChat({ id, messages, activeStreamId: null });\\n    },\\n    async consumeSseStream({ stream }) {\\n      const streamId = generateId();\\n\\n      // Create a resumable stream from the SSE stream\\n      const streamContext = createResumableStreamContext({ waitUntil: after });\\n      await streamContext.createNewResumableStream(streamId, () => stream);\\n\\n      // Update the chat with the active stream ID\\n      saveChat({ id, activeStreamId: streamId });\\n    },\\n  });\\n}\\n```\\n\\n### 3. Implement the GET handler\\n\\nCreate a GET handler at `/api/chat/[id]/stream` that:\\n\\n1. Reads the chat ID from the route params\\n2. Loads the chat data to check for an active stream\\n3. Returns 204 (No Content) if no stream is active\\n4. Resumes the existing stream if one is found\\n\\n```ts filename=\"app/api/chat/[id]/stream/route.ts\"\\nimport { readChat } from \\'@util/chat-store\\';\\nimport { UI_MESSAGE_STREAM_HEADERS } from \\'ai\\';\\nimport { after } from \\'next/server\\';\\nimport { createResumableStreamContext } from \\'resumable-stream\\';\\n\\nexport async function GET(\\n  _: Request,\\n  { params }: { params: Promise<{ id: string }> },\\n) {\\n  const { id } = await params;\\n\\n  const chat = await readChat(id);\\n\\n  if (chat.activeStreamId == null) {\\n    // no content response when there is no active stream\\n    return new Response(null, { status: 204 });\\n  }\\n\\n  const streamContext = createResumableStreamContext({\\n    waitUntil: after,\\n  });\\n\\n  return new Response(\\n    await streamContext.resumeExistingStream(chat.activeStreamId),\\n    { headers: UI_MESSAGE_STREAM_HEADERS },\\n  );\\n}\\n```\\n\\n<Note>\\n  The `after` function from Next.js allows work to continue after the response\\n  has been sent. This ensures that the resumable stream persists in Redis even\\n  after the initial response is returned to the client, enabling reconnection\\n  later.\\n</Note>\\n\\n## How it works\\n\\n### Request lifecycle\\n\\n![Diagram showing the architecture and lifecycle of resumable stream requests](https://e742qlubrjnjqpp0.public.blob.vercel-storage.com/resume-stream-diagram.png)\\n\\nThe diagram above shows the complete lifecycle of a resumable stream:\\n\\n1. **Stream creation**: When you send a new message, the POST handler uses `streamText` to generate the response. The `consumeSseStream` callback creates a resumable stream with a unique ID and stores it in Redis through the `resumable-stream` package\\n2. **Stream tracking**: Your persistence layer saves the `activeStreamId` in the chat data\\n3. **Client reconnection**: When the client reconnects (page reload), the `resume` option triggers a GET request to `/api/chat/[id]/stream`\\n4. **Stream recovery**: The GET handler checks for an `activeStreamId` and uses `resumeExistingStream` to reconnect. If no active stream exists, it returns a 204 (No Content) response\\n5. **Completion cleanup**: When the stream finishes, the `onFinish` callback clears the `activeStreamId` by setting it to `null`\\n\\n## Customize the resume endpoint\\n\\nBy default, the `useChat` hook makes a GET request to `/api/chat/[id]/stream` when resuming. Customize this endpoint, credentials, and headers, using the `prepareReconnectToStreamRequest` option in `DefaultChatTransport`:\\n\\n```tsx filename=\"app/chat/[chatId]/chat.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\n\\nexport function Chat({ chatData, resume }) {\\n  const { messages, sendMessage } = useChat({\\n    id: chatData.id,\\n    messages: chatData.messages,\\n    resume,\\n    transport: new DefaultChatTransport({\\n      // Customize reconnect settings (optional)\\n      prepareReconnectToStreamRequest: ({ id }) => {\\n        return {\\n          api: `/api/chat/${id}/stream`, // Default pattern\\n          // Or use a different pattern:\\n          // api: `/api/streams/${id}/resume`,\\n          // api: `/api/resume-chat?id=${id}`,\\n          credentials: \\'include\\', // Include cookies/auth\\n          headers: {\\n            Authorization: \\'Bearer token\\',\\n            \\'X-Custom-Header\\': \\'value\\',\\n          },\\n        };\\n      },\\n    }),\\n  });\\n\\n  return <div>{/* Your chat UI */}</div>;\\n}\\n```\\n\\nThis lets you:\\n\\n- Match your existing API route structure\\n- Add query parameters or custom paths\\n- Integrate with different backend architectures\\n\\n## Important considerations\\n\\n- **Incompatibility with abort**: Stream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use `resume: true` if you need abort functionality in your application\\n- **Stream expiration**: Streams in Redis expire after a set time (configurable in the `resumable-stream` package)\\n- **Multiple clients**: Multiple clients can connect to the same stream simultaneously\\n- **Error handling**: When no active stream exists, the GET handler returns a 204 (No Content) status code\\n- **Security**: Ensure proper authentication and authorization for both creating and resuming streams\\n- **Race conditions**: Clear the `activeStreamId` when starting a new stream to prevent resuming outdated streams\\n\\n<br />\\n<GithubLink link=\"https://github.com/vercel/ai/blob/main/examples/next\" />\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/03-chatbot-tool-usage.mdx'), name='03-chatbot-tool-usage.mdx', displayName='03-chatbot-tool-usage.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Chatbot Tool Usage\\ndescription: Learn how to use tools with the useChat hook.\\n---\\n\\n# Chatbot Tool Usage\\n\\nWith [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`streamText`](/docs/reference/ai-sdk-core/stream-text), you can use tools in your chatbot application.\\nThe AI SDK supports three types of tools in this context:\\n\\n1. Automatically executed server-side tools\\n2. Automatically executed client-side tools\\n3. Tools that require user interaction, such as confirmation dialogs\\n\\nThe flow is as follows:\\n\\n1. The user enters a message in the chat UI.\\n1. The message is sent to the API route.\\n1. In your server side route, the language model generates tool calls during the `streamText` call.\\n1. All tool calls are forwarded to the client.\\n1. Server-side tools are executed using their `execute` method and their results are forwarded to the client.\\n1. Client-side tools that should be automatically executed are handled with the `onToolCall` callback.\\n   You must call `addToolOutput` to provide the tool result.\\n1. Client-side tool that require user interactions can be displayed in the UI.\\n   The tool calls and results are available as tool invocation parts in the `parts` property of the last assistant message.\\n1. When the user interaction is done, `addToolOutput` can be used to add the tool result to the chat.\\n1. The chat can be configured to automatically submit when all tool results are available using `sendAutomaticallyWhen`.\\n   This triggers another iteration of this flow.\\n\\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts.\\nA tool part is at first a tool call, and then it becomes a tool result when the tool is executed.\\nThe tool result contains all information about the tool call as well as the result of the tool execution.\\n\\n<Note>\\n  Tool result submission can be configured using the `sendAutomaticallyWhen`\\n  option. You can use the `lastAssistantMessageIsCompleteWithToolCalls` helper\\n  to automatically submit when all tool results are available. This simplifies\\n  the client-side code while still allowing full control when needed.\\n</Note>\\n\\n## Example\\n\\nIn this example, we\\'ll use three tools:\\n\\n- `getWeatherInformation`: An automatically executed server-side tool that returns the weather in a given city.\\n- `askForConfirmation`: A user-interaction client-side tool that asks the user for confirmation.\\n- `getLocation`: An automatically executed client-side tool that returns a random city.\\n\\n### API route\\n\\n```tsx filename=\\'app/api/chat/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      // server-side tool with execute function:\\n      getWeatherInformation: {\\n        description: \\'show the weather in a given city to the user\\',\\n        inputSchema: z.object({ city: z.string() }),\\n        execute: async ({}: { city: string }) => {\\n          const weatherOptions = [\\'sunny\\', \\'cloudy\\', \\'rainy\\', \\'snowy\\', \\'windy\\'];\\n          return weatherOptions[\\n            Math.floor(Math.random() * weatherOptions.length)\\n          ];\\n        },\\n      },\\n      // client-side tool that starts user interaction:\\n      askForConfirmation: {\\n        description: \\'Ask the user for confirmation.\\',\\n        inputSchema: z.object({\\n          message: z.string().describe(\\'The message to ask for confirmation.\\'),\\n        }),\\n      },\\n      // client-side tool that is automatically executed on the client:\\n      getLocation: {\\n        description:\\n          \\'Get the user location. Always ask for confirmation before using this tool.\\',\\n        inputSchema: z.object({}),\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Client-side page\\n\\nThe client-side page uses the `useChat` hook to create a chatbot application with real-time message streaming.\\nTool calls are displayed in the chat UI as typed tool parts.\\nPlease make sure to render the messages using the `parts` property of the message.\\n\\nThere are three things worth mentioning:\\n\\n1. The [`onToolCall`](/docs/reference/ai-sdk-ui/use-chat#on-tool-call) callback is used to handle client-side tools that should be automatically executed.\\n   In this example, the `getLocation` tool is a client-side tool that returns a random city.\\n   You call `addToolOutput` to provide the result (without `await` to avoid potential deadlocks).\\n\\n   <Note>\\n     Always check `if (toolCall.dynamic)` first in your `onToolCall` handler.\\n     Without this check, TypeScript will throw an error like: `Type \\'string\\' is\\n     not assignable to type \\'\"toolName1\" | \"toolName2\"\\'` when you try to use\\n     `toolCall.toolName` in `addToolOutput`.\\n   </Note>\\n\\n2. The [`sendAutomaticallyWhen`](/docs/reference/ai-sdk-ui/use-chat#send-automatically-when) option with `lastAssistantMessageIsCompleteWithToolCalls` helper automatically submits when all tool results are available.\\n\\n3. The `parts` array of assistant messages contains tool parts with typed names like `tool-askForConfirmation`.\\n   The client-side tool `askForConfirmation` is displayed in the UI.\\n   It asks the user for confirmation and displays the result once the user confirms or denies the execution.\\n   The result is added to the chat using `addToolOutput` with the `tool` parameter for type safety.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"2,6,10,14-20\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport {\\n  DefaultChatTransport,\\n  lastAssistantMessageIsCompleteWithToolCalls,\\n} from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, addToolOutput } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n\\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n\\n    // run client-side tools that are automatically executed:\\n    async onToolCall({ toolCall }) {\\n      // Check if it\\'s a dynamic tool first for proper type narrowing\\n      if (toolCall.dynamic) {\\n        return;\\n      }\\n\\n      if (toolCall.toolName === \\'getLocation\\') {\\n        const cities = [\\'New York\\', \\'Los Angeles\\', \\'Chicago\\', \\'San Francisco\\'];\\n\\n        // No await - avoids potential deadlocks\\n        addToolOutput({\\n          tool: \\'getLocation\\',\\n          toolCallId: toolCall.toolCallId,\\n          output: cities[Math.floor(Math.random() * cities.length)],\\n        });\\n      }\\n    },\\n  });\\n  const [input, setInput] = useState(\\'\\');\\n\\n  return (\\n    <>\\n      {messages?.map(message => (\\n        <div key={message.id}>\\n          <strong>{`${message.role}: `}</strong>\\n          {message.parts.map(part => {\\n            switch (part.type) {\\n              // render text parts as simple text:\\n              case \\'text\\':\\n                return part.text;\\n\\n              // for tool parts, use the typed tool part names:\\n              case \\'tool-askForConfirmation\\': {\\n                const callId = part.toolCallId;\\n\\n                switch (part.state) {\\n                  case \\'input-streaming\\':\\n                    return (\\n                      <div key={callId}>Loading confirmation request...</div>\\n                    );\\n                  case \\'input-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        {part.input.message}\\n                        <div>\\n                          <button\\n                            onClick={() =>\\n                              addToolOutput({\\n                                tool: \\'askForConfirmation\\',\\n                                toolCallId: callId,\\n                                output: \\'Yes, confirmed.\\',\\n                              })\\n                            }\\n                          >\\n                            Yes\\n                          </button>\\n                          <button\\n                            onClick={() =>\\n                              addToolOutput({\\n                                tool: \\'askForConfirmation\\',\\n                                toolCallId: callId,\\n                                output: \\'No, denied\\',\\n                              })\\n                            }\\n                          >\\n                            No\\n                          </button>\\n                        </div>\\n                      </div>\\n                    );\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        Location access allowed: {part.output}\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={callId}>Error: {part.errorText}</div>;\\n                }\\n                break;\\n              }\\n\\n              case \\'tool-getLocation\\': {\\n                const callId = part.toolCallId;\\n\\n                switch (part.state) {\\n                  case \\'input-streaming\\':\\n                    return (\\n                      <div key={callId}>Preparing location request...</div>\\n                    );\\n                  case \\'input-available\\':\\n                    return <div key={callId}>Getting location...</div>;\\n                  case \\'output-available\\':\\n                    return <div key={callId}>Location: {part.output}</div>;\\n                  case \\'output-error\\':\\n                    return (\\n                      <div key={callId}>\\n                        Error getting location: {part.errorText}\\n                      </div>\\n                    );\\n                }\\n                break;\\n              }\\n\\n              case \\'tool-getWeatherInformation\\': {\\n                const callId = part.toolCallId;\\n\\n                switch (part.state) {\\n                  // example of pre-rendering streaming tool inputs:\\n                  case \\'input-streaming\\':\\n                    return (\\n                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\\n                    );\\n                  case \\'input-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        Getting weather information for {part.input.city}...\\n                      </div>\\n                    );\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={callId}>\\n                        Weather in {part.input.city}: {part.output}\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return (\\n                      <div key={callId}>\\n                        Error getting weather for {part.input.city}:{\\' \\'}\\n                        {part.errorText}\\n                      </div>\\n                    );\\n                }\\n                break;\\n              }\\n            }\\n          })}\\n          <br />\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          if (input.trim()) {\\n            sendMessage({ text: input });\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n### Error handling\\n\\nSometimes an error may occur during client-side tool execution. Use the `addToolOutput` method with a `state` of `output-error` and `errorText` value instead of `output` record the error.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"19,36-41\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport {\\n  DefaultChatTransport,\\n  lastAssistantMessageIsCompleteWithToolCalls,\\n} from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const { messages, sendMessage, addToolOutput } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n\\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n\\n    // run client-side tools that are automatically executed:\\n    async onToolCall({ toolCall }) {\\n      // Check if it\\'s a dynamic tool first for proper type narrowing\\n      if (toolCall.dynamic) {\\n        return;\\n      }\\n\\n      if (toolCall.toolName === \\'getWeatherInformation\\') {\\n        try {\\n          const weather = await getWeatherInformation(toolCall.input);\\n\\n          // No await - avoids potential deadlocks\\n          addToolOutput({\\n            tool: \\'getWeatherInformation\\',\\n            toolCallId: toolCall.toolCallId,\\n            output: weather,\\n          });\\n        } catch (err) {\\n          addToolOutput({\\n            tool: \\'getWeatherInformation\\',\\n            toolCallId: toolCall.toolCallId,\\n            state: \\'output-error\\',\\n            errorText: \\'Unable to get the weather information\\',\\n          });\\n        }\\n      }\\n    },\\n  });\\n}\\n```\\n\\n## Dynamic Tools\\n\\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic `dynamic-tool` type instead of specific tool types:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n{\\n  message.parts.map((part, index) => {\\n    switch (part.type) {\\n      // Static tools with specific (`tool-${toolName}`) types\\n      case \\'tool-getWeatherInformation\\':\\n        return <WeatherDisplay part={part} />;\\n\\n      // Dynamic tools use generic `dynamic-tool` type\\n      case \\'dynamic-tool\\':\\n        return (\\n          <div key={index}>\\n            <h4>Tool: {part.toolName}</h4>\\n            {part.state === \\'input-streaming\\' && (\\n              <pre>{JSON.stringify(part.input, null, 2)}</pre>\\n            )}\\n            {part.state === \\'output-available\\' && (\\n              <pre>{JSON.stringify(part.output, null, 2)}</pre>\\n            )}\\n            {part.state === \\'output-error\\' && (\\n              <div>Error: {part.errorText}</div>\\n            )}\\n          </div>\\n        );\\n    }\\n  });\\n}\\n```\\n\\nDynamic tools are useful when integrating with:\\n\\n- MCP (Model Context Protocol) tools without schemas\\n- User-defined functions loaded at runtime\\n- External tool providers\\n\\n## Tool call streaming\\n\\nTool call streaming is **enabled by default** in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\\n\\n```tsx filename=\\'app/api/chat/route.ts\\'\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    // toolCallStreaming is enabled by default in v5\\n    // ...\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream.\\nThey are available through the `useChat` hook.\\nThe typed tool parts of assistant messages will also contain partial tool calls.\\nYou can use the `state` property of the tool part to render the correct UI.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"9,10\"\\nexport default function Chat() {\\n  // ...\\n  return (\\n    <>\\n      {messages?.map(message => (\\n        <div key={message.id}>\\n          {message.parts.map(part => {\\n            switch (part.type) {\\n              case \\'tool-askForConfirmation\\':\\n              case \\'tool-getLocation\\':\\n              case \\'tool-getWeatherInformation\\':\\n                switch (part.state) {\\n                  case \\'input-streaming\\':\\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\\n                  case \\'input-available\\':\\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\\n                  case \\'output-available\\':\\n                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\\n                  case \\'output-error\\':\\n                    return <div>Error: {part.errorText}</div>;\\n                }\\n            }\\n          })}\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n## Step start parts\\n\\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages.\\nIf you want to display boundaries between tool calls, you can use the `step-start` parts as follows:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n// ...\\n// where you render the message parts:\\nmessage.parts.map((part, index) => {\\n  switch (part.type) {\\n    case \\'step-start\\':\\n      // show step boundaries as horizontal lines:\\n      return index > 0 ? (\\n        <div key={index} className=\"text-gray-500\">\\n          <hr className=\"my-2 border-gray-300\" />\\n        </div>\\n      ) : null;\\n    case \\'text\\':\\n    // ...\\n    case \\'tool-askForConfirmation\\':\\n    case \\'tool-getLocation\\':\\n    case \\'tool-getWeatherInformation\\':\\n    // ...\\n  }\\n});\\n// ...\\n```\\n\\n## Server-side Multi-Step Calls\\n\\nYou can also use multi-step calls on the server-side with `streamText`.\\nThis works when all invoked tools have an `execute` function on the server side.\\n\\n```tsx filename=\\'app/api/chat/route.ts\\' highlight=\"15-21,24\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText, UIMessage, stepCountIs } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      getWeatherInformation: {\\n        description: \\'show the weather in a given city to the user\\',\\n        inputSchema: z.object({ city: z.string() }),\\n        // tool has execute function:\\n        execute: async ({}: { city: string }) => {\\n          const weatherOptions = [\\'sunny\\', \\'cloudy\\', \\'rainy\\', \\'snowy\\', \\'windy\\'];\\n          return weatherOptions[\\n            Math.floor(Math.random() * weatherOptions.length)\\n          ];\\n        },\\n      },\\n    },\\n    stopWhen: stepCountIs(5),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Errors\\n\\nLanguage models can make errors when calling tools.\\nBy default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\\n\\nTo surface the errors, you can use the `onError` function when calling `toUIMessageResponse`.\\n\\n```tsx\\nexport function errorHandler(error: unknown) {\\n  if (error == null) {\\n    return \\'unknown error\\';\\n  }\\n\\n  if (typeof error === \\'string\\') {\\n    return error;\\n  }\\n\\n  if (error instanceof Error) {\\n    return error.message;\\n  }\\n\\n  return JSON.stringify(error);\\n}\\n```\\n\\n```tsx\\nconst result = streamText({\\n  // ...\\n});\\n\\nreturn result.toUIMessageStreamResponse({\\n  onError: errorHandler,\\n});\\n```\\n\\nIn case you are using `createUIMessageResponse`, you can use the `onError` function when calling `toUIMessageResponse`:\\n\\n```tsx\\nconst response = createUIMessageResponse({\\n  // ...\\n  async execute(dataStream) {\\n    // ...\\n  },\\n  onError: error => `Custom error: ${error.message}`,\\n});\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/04-generative-user-interfaces.mdx'), name='04-generative-user-interfaces.mdx', displayName='04-generative-user-interfaces.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Generative User Interfaces\\ndescription: Learn how to build Generative UI with AI SDK UI.\\n---\\n\\n# Generative User Interfaces\\n\\nGenerative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and \"generate UI\". This creates a more engaging and AI-native experience for users.\\n\\n<WeatherSearch />\\n\\nAt the core of generative UI are [ tools ](/docs/ai-sdk-core/tools-and-tool-calling), which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.\\n\\nGenerative UI is the process of connecting the results of a tool call to a React component. Here\\'s how it works:\\n\\n1. You provide the model with a prompt or conversation history, along with a set of tools.\\n2. Based on the context, the model may decide to call a tool.\\n3. If a tool is called, it will execute and return data.\\n4. This data can then be passed to a React component for rendering.\\n\\nBy passing the tool results to React components, you can create a generative UI experience that\\'s more engaging and adaptive to your needs.\\n\\n## Build a Generative UI Chat Interface\\n\\nLet\\'s create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.\\n\\n### Basic Chat Implementation\\n\\nStart with a basic chat implementation using the `useChat` hook:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}</div>\\n          <div>\\n            {message.parts.map((part, index) => {\\n              if (part.type === \\'text\\') {\\n                return <span key={index}>{part.text}</span>;\\n              }\\n              return null;\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Type a message...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nTo handle the chat requests and model responses, set up an API route:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from \\'ai\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'You are a friendly assistant!\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nThis API route uses the `streamText` function to process chat messages and stream the model\\'s responses back to the client.\\n\\n### Create a Tool\\n\\nBefore enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.\\n\\nCreate a new file called `ai/tools.ts` with the following content:\\n\\n```ts filename=\"ai/tools.ts\"\\nimport { tool as createTool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const weatherTool = createTool({\\n  description: \\'Display the weather for a location\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The location to get the weather for\\'),\\n  }),\\n  execute: async function ({ location }) {\\n    await new Promise(resolve => setTimeout(resolve, 2000));\\n    return { weather: \\'Sunny\\', temperature: 75, location };\\n  },\\n});\\n\\nexport const tools = {\\n  displayWeather: weatherTool,\\n};\\n```\\n\\nIn this file, you\\'ve created a tool called `weatherTool`. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.\\n\\n### Update the API Route\\n\\nUpdate the API route to include the tool you\\'ve defined:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"3,8,14\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from \\'ai\\';\\nimport { tools } from \\'@/ai/tools\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages }: { messages: UIMessage[] } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'You are a friendly assistant!\\',\\n    messages: convertToModelMessages(messages),\\n    stopWhen: stepCountIs(5),\\n    tools,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nNow that you\\'ve defined the tool and added it to your `streamText` call, let\\'s build a React component to display the weather information it returns.\\n\\n### Create UI Components\\n\\nCreate a new file called `components/weather.tsx`:\\n\\n```tsx filename=\"components/weather.tsx\"\\ntype WeatherProps = {\\n  temperature: number;\\n  weather: string;\\n  location: string;\\n};\\n\\nexport const Weather = ({ temperature, weather, location }: WeatherProps) => {\\n  return (\\n    <div>\\n      <h2>Current Weather for {location}</h2>\\n      <p>Condition: {weather}</p>\\n      <p>Temperature: {temperature}°C</p>\\n    </div>\\n  );\\n};\\n```\\n\\nThis component will display the weather information for a given location. It takes three props: `temperature`, `weather`, and `location` (exactly what the `weatherTool` returns).\\n\\n### Render the Weather Component\\n\\nNow that you have your tool and corresponding React component, let\\'s integrate them into your chat interface. You\\'ll render the Weather component when the model calls the weather tool.\\n\\nTo check if the model has called a tool, you can check the `parts` array of the UIMessage object for tool-specific parts. In AI SDK 5.0, tool parts use typed naming: `tool-${toolName}` instead of generic types.\\n\\nUpdate your `page.tsx` file:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"4,9,14-15,19-46\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport { Weather } from \\'@/components/weather\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}</div>\\n          <div>\\n            {message.parts.map((part, index) => {\\n              if (part.type === \\'text\\') {\\n                return <span key={index}>{part.text}</span>;\\n              }\\n\\n              if (part.type === \\'tool-displayWeather\\') {\\n                switch (part.state) {\\n                  case \\'input-available\\':\\n                    return <div key={index}>Loading weather...</div>;\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={index}>\\n                        <Weather {...part.output} />\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={index}>Error: {part.errorText}</div>;\\n                  default:\\n                    return null;\\n                }\\n              }\\n\\n              return null;\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Type a message...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nIn this updated code snippet, you:\\n\\n1. Use manual input state management with `useState` instead of the built-in `input` and `handleInputChange`.\\n2. Use `sendMessage` instead of `handleSubmit` to send messages.\\n3. Check the `parts` array of each message for different content types.\\n4. Handle tool parts with type `tool-displayWeather` and their different states (`input-available`, `output-available`, `output-error`).\\n\\nThis approach allows you to dynamically render UI components based on the model\\'s responses, creating a more interactive and context-aware chat experience.\\n\\n## Expanding Your Generative UI Application\\n\\nYou can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here\\'s how you can expand your application:\\n\\n### Adding More Tools\\n\\nTo add more tools, simply define them in your `ai/tools.ts` file:\\n\\n```ts\\n// Add a new stock tool\\nexport const stockTool = createTool({\\n  description: \\'Get price for a stock\\',\\n  inputSchema: z.object({\\n    symbol: z.string().describe(\\'The stock symbol to get the price for\\'),\\n  }),\\n  execute: async function ({ symbol }) {\\n    // Simulated API call\\n    await new Promise(resolve => setTimeout(resolve, 2000));\\n    return { symbol, price: 100 };\\n  },\\n});\\n\\n// Update the tools object\\nexport const tools = {\\n  displayWeather: weatherTool,\\n  getStockPrice: stockTool,\\n};\\n```\\n\\nNow, create a new file called `components/stock.tsx`:\\n\\n```tsx\\ntype StockProps = {\\n  price: number;\\n  symbol: string;\\n};\\n\\nexport const Stock = ({ price, symbol }: StockProps) => {\\n  return (\\n    <div>\\n      <h2>Stock Information</h2>\\n      <p>Symbol: {symbol}</p>\\n      <p>Price: ${price}</p>\\n    </div>\\n  );\\n};\\n```\\n\\nFinally, update your `page.tsx` file to include the new Stock component:\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport { Weather } from \\'@/components/weather\\';\\nimport { Stock } from \\'@/components/stock\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>\\n            {message.parts.map((part, index) => {\\n              if (part.type === \\'text\\') {\\n                return <span key={index}>{part.text}</span>;\\n              }\\n\\n              if (part.type === \\'tool-displayWeather\\') {\\n                switch (part.state) {\\n                  case \\'input-available\\':\\n                    return <div key={index}>Loading weather...</div>;\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={index}>\\n                        <Weather {...part.output} />\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={index}>Error: {part.errorText}</div>;\\n                  default:\\n                    return null;\\n                }\\n              }\\n\\n              if (part.type === \\'tool-getStockPrice\\') {\\n                switch (part.state) {\\n                  case \\'input-available\\':\\n                    return <div key={index}>Loading stock price...</div>;\\n                  case \\'output-available\\':\\n                    return (\\n                      <div key={index}>\\n                        <Stock {...part.output} />\\n                      </div>\\n                    );\\n                  case \\'output-error\\':\\n                    return <div key={index}>Error: {part.errorText}</div>;\\n                  default:\\n                    return null;\\n                }\\n              }\\n\\n              return null;\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nBy following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/05-completion.mdx'), name='05-completion.mdx', displayName='05-completion.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Completion\\ndescription: Learn how to use the useCompletion hook.\\n---\\n\\n# Completion\\n\\nThe `useCompletion` hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\\n\\n<Note>\\n  The `useCompletion` hook is now part of the `@ai-sdk/react` package.\\n</Note>\\n\\nIn this guide, you will learn how to use the `useCompletion` hook in your application to generate text completions and stream them in real-time to your users.\\n\\n## Example\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useCompletion } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\\n    api: \\'/api/completion\\',\\n  });\\n\\n  return (\\n    <form onSubmit={handleSubmit}>\\n      <input\\n        name=\"prompt\"\\n        value={input}\\n        onChange={handleInputChange}\\n        id=\"input\"\\n      />\\n      <button type=\"submit\">Submit</button>\\n      <div>{completion}</div>\\n    </form>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/completion/route.ts\\'\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { prompt }: { prompt: string } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nIn the `Page` component, the `useCompletion` hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.\\n\\nThis enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\\n\\n## Customized UI\\n\\n`useCompletion` also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.\\n\\n### Loading and error states\\n\\nTo show a loading spinner while the chatbot is processing the user\\'s message, you can use the `isLoading` state returned by the `useCompletion` hook:\\n\\n```tsx\\nconst { isLoading, ... } = useCompletion()\\n\\nreturn(\\n  <>\\n    {isLoading ? <Spinner /> : null}\\n  </>\\n)\\n```\\n\\nSimilarly, the `error` state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:\\n\\n```tsx\\nconst { error, ... } = useCompletion()\\n\\nuseEffect(() => {\\n  if (error) {\\n    toast.error(error.message)\\n  }\\n}, [error])\\n\\n// Or display the error message in the UI:\\nreturn (\\n  <>\\n    {error ? <div>{error.message}</div> : null}\\n  </>\\n)\\n```\\n\\n### Controlled input\\n\\nIn the initial example, we have `handleSubmit` and `handleInputChange` callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.\\n\\nThe following example demonstrates how to use more granular APIs like `setInput` with your custom input and submit button components:\\n\\n```tsx\\nconst { input, setInput } = useCompletion();\\n\\nreturn (\\n  <>\\n    <MyCustomInput value={input} onChange={value => setInput(value)} />\\n  </>\\n);\\n```\\n\\n### Cancelation\\n\\nIt\\'s also a common use case to abort the response message while it\\'s still streaming back from the AI provider. You can do this by calling the `stop` function returned by the `useCompletion` hook.\\n\\n```tsx\\nconst { stop, isLoading, ... } = useCompletion()\\n\\nreturn (\\n  <>\\n    <button onClick={stop} disabled={!isLoading}>Stop</button>\\n  </>\\n)\\n```\\n\\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.\\n\\n### Throttling UI Updates\\n\\n<Note>This feature is currently only available for React.</Note>\\n\\nBy default, the `useCompletion` hook will trigger a render every time a new chunk is received.\\nYou can throttle the UI updates with the `experimental_throttle` option.\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { completion, ... } = useCompletion({\\n  // Throttle the completion and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n\\n## Event Callbacks\\n\\n`useCompletion` also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\\n\\n```tsx\\nconst { ... } = useCompletion({\\n  onResponse: (response: Response) => {\\n    console.log(\\'Received response from server:\\', response)\\n  },\\n  onFinish: (prompt: string, completion: string) => {\\n    console.log(\\'Finished streaming completion:\\', completion)\\n  },\\n  onError: (error: Error) => {\\n    console.error(\\'An error occurred:\\', error)\\n  },\\n})\\n```\\n\\nIt\\'s worth noting that you can abort the processing by throwing an error in the `onResponse` callback. This will trigger the `onError` callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\\n\\n## Configure Request Options\\n\\nBy default, the `useCompletion` hook sends a HTTP POST request to the `/api/completion` endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the `useCompletion` hook:\\n\\n```tsx\\nconst { messages, input, handleInputChange, handleSubmit } = useCompletion({\\n  api: \\'/api/custom-completion\\',\\n  headers: {\\n    Authorization: \\'your_token\\',\\n  },\\n  body: {\\n    user_id: \\'123\\',\\n  },\\n  credentials: \\'same-origin\\',\\n});\\n```\\n\\nIn this example, the `useCompletion` hook sends a POST request to the `/api/completion` endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/08-object-generation.mdx'), name='08-object-generation.mdx', displayName='08-object-generation.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Object Generation\\ndescription: Learn how to use the useObject hook.\\n---\\n\\n# Object Generation\\n\\n<Note>\\n  `useObject` is an experimental feature and only available in React, Svelte,\\n  and Vue.\\n</Note>\\n\\nThe [`useObject`](/docs/reference/ai-sdk-ui/use-object) hook allows you to create interfaces that represent a structured JSON object that is being streamed.\\n\\nIn this guide, you will learn how to use the `useObject` hook in your application to generate UIs for structured data on the fly.\\n\\n## Example\\n\\nThe example shows a small notifications demo app that generates fake notifications in real-time.\\n\\n### Schema\\n\\nIt is helpful to set up the schema in a separate file that is imported on both the client and server.\\n\\n```ts filename=\\'app/api/notifications/schema.ts\\'\\nimport { z } from \\'zod\\';\\n\\n// define a schema for the notifications\\nexport const notificationSchema = z.object({\\n  notifications: z.array(\\n    z.object({\\n      name: z.string().describe(\\'Name of a fictional person.\\'),\\n      message: z.string().describe(\\'Message. Do not use emojis or links.\\'),\\n    }),\\n  ),\\n});\\n```\\n\\n### Client\\n\\nThe client uses [`useObject`](/docs/reference/ai-sdk-ui/use-object) to stream the object generation process.\\n\\nThe results are partial and are displayed as they are received.\\nPlease note the code for handling `undefined` values in the JSX.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\nimport { notificationSchema } from \\'./api/notifications/schema\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n### Server\\n\\nOn the server, we use [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to stream the object generation process.\\n\\n```typescript filename=\\'app/api/notifications/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\nimport { notificationSchema } from \\'./schema\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const context = await req.json();\\n\\n  const result = streamObject({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    schema: notificationSchema,\\n    prompt:\\n      `Generate 3 notifications for a messages app in this context:` + context,\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## Enum Output Mode\\n\\nWhen you need to classify or categorize input into predefined options, you can use the `enum` output mode with `useObject`. This requires a specific schema structure where the object has `enum` as a key with `z.enum` containing your possible values.\\n\\n### Example: Text Classification\\n\\nThis example shows how to build a simple text classifier that categorizes statements as true or false.\\n\\n#### Client\\n\\nWhen using `useObject` with enum output mode, your schema must be an object with `enum` as the key:\\n\\n```tsx filename=\\'app/classify/page.tsx\\'\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\nimport { z } from \\'zod\\';\\n\\nexport default function ClassifyPage() {\\n  const { object, submit, isLoading } = useObject({\\n    api: \\'/api/classify\\',\\n    schema: z.object({ enum: z.enum([\\'true\\', \\'false\\']) }),\\n  });\\n\\n  return (\\n    <>\\n      <button onClick={() => submit(\\'The earth is flat\\')} disabled={isLoading}>\\n        Classify statement\\n      </button>\\n\\n      {object && <div>Classification: {object.enum}</div>}\\n    </>\\n  );\\n}\\n```\\n\\n#### Server\\n\\nOn the server, use `streamObject` with `output: \\'enum\\'` to stream the classification result:\\n\\n```typescript filename=\\'app/api/classify/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const context = await req.json();\\n\\n  const result = streamObject({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    output: \\'enum\\',\\n    enum: [\\'true\\', \\'false\\'],\\n    prompt: `Classify this statement as true or false: ${context}`,\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## Customized UI\\n\\n`useObject` also provides ways to show loading and error states:\\n\\n### Loading State\\n\\nThe `isLoading` state returned by the `useObject` hook can be used for several\\npurposes:\\n\\n- To show a loading spinner while the object is generated.\\n- To disable the submit button.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"6,13-20,24\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { isLoading, object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      {isLoading && <Spinner />}\\n\\n      <button\\n        onClick={() => submit(\\'Messages during finals week.\\')}\\n        disabled={isLoading}\\n      >\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n### Stop Handler\\n\\nThe `stop` function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"6,14-16\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { isLoading, stop, object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      {isLoading && (\\n        <button type=\"button\" onClick={() => stop()}>\\n          Stop\\n        </button>\\n      )}\\n\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n### Error State\\n\\nSimilarly, the `error` state reflects the error object thrown during the fetch request.\\nIt can be used to display an error message, or to disable the submit button:\\n\\n<Note>\\n  We recommend showing a generic error message to the user, such as \"Something\\n  went wrong.\" This is a good practice to avoid leaking information from the\\n  server.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"6,13\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { error, object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <>\\n      {error && <div>An error occurred.</div>}\\n\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </>\\n  );\\n}\\n```\\n\\n## Event Callbacks\\n\\n`useObject` provides optional event callbacks that you can use to handle life-cycle events.\\n\\n- `onFinish`: Called when the object generation is completed.\\n- `onError`: Called when an error occurs during the fetch request.\\n\\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"10-20\"\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\nimport { notificationSchema } from \\'./api/notifications/schema\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/notifications\\',\\n    schema: notificationSchema,\\n    onFinish({ object, error }) {\\n      // typed object, undefined if schema validation fails:\\n      console.log(\\'Object generation completed:\\', object);\\n\\n      // error, undefined if schema validation succeeds:\\n      console.log(\\'Schema validation error:\\', error);\\n    },\\n    onError(error) {\\n      // error during fetch request:\\n      console.error(\\'An error occurred:\\', error);\\n    },\\n  });\\n\\n  return (\\n    <div>\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n## Configure Request Options\\n\\nYou can configure the API endpoint, optional headers and credentials using the `api`, `headers` and `credentials` settings.\\n\\n```tsx highlight=\"2-5\"\\nconst { submit, object } = useObject({\\n  api: \\'/api/use-object\\',\\n  headers: {\\n    \\'X-Custom-Header\\': \\'CustomValue\\',\\n  },\\n  credentials: \\'include\\',\\n  schema: yourSchema,\\n});\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/20-streaming-data.mdx'), name='20-streaming-data.mdx', displayName='20-streaming-data.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming Custom Data\\ndescription: Learn how to stream custom data from the server to the client.\\n---\\n\\n# Streaming Custom Data\\n\\nIt is often useful to send additional data alongside the model\\'s response.\\nFor example, you may want to send status information, the message ids after storing them,\\nor references to content that the language model is referring to.\\n\\nThe AI SDK provides several helpers that allows you to stream additional data to the client\\nand attach it to the `UIMessage` parts array:\\n\\n- `createUIMessageStream`: creates a data stream\\n- `createUIMessageStreamResponse`: creates a response object that streams data\\n- `pipeUIMessageStreamToResponse`: pipes a data stream to a server response object\\n\\nThe data is streamed as part of the response stream using Server-Sent Events.\\n\\n## Setting Up Type-Safe Data Streaming\\n\\nFirst, define your custom message type with data part schemas for type safety:\\n\\n```tsx filename=\"ai/types.ts\"\\nimport { UIMessage } from \\'ai\\';\\n\\n// Define your custom message type with data part schemas\\nexport type MyUIMessage = UIMessage<\\n  never, // metadata type\\n  {\\n    weather: {\\n      city: string;\\n      weather?: string;\\n      status: \\'loading\\' | \\'success\\';\\n    };\\n    notification: {\\n      message: string;\\n      level: \\'info\\' | \\'warning\\' | \\'error\\';\\n    };\\n  } // data parts type\\n>;\\n```\\n\\n## Streaming Data from the Server\\n\\nIn your server-side route handler, you can create a `UIMessageStream` and then pass it to `createUIMessageStreamResponse`:\\n\\n```tsx filename=\"route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport {\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n  streamText,\\n  convertToModelMessages,\\n} from \\'ai\\';\\nimport type { MyUIMessage } from \\'@/ai/types\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const stream = createUIMessageStream<MyUIMessage>({\\n    execute: ({ writer }) => {\\n      // 1. Send initial status (transient - won\\'t be added to message history)\\n      writer.write({\\n        type: \\'data-notification\\',\\n        data: { message: \\'Processing your request...\\', level: \\'info\\' },\\n        transient: true, // This part won\\'t be added to message history\\n      });\\n\\n      // 2. Send sources (useful for RAG use cases)\\n      writer.write({\\n        type: \\'source\\',\\n        value: {\\n          type: \\'source\\',\\n          sourceType: \\'url\\',\\n          id: \\'source-1\\',\\n          url: \\'https://weather.com\\',\\n          title: \\'Weather Data Source\\',\\n        },\\n      });\\n\\n      // 3. Send data parts with loading state\\n      writer.write({\\n        type: \\'data-weather\\',\\n        id: \\'weather-1\\',\\n        data: { city: \\'San Francisco\\', status: \\'loading\\' },\\n      });\\n\\n      const result = streamText({\\n        model: \\'anthropic/claude-sonnet-4.5\\',\\n        messages: convertToModelMessages(messages),\\n        onFinish() {\\n          // 4. Update the same data part (reconciliation)\\n          writer.write({\\n            type: \\'data-weather\\',\\n            id: \\'weather-1\\', // Same ID = update existing part\\n            data: {\\n              city: \\'San Francisco\\',\\n              weather: \\'sunny\\',\\n              status: \\'success\\',\\n            },\\n          });\\n\\n          // 5. Send completion notification (transient)\\n          writer.write({\\n            type: \\'data-notification\\',\\n            data: { message: \\'Request completed\\', level: \\'info\\' },\\n            transient: true, // Won\\'t be added to message history\\n          });\\n        },\\n      });\\n\\n      writer.merge(result.toUIMessageStream());\\n    },\\n  });\\n\\n  return createUIMessageStreamResponse({ stream });\\n}\\n```\\n\\n<Note>\\n  You can also send stream data from custom backends, e.g. Python / FastAPI,\\n  using the [UI Message Stream\\n  Protocol](/docs/ai-sdk-ui/stream-protocol#ui-message-stream-protocol).\\n</Note>\\n\\n## Types of Streamable Data\\n\\n### Data Parts (Persistent)\\n\\nRegular data parts are added to the message history and appear in `message.parts`:\\n\\n```tsx\\nwriter.write({\\n  type: \\'data-weather\\',\\n  id: \\'weather-1\\', // Optional: enables reconciliation\\n  data: { city: \\'San Francisco\\', status: \\'loading\\' },\\n});\\n```\\n\\n### Sources\\n\\nSources are useful for RAG implementations where you want to show which documents or URLs were referenced:\\n\\n```tsx\\nwriter.write({\\n  type: \\'source\\',\\n  value: {\\n    type: \\'source\\',\\n    sourceType: \\'url\\',\\n    id: \\'source-1\\',\\n    url: \\'https://example.com\\',\\n    title: \\'Example Source\\',\\n  },\\n});\\n```\\n\\n### Transient Data Parts (Ephemeral)\\n\\nTransient parts are sent to the client but not added to the message history. They are only accessible via the `onData` useChat handler:\\n\\n```tsx\\n// server\\nwriter.write({\\n  type: \\'data-notification\\',\\n  data: { message: \\'Processing...\\', level: \\'info\\' },\\n  transient: true, // Won\\'t be added to message history\\n});\\n\\n// client\\nconst [notification, setNotification] = useState();\\n\\nconst { messages } = useChat({\\n  onData: ({ data, type }) => {\\n    if (type === \\'data-notification\\') {\\n      setNotification({ message: data.message, level: data.level });\\n    }\\n  },\\n});\\n```\\n\\n## Data Part Reconciliation\\n\\nWhen you write to a data part with the same ID, the client automatically reconciles and updates that part. This enables powerful dynamic experiences like:\\n\\n- **Collaborative artifacts** - Update code, documents, or designs in real-time\\n- **Progressive data loading** - Show loading states that transform into final results\\n- **Live status updates** - Update progress bars, counters, or status indicators\\n- **Interactive components** - Build UI elements that evolve based on user interaction\\n\\nThe reconciliation happens automatically - simply use the same `id` when writing to the stream.\\n\\n## Processing Data on the Client\\n\\n### Using the onData Callback\\n\\nThe `onData` callback is essential for handling streaming data, especially transient parts:\\n\\n```tsx filename=\"page.tsx\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport type { MyUIMessage } from \\'@/ai/types\\';\\n\\nconst { messages } = useChat<MyUIMessage>({\\n  api: \\'/api/chat\\',\\n  onData: dataPart => {\\n    // Handle all data parts as they arrive (including transient parts)\\n    console.log(\\'Received data part:\\', dataPart);\\n\\n    // Handle different data part types\\n    if (dataPart.type === \\'data-weather\\') {\\n      console.log(\\'Weather update:\\', dataPart.data);\\n    }\\n\\n    // Handle transient notifications (ONLY available here, not in message.parts)\\n    if (dataPart.type === \\'data-notification\\') {\\n      showToast(dataPart.data.message, dataPart.data.level);\\n    }\\n  },\\n});\\n```\\n\\n**Important:** Transient data parts are **only** available through the `onData` callback. They will not appear in the `message.parts` array since they\\'re not added to message history.\\n\\n### Rendering Persistent Data Parts\\n\\nYou can filter and render data parts from the message parts array:\\n\\n```tsx filename=\"page.tsx\"\\nconst result = (\\n  <>\\n    {messages?.map(message => (\\n      <div key={message.id}>\\n        {/* Render weather data parts */}\\n        {message.parts\\n          .filter(part => part.type === \\'data-weather\\')\\n          .map((part, index) => (\\n            <div key={index} className=\"weather-widget\">\\n              {part.data.status === \\'loading\\' ? (\\n                <>Getting weather for {part.data.city}...</>\\n              ) : (\\n                <>\\n                  Weather in {part.data.city}: {part.data.weather}\\n                </>\\n              )}\\n            </div>\\n          ))}\\n\\n        {/* Render text content */}\\n        {message.parts\\n          .filter(part => part.type === \\'text\\')\\n          .map((part, index) => (\\n            <div key={index}>{part.text}</div>\\n          ))}\\n\\n        {/* Render sources */}\\n        {message.parts\\n          .filter(part => part.type === \\'source\\')\\n          .map((part, index) => (\\n            <div key={index} className=\"source\">\\n              Source: <a href={part.url}>{part.title}</a>\\n            </div>\\n          ))}\\n      </div>\\n    ))}\\n  </>\\n);\\n```\\n\\n### Complete Example\\n\\n```tsx filename=\"page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\nimport type { MyUIMessage } from \\'@/ai/types\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n\\n  const { messages, sendMessage } = useChat<MyUIMessage>({\\n    api: \\'/api/chat\\',\\n    onData: dataPart => {\\n      // Handle transient notifications\\n      if (dataPart.type === \\'data-notification\\') {\\n        console.log(\\'Notification:\\', dataPart.data.message);\\n      }\\n    },\\n  });\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <>\\n      {messages?.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n\\n          {/* Render weather data */}\\n          {message.parts\\n            .filter(part => part.type === \\'data-weather\\')\\n            .map((part, index) => (\\n              <span key={index} className=\"weather-update\">\\n                {part.data.status === \\'loading\\' ? (\\n                  <>Getting weather for {part.data.city}...</>\\n                ) : (\\n                  <>\\n                    Weather in {part.data.city}: {part.data.weather}\\n                  </>\\n                )}\\n              </span>\\n            ))}\\n\\n          {/* Render text content */}\\n          {message.parts\\n            .filter(part => part.type === \\'text\\')\\n            .map((part, index) => (\\n              <div key={index}>{part.text}</div>\\n            ))}\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          placeholder=\"Ask about the weather...\"\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\n## Use Cases\\n\\n- **RAG Applications** - Stream sources and retrieved documents\\n- **Real-time Status** - Show loading states and progress updates\\n- **Collaborative Tools** - Stream live updates to shared artifacts\\n- **Analytics** - Send usage data without cluttering message history\\n- **Notifications** - Display temporary alerts and status messages\\n\\n## Message Metadata vs Data Parts\\n\\nBoth [message metadata](/docs/ai-sdk-ui/message-metadata) and data parts allow you to send additional information alongside messages, but they serve different purposes:\\n\\n### Message Metadata\\n\\nMessage metadata is best for **message-level information** that describes the message as a whole:\\n\\n- Attached at the message level via `message.metadata`\\n- Sent using the `messageMetadata` callback in `toUIMessageStreamResponse`\\n- Ideal for: timestamps, model info, token usage, user context\\n- Type-safe with custom metadata types\\n\\n```ts\\n// Server: Send metadata about the message\\nreturn result.toUIMessageStreamResponse({\\n  messageMetadata: ({ part }) => {\\n    if (part.type === \\'finish\\') {\\n      return {\\n        model: part.response.modelId,\\n        totalTokens: part.totalUsage.totalTokens,\\n        createdAt: Date.now(),\\n      };\\n    }\\n  },\\n});\\n```\\n\\n### Data Parts\\n\\nData parts are best for streaming **dynamic arbitrary data**:\\n\\n- Added to the message parts array via `message.parts`\\n- Streamed using `createUIMessageStream` and `writer.write()`\\n- Can be reconciled/updated using the same ID\\n- Support transient parts that don\\'t persist\\n- Ideal for: dynamic content, loading states, interactive components\\n\\n```ts\\n// Server: Stream data as part of message content\\nwriter.write({\\n  type: \\'data-weather\\',\\n  id: \\'weather-1\\',\\n  data: { city: \\'San Francisco\\', status: \\'loading\\' },\\n});\\n```\\n\\nFor more details on message metadata, see the [Message Metadata documentation](/docs/ai-sdk-ui/message-metadata).\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/21-error-handling.mdx'), name='21-error-handling.mdx', displayName='21-error-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Error Handling\\ndescription: Learn how to handle errors in the AI SDK UI\\n---\\n\\n# Error Handling and warnings\\n\\n## Warnings\\n\\nThe AI SDK shows warnings when something might not work as expected.\\nThese warnings help you fix problems before they cause errors.\\n\\n### When Warnings Appear\\n\\nWarnings are shown in the browser console when:\\n\\n- **Unsupported features**: You use a feature or setting that is not supported by the AI model (e.g., certain options or parameters).\\n- **Compatibility warnings**: A feature is used in a compatibility mode, which might work differently or less optimally than intended.\\n- **Other warnings**: The AI model reports another type of issue, such as general problems or advisory messages.\\n\\n### Warning Messages\\n\\nAll warnings start with \"AI SDK Warning:\" so you can easily find them. For example:\\n\\n```\\nAI SDK Warning: The feature \"temperature\" is not supported by this model\\n```\\n\\n### Turning Off Warnings\\n\\nBy default, warnings are shown in the console. You can control this behavior:\\n\\n#### Turn Off All Warnings\\n\\nSet a global variable to turn off warnings completely:\\n\\n```ts\\nglobalThis.AI_SDK_LOG_WARNINGS = false;\\n```\\n\\n#### Custom Warning Handler\\n\\nYou can also provide your own function to handle warnings.\\nIt receives provider id, model id, and a list of warnings.\\n\\n```ts\\nglobalThis.AI_SDK_LOG_WARNINGS = ({ warnings, provider, model }) => {\\n  // Handle warnings your own way\\n};\\n```\\n\\n## Error Handling\\n\\n### Error Helper Object\\n\\nEach AI SDK UI hook also returns an [error](/docs/reference/ai-sdk-ui/use-chat#error) object that you can use to render the error in your UI.\\nYou can use the error object to show an error message, disable the submit button, or show a retry button.\\n\\n<Note>\\n  We recommend showing a generic error message to the user, such as \"Something\\n  went wrong.\" This is a good practice to avoid leaking information from the\\n  server.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"7,18-25,31\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage, error, regenerate } = useChat();\\n\\n  const handleSubmit = (e: React.FormEvent) => {\\n    e.preventDefault();\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  };\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts\\n            .filter(part => part.type === \\'text\\')\\n            .map(part => part.text)\\n            .join(\\'\\')}\\n        </div>\\n      ))}\\n\\n      {error && (\\n        <>\\n          <div>An error occurred.</div>\\n          <button type=\"button\" onClick={() => regenerate()}>\\n            Retry\\n          </button>\\n        </>\\n      )}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          value={input}\\n          onChange={e => setInput(e.target.value)}\\n          disabled={error != null}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n#### Alternative: replace last message\\n\\nAlternatively you can write a custom submit handler that replaces the last message when an error is present.\\n\\n```tsx file=\"app/page.tsx\" highlight=\"17-23,35\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { sendMessage, error, messages, setMessages } = useChat();\\n\\n  function customSubmit(event: React.FormEvent<HTMLFormElement>) {\\n    event.preventDefault();\\n\\n    if (error != null) {\\n      setMessages(messages.slice(0, -1)); // remove last message\\n    }\\n\\n    sendMessage({ text: input });\\n    setInput(\\'\\');\\n  }\\n\\n  return (\\n    <div>\\n      {messages.map(m => (\\n        <div key={m.id}>\\n          {m.role}:{\\' \\'}\\n          {m.parts\\n            .filter(part => part.type === \\'text\\')\\n            .map(part => part.text)\\n            .join(\\'\\')}\\n        </div>\\n      ))}\\n\\n      {error && <div>An error occurred.</div>}\\n\\n      <form onSubmit={customSubmit}>\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Error Handling Callback\\n\\nErrors can be processed by passing an [`onError`](/docs/reference/ai-sdk-ui/use-chat#on-error) callback function as an option to the [`useChat`](/docs/reference/ai-sdk-ui/use-chat) or [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) hooks.\\nThe callback function receives an error object as an argument.\\n\\n```tsx file=\"app/page.tsx\" highlight=\"6-9\"\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const {\\n    /* ... */\\n  } = useChat({\\n    // handle error:\\n    onError: error => {\\n      console.error(error);\\n    },\\n  });\\n}\\n```\\n\\n### Injecting Errors for Testing\\n\\nYou might want to create errors for testing.\\nYou can easily do so by throwing an error in your route handler:\\n\\n```ts file=\"app/api/chat/route.ts\"\\nexport async function POST(req: Request) {\\n  throw new Error(\\'This is a test error\\');\\n}\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/21-transport.mdx'), name='21-transport.mdx', displayName='21-transport.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Transport\\ndescription: Learn how to use custom transports with useChat.\\n---\\n\\n# Transport\\n\\nThe `useChat` transport system provides fine-grained control over how messages are sent to your API endpoints and how responses are processed. This is particularly useful for alternative communication protocols like WebSockets, custom authentication patterns, or specialized backend integrations.\\n\\n## Default Transport\\n\\nBy default, `useChat` uses HTTP POST requests to send messages to `/api/chat`:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\n\\n// Uses default HTTP transport\\nconst { messages, sendMessage } = useChat();\\n```\\n\\nThis is equivalent to:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n  }),\\n});\\n```\\n\\n## Custom Transport Configuration\\n\\nConfigure the default transport with custom options:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/custom-chat',\\n    headers: {\\n      Authorization: 'Bearer your-token',\\n      'X-API-Version': '2024-01',\\n    },\\n    credentials: 'include',\\n  }),\\n});\\n```\\n\\n### Dynamic Configuration\\n\\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: () => ({\\n      Authorization: `Bearer ${getAuthToken()}`,\\n      'X-User-ID': getCurrentUserId(),\\n    }),\\n    body: () => ({\\n      sessionId: getCurrentSessionId(),\\n      preferences: getUserPreferences(),\\n    }),\\n    credentials: () => 'include',\\n  }),\\n});\\n```\\n\\n### Request Transformation\\n\\nTransform requests before sending to your API:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\\n      return {\\n        headers: {\\n          'X-Session-ID': id,\\n        },\\n        body: {\\n          messages: messages.slice(-10), // Only send last 10 messages\\n          trigger,\\n          messageId,\\n        },\\n      };\\n    },\\n  }),\\n});\\n```\\n\\n## Building Custom Transports\\n\\nTo understand how to build your own transport, refer to the source code of the default implementation:\\n\\n- **[DefaultChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/default-chat-transport.ts)** - The complete default HTTP transport implementation\\n- **[HttpChatTransport](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/http-chat-transport.ts)** - Base HTTP transport with request handling\\n- **[ChatTransport Interface](https://github.com/vercel/ai/blob/main/packages/ai/src/ui/chat-transport.ts)** - The transport interface you need to implement\\n\\nThese implementations show you exactly how to:\\n\\n- Handle the `sendMessages` method\\n- Process UI message streams\\n- Transform requests and responses\\n- Handle errors and connection management\\n\\nThe transport system gives you complete control over how your chat application communicates, enabling integration with any backend protocol or service.\\n\", children=[]), DocItem(origPath=Path('04-ai-sdk-ui/24-reading-ui-message-streams.mdx'), name='24-reading-ui-message-streams.mdx', displayName='24-reading-ui-message-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Reading UIMessage Streams\\ndescription: Learn how to read UIMessage streams.\\n---\\n\\n# Reading UI Message Streams\\n\\n`UIMessage` streams are useful outside of traditional chat use cases. You can consume them for terminal UIs, custom stream processing on the client, or React Server Components (RSC).\\n\\nThe `readUIMessageStream` helper transforms a stream of `UIMessageChunk` objects into an `AsyncIterableStream` of `UIMessage` objects, allowing you to process messages as they're being constructed.\\n\\n## Basic Usage\\n\\n```tsx\\nimport { openai } from '@ai-sdk/openai';\\nimport { readUIMessageStream, streamText } from 'ai';\\n\\nasync function main() {\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    prompt: 'Write a short story about a robot.',\\n  });\\n\\n  for await (const uiMessage of readUIMessageStream({\\n    stream: result.toUIMessageStream(),\\n  })) {\\n    console.log('Current message state:', uiMessage);\\n  }\\n}\\n```\\n\\n## Tool Calls Integration\\n\\nHandle streaming responses that include tool calls:\\n\\n```tsx\\nimport { openai } from '@ai-sdk/openai';\\nimport { readUIMessageStream, streamText, tool } from 'ai';\\nimport { z } from 'zod';\\n\\nasync function handleToolCalls() {\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    tools: {\\n      weather: tool({\\n        description: 'Get the weather in a location',\\n        inputSchema: z.object({\\n          location: z.string().describe('The location to get the weather for'),\\n        }),\\n        execute: ({ location }) => ({\\n          location,\\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n        }),\\n      }),\\n    },\\n    prompt: 'What is the weather in Tokyo?',\\n  });\\n\\n  for await (const uiMessage of readUIMessageStream({\\n    stream: result.toUIMessageStream(),\\n  })) {\\n    // Handle different part types\\n    uiMessage.parts.forEach(part => {\\n      switch (part.type) {\\n        case 'text':\\n          console.log('Text:', part.text);\\n          break;\\n        case 'tool-call':\\n          console.log('Tool called:', part.toolName, 'with args:', part.args);\\n          break;\\n        case 'tool-result':\\n          console.log('Tool result:', part.result);\\n          break;\\n      }\\n    });\\n  }\\n}\\n```\\n\\n## Resuming Conversations\\n\\nResume streaming from a previous message state:\\n\\n```tsx\\nimport { readUIMessageStream, streamText } from 'ai';\\n\\nasync function resumeConversation(lastMessage: UIMessage) {\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages: [\\n      { role: 'user', content: 'Continue our previous conversation.' },\\n    ],\\n  });\\n\\n  // Resume from the last message\\n  for await (const uiMessage of readUIMessageStream({\\n    stream: result.toUIMessageStream(),\\n    message: lastMessage, // Resume from this message\\n  })) {\\n    console.log('Resumed message:', uiMessage);\\n  }\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('04-ai-sdk-ui/25-message-metadata.mdx'), name='25-message-metadata.mdx', displayName='25-message-metadata.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Message Metadata\\ndescription: Learn how to attach and use metadata with messages in AI SDK UI\\n---\\n\\n# Message Metadata\\n\\nMessage metadata allows you to attach custom information to messages at the message level. This is useful for tracking timestamps, model information, token usage, user context, and other message-level data.\\n\\n## Overview\\n\\nMessage metadata differs from [data parts](/docs/ai-sdk-ui/streaming-data) in that it\\'s attached at the message level rather than being part of the message content. While data parts are ideal for dynamic content that forms part of the message, metadata is perfect for information about the message itself.\\n\\n## Getting Started\\n\\nHere\\'s a simple example of using message metadata to track timestamps and model information:\\n\\n### Defining Metadata Types\\n\\nFirst, define your metadata type for type safety:\\n\\n```tsx filename=\"app/types.ts\"\\nimport { UIMessage } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Define your metadata schema\\nexport const messageMetadataSchema = z.object({\\n  createdAt: z.number().optional(),\\n  model: z.string().optional(),\\n  totalTokens: z.number().optional(),\\n});\\n\\nexport type MessageMetadata = z.infer<typeof messageMetadataSchema>;\\n\\n// Create a typed UIMessage\\nexport type MyUIMessage = UIMessage<MessageMetadata>;\\n```\\n\\n### Sending Metadata from the Server\\n\\nUse the `messageMetadata` callback in `toUIMessageStreamResponse` to send metadata at different streaming stages:\\n\\n```ts filename=\"app/api/chat/route.ts\" highlight=\"11-20\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\nimport type { MyUIMessage } from \\'@/types\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages, // pass this in for type-safe return objects\\n    messageMetadata: ({ part }) => {\\n      // Send metadata when streaming starts\\n      if (part.type === \\'start\\') {\\n        return {\\n          createdAt: Date.now(),\\n          model: \\'gpt-5.1\\',\\n        };\\n      }\\n\\n      // Send additional metadata when streaming completes\\n      if (part.type === \\'finish\\') {\\n        return {\\n          totalTokens: part.totalUsage.totalTokens,\\n        };\\n      }\\n    },\\n  });\\n}\\n```\\n\\n<Note>\\n  To enable type-safe metadata return object in `messageMetadata`, pass in the\\n  `originalMessages` parameter typed to your UIMessage type.\\n</Note>\\n\\n### Accessing Metadata on the Client\\n\\nAccess metadata through the `message.metadata` property:\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"8,18-23\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { DefaultChatTransport } from \\'ai\\';\\nimport type { MyUIMessage } from \\'@/types\\';\\n\\nexport default function Chat() {\\n  const { messages } = useChat<MyUIMessage>({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>\\n            {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n            {message.metadata?.createdAt && (\\n              <span className=\"text-sm text-gray-500\">\\n                {new Date(message.metadata.createdAt).toLocaleTimeString()}\\n              </span>\\n            )}\\n          </div>\\n\\n          {/* Render message content */}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <div key={index}>{part.text}</div> : null,\\n          )}\\n\\n          {/* Display additional metadata */}\\n          {message.metadata?.totalTokens && (\\n            <div className=\"text-xs text-gray-400\">\\n              {message.metadata.totalTokens} tokens\\n            </div>\\n          )}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n<Note>\\n  For streaming arbitrary data that changes during generation, consider using\\n  [data parts](/docs/ai-sdk-ui/streaming-data) instead.\\n</Note>\\n\\n## Common Use Cases\\n\\nMessage metadata is ideal for:\\n\\n- **Timestamps**: When messages were created or completed\\n- **Model Information**: Which AI model was used\\n- **Token Usage**: Track costs and usage limits\\n- **User Context**: User IDs, session information\\n- **Performance Metrics**: Generation time, time to first token\\n- **Quality Indicators**: Finish reason, confidence scores\\n\\n## See Also\\n\\n- [Chatbot Guide](/docs/ai-sdk-ui/chatbot#message-metadata) - Message metadata in the context of building chatbots\\n- [Streaming Data](/docs/ai-sdk-ui/streaming-data#message-metadata-vs-data-parts) - Comparison with data parts\\n- [UIMessage Reference](/docs/reference/ai-sdk-core/ui-message) - Complete UIMessage type reference\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/50-stream-protocol.mdx'), name='50-stream-protocol.mdx', displayName='50-stream-protocol.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stream Protocols\\ndescription: Learn more about the supported stream protocols in the AI SDK.\\n---\\n\\n# Stream Protocols\\n\\nAI SDK UI functions such as `useChat` and `useCompletion` support both text streams and data streams.\\nThe stream protocol defines how the data is streamed to the frontend on top of the HTTP protocol.\\n\\nThis page describes both protocols and how to use them in the backend and frontend.\\n\\nYou can use this information to develop custom backends and frontends for your use case, e.g.,\\nto provide compatible API endpoints that are implemented in a different language such as Python.\\n\\nFor instance, here\\'s an example using [FastAPI](https://github.com/vercel/ai/tree/main/examples/next-fastapi) as a backend.\\n\\n## Text Stream Protocol\\n\\nA text stream contains chunks in plain text, that are streamed to the frontend.\\nEach chunk is then appended together to form a full text response.\\n\\nText streams are supported by `useChat`, `useCompletion`, and `useObject`.\\nWhen you use `useChat` or `useCompletion`, you need to enable text streaming\\nby setting the `streamProtocol` options to `text`.\\n\\nYou can generate text streams with `streamText` in the backend.\\nWhen you call `toTextStreamResponse()` on the result object,\\na streaming HTTP response is returned.\\n\\n<Note>\\n  Text streams only support basic text data. If you need to stream other types\\n  of data such as tool calls, use data streams.\\n</Note>\\n\\n### Text Stream Example\\n\\nHere is a Next.js example that uses the text stream protocol:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { TextStreamChatTransport } from \\'ai\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat({\\n    transport: new TextStreamChatTransport({ api: \\'/api/chat\\' }),\\n  });\\n\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/chat/route.ts\\'\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## Data Stream Protocol\\n\\nA data stream follows a special protocol that the AI SDK provides to send information to the frontend.\\n\\nThe data stream protocol uses Server-Sent Events (SSE) format for improved standardization, keep-alive through ping, reconnect capabilities, and better cache handling.\\n\\n<Note>\\n  When you provide data streams from a custom backend, you need to set the\\n  `x-vercel-ai-ui-message-stream` header to `v1`.\\n</Note>\\n\\nThe following stream parts are currently supported:\\n\\n### Message Start Part\\n\\nIndicates the beginning of a new message with metadata.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"start\",\"messageId\":\"...\"}\\n\\n```\\n\\n### Text Parts\\n\\nText content is streamed using a start/delta/end pattern with unique IDs for each text block.\\n\\n#### Text Start Part\\n\\nIndicates the beginning of a text block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"text-start\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\\n\\n```\\n\\n#### Text Delta Part\\n\\nContains incremental text content for the text block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"text-delta\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\",\"delta\":\"Hello\"}\\n\\n```\\n\\n#### Text End Part\\n\\nIndicates the completion of a text block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"text-end\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\\n\\n```\\n\\n### Reasoning Parts\\n\\nReasoning content is streamed using a start/delta/end pattern with unique IDs for each reasoning block.\\n\\n#### Reasoning Start Part\\n\\nIndicates the beginning of a reasoning block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"reasoning-start\",\"id\":\"reasoning_123\"}\\n\\n```\\n\\n#### Reasoning Delta Part\\n\\nContains incremental reasoning content for the reasoning block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"reasoning-delta\",\"id\":\"reasoning_123\",\"delta\":\"This is some reasoning\"}\\n\\n```\\n\\n#### Reasoning End Part\\n\\nIndicates the completion of a reasoning block.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"reasoning-end\",\"id\":\"reasoning_123\"}\\n\\n```\\n\\n### Source Parts\\n\\nSource parts provide references to external content sources.\\n\\n#### Source URL Part\\n\\nReferences to external URLs.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"source-url\",\"sourceId\":\"https://example.com\",\"url\":\"https://example.com\"}\\n\\n```\\n\\n#### Source Document Part\\n\\nReferences to documents or files.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"source-document\",\"sourceId\":\"https://example.com\",\"mediaType\":\"file\",\"title\":\"Title\"}\\n\\n```\\n\\n### File Part\\n\\nThe file parts contain references to files with their media type.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"file\",\"url\":\"https://example.com/file.png\",\"mediaType\":\"image/png\"}\\n\\n```\\n\\n### Data Parts\\n\\nCustom data parts allow streaming of arbitrary structured data with type-specific handling.\\n\\nFormat: Server-Sent Event with JSON object where the type includes a custom suffix\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"data-weather\",\"data\":{\"location\":\"SF\",\"temperature\":100}}\\n\\n```\\n\\nThe `data-*` type pattern allows you to define custom data types that your frontend can handle specifically.\\n\\n### Error Part\\n\\nThe error parts are appended to the message as they are received.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"error\",\"errorText\":\"error message\"}\\n\\n```\\n\\n### Tool Input Start Part\\n\\nIndicates the beginning of tool input streaming.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-input-start\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\"}\\n\\n```\\n\\n### Tool Input Delta Part\\n\\nIncremental chunks of tool input as it\\'s being generated.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-input-delta\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"inputTextDelta\":\"San Francisco\"}\\n\\n```\\n\\n### Tool Input Available Part\\n\\nIndicates that tool input is complete and ready for execution.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-input-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\",\"input\":{\"city\":\"San Francisco\"}}\\n\\n```\\n\\n### Tool Output Available Part\\n\\nContains the result of tool execution.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"tool-output-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"output\":{\"city\":\"San Francisco\",\"weather\":\"sunny\"}}\\n\\n```\\n\\n### Start Step Part\\n\\nA part indicating the start of a step.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"start-step\"}\\n\\n```\\n\\n### Finish Step Part\\n\\nA part indicating that a step (i.e., one LLM API call in the backend) has been completed.\\n\\nThis part is necessary to correctly process multiple stitched assistant calls, e.g. when calling tools in the backend, and using steps in `useChat` at the same time.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"finish-step\"}\\n\\n```\\n\\n### Finish Message Part\\n\\nA part indicating the completion of a message.\\n\\nFormat: Server-Sent Event with JSON object\\n\\nExample:\\n\\n```\\ndata: {\"type\":\"finish\"}\\n\\n```\\n\\n### Stream Termination\\n\\nThe stream ends with a special `[DONE]` marker.\\n\\nFormat: Server-Sent Event with literal `[DONE]`\\n\\nExample:\\n\\n```\\ndata: [DONE]\\n\\n```\\n\\nThe data stream protocol is supported\\nby `useChat` and `useCompletion` on the frontend and used by default.\\n`useCompletion` only supports the `text` and `data` stream parts.\\n\\nOn the backend, you can use `toUIMessageStreamResponse()` from the `streamText` result object to return a streaming HTTP response.\\n\\n### UI Message Stream Example\\n\\nHere is a Next.js example that uses the UI message stream protocol:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { useState } from \\'react\\';\\n\\nexport default function Chat() {\\n  const [input, setInput] = useState(\\'\\');\\n  const { messages, sendMessage } = useChat();\\n\\n  return (\\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\\n      {messages.map(message => (\\n        <div key={message.id} className=\"whitespace-pre-wrap\">\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, i) => {\\n            switch (part.type) {\\n              case \\'text\\':\\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\\n            }\\n          })}\\n        </div>\\n      ))}\\n\\n      <form\\n        onSubmit={e => {\\n          e.preventDefault();\\n          sendMessage({ text: input });\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\\n          value={input}\\n          placeholder=\"Say something...\"\\n          onChange={e => setInput(e.currentTarget.value)}\\n        />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n```ts filename=\\'app/api/chat/route.ts\\'\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText, UIMessage, convertToModelMessages } from \\'ai\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n', children=[]), DocItem(origPath=Path('04-ai-sdk-ui/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK UI\\ndescription: Learn about the AI SDK UI.\\n---\\n\\n# AI SDK UI\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'Overview',\\n      description: 'Get an overview about the AI SDK UI.',\\n      href: '/docs/ai-sdk-ui/overview'\\n    },\\n    {\\n      title: 'Chatbot',\\n      description: 'Learn how to integrate an interface for a chatbot.',\\n      href: '/docs/ai-sdk-ui/chatbot'\\n    },\\n    {\\n      title: 'Chatbot Message Persistence',\\n      description: 'Learn how to store and load chat messages in a chatbot.',\\n      href: '/docs/ai-sdk-ui/chatbot-message-persistence'\\n    },\\n    {\\n      title: 'Chatbot Tool Usage',\\n      description:\\n        'Learn how to integrate an interface for a chatbot with tool calling.',\\n      href: '/docs/ai-sdk-ui/chatbot-tool-usage'\\n    },\\n    {\\n      title: 'Completion',\\n      description: 'Learn how to integrate an interface for text completion.',\\n      href: '/docs/ai-sdk-ui/completion'\\n    },\\n    {\\n      title: 'Object Generation',\\n      description: 'Learn how to integrate an interface for object generation.',\\n      href: '/docs/ai-sdk-ui/object-generation'\\n    },\\n    {\\n      title: 'Streaming Data',\\n      description: 'Learn how to stream data.',\\n      href: '/docs/ai-sdk-ui/streaming-data'\\n    },\\n    {\\n      title: 'Reading UI Message Streams',\\n      description: 'Learn how to read UIMessage streams for terminal UIs, custom clients, and server components.',\\n      href: '/docs/ai-sdk-ui/reading-ui-message-streams'\\n    },\\n    {\\n      title: 'Error Handling',\\n      description: 'Learn how to handle errors.',\\n      href: '/docs/ai-sdk-ui/error-handling'\\n    },\\n    {\\n      title: 'Stream Protocol',\\n      description:\\n        'The stream protocol defines how data is sent from the backend to the AI SDK UI frontend.',\\n      href: '/docs/ai-sdk-ui/stream-protocol'\\n    }\\n\\n]}\\n/>\\n\", children=[])]),\n",
       " DocItem(origPath=Path('05-ai-sdk-rsc'), name='05-ai-sdk-rsc', displayName='05-ai-sdk-rsc', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('05-ai-sdk-rsc/01-overview.mdx'), name='01-overview.mdx', displayName='01-overview.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Overview\\ndescription: An overview of AI SDK RSC.\\n---\\n\\n# AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n<Note>\\n  The `@ai-sdk/rsc` package is compatible with frameworks that support React\\n  Server Components.\\n</Note>\\n\\n[React Server Components](https://nextjs.org/docs/app/building-your-application/rendering/server-components) (RSC) allow you to write UI that can be rendered on the server and streamed to the client. RSCs enable [ Server Actions ](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components), a new way to call server-side code directly from the client just like any other function with end-to-end type-safety. This combination opens the door to a new way of building AI applications, allowing the large language model (LLM) to generate and stream UI directly from the server to the client.\\n\\n## AI SDK RSC Functions\\n\\nAI SDK RSC has various functions designed to help you build AI-native applications with React Server Components. These functions:\\n\\n1. Provide abstractions for building Generative UI applications.\\n   - [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui): calls a model and allows it to respond with React Server Components.\\n   - [`useUIState`](/docs/reference/ai-sdk-rsc/use-ui-state): returns the current UI state and a function to update the UI State (like React\\'s `useState`). UI State is the visual representation of the AI state.\\n   - [`useAIState`](/docs/reference/ai-sdk-rsc/use-ai-state): returns the current AI state and a function to update the AI State (like React\\'s `useState`). The AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.\\n   - [`useActions`](/docs/reference/ai-sdk-rsc/use-actions): provides access to your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.\\n   - [`createAI`](/docs/reference/ai-sdk-rsc/create-ai): creates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.\\n2. Make it simple to work with streamable values between the server and client.\\n   - [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value): creates a stream that sends values from the server to the client. The value can be any serializable data.\\n   - [`readStreamableValue`](/docs/reference/ai-sdk-rsc/read-streamable-value): reads a streamable value from the client that was originally created using `createStreamableValue`.\\n   - [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui): creates a stream that sends UI from the server to the client.\\n   - [`useStreamableValue`](/docs/reference/ai-sdk-rsc/use-streamable-value): accepts a streamable value created using `createStreamableValue` and returns the current value, error, and pending state.\\n\\n## Templates\\n\\nCheck out the following templates to see AI SDK RSC in action.\\n\\n<Templates type=\"generative-ui\" />\\n\\n## API Reference\\n\\nPlease check out the [AI SDK RSC API Reference](/docs/reference/ai-sdk-rsc) for more details on each function.\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/02-streaming-react-components.mdx'), name='02-streaming-react-components.mdx', displayName='02-streaming-react-components.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming React Components\\ndescription: Overview of streaming RSCs\\n---\\n\\nimport { UIPreviewCard, Card } from \\'@/components/home/card\\';\\nimport { EventPlanning } from \\'@/components/home/event-planning\\';\\nimport { Searching } from \\'@/components/home/searching\\';\\nimport { Weather } from \\'@/components/home/weather\\';\\n\\n# Streaming React Components\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nThe RSC API allows you to stream React components from the server to the client with the [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui) function. This is useful when you want to go beyond raw text and stream components to the client in real-time.\\n\\nSimilar to [ AI SDK Core ](/docs/ai-sdk-core/overview) APIs (like [ `streamText` ](/docs/reference/ai-sdk-core/stream-text) and [ `streamObject` ](/docs/reference/ai-sdk-core/stream-object)), `streamUI` provides a single function to call a model and allow it to respond with React Server Components.\\nIt supports the same model interfaces as AI SDK Core APIs.\\n\\n### Concepts\\n\\nTo give the model the ability to respond to a user\\'s prompt with a React component, you can leverage [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n<Note>\\n  Remember, tools are like programs you can give to the model, and the model can\\n  decide as and when to use based on the context of the conversation.\\n</Note>\\n\\nWith the `streamUI` function, **you provide tools that return React components**. With the ability to stream components, the model is akin to a dynamic router that is able to understand the user\\'s intention and display relevant UI.\\n\\nAt a high level, the `streamUI` works like other AI SDK Core functions: you can provide the model with a prompt or some conversation history and, optionally, some tools. If the model decides, based on the context of the conversation, to call a tool, it will generate a tool call. The `streamUI` function will then run the respective tool, returning a React component. If the model doesn\\'t have a relevant tool to use, it will return a text generation, which will be passed to the `text` function, for you to handle (render and return as a React component).\\n\\n<Note>Remember, the `streamUI` function must return a React component. </Note>\\n\\n```tsx\\nconst result = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  prompt: \\'Get the weather for San Francisco\\',\\n  text: ({ content }) => <div>{content}</div>,\\n  tools: {},\\n});\\n```\\n\\nThis example calls the `streamUI` function using OpenAI\\'s `gpt-4o` model, passes a prompt, specifies how the model\\'s plain text response (`content`) should be rendered, and then provides an empty object for tools. Even though this example does not define any tools, it will stream the model\\'s response as a `div` rather than plain text.\\n\\n### Adding A Tool\\n\\nUsing tools with `streamUI` is similar to how you use tools with `generateText` and `streamText`.\\nA tool is an object that has:\\n\\n- `description`: a string telling the model what the tool does and when to use it\\n- `inputSchema`: a Zod schema describing what the tool needs in order to run\\n- `generate`: an asynchronous function that will be run if the model calls the tool. This must return a React component\\n\\nLet\\'s expand the previous example to add a tool.\\n\\n```tsx highlight=\"6-14\"\\nconst result = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  prompt: \\'Get the weather for San Francisco\\',\\n  text: ({ content }) => <div>{content}</div>,\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      inputSchema: z.object({ location: z.string() }),\\n      generate: async function* ({ location }) {\\n        yield <LoadingComponent />;\\n        const weather = await getWeather(location);\\n        return <WeatherComponent weather={weather} location={location} />;\\n      },\\n    },\\n  },\\n});\\n```\\n\\nThis tool would be run if the user asks for the weather for their location. If the user hasn\\'t specified a location, the model will ask for it before calling the tool. When the model calls the tool, the generate function will initially return a loading component. This component will show until the awaited call to `getWeather` is resolved, at which point, the model will stream the `<WeatherComponent />` to the user.\\n\\n<Note>\\n  Note: This example uses a [ generator function\\n  ](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*)\\n  (`function*`), which allows you to pause its execution and return a value,\\n  then resume from where it left off on the next call. This is useful for\\n  handling data streams, as you can fetch and return data from an asynchronous\\n  source like an API, then resume the function to fetch the next chunk when\\n  needed. By yielding values one at a time, generator functions enable efficient\\n  processing of streaming data without blocking the main thread.\\n</Note>\\n\\n## Using `streamUI` with Next.js\\n\\nLet\\'s see how you can use the example above in a Next.js application.\\n\\nTo use `streamUI` in a Next.js application, you will need two things:\\n\\n1. A Server Action (where you will call `streamUI`)\\n2. A page to call the Server Action and render the resulting components\\n\\n### Step 1: Create a Server Action\\n\\n<Note>\\n  Server Actions are server-side functions that you can call directly from the\\n  frontend. For more info, see [the\\n  documentation](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations#with-client-components).\\n</Note>\\n\\nCreate a Server Action at `app/actions.tsx` and add the following code:\\n\\n```tsx filename=\"app/actions.tsx\"\\n\\'use server\\';\\n\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst LoadingComponent = () => (\\n  <div className=\"animate-pulse p-4\">getting weather...</div>\\n);\\n\\nconst getWeather = async (location: string) => {\\n  await new Promise(resolve => setTimeout(resolve, 2000));\\n  return \\'82°F️ ☀️\\';\\n};\\n\\ninterface WeatherProps {\\n  location: string;\\n  weather: string;\\n}\\n\\nconst WeatherComponent = (props: WeatherProps) => (\\n  <div className=\"border border-neutral-200 p-4 rounded-lg max-w-fit\">\\n    The weather in {props.location} is {props.weather}\\n  </div>\\n);\\n\\nexport async function streamComponent() {\\n  const result = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    prompt: \\'Get the weather for San Francisco\\',\\n    text: ({ content }) => <div>{content}</div>,\\n    tools: {\\n      getWeather: {\\n        description: \\'Get the weather for a location\\',\\n        inputSchema: z.object({\\n          location: z.string(),\\n        }),\\n        generate: async function* ({ location }) {\\n          yield <LoadingComponent />;\\n          const weather = await getWeather(location);\\n          return <WeatherComponent weather={weather} location={location} />;\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.value;\\n}\\n```\\n\\nThe `getWeather` tool should look familiar as it is identical to the example in the previous section. In order for this tool to work:\\n\\n1. First define a `LoadingComponent`, which renders a pulsing `div` that will show some loading text.\\n2. Next, define a `getWeather` function that will timeout for 2 seconds (to simulate fetching the weather externally) before returning the \"weather\" for a `location`. Note: you could run any asynchronous TypeScript code here.\\n3. Finally, define a `WeatherComponent` which takes in `location` and `weather` as props, which are then rendered within a `div`.\\n\\nYour Server Action is an asynchronous function called `streamComponent` that takes no inputs, and returns a `ReactNode`. Within the action, you call the `streamUI` function, specifying the model (`gpt-4o`), the prompt, the component that should be rendered if the model chooses to return text, and finally, your `getWeather` tool. Last but not least, you return the resulting component generated by the model with `result.value`.\\n\\nTo call this Server Action and display the resulting React Component, you will need a page.\\n\\n### Step 2: Create a Page\\n\\nCreate or update your root page (`app/page.tsx`) with the following code:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { Button } from \\'@/components/ui/button\\';\\nimport { streamComponent } from \\'./actions\\';\\n\\nexport default function Page() {\\n  const [component, setComponent] = useState<React.ReactNode>();\\n\\n  return (\\n    <div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          setComponent(await streamComponent());\\n        }}\\n      >\\n        <Button>Stream Component</Button>\\n      </form>\\n      <div>{component}</div>\\n    </div>\\n  );\\n}\\n```\\n\\nThis page is first marked as a client component with the `\"use client\";` directive given it will be using hooks and interactivity. On the page, you render a form. When that form is submitted, you call the `streamComponent` action created in the previous step (just like any other function). The `streamComponent` action returns a `ReactNode` that you can then render on the page using React state (`setComponent`).\\n\\n## Going beyond a single prompt\\n\\nYou can now allow the model to respond to your prompt with a React component. However, this example is limited to a static prompt that is set within your Server Action. You could make this example interactive by turning it into a chatbot.\\n\\nLearn how to stream React components with the Next.js App Router using `streamUI` with this [example](/examples/next-app/interface/route-components).\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/03-generative-ui-state.mdx'), name='03-generative-ui-state.mdx', displayName='03-generative-ui-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Managing Generative UI State\\ndescription: Overview of the AI and UI states\\n---\\n\\n# Managing Generative UI State\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nState is an essential part of any application. State is particularly important in AI applications as it is passed to large language models (LLMs) on each request to ensure they have the necessary context to produce a great generation. Traditional chatbots are text-based and have a structure that mirrors that of any chat application.\\n\\nFor example, in a chatbot, state is an array of `messages` where each `message` has:\\n\\n- `id`: a unique identifier\\n- `role`: who sent the message (user/assistant/system/tool)\\n- `content`: the content of the message\\n\\nThis state can be rendered in the UI and sent to the model without any modifications.\\n\\nWith Generative UI, the model can now return a React component, rather than a plain text message. The client can render that component without issue, but that state can\\'t be sent back to the model because React components aren\\'t serialisable. So, what can you do?\\n\\n**The solution is to split the state in two, where one (AI State) becomes a proxy for the other (UI State)**.\\n\\nOne way to understand this concept is through a Lego analogy. Imagine a 10,000 piece Lego model that, once built, cannot be easily transported because it is fragile. By taking the model apart, it can be easily transported, and then rebuilt following the steps outlined in the instructions pamphlet. In this way, the instructions pamphlet is a proxy to the physical structure. Similarly, AI State provides a serialisable (JSON) representation of your UI that can be passed back and forth to the model.\\n\\n## What is AI and UI State?\\n\\nThe RSC API simplifies how you manage AI State and UI State, providing a robust way to keep them in sync between your database, server and client.\\n\\n### AI State\\n\\nAI State refers to the state of your application in a serialisable format that will be used on the server and can be shared with the language model.\\n\\nFor a chat app, the AI State is the conversation history (messages) between the user and the assistant. Components generated by the model would be represented in a JSON format as a tool alongside any necessary props. AI State can also be used to store other values and meta information such as `createdAt` for each message and `chatId` for each conversation. The LLM reads this history so it can generate the next message. This state serves as the source of truth for the current application state.\\n\\n<Note>\\n  **Note**: AI state can be accessed/modified from both the server and the\\n  client.\\n</Note>\\n\\n### UI State\\n\\nUI State refers to the state of your application that is rendered on the client. It is a fully client-side state (similar to `useState`) that can store anything from Javascript values to React elements. UI state is a list of actual UI elements that are rendered on the client.\\n\\n<Note>**Note**: UI State can only be accessed client-side.</Note>\\n\\n## Using AI / UI State\\n\\n### Creating the AI Context\\n\\nAI SDK RSC simplifies managing AI and UI state across your application by providing several hooks. These hooks are powered by [ React context ](https://react.dev/reference/react/hooks#context-hooks) under the hood.\\n\\nNotably, this means you do not have to pass the message history to the server explicitly for each request. You also can access and update your application state in any child component of the context provider. As you begin building [multistep generative interfaces](/docs/ai-sdk-rsc/multistep-interfaces), this will be particularly helpful.\\n\\nTo use `@ai-sdk/rsc` to manage AI and UI State in your application, you can create a React context using [`createAI`](/docs/reference/ai-sdk-rsc/create-ai):\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n// Define the AI state and UI state types\\nexport type ServerMessage = {\\n  role: \\'user\\' | \\'assistant\\';\\n  content: string;\\n};\\n\\nexport type ClientMessage = {\\n  id: string;\\n  role: \\'user\\' | \\'assistant\\';\\n  display: ReactNode;\\n};\\n\\nexport const sendMessage = async (input: string): Promise<ClientMessage> => {\\n  \"use server\"\\n  ...\\n}\\n```\\n\\n```tsx filename=\\'app/ai.ts\\'\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { ClientMessage, ServerMessage, sendMessage } from \\'./actions\\';\\n\\nexport type AIState = ServerMessage[];\\nexport type UIState = ClientMessage[];\\n\\n// Create the AI provider with the initial states and allowed actions\\nexport const AI = createAI<AIState, UIState>({\\n  initialAIState: [],\\n  initialUIState: [],\\n  actions: {\\n    sendMessage,\\n  },\\n});\\n```\\n\\n<Note>You must pass Server Actions to the `actions` object.</Note>\\n\\nIn this example, you define types for AI State and UI State, respectively.\\n\\nNext, wrap your application with your newly created context. With that, you can get and set AI and UI State across your entire application.\\n\\n```tsx filename=\\'app/layout.tsx\\'\\nimport { type ReactNode } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default function RootLayout({\\n  children,\\n}: Readonly<{ children: ReactNode }>) {\\n  return (\\n    <AI>\\n      <html lang=\"en\">\\n        <body>{children}</body>\\n      </html>\\n    </AI>\\n  );\\n}\\n```\\n\\n## Reading UI State in Client\\n\\nThe UI state can be accessed in Client Components using the [`useUIState`](/docs/reference/ai-sdk-rsc/use-ui-state) hook provided by the RSC API. The hook returns the current UI state and a function to update the UI state like React\\'s `useState`.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [messages, setMessages] = useUIState();\\n\\n  return (\\n    <ul>\\n      {messages.map(message => (\\n        <li key={message.id}>{message.display}</li>\\n      ))}\\n    </ul>\\n  );\\n}\\n```\\n\\n## Reading AI State in Client\\n\\nThe AI state can be accessed in Client Components using the [`useAIState`](/docs/reference/ai-sdk-rsc/use-ai-state) hook provided by the RSC API. The hook returns the current AI state.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useAIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [messages, setMessages] = useAIState();\\n\\n  return (\\n    <ul>\\n      {messages.map(message => (\\n        <li key={message.id}>{message.content}</li>\\n      ))}\\n    </ul>\\n  );\\n}\\n```\\n\\n## Reading AI State on Server\\n\\nThe AI State can be accessed within any Server Action provided to the `createAI` context using the [`getAIState`](/docs/reference/ai-sdk-rsc/get-ai-state) function. It returns the current AI state as a read-only value:\\n\\n```tsx filename=\\'app/actions.ts\\'\\nimport { getAIState } from \\'@ai-sdk/rsc\\';\\n\\nexport async function sendMessage(message: string) {\\n  \\'use server\\';\\n\\n  const history = getAIState();\\n\\n  const response = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: [...history, { role: \\'user\\', content: message }],\\n  });\\n\\n  return response;\\n}\\n```\\n\\n<Note>\\n  Remember, you can only access state within actions that have been passed to\\n  the `createAI` context within the `actions` key.\\n</Note>\\n\\n## Updating AI State on Server\\n\\nThe AI State can also be updated from within your Server Action with the [`getMutableAIState`](/docs/reference/ai-sdk-rsc/get-mutable-ai-state) function. This function is similar to `getAIState`, but it returns the state with methods to read and update it:\\n\\n```tsx filename=\\'app/actions.ts\\'\\nimport { getMutableAIState } from \\'@ai-sdk/rsc\\';\\n\\nexport async function sendMessage(message: string) {\\n  \\'use server\\';\\n\\n  const history = getMutableAIState();\\n\\n  // Update the AI state with the new user message.\\n  history.update([...history.get(), { role: \\'user\\', content: message }]);\\n\\n  const response = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: history.get(),\\n  });\\n\\n  // Update the AI state again with the response from the model.\\n  history.done([...history.get(), { role: \\'assistant\\', content: response }]);\\n\\n  return response;\\n}\\n```\\n\\n<Note>\\n  It is important to update the AI State with new responses using `.update()`\\n  and `.done()` to keep the conversation history in sync.\\n</Note>\\n\\n## Calling Server Actions from the Client\\n\\nTo call the `sendMessage` action from the client, you can use the [`useActions`](/docs/reference/ai-sdk-rsc/use-actions) hook. The hook returns all the available Actions that were provided to `createAI`:\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default function Page() {\\n  const { sendMessage } = useActions<typeof AI>();\\n  const [messages, setMessages] = useUIState();\\n\\n  const handleSubmit = async event => {\\n    event.preventDefault();\\n\\n    setMessages([\\n      ...messages,\\n      { id: Date.now(), role: \\'user\\', display: event.target.message.value },\\n    ]);\\n\\n    const response = await sendMessage(event.target.message.value);\\n\\n    setMessages([\\n      ...messages,\\n      { id: Date.now(), role: \\'assistant\\', display: response },\\n    ]);\\n  };\\n\\n  return (\\n    <>\\n      <ul>\\n        {messages.map(message => (\\n          <li key={message.id}>{message.display}</li>\\n        ))}\\n      </ul>\\n      <form onSubmit={handleSubmit}>\\n        <input type=\"text\" name=\"message\" />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </>\\n  );\\n}\\n```\\n\\nWhen the user submits a message, the `sendMessage` action is called with the message content. The response from the action is then added to the UI state, updating the displayed messages.\\n\\n<Note>\\n  Important! Don\\'t forget to update the UI State after you call your Server\\n  Action otherwise the streamed component will not show in the UI.\\n</Note>\\n\\nTo learn more, check out this [example](/examples/next-app/state-management/ai-ui-states) on managing AI and UI state using `@ai-sdk/rsc`.\\n\\n---\\n\\nNext, you will learn how you can save and restore state with `@ai-sdk/rsc`.\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/03-saving-and-restoring-states.mdx'), name='03-saving-and-restoring-states.mdx', displayName='03-saving-and-restoring-states.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Saving and Restoring States\\ndescription: Saving and restoring AI and UI states with onGetUIState and onSetAIState\\n---\\n\\n# Saving and Restoring States\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nAI SDK RSC provides convenient methods for saving and restoring AI and UI state. This is useful for saving the state of your application after every model generation, and restoring it when the user revisits the generations.\\n\\n## AI State\\n\\n### Saving AI state\\n\\nThe AI state can be saved using the [`onSetAIState`](/docs/reference/ai-sdk-rsc/create-ai#on-set-ai-state) callback, which gets called whenever the AI state is updated. In the following example, you save the chat history to a database whenever the generation is marked as done.\\n\\n```tsx filename=\\'app/ai.ts\\'\\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\\n  actions: {\\n    continueConversation,\\n  },\\n  onSetAIState: async ({ state, done }) => {\\n    \\'use server\\';\\n\\n    if (done) {\\n      saveChatToDB(state);\\n    }\\n  },\\n});\\n```\\n\\n### Restoring AI state\\n\\nThe AI state can be restored using the [`initialAIState`](/docs/reference/ai-sdk-rsc/create-ai#initial-ai-state) prop passed to the context provider created by the [`createAI`](/docs/reference/ai-sdk-rsc/create-ai) function. In the following example, you restore the chat history from a database when the component is mounted.\\n\\n```tsx file=\\'app/layout.tsx\\'\\nimport { ReactNode } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default async function RootLayout({\\n  children,\\n}: Readonly<{ children: ReactNode }>) {\\n  const chat = await loadChatFromDB();\\n\\n  return (\\n    <html lang=\"en\">\\n      <body>\\n        <AI initialAIState={chat}>{children}</AI>\\n      </body>\\n    </html>\\n  );\\n}\\n```\\n\\n## UI State\\n\\n### Saving UI state\\n\\nThe UI state cannot be saved directly, since the contents aren\\'t yet serializable. Instead, you can use the AI state as proxy to store details about the UI state and use it to restore the UI state when needed.\\n\\n### Restoring UI state\\n\\nThe UI state can be restored using the AI state as a proxy. In the following example, you restore the chat history from the AI state when the component is mounted. You use the [`onGetUIState`](/docs/reference/ai-sdk-rsc/create-ai#on-get-ui-state) callback to listen for SSR events and restore the UI state.\\n\\n```tsx filename=\\'app/ai.ts\\'\\nexport const AI = createAI<ServerMessage[], ClientMessage[]>({\\n  actions: {\\n    continueConversation,\\n  },\\n  onGetUIState: async () => {\\n    \\'use server\\';\\n\\n    const historyFromDB: ServerMessage[] = await loadChatFromDB();\\n    const historyFromApp: ServerMessage[] = getAIState();\\n\\n    // If the history from the database is different from the\\n    // history in the app, they\\'re not in sync so return the UIState\\n    // based on the history from the database\\n\\n    if (historyFromDB.length !== historyFromApp.length) {\\n      return historyFromDB.map(({ role, content }) => ({\\n        id: generateId(),\\n        role,\\n        display:\\n          role === \\'function\\' ? (\\n            <Component {...JSON.parse(content)} />\\n          ) : (\\n            content\\n          ),\\n      }));\\n    }\\n  },\\n});\\n```\\n\\nTo learn more, check out this [example](/examples/next-app/state-management/save-and-restore-states) that persists and restores states in your Next.js application.\\n\\n---\\n\\nNext, you will learn how you can use `@ai-sdk/rsc` functions like `useActions` and `useUIState` to create interactive, multistep interfaces.\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/04-multistep-interfaces.mdx'), name='04-multistep-interfaces.mdx', displayName='04-multistep-interfaces.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Multistep Interfaces\\ndescription: Overview of Building Multistep Interfaces with AI SDK RSC\\n---\\n\\n# Designing Multistep Interfaces\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nMultistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.\\n\\nFor example, if you wanted to build a Generative UI chatbot capable of booking flights, it could have three steps:\\n\\n- Search all flights\\n- Pick flight\\n- Check availability\\n\\nTo build this kind of application you will leverage two concepts, **tool composition** and **application context**.\\n\\n**Tool composition** is the process of combining multiple [tools](/docs/ai-sdk-core/tools-and-tool-calling) to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps. In the example above, _\"search all flights\"_, _\"pick flight\"_, and _\"check availability\"_ come together to create a holistic _\"book flight\"_ tool.\\n\\n**Application context** refers to the state of the application at any given point in time. This includes the user\\'s input, the output of the language model, and any other relevant information. In the example above, the flight selected in _\"pick flight\"_ would be used as context necessary to complete the _\"check availability\"_ task.\\n\\n## Overview\\n\\nIn order to build a multistep interface with `@ai-sdk/rsc`, you will need a few things:\\n\\n- A Server Action that calls and returns the result from the `streamUI` function\\n- Tool(s) (sub-tasks necessary to complete your overall task)\\n- React component(s) that should be rendered when the tool is called\\n- A page to render your chatbot\\n\\nThe general flow that you will follow is:\\n\\n- User sends a message (calls your Server Action with `useActions`, passing the message as an input)\\n- Message is appended to the AI State and then passed to the model alongside a number of tools\\n- Model can decide to call a tool, which will render the `<SomeTool />` component\\n- Within that component, you can add interactivity by using `useActions` to call the model with your Server Action and `useUIState` to append the model\\'s response (`<SomeOtherTool />`) to the UI State\\n- And so on...\\n\\n## Implementation\\n\\nThe turn-by-turn implementation is the simplest form of multistep interfaces. In this implementation, the user and the model take turns during the conversation. For every user input, the model generates a response, and the conversation continues in this turn-by-turn fashion.\\n\\nIn the following example, you specify two tools (`searchFlights` and `lookupFlight`) that the model can use to search for flights and lookup details for a specific flight.\\n\\n```tsx filename=\"app/actions.tsx\"\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst searchFlights = async (\\n  source: string,\\n  destination: string,\\n  date: string,\\n) => {\\n  return [\\n    {\\n      id: \\'1\\',\\n      flightNumber: \\'AA123\\',\\n    },\\n    {\\n      id: \\'2\\',\\n      flightNumber: \\'AA456\\',\\n    },\\n  ];\\n};\\n\\nconst lookupFlight = async (flightNumber: string) => {\\n  return {\\n    flightNumber: flightNumber,\\n    departureTime: \\'10:00 AM\\',\\n    arrivalTime: \\'12:00 PM\\',\\n  };\\n};\\n\\nexport async function submitUserMessage(input: string) {\\n  \\'use server\\';\\n\\n  const ui = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    system: \\'you are a flight booking assistant\\',\\n    prompt: input,\\n    text: async ({ content }) => <div>{content}</div>,\\n    tools: {\\n      searchFlights: {\\n        description: \\'search for flights\\',\\n        inputSchema: z.object({\\n          source: z.string().describe(\\'The origin of the flight\\'),\\n          destination: z.string().describe(\\'The destination of the flight\\'),\\n          date: z.string().describe(\\'The date of the flight\\'),\\n        }),\\n        generate: async function* ({ source, destination, date }) {\\n          yield `Searching for flights from ${source} to ${destination} on ${date}...`;\\n          const results = await searchFlights(source, destination, date);\\n\\n          return (\\n            <div>\\n              {results.map(result => (\\n                <div key={result.id}>\\n                  <div>{result.flightNumber}</div>\\n                </div>\\n              ))}\\n            </div>\\n          );\\n        },\\n      },\\n      lookupFlight: {\\n        description: \\'lookup details for a flight\\',\\n        parameters: z.object({\\n          flightNumber: z.string().describe(\\'The flight number\\'),\\n        }),\\n        generate: async function* ({ flightNumber }) {\\n          yield `Looking up details for flight ${flightNumber}...`;\\n          const details = await lookupFlight(flightNumber);\\n\\n          return (\\n            <div>\\n              <div>Flight Number: {details.flightNumber}</div>\\n              <div>Departure Time: {details.departureTime}</div>\\n              <div>Arrival Time: {details.arrivalTime}</div>\\n            </div>\\n          );\\n        },\\n      },\\n    },\\n  });\\n\\n  return ui.value;\\n}\\n```\\n\\nNext, create an AI context that will hold the UI State and AI State.\\n\\n```ts filename=\\'app/ai.ts\\'\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { submitUserMessage } from \\'./actions\\';\\n\\nexport const AI = createAI<any[], React.ReactNode[]>({\\n  initialUIState: [],\\n  initialAIState: [],\\n  actions: {\\n    submitUserMessage,\\n  },\\n});\\n```\\n\\nNext, wrap your application with your newly created context.\\n\\n```tsx filename=\\'app/layout.tsx\\'\\nimport { type ReactNode } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\n\\nexport default function RootLayout({\\n  children,\\n}: Readonly<{ children: ReactNode }>) {\\n  return (\\n    <AI>\\n      <html lang=\"en\">\\n        <body>{children}</body>\\n      </html>\\n    </AI>\\n  );\\n}\\n```\\n\\nTo call your Server Action, update your root page with the following:\\n\\n```tsx filename=\"app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { AI } from \\'./ai\\';\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [conversation, setConversation] = useUIState<typeof AI>();\\n  const { submitUserMessage } = useActions();\\n\\n  const handleSubmit = async (e: React.FormEvent<HTMLFormElement>) => {\\n    e.preventDefault();\\n    setInput(\\'\\');\\n    setConversation(currentConversation => [\\n      ...currentConversation,\\n      <div>{input}</div>,\\n    ]);\\n    const message = await submitUserMessage(input);\\n    setConversation(currentConversation => [...currentConversation, message]);\\n  };\\n\\n  return (\\n    <div>\\n      <div>\\n        {conversation.map((message, i) => (\\n          <div key={i}>{message}</div>\\n        ))}\\n      </div>\\n      <div>\\n        <form onSubmit={handleSubmit}>\\n          <input\\n            type=\"text\"\\n            value={input}\\n            onChange={e => setInput(e.target.value)}\\n          />\\n          <button>Send Message</button>\\n        </form>\\n      </div>\\n    </div>\\n  );\\n}\\n```\\n\\nThis page pulls in the current UI State using the `useUIState` hook, which is then mapped over and rendered in the UI. To access the Server Action, you use the `useActions` hook which will return all actions that were passed to the `actions` key of the `createAI` function in your `actions.tsx` file. Finally, you call the `submitUserMessage` function like any other TypeScript function. This function returns a React component (`message`) that is then rendered in the UI by updating the UI State with `setConversation`.\\n\\nIn this example, to call the next tool, the user must respond with plain text. **Given you are streaming a React component, you can add a button to trigger the next step in the conversation**.\\n\\nTo add user interaction, you will have to convert the component into a client component and use the `useAction` hook to trigger the next step in the conversation.\\n\\n```tsx filename=\"components/flights.tsx\"\\n\\'use client\\';\\n\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\nimport { ReactNode } from \\'react\\';\\n\\ninterface FlightsProps {\\n  flights: { id: string; flightNumber: string }[];\\n}\\n\\nexport const Flights = ({ flights }: FlightsProps) => {\\n  const { submitUserMessage } = useActions();\\n  const [_, setMessages] = useUIState();\\n\\n  return (\\n    <div>\\n      {flights.map(result => (\\n        <div key={result.id}>\\n          <div\\n            onClick={async () => {\\n              const display = await submitUserMessage(\\n                `lookupFlight ${result.flightNumber}`,\\n              );\\n\\n              setMessages((messages: ReactNode[]) => [...messages, display]);\\n            }}\\n          >\\n            {result.flightNumber}\\n          </div>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n};\\n```\\n\\nNow, update your `searchFlights` tool to render the new `<Flights />` component.\\n\\n```tsx filename=\"actions.tsx\"\\n...\\nsearchFlights: {\\n  description: \\'search for flights\\',\\n  parameters: z.object({\\n    source: z.string().describe(\\'The origin of the flight\\'),\\n    destination: z.string().describe(\\'The destination of the flight\\'),\\n    date: z.string().describe(\\'The date of the flight\\'),\\n  }),\\n  generate: async function* ({ source, destination, date }) {\\n    yield `Searching for flights from ${source} to ${destination} on ${date}...`;\\n    const results = await searchFlights(source, destination, date);\\n    return (<Flights flights={results} />);\\n  },\\n}\\n...\\n```\\n\\nIn the above example, the `Flights` component is used to display the search results. When the user clicks on a flight number, the `lookupFlight` tool is called with the flight number as a parameter. The `submitUserMessage` action is then called to trigger the next step in the conversation.\\n\\nLearn more about tool calling in Next.js App Router by checking out examples [here](/examples/next-app/tools).\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/05-streaming-values.mdx'), name='05-streaming-values.mdx', displayName='05-streaming-values.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming Values\\ndescription: Overview of streaming RSCs\\n---\\n\\nimport { UIPreviewCard, Card } from \\'@/components/home/card\\';\\nimport { EventPlanning } from \\'@/components/home/event-planning\\';\\nimport { Searching } from \\'@/components/home/searching\\';\\nimport { Weather } from \\'@/components/home/weather\\';\\n\\n# Streaming Values\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nThe RSC API provides several utility functions to allow you to stream values from the server to the client. This is useful when you need more granular control over what you are streaming and how you are streaming it.\\n\\n<Note>\\n  These utilities can also be paired with [AI SDK Core](/docs/ai-sdk-core)\\n  functions like [`streamText`](/docs/reference/ai-sdk-core/stream-text) and\\n  [`streamObject`](/docs/reference/ai-sdk-core/stream-object) to easily stream\\n  LLM generations from the server to the client.\\n</Note>\\n\\nThere are two functions provided by the RSC API that allow you to create streamable values:\\n\\n- [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) - creates a streamable (serializable) value, with full control over how you create, update, and close the stream.\\n- [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) - creates a streamable React component, with full control over how you create, update, and close the stream.\\n\\n## `createStreamableValue`\\n\\nThe RSC API allows you to stream serializable Javascript values from the server to the client using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value), such as strings, numbers, objects, and arrays.\\n\\nThis is useful when you want to stream:\\n\\n- Text generations from the language model in real-time.\\n- Buffer values of image and audio generations from multi-modal models.\\n- Progress updates from multi-step agent runs.\\n\\n## Creating a Streamable Value\\n\\nYou can import `createStreamableValue` from `@ai-sdk/rsc` and use it to create a streamable value.\\n\\n```tsx file=\\'app/actions.ts\\'\\n\\'use server\\';\\n\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport const runThread = async () => {\\n  const streamableStatus = createStreamableValue(\\'thread.init\\');\\n\\n  setTimeout(() => {\\n    streamableStatus.update(\\'thread.run.create\\');\\n    streamableStatus.update(\\'thread.run.update\\');\\n    streamableStatus.update(\\'thread.run.end\\');\\n    streamableStatus.done(\\'thread.end\\');\\n  }, 1000);\\n\\n  return {\\n    status: streamableStatus.value,\\n  };\\n};\\n```\\n\\n## Reading a Streamable Value\\n\\nYou can read streamable values on the client using `readStreamableValue`. It returns an async iterator that yields the value of the streamable as it is updated:\\n\\n```tsx file=\\'app/page.tsx\\'\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { runThread } from \\'@/actions\\';\\n\\nexport default function Page() {\\n  return (\\n    <button\\n      onClick={async () => {\\n        const { status } = await runThread();\\n\\n        for await (const value of readStreamableValue(status)) {\\n          console.log(value);\\n        }\\n      }}\\n    >\\n      Ask\\n    </button>\\n  );\\n}\\n```\\n\\nLearn how to stream a text generation (with `streamText`) using the Next.js App Router and `createStreamableValue` in this [example](/examples/next-app/basics/streaming-text-generation).\\n\\n## `createStreamableUI`\\n\\n`createStreamableUI` creates a stream that holds a React component. Unlike AI SDK Core APIs, this function does not call a large language model. Instead, it provides a primitive that can be used to have granular control over streaming a React component.\\n\\n## Using `createStreamableUI`\\n\\nLet\\'s look at how you can use the `createStreamableUI` function with a Server Action.\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n\\'use server\\';\\n\\nimport { createStreamableUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function getWeather() {\\n  const weatherUI = createStreamableUI();\\n\\n  weatherUI.update(<div style={{ color: \\'gray\\' }}>Loading...</div>);\\n\\n  setTimeout(() => {\\n    weatherUI.done(<div>It&apos;s a sunny day!</div>);\\n  }, 1000);\\n\\n  return weatherUI.value;\\n}\\n```\\n\\nFirst, you create a streamable UI with an empty state and then update it with a loading message. After 1 second, you mark the stream as done passing in the actual weather information as its final value. The `.value` property contains the actual UI that can be sent to the client.\\n\\n## Reading a Streamable UI\\n\\nOn the client side, you can call the `getWeather` Server Action and render the returned UI like any other React component.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { getWeather } from \\'@/actions\\';\\n\\nexport default function Page() {\\n  const [weather, setWeather] = useState<React.ReactNode | null>(null);\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const weatherUI = await getWeather();\\n          setWeather(weatherUI);\\n        }}\\n      >\\n        What&apos;s the weather?\\n      </button>\\n\\n      {weather}\\n    </div>\\n  );\\n}\\n```\\n\\nWhen the button is clicked, the `getWeather` function is called, and the returned UI is set to the `weather` state and rendered on the page. Users will see the loading message first and then the actual weather information after 1 second.\\n\\nLearn more about handling multiple streams in a single request in the [Multiple Streamables](/docs/advanced/multiple-streamables) guide.\\n\\nLearn more about handling state for more complex use cases with [ AI/UI State ](/docs/ai-sdk-rsc/generative-ui-state).\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/06-loading-state.mdx'), name='06-loading-state.mdx', displayName='06-loading-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Handling Loading State\\ndescription: Overview of handling loading state with AI SDK RSC\\n---\\n\\n# Handling Loading State\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nGiven that responses from language models can often take a while to complete, it\\'s crucial to be able to show loading state to users. This provides visual feedback that the system is working on their request and helps maintain a positive user experience.\\n\\nThere are three approaches you can take to handle loading state with the AI SDK RSC:\\n\\n- Managing loading state similar to how you would in a traditional Next.js application. This involves setting a loading state variable in the client and updating it when the response is received.\\n- Streaming loading state from the server to the client. This approach allows you to track loading state on a more granular level and provide more detailed feedback to the user.\\n- Streaming loading component from the server to the client. This approach allows you to stream a React Server Component to the client while awaiting the model\\'s response.\\n\\n## Handling Loading State on the Client\\n\\n### Client\\n\\nLet\\'s create a simple Next.js page that will call the `generateResponse` function when the form is submitted. The function will take in the user\\'s prompt (`input`) and then generate a response (`response`). To handle the loading state, use the `loading` state variable. When the form is submitted, set `loading` to `true`, and when the response is received, set it back to `false`. While the response is being streamed, the input field will be disabled.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { generateResponse } from \\'./actions\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport default function Home() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [generation, setGeneration] = useState<string>(\\'\\');\\n  const [loading, setLoading] = useState<boolean>(false);\\n\\n  return (\\n    <div>\\n      <div>{generation}</div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          setLoading(true);\\n          const response = await generateResponse(input);\\n\\n          let textContent = \\'\\';\\n\\n          for await (const delta of readStreamableValue(response)) {\\n            textContent = `${textContent}${delta}`;\\n            setGeneration(textContent);\\n          }\\n          setInput(\\'\\');\\n          setLoading(false);\\n        }}\\n      >\\n        <input\\n          type=\"text\"\\n          value={input}\\n          disabled={loading}\\n          className=\"disabled:opacity-50\"\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button>Send Message</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Server\\n\\nNow let\\'s implement the `generateResponse` function. Use the `streamText` function to generate a response to the input.\\n\\n```typescript filename=\\'app/actions.ts\\'\\n\\'use server\\';\\n\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport async function generateResponse(prompt: string) {\\n  const stream = createStreamableValue();\\n\\n  (async () => {\\n    const { textStream } = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      prompt,\\n    });\\n\\n    for await (const text of textStream) {\\n      stream.update(text);\\n    }\\n\\n    stream.done();\\n  })();\\n\\n  return stream.value;\\n}\\n```\\n\\n## Streaming Loading State from the Server\\n\\nIf you are looking to track loading state on a more granular level, you can create a new streamable value to store a custom variable and then read this on the frontend. Let\\'s update the example to create a new streamable value for tracking loading state:\\n\\n### Server\\n\\n```typescript filename=\\'app/actions.ts\\' highlight=\\'9,22,25\\'\\n\\'use server\\';\\n\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport async function generateResponse(prompt: string) {\\n  const stream = createStreamableValue();\\n  const loadingState = createStreamableValue({ loading: true });\\n\\n  (async () => {\\n    const { textStream } = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      prompt,\\n    });\\n\\n    for await (const text of textStream) {\\n      stream.update(text);\\n    }\\n\\n    stream.done();\\n    loadingState.done({ loading: false });\\n  })();\\n\\n  return { response: stream.value, loadingState: loadingState.value };\\n}\\n```\\n\\n### Client\\n\\n```tsx filename=\\'app/page.tsx\\' highlight=\"22,30-34\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { generateResponse } from \\'./actions\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport default function Home() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [generation, setGeneration] = useState<string>(\\'\\');\\n  const [loading, setLoading] = useState<boolean>(false);\\n\\n  return (\\n    <div>\\n      <div>{generation}</div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          setLoading(true);\\n          const { response, loadingState } = await generateResponse(input);\\n\\n          let textContent = \\'\\';\\n\\n          for await (const responseDelta of readStreamableValue(response)) {\\n            textContent = `${textContent}${responseDelta}`;\\n            setGeneration(textContent);\\n          }\\n          for await (const loadingDelta of readStreamableValue(loadingState)) {\\n            if (loadingDelta) {\\n              setLoading(loadingDelta.loading);\\n            }\\n          }\\n          setInput(\\'\\');\\n          setLoading(false);\\n        }}\\n      >\\n        <input\\n          type=\"text\"\\n          value={input}\\n          disabled={loading}\\n          className=\"disabled:opacity-50\"\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button>Send Message</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThis allows you to provide more detailed feedback about the generation process to your users.\\n\\n## Streaming Loading Components with `streamUI`\\n\\nIf you are using the [ `streamUI` ](/docs/reference/ai-sdk-rsc/stream-ui) function, you can stream the loading state to the client in the form of a React component. `streamUI` supports the usage of [ JavaScript generator functions ](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function*), which allow you to yield some value (in this case a React component) while some other blocking work completes.\\n\\n## Server\\n\\n```ts\\n\\'use server\\';\\n\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function generateResponse(prompt: string) {\\n  const result = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    prompt,\\n    text: async function* ({ content }) {\\n      yield <div>loading...</div>;\\n      return <div>{content}</div>;\\n    },\\n  });\\n\\n  return result.value;\\n}\\n```\\n\\n<Note>\\n  Remember to update the file from `.ts` to `.tsx` because you are defining a\\n  React component in the `streamUI` function.\\n</Note>\\n\\n## Client\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { generateResponse } from \\'./actions\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\n// Force the page to be dynamic and allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nexport default function Home() {\\n  const [input, setInput] = useState<string>(\\'\\');\\n  const [generation, setGeneration] = useState<React.ReactNode>();\\n\\n  return (\\n    <div>\\n      <div>{generation}</div>\\n      <form\\n        onSubmit={async e => {\\n          e.preventDefault();\\n          const result = await generateResponse(input);\\n          setGeneration(result);\\n          setInput(\\'\\');\\n        }}\\n      >\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button>Send Message</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/08-error-handling.mdx'), name='08-error-handling.mdx', displayName='08-error-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Error Handling\\ndescription: Learn how to handle errors with the AI SDK.\\n---\\n\\n# Error Handling\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nTwo categories of errors can occur when working with the RSC API: errors while streaming user interfaces and errors while streaming other values.\\n\\n## Handling UI Errors\\n\\nTo handle errors while generating UI, the [`streamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) object exposes an `error()` method.\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n\\'use server\\';\\n\\nimport { createStreamableUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function getStreamedUI() {\\n  const ui = createStreamableUI();\\n\\n  (async () => {\\n    ui.update(<div>loading</div>);\\n    const data = await fetchData();\\n    ui.done(<div>{data}</div>);\\n  })().catch(e => {\\n    ui.error(<div>Error: {e.message}</div>);\\n  });\\n\\n  return ui.value;\\n}\\n```\\n\\nWith this method, you can catch any error with the stream, and return relevant UI. On the client, you can also use a [React Error Boundary](https://react.dev/reference/react/Component#catching-rendering-errors-with-an-error-boundary) to wrap the streamed component and catch any additional errors.\\n\\n```tsx filename=\\'app/page.tsx\\'\\nimport { getStreamedUI } from \\'@/actions\\';\\nimport { useState } from \\'react\\';\\nimport { ErrorBoundary } from \\'./ErrorBoundary\\';\\n\\nexport default function Page() {\\n  const [streamedUI, setStreamedUI] = useState(null);\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const newUI = await getStreamedUI();\\n          setStreamedUI(newUI);\\n        }}\\n      >\\n        What does the new UI look like?\\n      </button>\\n      <ErrorBoundary>{streamedUI}</ErrorBoundary>\\n    </div>\\n  );\\n}\\n```\\n\\n## Handling Other Errors\\n\\nTo handle other errors while streaming, you can return an error object that the receiver can use to determine why the failure occurred.\\n\\n```tsx filename=\\'app/actions.tsx\\'\\n\\'use server\\';\\n\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { fetchData, emptyData } from \\'../utils/data\\';\\n\\nexport const getStreamedData = async () => {\\n  const streamableData = createStreamableValue<string>(emptyData);\\n\\n  try {\\n    (() => {\\n      const data1 = await fetchData();\\n      streamableData.update(data1);\\n\\n      const data2 = await fetchData();\\n      streamableData.update(data2);\\n\\n      const data3 = await fetchData();\\n      streamableData.done(data3);\\n    })();\\n\\n    return { data: streamableData.value };\\n  } catch (e) {\\n    return { error: e.message };\\n  }\\n};\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/09-authentication.mdx'), name='09-authentication.mdx', displayName='09-authentication.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Handling Authentication\\ndescription: Learn how to authenticate with the AI SDK.\\n---\\n\\n# Authentication\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nThe RSC API makes extensive use of [`Server Actions`](https://nextjs.org/docs/app/building-your-application/data-fetching/server-actions-and-mutations) to power streaming values and UI from the server.\\n\\nServer Actions are exposed as public, unprotected endpoints. As a result, you should treat Server Actions as you would public-facing API endpoints and ensure that the user is authorized to perform the action before returning any data.\\n\\n```tsx filename=\"app/actions.tsx\"\\n\\'use server\\';\\n\\nimport { cookies } from \\'next/headers\\';\\nimport { createStremableUI } from \\'@ai-sdk/rsc\\';\\nimport { validateToken } from \\'../utils/auth\\';\\n\\nexport const getWeather = async () => {\\n  const token = cookies().get(\\'token\\');\\n\\n  if (!token || !validateToken(token)) {\\n    return {\\n      error: \\'This action requires authentication\\',\\n    };\\n  }\\n  const streamableDisplay = createStreamableUI(null);\\n\\n  streamableDisplay.update(<Skeleton />);\\n  streamableDisplay.done(<Weather />);\\n\\n  return {\\n    display: streamableDisplay.value,\\n  };\\n};\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/10-migrating-to-ui.mdx'), name='10-migrating-to-ui.mdx', displayName='10-migrating-to-ui.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Migrating from RSC to UI\\ndescription: Learn how to migrate from AI SDK RSC to AI SDK UI.\\n---\\n\\n# Migrating from RSC to UI\\n\\nThis guide helps you migrate from AI SDK RSC to AI SDK UI.\\n\\n## Background\\n\\nThe AI SDK has two packages that help you build the frontend for your applications – [AI SDK UI](/docs/ai-sdk-ui) and [AI SDK RSC](/docs/ai-sdk-rsc).\\n\\nWe introduced support for using [React Server Components](https://react.dev/reference/rsc/server-components) (RSC) within the AI SDK to simplify building generative user interfaces for frameworks that support RSC.\\n\\nHowever, given we\\'re pushing the boundaries of this technology, AI SDK RSC currently faces significant limitations that make it unsuitable for stable production use.\\n\\n- It is not possible to abort a stream using server actions. This will be improved in future releases of React and Next.js [(1122)](https://github.com/vercel/ai/issues/1122).\\n- When using `createStreamableUI` and `streamUI`, components remount on `.done()`, causing them to flicker [(2939)](https://github.com/vercel/ai/issues/2939).\\n- Many suspense boundaries can lead to crashes [(2843)](https://github.com/vercel/ai/issues/2843).\\n- Using\\xa0`createStreamableUI`\\xa0can lead to quadratic data transfer. You can avoid this using\\xa0createStreamableValue\\xa0instead, and rendering the component client-side.\\n- Closed RSC streams cause update issues [(3007)](https://github.com/vercel/ai/issues/3007).\\n\\nDue to these limitations, AI SDK RSC is marked as experimental, and we do not recommend using it for stable production environments.\\n\\nAs a result, we strongly recommend migrating to AI SDK UI, which has undergone extensive development to provide a more stable and production grade experience.\\n\\nIn building [v0](https://v0.dev), we have invested considerable time exploring how to create the best chat experience on the web. AI SDK UI ships with many of these best practices and commonly used patterns like [language model middleware](/docs/ai-sdk-core/middleware), [multi-step tool calls](/docs/ai-sdk-core/tools-and-tool-calling#multi-step-calls), [attachments](/docs/ai-sdk-ui/chatbot#attachments-experimental), [telemetry](/docs/ai-sdk-core/telemetry), [provider registry](/docs/ai-sdk-core/provider-management#provider-registry), and many more. These features have been considerately designed into a neat abstraction that you can use to reliably integrate AI into your applications.\\n\\n## Streaming Chat Completions\\n\\n### Basic Setup\\n\\nThe `streamUI` function executes as part of a server action as illustrated below.\\n\\n#### Before: Handle generation and rendering in a single server action\\n\\n```tsx filename=\"@/app/actions.tsx\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { getMutableAIState, streamUI } from \\'@ai-sdk/rsc\\';\\n\\nexport async function sendMessage(message: string) {\\n  \\'use server\\';\\n\\n  const messages = getMutableAIState(\\'messages\\');\\n\\n  messages.update([...messages.get(), { role: \\'user\\', content: message }]);\\n\\n  const { value: stream } = await streamUI({\\n    model: openai(\\'gpt-4o\\'),\\n    system: \\'you are a friendly assistant!\\',\\n    messages: messages.get(),\\n    text: async function* ({ content, done }) {\\n      // process text\\n    },\\n    tools: {\\n      // tool definitions\\n    },\\n  });\\n\\n  return stream;\\n}\\n```\\n\\n#### Before: Call server action and update UI state\\n\\nThe chat interface calls the server action. The response is then saved using the `useUIState` hook.\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState, ReactNode } from \\'react\\';\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const { sendMessage } = useActions();\\n  const [input, setInput] = useState(\\'\\');\\n  const [messages, setMessages] = useUIState();\\n\\n  return (\\n    <div>\\n      {messages.map(message => message)}\\n\\n      <form\\n        onSubmit={async () => {\\n          const response: ReactNode = await sendMessage(input);\\n          setMessages(msgs => [...msgs, response]);\\n        }}\\n      >\\n        <input type=\"text\" />\\n        <button type=\"submit\">Submit</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\nThe `streamUI` function combines generating text and rendering the user interface. To migrate to AI SDK UI, you need to **separate these concerns** –\\xa0streaming generations with `streamText` and rendering the UI with `useChat`.\\n\\n#### After: Replace server action with route handler\\n\\nThe `streamText` function executes as part of a route handler and streams the response to the client. The `useChat` hook on the client decodes this stream and renders the response within the chat interface.\\n\\n```ts filename=\"@/app/api/chat/route.ts\"\\nimport { streamText } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nexport async function POST(request) {\\n  const { messages } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly assistant!\\',\\n    messages,\\n    tools: {\\n      // tool definitions\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n#### After: Update client to use chat hook\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { messages, input, setInput, handleSubmit } = useChat();\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>{message.content}</div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Parallel Tool Calls\\n\\nIn AI SDK RSC, `streamUI` does not support parallel tool calls. You will have to use a combination of `streamText`, `createStreamableUI` and `createStreamableValue`.\\n\\nWith AI SDK UI, `useChat` comes with built-in support for parallel tool calls. You can define multiple tools in the `streamText` and have them called them in parallel. The `useChat` hook will then handle the parallel tool calls for you automatically.\\n\\n### Multi-Step Tool Calls\\n\\nIn AI SDK RSC, `streamUI` does not support multi-step tool calls. You will have to use a combination of `streamText`, `createStreamableUI` and `createStreamableValue`.\\n\\nWith AI SDK UI, `useChat` comes with built-in support for multi-step tool calls. You can set `maxSteps` in the `streamText` function to define the number of steps the language model can make in a single call. The `useChat` hook will then handle the multi-step tool calls for you automatically.\\n\\n### Generative User Interfaces\\n\\nThe `streamUI` function uses `tools` as a way to execute functions based on user input and renders React components based on the function output to go beyond text in the chat interface.\\n\\n#### Before: Render components within the server action and stream to client\\n\\n```tsx filename=\"@/app/actions.tsx\"\\nimport { z } from \\'zod\\';\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { getWeather } from \\'@/utils/queries\\';\\nimport { Weather } from \\'@/components/weather\\';\\n\\nconst { value: stream } = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  system: \\'you are a friendly assistant!\\',\\n  messages,\\n  text: async function* ({ content, done }) {\\n    // process text\\n  },\\n  tools: {\\n    displayWeather: {\\n      description: \\'Display the weather for a location\\',\\n      inputSchema: z.object({\\n        latitude: z.number(),\\n        longitude: z.number(),\\n      }),\\n      generate: async function* ({ latitude, longitude }) {\\n        yield <div>Loading weather...</div>;\\n\\n        const { value, unit } = await getWeather({ latitude, longitude });\\n\\n        return <Weather value={value} unit={unit} />;\\n      },\\n    },\\n  },\\n});\\n```\\n\\nAs mentioned earlier, `streamUI` generates text and renders the React component in a single server action call.\\n\\n#### After: Replace with route handler and stream props data to client\\n\\nThe `streamText` function streams the props data as response to the client, while `useChat` decode the stream as `toolInvocations` and renders the chat interface.\\n\\n```ts filename=\"@/app/api/chat/route.ts\"\\nimport { z } from \\'zod\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { getWeather } from \\'@/utils/queries\\';\\nimport { streamText } from \\'ai\\';\\n\\nexport async function POST(request) {\\n  const { messages } = await request.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly assistant!\\',\\n    messages,\\n    tools: {\\n      displayWeather: {\\n        description: \\'Display the weather for a location\\',\\n        parameters: z.object({\\n          latitude: z.number(),\\n          longitude: z.number(),\\n        }),\\n        execute: async function ({ latitude, longitude }) {\\n          const props = await getWeather({ latitude, longitude });\\n          return props;\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n#### After: Update client to use chat hook and render components using tool invocations\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport { Weather } from \\'@/components/weather\\';\\n\\nexport default function Page() {\\n  const { messages, input, setInput, handleSubmit } = useChat();\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>{message.content}</div>\\n\\n          <div>\\n            {message.toolInvocations.map(toolInvocation => {\\n              const { toolName, toolCallId, state } = toolInvocation;\\n\\n              if (state === \\'result\\') {\\n                const { result } = toolInvocation;\\n\\n                return (\\n                  <div key={toolCallId}>\\n                    {toolName === \\'displayWeather\\' ? (\\n                      <Weather weatherAtLocation={result} />\\n                    ) : null}\\n                  </div>\\n                );\\n              } else {\\n                return (\\n                  <div key={toolCallId}>\\n                    {toolName === \\'displayWeather\\' ? (\\n                      <div>Loading weather...</div>\\n                    ) : null}\\n                  </div>\\n                );\\n              }\\n            })}\\n          </div>\\n        </div>\\n      ))}\\n\\n      <form onSubmit={handleSubmit}>\\n        <input\\n          type=\"text\"\\n          value={input}\\n          onChange={event => {\\n            setInput(event.target.value);\\n          }}\\n        />\\n        <button type=\"submit\">Send</button>\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Handling Client Interactions\\n\\nWith AI SDK RSC, components streamed to the client can trigger subsequent generations by calling the relevant server action using the `useActions` hooks. This is possible as long as the component is a descendant of the `<AI/>` context provider.\\n\\n#### Before: Use actions hook to send messages\\n\\n```tsx filename=\"@/app/components/list-flights.tsx\"\\n\\'use client\\';\\n\\nimport { useActions, useUIState } from \\'@ai-sdk/rsc\\';\\n\\nexport function ListFlights({ flights }) {\\n  const { sendMessage } = useActions();\\n  const [_, setMessages] = useUIState();\\n\\n  return (\\n    <div>\\n      {flights.map(flight => (\\n        <div\\n          key={flight.id}\\n          onClick={async () => {\\n            const response = await sendMessage(\\n              `I would like to choose flight ${flight.id}!`,\\n            );\\n\\n            setMessages(msgs => [...msgs, response]);\\n          }}\\n        >\\n          {flight.name}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n#### After: Use another chat hook with same ID from the component\\n\\nAfter switching to AI SDK UI, these messages are synced by initializing the `useChat` hook in the component with the same `id` as the parent component.\\n\\n```tsx filename=\"@/app/components/list-flights.tsx\"\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport function ListFlights({ chatId, flights }) {\\n  const { append } = useChat({\\n    id: chatId,\\n    body: { id: chatId },\\n    maxSteps: 5,\\n  });\\n\\n  return (\\n    <div>\\n      {flights.map(flight => (\\n        <div\\n          key={flight.id}\\n          onClick={async () => {\\n            await append({\\n              role: \\'user\\',\\n              content: `I would like to choose flight ${flight.id}!`,\\n            });\\n          }}\\n        >\\n          {flight.name}\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n### Loading Indicators\\n\\nIn AI SDK RSC, you can use the `initial` parameter of `streamUI` to define the component to display while the generation is in progress.\\n\\n#### Before: Use `loading` to show loading indicator\\n\\n```tsx filename=\"@/app/actions.tsx\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamUI } from \\'@ai-sdk/rsc\\';\\n\\nconst { value: stream } = await streamUI({\\n  model: openai(\\'gpt-4o\\'),\\n  system: \\'you are a friendly assistant!\\',\\n  messages,\\n  initial: <div>Loading...</div>,\\n  text: async function* ({ content, done }) {\\n    // process text\\n  },\\n  tools: {\\n    // tool definitions\\n  },\\n});\\n\\nreturn stream;\\n```\\n\\nWith AI SDK UI, you can use the tool invocation state to show a loading indicator while the tool is executing.\\n\\n#### After: Use tool invocation state to show loading indicator\\n\\n```tsx filename=\"@/app/components/message.tsx\"\\n\\'use client\\';\\n\\nexport function Message({ role, content, toolInvocations }) {\\n  return (\\n    <div>\\n      <div>{role}</div>\\n      <div>{content}</div>\\n\\n      {toolInvocations && (\\n        <div>\\n          {toolInvocations.map(toolInvocation => {\\n            const { toolName, toolCallId, state } = toolInvocation;\\n\\n            if (state === \\'result\\') {\\n              const { result } = toolInvocation;\\n\\n              return (\\n                <div key={toolCallId}>\\n                  {toolName === \\'getWeather\\' ? (\\n                    <Weather weatherAtLocation={result} />\\n                  ) : null}\\n                </div>\\n              );\\n            } else {\\n              return (\\n                <div key={toolCallId}>\\n                  {toolName === \\'getWeather\\' ? (\\n                    <Weather isLoading={true} />\\n                  ) : (\\n                    <div>Loading...</div>\\n                  )}\\n                </div>\\n              );\\n            }\\n          })}\\n        </div>\\n      )}\\n    </div>\\n  );\\n}\\n```\\n\\n### Saving Chats\\n\\nBefore implementing `streamUI` as a server action, you should create an `<AI/>` provider and wrap your application at the root layout to sync the AI and UI states. During initialization, you typically use the `onSetAIState` callback function to track updates to the AI state and save it to the database when `done(...)` is called.\\n\\n#### Before: Save chats using callback function of context provider\\n\\n```ts filename=\"@/app/actions.ts\"\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { saveChat } from \\'@/utils/queries\\';\\n\\nexport const AI = createAI({\\n  initialAIState: {},\\n  initialUIState: {},\\n  actions: {\\n    // server actions\\n  },\\n  onSetAIState: async ({ state, done }) => {\\n    \\'use server\\';\\n\\n    if (done) {\\n      await saveChat(state);\\n    }\\n  },\\n});\\n```\\n\\n#### After: Save chats using callback function of `streamText`\\n\\nWith AI SDK UI, you will save chats using the `onFinish` callback function of `streamText` in your route handler.\\n\\n```ts filename=\"@/app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { saveChat } from \\'@/utils/queries\\';\\nimport { streamText, convertToModelMessages } from \\'ai\\';\\n\\nexport async function POST(request) {\\n  const { id, messages } = await request.json();\\n\\n  const coreMessages = convertToModelMessages(messages);\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly assistant!\\',\\n    messages: coreMessages,\\n    onFinish: async ({ response }) => {\\n      try {\\n        await saveChat({\\n          id,\\n          messages: [...coreMessages, ...response.messages],\\n        });\\n      } catch (error) {\\n        console.error(\\'Failed to save chat\\');\\n      }\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Restoring Chats\\n\\nWhen using AI SDK RSC, the `useUIState` hook contains the UI state of the chat. When restoring a previously saved chat, the UI state needs to be loaded with messages.\\n\\nSimilar to how you typically save chats in AI SDK RSC, you should use the `onGetUIState` callback function to retrieve the chat from the database, convert it into UI state, and return it to be accessible through `useUIState`.\\n\\n#### Before: Load chat from database using callback function of context provider\\n\\n```ts filename=\"@/app/actions.ts\"\\nimport { createAI } from \\'@ai-sdk/rsc\\';\\nimport { loadChatFromDB, convertToUIState } from \\'@/utils/queries\\';\\n\\nexport const AI = createAI({\\n  actions: {\\n    // server actions\\n  },\\n  onGetUIState: async () => {\\n    \\'use server\\';\\n\\n    const chat = await loadChatFromDB();\\n    const uiState = convertToUIState(chat);\\n\\n    return uiState;\\n  },\\n});\\n```\\n\\nAI SDK UI uses the `messages` field of `useChat` to store messages. To load messages when `useChat` is mounted, you should use `initialMessages`.\\n\\nAs messages are typically loaded from the database, we can use a server actions inside a Page component to fetch an older chat from the database during static generation and pass the messages as props to the `<Chat/>` component.\\n\\n#### After: Load chat from database during static generation of page\\n\\n```tsx filename=\"@/app/chat/[id]/page.tsx\"\\nimport { Chat } from \\'@/app/components/chat\\';\\nimport { getChatById } from \\'@/utils/queries\\';\\n\\n// link to example implementation: https://github.com/vercel/ai-chatbot/blob/00b125378c998d19ef60b73fe576df0fe5a0e9d4/lib/utils.ts#L87-L127\\nimport { convertToUIMessages } from \\'@/utils/functions\\';\\n\\nexport default async function Page({ params }: { params: any }) {\\n  const { id } = params;\\n  const chatFromDb = await getChatById({ id });\\n\\n  const chat: Chat = {\\n    ...chatFromDb,\\n    messages: convertToUIMessages(chatFromDb.messages),\\n  };\\n\\n  return <Chat key={id} id={chat.id} initialMessages={chat.messages} />;\\n}\\n```\\n\\n#### After: Pass chat messages as props and load into chat hook\\n\\n```tsx filename=\"@/app/components/chat.tsx\"\\n\\'use client\\';\\n\\nimport { Message } from \\'ai\\';\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport function Chat({\\n  id,\\n  initialMessages,\\n}: {\\n  id;\\n  initialMessages: Array<Message>;\\n}) {\\n  const { messages } = useChat({\\n    id,\\n    initialMessages,\\n  });\\n\\n  return (\\n    <div>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          <div>{message.role}</div>\\n          <div>{message.content}</div>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n\\n## Streaming Object Generation\\n\\nThe `createStreamableValue` function streams any serializable data from the server to the client. As a result, this function allows you to stream object generations from the server to the client when paired with `streamObject`.\\n\\n#### Before: Use streamable value to stream object generations\\n\\n```ts filename=\"@/app/actions.ts\"\\nimport { streamObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { createStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { notificationsSchema } from \\'@/utils/schemas\\';\\n\\nexport async function generateSampleNotifications() {\\n  \\'use server\\';\\n\\n  const stream = createStreamableValue();\\n\\n  (async () => {\\n    const { partialObjectStream } = streamObject({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      system: \\'generate sample ios messages for testing\\',\\n      prompt: \\'messages from a family group chat during diwali, max 4\\',\\n      schema: notificationsSchema,\\n    });\\n\\n    for await (const partialObject of partialObjectStream) {\\n      stream.update(partialObject);\\n    }\\n  })();\\n\\n  stream.done();\\n\\n  return { partialNotificationsStream: stream.value };\\n}\\n```\\n\\n#### Before: Read streamable value and update object\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useState } from \\'react\\';\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\nimport { generateSampleNotifications } from \\'@/app/actions\\';\\n\\nexport default function Page() {\\n  const [notifications, setNotifications] = useState(null);\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const { partialNotificationsStream } =\\n            await generateSampleNotifications();\\n\\n          for await (const partialNotifications of readStreamableValue(\\n            partialNotificationsStream,\\n          )) {\\n            if (partialNotifications) {\\n              setNotifications(partialNotifications.notifications);\\n            }\\n          }\\n        }}\\n      >\\n        Generate\\n      </button>\\n    </div>\\n  );\\n}\\n```\\n\\nTo migrate to AI SDK UI, you should use the `useObject` hook and implement `streamObject` within your route handler.\\n\\n#### After: Replace with route handler and stream text response\\n\\n```ts filename=\"@/app/api/object/route.ts\"\\nimport { streamObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { notificationSchema } from \\'@/utils/schemas\\';\\n\\nexport async function POST(req: Request) {\\n  const context = await req.json();\\n\\n  const result = streamObject({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    schema: notificationSchema,\\n    prompt:\\n      `Generate 3 notifications for a messages app in this context:` + context,\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n#### After: Use object hook to decode stream and update object\\n\\n```tsx filename=\"@/app/page.tsx\"\\n\\'use client\\';\\n\\nimport { useObject } from \\'@ai-sdk/react\\';\\nimport { notificationSchema } from \\'@/utils/schemas\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/object\\',\\n    schema: notificationSchema,\\n  });\\n\\n  return (\\n    <div>\\n      <button onClick={() => submit(\\'Messages during finals week.\\')}>\\n        Generate notifications\\n      </button>\\n\\n      {object?.notifications?.map((notification, index) => (\\n        <div key={index}>\\n          <p>{notification?.name}</p>\\n          <p>{notification?.message}</p>\\n        </div>\\n      ))}\\n    </div>\\n  );\\n}\\n```\\n', children=[]), DocItem(origPath=Path('05-ai-sdk-rsc/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK RSC\\ndescription: Learn about AI SDK RSC.\\ncollapsed: true\\n---\\n\\n# AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: \\'Overview\\',\\n      description: \\'Learn about AI SDK RSC.\\',\\n      href: \\'/docs/ai-sdk-rsc/overview\\',\\n    },\\n    {\\n      title: \\'Streaming React Components\\',\\n      description: \\'Learn how to stream React components.\\',\\n      href: \\'/docs/ai-sdk-rsc/streaming-react-components\\',\\n    },\\n    {\\n      title: \\'Managing Generative UI State\\',\\n      description: \\'Learn how to manage generative UI state.\\',\\n      href: \\'/docs/ai-sdk-rsc/generative-ui-state\\',\\n    },\\n    {\\n      title: \\'Saving and Restoring States\\',\\n      description: \\'Learn how to save and restore states.\\',\\n      href: \\'/docs/ai-sdk-rsc/saving-and-restoring-states\\',\\n    },\\n    {\\n      title: \\'Multi-step Interfaces\\',\\n      description: \\'Learn how to build multi-step interfaces.\\',\\n      href: \\'/docs/ai-sdk-rsc/multistep-interfaces\\',\\n    },\\n    {\\n      title: \\'Streaming Values\\',\\n      description: \\'Learn how to stream values with AI SDK RSC.\\',\\n      href: \\'/docs/ai-sdk-rsc/streaming-values\\',\\n    },\\n    {\\n      title: \\'Error Handling\\',\\n      description: \\'Learn how to handle errors.\\',\\n      href: \\'/docs/ai-sdk-rsc/error-handling\\',\\n    },\\n    {\\n      title: \\'Authentication\\',\\n      description: \\'Learn how to authenticate users.\\',\\n      href: \\'/docs/ai-sdk-rsc/authentication\\',\\n    },\\n  ]}\\n/>\\n', children=[])]),\n",
       " DocItem(origPath=Path('06-advanced'), name='06-advanced', displayName='06-advanced', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('06-advanced/01-prompt-engineering.mdx'), name='01-prompt-engineering.mdx', displayName='01-prompt-engineering.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Prompt Engineering\\ndescription: Learn how to engineer prompts for LLMs with the AI SDK\\n---\\n\\n# Prompt Engineering\\n\\n## What is a Large Language Model (LLM)?\\n\\nA Large Language Model is essentially a prediction engine that takes a sequence of words as input and aims to predict the most likely sequence to follow. It does this by assigning probabilities to potential next sequences and then selecting one. The model continues to generate sequences until it meets a specified stopping criterion.\\n\\nThese models learn by training on massive text corpuses, which means they will be better suited to some use cases than others. For example, a model trained on GitHub data would understand the probabilities of sequences in source code particularly well. However, it\\'s crucial to understand that the generated sequences, while often seeming plausible, can sometimes be random and not grounded in reality. As these models become more accurate, many surprising abilities and applications emerge.\\n\\n## What is a prompt?\\n\\nPrompts are the starting points for LLMs. They are the inputs that trigger the model to generate text. The scope of prompt engineering involves not just crafting these prompts but also understanding related concepts such as hidden prompts, tokens, token limits, and the potential for prompt hacking, which includes phenomena like jailbreaks and leaks.\\n\\n## Why is prompt engineering needed?\\n\\nPrompt engineering currently plays a pivotal role in shaping the responses of LLMs. It allows us to tweak the model to respond more effectively to a broader range of queries. This includes the use of techniques like semantic search, command grammars, and the ReActive model architecture. The performance, context window, and cost of LLMs varies between models and model providers which adds further constraints to the mix. For example, the GPT-4 model is more expensive than GPT-3.5-turbo and significantly slower, but it can also be more effective at certain tasks. And so, like many things in software engineering, there is a trade-offs between cost and performance.\\n\\nTo assist with comparing and tweaking LLMs, we\\'ve built an AI playground that allows you to compare the performance of different models side-by-side online. When you\\'re ready, you can even generate code with the AI SDK to quickly use your prompt and your selected model into your own applications.\\n\\n## Example: Build a Slogan Generator\\n\\n### Start with an instruction\\n\\nImagine you want to build a slogan generator for marketing campaigns. Creating catchy slogans isn\\'t always straightforward!\\n\\nFirst, you\\'ll need a prompt that makes it clear what you want. Let\\'s start with an instruction. Submit this prompt to generate your first completion.\\n\\n<InlinePrompt initialInput=\"Create a slogan for a coffee shop.\" />\\n\\nNot bad! Now, try making your instruction more specific.\\n\\n<InlinePrompt initialInput=\"Create a slogan for an organic coffee shop.\" />\\n\\nIntroducing a single descriptive term to our prompt influences the completion. Essentially, crafting your prompt is the means by which you \"instruct\" or \"program\" the model.\\n\\n### Include examples\\n\\nClear instructions are key for quality outcomes, but that might not always be enough. Let\\'s try to enhance your instruction further.\\n\\n<InlinePrompt initialInput=\"Create three slogans for a coffee shop with live music.\" />\\n\\nThese slogans are fine, but could be even better. It appears the model overlooked the \\'live\\' part in our prompt. Let\\'s change it slightly to generate more appropriate suggestions.\\n\\nOften, it\\'s beneficial to both demonstrate and tell the model your requirements. Incorporating examples in your prompt can aid in conveying patterns or subtleties. Test this prompt that carries a few examples.\\n\\n<InlinePrompt\\n  initialInput={`Create three slogans for a business with unique features.\\n\\nBusiness: Bookstore with cats\\nSlogans: \"Purr-fect Pages\", \"Books and Whiskers\", \"Novels and Nuzzles\"\\nBusiness: Gym with rock climbing\\nSlogans: \"Peak Performance\", \"Reach New Heights\", \"Climb Your Way Fit\"\\nBusiness: Coffee shop with live music\\nSlogans:`}\\n/>\\n\\nGreat! Incorporating examples of expected output for a certain input prompted the model to generate the kind of names we aimed for.\\n\\n### Tweak your settings\\n\\nApart from designing prompts, you can influence completions by tweaking model settings. A crucial setting is the **temperature**.\\n\\nYou might have seen that the same prompt, when repeated, yielded the same or nearly the same completions. This happens when your temperature is at 0.\\n\\nAttempt to re-submit the identical prompt a few times with temperature set to 1.\\n\\n<InlinePrompt\\n  initialInput={`Create three slogans for a business with unique features.\\n\\nBusiness: Bookstore with cats\\nSlogans: \"Purr-fect Pages\", \"Books and Whiskers\", \"Novels and Nuzzles\"\\nBusiness: Gym with rock climbing\\nSlogans: \"Peak Performance\", \"Reach New Heights\", \"Climb Your Way Fit\"\\nBusiness: Coffee shop with live music\\nSlogans:`}\\nshowTemp={true}\\ninitialTemperature={1}\\n/>\\n\\nNotice the difference? With a temperature above 0, the same prompt delivers varied completions each time.\\n\\nKeep in mind that the model forecasts the text most likely to follow the preceding text. Temperature, a value from 0 to 1, essentially governs the model\\'s confidence level in making these predictions. A lower temperature implies lesser risks, leading to more precise and deterministic completions. A higher temperature yields a broader range of completions.\\n\\nFor your slogan generator, you might want a large pool of name suggestions. A moderate temperature of 0.6 should serve well.\\n\\n## Recommended Resources\\n\\nPrompt Engineering is evolving rapidly, with new methods and research papers surfacing every week. Here are some resources that we\\'ve found useful for learning about and experimenting with prompt engineering:\\n\\n- [The Vercel AI Playground](/playground)\\n- [Brex Prompt Engineering](https://github.com/brexhq/prompt-engineering)\\n- [Prompt Engineering Guide by Dair AI](https://www.promptingguide.ai/)\\n', children=[]), DocItem(origPath=Path('06-advanced/02-stopping-streams.mdx'), name='02-stopping-streams.mdx', displayName='02-stopping-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stopping Streams\\ndescription: Learn how to cancel streams with the AI SDK\\n---\\n\\n# Stopping Streams\\n\\nCancelling ongoing streams is often needed.\\nFor example, users might want to stop a stream when they realize that the response is not what they want.\\n\\nThe different parts of the AI SDK support cancelling streams in different ways.\\n\\n## AI SDK Core\\n\\nThe AI SDK functions have an `abortSignal` argument that you can use to cancel a stream.\\nYou would use this if you want to cancel a stream from the server side to the LLM API, e.g. by\\nforwarding the `abortSignal` from the request.\\n\\n```tsx highlight=\"10,11,12-16\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt,\\n    // forward the abort signal:\\n    abortSignal: req.signal,\\n    onAbort: ({ steps }) => {\\n      // Handle cleanup when stream is aborted\\n      console.log(\\'Stream aborted after\\', steps.length, \\'steps\\');\\n      // Persist partial results to database\\n    },\\n  });\\n\\n  return result.toTextStreamResponse();\\n}\\n```\\n\\n## AI SDK UI\\n\\nThe hooks, e.g. `useChat` or `useCompletion`, provide a `stop` helper function that can be used to cancel a stream.\\nThis will cancel the stream from the client side to the server.\\n\\n<Note type=\"warning\">\\n  Stream abort functionality is not compatible with stream resumption. If you\\'re\\n  using `resume: true` in `useChat`, the abort functionality will break the\\n  resumption mechanism. Choose either abort or resume functionality, but not\\n  both.\\n</Note>\\n\\n```tsx file=\"app/page.tsx\" highlight=\"9,18-20\"\\n\\'use client\\';\\n\\nimport { useCompletion } from \\'@ai-sdk/react\\';\\n\\nexport default function Chat() {\\n  const { input, completion, stop, status, handleSubmit, handleInputChange } =\\n    useCompletion();\\n\\n  return (\\n    <div>\\n      {(status === \\'submitted\\' || status === \\'streaming\\') && (\\n        <button type=\"button\" onClick={() => stop()}>\\n          Stop\\n        </button>\\n      )}\\n      {completion}\\n      <form onSubmit={handleSubmit}>\\n        <input value={input} onChange={handleInputChange} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n## Handling stream abort cleanup\\n\\nWhen streams are aborted, you may need to perform cleanup operations such as persisting partial results or cleaning up resources. The `onAbort` callback provides a way to handle these scenarios on the server side.\\n\\nUnlike `onFinish`, which is called when a stream completes normally, `onAbort` is specifically called when a stream is aborted via `AbortSignal`. This distinction allows you to handle normal completion and aborted streams differently.\\n\\n<Note>\\n  For UI message streams (`toUIMessageStreamResponse`), the `onFinish` callback\\n  also receives an `isAborted` parameter that indicates whether the stream was\\n  aborted. This allows you to handle both completion and abort scenarios in a\\n  single callback.\\n</Note>\\n\\n```tsx highlight=\"8-12\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Write a long story...\\',\\n  abortSignal: controller.signal,\\n  onAbort: ({ steps }) => {\\n    // Called when stream is aborted - persist partial results\\n    await savePartialResults(steps);\\n    await logAbortEvent(steps.length);\\n  },\\n  onFinish: ({ steps, totalUsage }) => {\\n    // Called when stream completes normally\\n    await saveFinalResults(steps, totalUsage);\\n  },\\n});\\n```\\n\\nThe `onAbort` callback receives:\\n\\n- `steps`: Array of all completed steps before the abort occurred\\n\\nThis is particularly useful for:\\n\\n- Persisting partial conversation history to database\\n- Saving partial progress for later continuation\\n- Cleaning up server-side resources or connections\\n- Logging abort events for analytics\\n\\nYou can also handle abort events directly in the stream using the `abort` stream part:\\n\\n```tsx highlight=\"8-12\"\\nfor await (const part of result.fullStream) {\\n  switch (part.type) {\\n    case \\'text-delta\\':\\n      // Handle text delta content\\n      break;\\n    case \\'abort\\':\\n      // Handle abort event directly in stream\\n      console.log(\\'Stream was aborted\\');\\n      break;\\n    // ... other cases\\n  }\\n}\\n```\\n\\n## UI Message Streams\\n\\nWhen using `toUIMessageStreamResponse`, you need to handle stream abortion slightly differently. The `onFinish` callback receives an `isAborted` parameter, and you should pass the `consumeStream` function to ensure proper abort handling:\\n\\n```tsx highlight=\"5,19,20-24,26\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport {\\n  consumeStream,\\n  convertToModelMessages,\\n  streamText,\\n  UIMessage,\\n} from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    abortSignal: req.signal,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onFinish: async ({ isAborted }) => {\\n      if (isAborted) {\\n        console.log(\\'Stream was aborted\\');\\n        // Handle abort-specific cleanup\\n      } else {\\n        console.log(\\'Stream completed normally\\');\\n        // Handle normal completion\\n      }\\n    },\\n    consumeSseStream: consumeStream,\\n  });\\n}\\n```\\n\\nThe `consumeStream` function is necessary for proper abort handling in UI message streams. It ensures that the stream is properly consumed even when aborted, preventing potential memory leaks or hanging connections.\\n\\n## AI SDK RSC\\n\\n<Note type=\"warning\">\\n  The AI SDK RSC does not currently support stopping streams.\\n</Note>\\n', children=[]), DocItem(origPath=Path('06-advanced/03-backpressure.mdx'), name='03-backpressure.mdx', displayName='03-backpressure.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Backpressure\\ndescription: How to handle backpressure and cancellation when working with the AI SDK\\n---\\n\\n# Stream Back-pressure and Cancellation\\n\\nThis page focuses on understanding back-pressure and cancellation when working with streams. You do not need to know this information to use the AI SDK, but for those interested, it offers a deeper dive on why and how the SDK optimally streams responses.\\n\\nIn the following sections, we\\'ll explore back-pressure and cancellation in the context of a simple example program. We\\'ll discuss the issues that can arise from an eager approach and demonstrate how a lazy approach can resolve them.\\n\\n## Back-pressure and Cancellation with Streams\\n\\nLet\\'s begin by setting up a simple example program:\\n\\n```jsx\\n// A generator that will yield positive integers\\nasync function* integers() {\\n  let i = 1;\\n  while (true) {\\n    console.log(`yielding ${i}`);\\n    yield i++;\\n\\n    await sleep(100);\\n  }\\n}\\nfunction sleep(ms) {\\n  return new Promise(resolve => setTimeout(resolve, ms));\\n}\\n\\n// Wraps a generator into a ReadableStream\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async start(controller) {\\n      for await (const v of iterator) {\\n        controller.enqueue(v);\\n      }\\n      controller.close();\\n    },\\n  });\\n}\\n\\n// Collect data from stream\\nasync function run() {\\n  // Set up a stream of integers\\n  const stream = createStream(integers());\\n\\n  // Read values from our stream\\n  const reader = stream.getReader();\\n  for (let i = 0; i < 10_000; i++) {\\n    // we know our stream is infinite, so there\\'s no need to check `done`.\\n    const { value } = await reader.read();\\n    console.log(`read ${value}`);\\n\\n    await sleep(1_000);\\n  }\\n}\\nrun();\\n```\\n\\nIn this example, we create an async-generator that yields positive integers, a `ReadableStream` that wraps our integer generator, and a reader which will read values out of our stream. Notice, too, that our integer generator logs out `\"yielding ${i}\"`, and our reader logs out `\"read ${value}\"`. Both take an arbitrary amount of time to process data, represented with a 100ms sleep in our generator, and a 1sec sleep in our reader.\\n\\n## Back-pressure\\n\\nIf you were to run this program, you\\'d notice something funny. We\\'ll see roughly 10 \"yield\" logs for every \"read\" log. This might seem obvious, the generator can push values 10x faster than the reader can pull them out. But it represents a problem, our `stream` has to maintain an ever expanding queue of items that have been pushed in but not pulled out.\\n\\nThe problem stems from the way we wrap our generator into a stream. Notice the use of `for await (…)` inside our `start` handler. This is an **eager** for-loop, and it is constantly running to get the next value from our generator to be enqueued in our stream. This means our stream does not respect back-pressure, the signal from the consumer to the producer that more values aren\\'t needed _yet_. We\\'ve essentially spawned a thread that will perpetually push more data into the stream, one that runs as fast as possible to push new data immediately. Worse, there\\'s no way to signal to this thread to stop running when we don\\'t need additional data.\\n\\nTo fix this, `ReadableStream` allows a `pull` handler. `pull` is called every time the consumer attempts to read more data from our stream (if there\\'s no data already queued internally). But it\\'s not enough to just move the `for await(…)` into `pull`, we also need to convert from an eager enqueuing to a **lazy** one. By making these 2 changes, we\\'ll be able to react to the consumer. If they need more data, we can easily produce it, and if they don\\'t, then we don\\'t need to spend any time doing unnecessary work.\\n\\n```jsx\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async pull(controller) {\\n      const { value, done } = await iterator.next();\\n\\n      if (done) {\\n        controller.close();\\n      } else {\\n        controller.enqueue(value);\\n      }\\n    },\\n  });\\n}\\n```\\n\\nOur `createStream` is a little more verbose now, but the new code is important. First, we need to manually call our `iterator.next()` method. This returns a `Promise` for an object with the type signature `{ done: boolean, value: T }`. If `done` is `true`, then we know that our iterator won\\'t yield any more values and we must `close` the stream (this allows the consumer to know that the stream is also finished producing values). Else, we need to `enqueue` our newly produced value.\\n\\nWhen we run this program, we see that our \"yield\" and \"read\" logs are now paired. We\\'re no longer yielding 10x integers for every read! And, our stream now only needs to maintain 1 item in its internal buffer. We\\'ve essentially given control to the consumer, so that it\\'s responsible for producing new values as it needs it. Neato!\\n\\n## Cancellation\\n\\nLet\\'s go back to our initial eager example, with 1 small edit. Now instead of reading 10,000 integers, we\\'re only going to read 3:\\n\\n```jsx\\n// A generator that will yield positive integers\\nasync function* integers() {\\n  let i = 1;\\n  while (true) {\\n    console.log(`yielding ${i}`);\\n    yield i++;\\n\\n    await sleep(100);\\n  }\\n}\\nfunction sleep(ms) {\\n  return new Promise(resolve => setTimeout(resolve, ms));\\n}\\n\\n// Wraps a generator into a ReadableStream\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async start(controller) {\\n      for await (const v of iterator) {\\n        controller.enqueue(v);\\n      }\\n      controller.close();\\n    },\\n  });\\n}\\n// Collect data from stream\\nasync function run() {\\n  // Set up a stream that of integers\\n  const stream = createStream(integers());\\n\\n  // Read values from our stream\\n  const reader = stream.getReader();\\n  // We\\'re only reading 3 items this time:\\n  for (let i = 0; i < 3; i++) {\\n    // we know our stream is infinite, so there\\'s no need to check `done`.\\n    const { value } = await reader.read();\\n    console.log(`read ${value}`);\\n\\n    await sleep(1000);\\n  }\\n}\\nrun();\\n```\\n\\nWe\\'re back to yielding 10x the number of values read. But notice now, after we\\'ve read 3 values, we\\'re continuing to yield new values. We know that our reader will never read another value, but our stream doesn\\'t! The eager `for await (…)` will continue forever, loudly enqueuing new values into our stream\\'s buffer and increasing our memory usage until it consumes all available program memory.\\n\\nThe fix to this is exactly the same: use `pull` and manual iteration. By producing values _**lazily**_, we tie the lifetime of our integer generator to the lifetime of the reader. Once the reads stop, the yields will stop too:\\n\\n```jsx\\n// Wraps a generator into a ReadableStream\\nfunction createStream(iterator) {\\n  return new ReadableStream({\\n    async pull(controller) {\\n      const { value, done } = await iterator.next();\\n\\n      if (done) {\\n        controller.close();\\n      } else {\\n        controller.enqueue(value);\\n      }\\n    },\\n  });\\n}\\n```\\n\\nSince the solution is the same as implementing back-pressure, it shows that they\\'re just 2 facets of the same problem: Pushing values into a stream should be done **lazily**, and doing it eagerly results in expected problems.\\n\\n## Tying Stream Laziness to AI Responses\\n\\nNow let\\'s imagine you\\'re integrating AIBot service into your product. Users will be able to prompt \"count from 1 to infinity\", the browser will fetch your AI API endpoint, and your servers connect to AIBot to get a response. But \"infinity\" is, well, infinite. The response will never end!\\n\\nAfter a few seconds, the user gets bored and navigates away. Or maybe you\\'re doing local development and a hot-module reload refreshes your page. The browser will have ended its connection to the API endpoint, but will your server end its connection with AIBot?\\n\\nIf you used the eager `for await (...)` approach, then the connection is still running and your server is asking for more and more data from AIBot. Our server spawned a \"thread\" and there\\'s no signal when we can end the eager pulls. Eventually, the server is going to run out of memory (remember, there\\'s no active fetch connection to read the buffering responses and free them).\\n\\n{/* When we started writing the streaming code for the AI SDK, we confirm aborting a fetch will end a streamed response from Next.js */}\\n\\nWith the lazy approach, this is taken care of for you. Because the stream will only request new data from AIBot when the consumer requests it, navigating away from the page naturally frees all resources. The fetch connection aborts and the server can clean up the response. The `ReadableStream` tied to that response can now be garbage collected. When that happens, the connection it holds to AIBot can then be freed.\\n', children=[]), DocItem(origPath=Path('06-advanced/04-caching.mdx'), name='04-caching.mdx', displayName='04-caching.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Caching\\ndescription: How to handle caching when working with the AI SDK\\n---\\n\\n# Caching Responses\\n\\nDepending on the type of application you\\'re building, you may want to cache the responses you receive from your AI provider, at least temporarily.\\n\\n## Using Language Model Middleware (Recommended)\\n\\nThe recommended approach to caching responses is using [language model middleware](/docs/ai-sdk-core/middleware)\\nand the [`simulateReadableStream`](/docs/reference/ai-sdk-core/simulate-readable-stream) function.\\n\\nLanguage model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model.\\nLet\\'s see how you can use language model middleware to cache responses.\\n\\n```ts filename=\"ai/middleware.ts\"\\nimport { Redis } from \\'@upstash/redis\\';\\nimport {\\n  type LanguageModelV3,\\n  type LanguageModelV3Middleware,\\n  type LanguageModelV3StreamPart,\\n  simulateReadableStream,\\n} from \\'ai\\';\\n\\nconst redis = new Redis({\\n  url: process.env.KV_URL,\\n  token: process.env.KV_TOKEN,\\n});\\n\\nexport const cacheMiddleware: LanguageModelV3Middleware = {\\n  wrapGenerate: async ({ doGenerate, params }) => {\\n    const cacheKey = JSON.stringify(params);\\n\\n    const cached = (await redis.get(cacheKey)) as Awaited<\\n      ReturnType<LanguageModelV3[\\'doGenerate\\']>\\n    > | null;\\n\\n    if (cached !== null) {\\n      return {\\n        ...cached,\\n        response: {\\n          ...cached.response,\\n          timestamp: cached?.response?.timestamp\\n            ? new Date(cached?.response?.timestamp)\\n            : undefined,\\n        },\\n      };\\n    }\\n\\n    const result = await doGenerate();\\n\\n    redis.set(cacheKey, result);\\n\\n    return result;\\n  },\\n  wrapStream: async ({ doStream, params }) => {\\n    const cacheKey = JSON.stringify(params);\\n\\n    // Check if the result is in the cache\\n    const cached = await redis.get(cacheKey);\\n\\n    // If cached, return a simulated ReadableStream that yields the cached result\\n    if (cached !== null) {\\n      // Format the timestamps in the cached response\\n      const formattedChunks = (cached as LanguageModelV3StreamPart[]).map(p => {\\n        if (p.type === \\'response-metadata\\' && p.timestamp) {\\n          return { ...p, timestamp: new Date(p.timestamp) };\\n        } else return p;\\n      });\\n      return {\\n        stream: simulateReadableStream({\\n          initialDelayInMs: 0,\\n          chunkDelayInMs: 10,\\n          chunks: formattedChunks,\\n        }),\\n      };\\n    }\\n\\n    // If not cached, proceed with streaming\\n    const { stream, ...rest } = await doStream();\\n\\n    const fullResponse: LanguageModelV3StreamPart[] = [];\\n\\n    const transformStream = new TransformStream<\\n      LanguageModelV3StreamPart,\\n      LanguageModelV3StreamPart\\n    >({\\n      transform(chunk, controller) {\\n        fullResponse.push(chunk);\\n        controller.enqueue(chunk);\\n      },\\n      flush() {\\n        // Store the full response in the cache after streaming is complete\\n        redis.set(cacheKey, fullResponse);\\n      },\\n    });\\n\\n    return {\\n      stream: stream.pipeThrough(transformStream),\\n      ...rest,\\n    };\\n  },\\n};\\n```\\n\\n<Note>\\n  This example uses `@upstash/redis` to store and retrieve the assistant\\'s\\n  responses but you can use any KV storage provider you would like.\\n</Note>\\n\\n`LanguageModelMiddleware` has two methods: `wrapGenerate` and `wrapStream`. `wrapGenerate` is called when using [`generateText`](/docs/reference/ai-sdk-core/generate-text) and [`generateObject`](/docs/reference/ai-sdk-core/generate-object), while `wrapStream` is called when using [`streamText`](/docs/reference/ai-sdk-core/stream-text) and [`streamObject`](/docs/reference/ai-sdk-core/stream-object).\\n\\nFor `wrapGenerate`, you can cache the response directly. Instead, for `wrapStream`, you cache an array of the stream parts, which can then be used with [`simulateReadableStream`](/docs/ai-sdk-core/testing#simulate-data-stream-protocol-responses) function to create a simulated `ReadableStream` that returns the cached response. In this way, the cached response is returned chunk-by-chunk as if it were being generated by the model. You can control the initial delay and delay between chunks by adjusting the `initialDelayInMs` and `chunkDelayInMs` parameters of `simulateReadableStream`.\\n\\nYou can see a full example of caching with Redis in a Next.js application in our [Caching Middleware Recipe](/cookbook/next/caching-middleware).\\n\\n## Using Lifecycle Callbacks\\n\\nAlternatively, each AI SDK Core function has special lifecycle callbacks you can use. The one of interest is likely `onFinish`, which is called when the generation is complete. This is where you can cache the full response.\\n\\nHere\\'s an example of how you can implement caching using Vercel KV and Next.js to cache the OpenAI response for 1 hour:\\n\\nThis example uses [Upstash Redis](https://upstash.com/docs/redis/overall/getstarted) and Next.js to cache the response for 1 hour.\\n\\n```tsx filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { formatDataStreamPart, streamText, UIMessage } from \\'ai\\';\\nimport { Redis } from \\'@upstash/redis\\';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\nconst redis = new Redis({\\n  url: process.env.KV_URL,\\n  token: process.env.KV_TOKEN,\\n});\\n\\nexport async function POST(req: Request) {\\n  const { messages }: { messages: UIMessage[] } = await req.json();\\n\\n  // come up with a key based on the request:\\n  const key = JSON.stringify(messages);\\n\\n  // Check if we have a cached response\\n  const cached = await redis.get(key);\\n  if (cached != null) {\\n    return new Response(formatDataStreamPart(\\'text\\', cached), {\\n      status: 200,\\n      headers: { \\'Content-Type\\': \\'text/plain\\' },\\n    });\\n  }\\n\\n  // Call the language model:\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n    async onFinish({ text }) {\\n      // Cache the response text:\\n      await redis.set(key, text);\\n      await redis.expire(key, 60 * 60);\\n    },\\n  });\\n\\n  // Respond with the stream\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n', children=[]), DocItem(origPath=Path('06-advanced/05-multiple-streamables.mdx'), name='05-multiple-streamables.mdx', displayName='05-multiple-streamables.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Multiple Streamables\\ndescription: Learn to handle multiple streamables in your application.\\n---\\n\\n# Multiple Streams\\n\\n## Multiple Streamable UIs\\n\\nThe AI SDK RSC APIs allow you to compose and return any number of streamable UIs, along with other data, in a single request. This can be useful when you want to decouple the UI into smaller components and stream them separately.\\n\\n```tsx file='app/actions.tsx'\\n'use server';\\n\\nimport { createStreamableUI } from '@ai-sdk/rsc';\\n\\nexport async function getWeather() {\\n  const weatherUI = createStreamableUI();\\n  const forecastUI = createStreamableUI();\\n\\n  weatherUI.update(<div>Loading weather...</div>);\\n  forecastUI.update(<div>Loading forecast...</div>);\\n\\n  getWeatherData().then(weatherData => {\\n    weatherUI.done(<div>{weatherData}</div>);\\n  });\\n\\n  getForecastData().then(forecastData => {\\n    forecastUI.done(<div>{forecastData}</div>);\\n  });\\n\\n  // Return both streamable UIs and other data fields.\\n  return {\\n    requestedAt: Date.now(),\\n    weather: weatherUI.value,\\n    forecast: forecastUI.value,\\n  };\\n}\\n```\\n\\nThe client side code is similar to the previous example, but the [tool call](/docs/ai-sdk-core/tools-and-tool-calling) will return the new data structure with the weather and forecast UIs. Depending on the speed of getting weather and forecast data, these two components might be updated independently.\\n\\n## Nested Streamable UIs\\n\\nYou can stream UI components within other UI components. This allows you to create complex UIs that are built up from smaller, reusable components. In the example below, we pass a `historyChart` streamable as a prop to a `StockCard` component. The StockCard can render the `historyChart` streamable, and it will automatically update as the server responds with new data.\\n\\n```tsx file='app/actions.tsx'\\nasync function getStockHistoryChart({ symbol: string }) {\\n  'use server';\\n\\n  const ui = createStreamableUI(<Spinner />);\\n\\n  // We need to wrap this in an async IIFE to avoid blocking.\\n  (async () => {\\n    const price = await getStockPrice({ symbol });\\n\\n    // Show a spinner as the history chart for now.\\n    const historyChart = createStreamableUI(<Spinner />);\\n    ui.done(<StockCard historyChart={historyChart.value} price={price} />);\\n\\n    // Getting the history data and then update that part of the UI.\\n    const historyData = await fetch('https://my-stock-data-api.com');\\n    historyChart.done(<HistoryChart data={historyData} />);\\n  })();\\n\\n  return ui;\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('06-advanced/06-rate-limiting.mdx'), name='06-rate-limiting.mdx', displayName='06-rate-limiting.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Rate Limiting\\ndescription: Learn how to rate limit your application.\\n---\\n\\n# Rate Limiting\\n\\nRate limiting helps you protect your APIs from abuse. It involves setting a\\nmaximum threshold on the number of requests a client can make within a\\nspecified timeframe. This simple technique acts as a gatekeeper,\\npreventing excessive usage that can degrade service performance and incur\\nunnecessary costs.\\n\\n## Rate Limiting with Vercel KV and Upstash Ratelimit\\n\\nIn this example, you will protect an API endpoint using [Vercel KV](https://vercel.com/storage/kv)\\nand [Upstash Ratelimit](https://github.com/upstash/ratelimit).\\n\\n```tsx filename='app/api/generate/route.ts'\\nimport kv from '@vercel/kv';\\nimport { openai } from '@ai-sdk/openai';\\nimport { streamText } from 'ai';\\nimport { Ratelimit } from '@upstash/ratelimit';\\nimport { NextRequest } from 'next/server';\\n\\n// Allow streaming responses up to 30 seconds\\nexport const maxDuration = 30;\\n\\n// Create Rate limit\\nconst ratelimit = new Ratelimit({\\n  redis: kv,\\n  limiter: Ratelimit.fixedWindow(5, '30s'),\\n});\\n\\nexport async function POST(req: NextRequest) {\\n  // call ratelimit with request ip\\n  const ip = req.ip ?? 'ip';\\n  const { success, remaining } = await ratelimit.limit(ip);\\n\\n  // block the request if unsuccessfull\\n  if (!success) {\\n    return new Response('Ratelimited!', { status: 429 });\\n  }\\n\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Simplify API Protection\\n\\nWith Vercel KV and Upstash Ratelimit, it is possible to protect your APIs\\nfrom such attacks with ease. To learn more about how Ratelimit works and\\nhow it can be configured to your needs, see [Ratelimit Documentation](https://upstash.com/docs/oss/sdks/ts/ratelimit/overview).\\n\", children=[]), DocItem(origPath=Path('06-advanced/07-rendering-ui-with-language-models.mdx'), name='07-rendering-ui-with-language-models.mdx', displayName='07-rendering-ui-with-language-models.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Rendering UI with Language Models\\ndescription: Rendering UI with Language Models\\n---\\n\\n# Rendering User Interfaces with Language Models\\n\\nLanguage models generate text, so at first it may seem like you would only need to render text in your application.\\n\\n```tsx highlight=\"16\" filename=\"app/actions.tsx\"\\nconst text = generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system: \\'You are a friendly assistant\\',\\n  prompt: \\'What is the weather in SF?\\',\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      parameters: z.object({\\n        city: z.string().describe(\\'The city to get the weather for\\'),\\n        unit: z\\n          .enum([\\'C\\', \\'F\\'])\\n          .describe(\\'The unit to display the temperature in\\'),\\n      }),\\n      execute: async ({ city, unit }) => {\\n        const weather = getWeather({ city, unit });\\n        return `It is currently ${weather.value}°${unit} and ${weather.description} in ${city}!`;\\n      },\\n    },\\n  },\\n});\\n```\\n\\nAbove, the language model is passed a [tool](/docs/ai-sdk-core/tools-and-tool-calling) called `getWeather` that returns the weather information as text. However, instead of returning text, if you return a JSON object that represents the weather information, you can use it to render a React component instead.\\n\\n```tsx highlight=\"18-23\" filename=\"app/action.ts\"\\nconst text = generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system: \\'You are a friendly assistant\\',\\n  prompt: \\'What is the weather in SF?\\',\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      parameters: z.object({\\n        city: z.string().describe(\\'The city to get the weather for\\'),\\n        unit: z\\n          .enum([\\'C\\', \\'F\\'])\\n          .describe(\\'The unit to display the temperature in\\'),\\n      }),\\n      execute: async ({ city, unit }) => {\\n        const weather = getWeather({ city, unit });\\n        const { temperature, unit, description, forecast } = weather;\\n\\n        return {\\n          temperature,\\n          unit,\\n          description,\\n          forecast,\\n        };\\n      },\\n    },\\n  },\\n});\\n```\\n\\nNow you can use the object returned by the `getWeather` function to conditionally render a React component `<WeatherCard/>` that displays the weather information by passing the object as props.\\n\\n```tsx filename=\"app/page.tsx\"\\nreturn (\\n  <div>\\n    {messages.map(message => {\\n      if (message.role === \\'function\\') {\\n        const { name, content } = message\\n        const { temperature, unit, description, forecast } = content;\\n\\n        return (\\n          <WeatherCard\\n            weather={{\\n              temperature: 47,\\n              unit: \\'F\\',\\n              description: \\'sunny\\'\\n              forecast,\\n            }}\\n          />\\n        )\\n      }\\n    })}\\n  </div>\\n)\\n```\\n\\nHere\\'s a little preview of what that might look like.\\n\\n<div className=\"not-prose flex flex-col2\">\\n  <CardPlayer\\n    type=\"weather\"\\n    title=\"Weather\"\\n    description=\"An example of an assistant that renders the weather information in a streamed component.\"\\n  />\\n</div>\\n\\nRendering interfaces as part of language model generations elevates the user experience of your application, allowing people to interact with language models beyond text.\\n\\nThey also make it easier for you to interpret [sequential tool calls](/docs/ai-sdk-rsc/multistep-interfaces) that take place in multiple steps and help identify and debug where the model reasoned incorrectly.\\n\\n## Rendering Multiple User Interfaces\\n\\nTo recap, an application has to go through the following steps to render user interfaces as part of model generations:\\n\\n1. The user prompts the language model.\\n2. The language model generates a response that includes a tool call.\\n3. The tool call returns a JSON object that represents the user interface.\\n4. The response is sent to the client.\\n5. The client receives the response and checks if the latest message was a tool call.\\n6. If it was a tool call, the client renders the user interface based on the JSON object returned by the tool call.\\n\\nMost applications have multiple tools that are called by the language model, and each tool can return a different user interface.\\n\\nFor example, a tool that searches for courses can return a list of courses, while a tool that searches for people can return a list of people. As this list grows, the complexity of your application will grow as well and it can become increasingly difficult to manage these user interfaces.\\n\\n```tsx filename=\\'app/page.tsx\\'\\n{\\n  message.role === \\'tool\\' ? (\\n    message.name === \\'api-search-course\\' ? (\\n      <Courses courses={message.content} />\\n    ) : message.name === \\'api-search-profile\\' ? (\\n      <People people={message.content} />\\n    ) : message.name === \\'api-meetings\\' ? (\\n      <Meetings meetings={message.content} />\\n    ) : message.name === \\'api-search-building\\' ? (\\n      <Buildings buildings={message.content} />\\n    ) : message.name === \\'api-events\\' ? (\\n      <Events events={message.content} />\\n    ) : message.name === \\'api-meals\\' ? (\\n      <Meals meals={message.content} />\\n    ) : null\\n  ) : (\\n    <div>{message.content}</div>\\n  );\\n}\\n```\\n\\n## Rendering User Interfaces on the Server\\n\\nThe **AI SDK RSC (`@ai-sdk/rsc`)** takes advantage of RSCs to solve the problem of managing all your React components on the client side, allowing you to render React components on the server and stream them to the client.\\n\\nRather than conditionally rendering user interfaces on the client based on the data returned by the language model, you can directly stream them from the server during a model generation.\\n\\n```tsx highlight=\"3,22-31,38\" filename=\"app/action.ts\"\\nimport { createStreamableUI } from \\'@ai-sdk/rsc\\'\\n\\nconst uiStream = createStreamableUI();\\n\\nconst text = generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  system: \\'you are a friendly assistant\\'\\n  prompt: \\'what is the weather in SF?\\'\\n  tools: {\\n    getWeather: {\\n      description: \\'Get the weather for a location\\',\\n      parameters: z.object({\\n        city: z.string().describe(\\'The city to get the weather for\\'),\\n        unit: z\\n          .enum([\\'C\\', \\'F\\'])\\n          .describe(\\'The unit to display the temperature in\\')\\n      }),\\n      execute: async ({ city, unit }) => {\\n        const weather = getWeather({ city, unit })\\n        const { temperature, unit, description, forecast } = weather\\n\\n        uiStream.done(\\n          <WeatherCard\\n            weather={{\\n              temperature: 47,\\n              unit: \\'F\\',\\n              description: \\'sunny\\'\\n              forecast,\\n            }}\\n          />\\n        )\\n      }\\n    }\\n  }\\n})\\n\\nreturn {\\n  display: uiStream.value\\n}\\n```\\n\\nThe [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) function belongs to the `@ai-sdk/rsc` module and creates a stream that can send React components to the client.\\n\\nOn the server, you render the `<WeatherCard/>` component with the props passed to it, and then stream it to the client. On the client side, you only need to render the UI that is streamed from the server.\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"4\"\\nreturn (\\n  <div>\\n    {messages.map(message => (\\n      <div>{message.display}</div>\\n    ))}\\n  </div>\\n);\\n```\\n\\nNow the steps involved are simplified:\\n\\n1. The user prompts the language model.\\n2. The language model generates a response that includes a tool call.\\n3. The tool call renders a React component along with relevant props that represent the user interface.\\n4. The response is streamed to the client and rendered directly.\\n\\n> **Note:** You can also render text on the server and stream it to the client using React Server Components. This way, all operations from language model generation to UI rendering can be done on the server, while the client only needs to render the UI that is streamed from the server.\\n\\nCheck out this [example](/examples/next-app/interface/stream-component-updates) for a full illustration of how to stream component updates with React Server Components in Next.js App Router.\\n', children=[]), DocItem(origPath=Path('06-advanced/08-model-as-router.mdx'), name='08-model-as-router.mdx', displayName='08-model-as-router.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Language Models as Routers\\ndescription: Generative User Interfaces and Language Models as Routers\\n---\\n\\n# Generative User Interfaces\\n\\nSince language models can render user interfaces as part of their generations, the resulting model generations are referred to as generative user interfaces.\\n\\nIn this section we will learn more about generative user interfaces and their impact on the way AI applications are built.\\n\\n## Deterministic Routes and Probabilistic Routing\\n\\nGenerative user interfaces are not deterministic in nature because they depend on the model\\'s generation output. Since these generations are probabilistic in nature, it is possible for every user query to result in a different user interface.\\n\\nUsers expect their experience using your application to be predictable, so non-deterministic user interfaces can sound like a bad idea at first. However, language models can be set up to limit their generations to a particular set of outputs using their ability to call functions.\\n\\nWhen language models are provided with a set of function definitions and instructed to execute any of them based on user query, they do either one of the following things:\\n\\n- Execute a function that is most relevant to the user query.\\n- Not execute any function if the user query is out of bounds of the set of functions available to them.\\n\\n```tsx filename=\\'app/actions.ts\\'\\nconst sendMessage = (prompt: string) =>\\n  generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    system: \\'you are a friendly weather assistant!\\',\\n    prompt,\\n    tools: {\\n      getWeather: {\\n        description: \\'Get the weather in a location\\',\\n        parameters: z.object({\\n          location: z.string().describe(\\'The location to get the weather for\\'),\\n        }),\\n        execute: async ({ location }: { location: string }) => ({\\n          location,\\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n        }),\\n      },\\n    },\\n  });\\n\\nsendMessage(\\'What is the weather in San Francisco?\\'); // getWeather is called\\nsendMessage(\\'What is the weather in New York?\\'); // getWeather is called\\nsendMessage(\\'What events are happening in London?\\'); // No function is called\\n```\\n\\nThis way, it is possible to ensure that the generations result in deterministic outputs, while the choice a model makes still remains to be probabilistic.\\n\\nThis emergent ability exhibited by a language model to choose whether a function needs to be executed or not based on a user query is believed to be models emulating \"reasoning\".\\n\\nAs a result, the combination of language models being able to reason which function to execute as well as render user interfaces at the same time gives you the ability to build applications where language models can be used as a router.\\n\\n## Language Models as Routers\\n\\nHistorically, developers had to write routing logic that connected different parts of an application to be navigable by a user and complete a specific task.\\n\\nIn web applications today, most of the routing logic takes place in the form of routes:\\n\\n- `/login` would navigate you to a page with a login form.\\n- `/user/john` would navigate you to a page with profile details about John.\\n- `/api/events?limit=5` would display the five most recent events from an events database.\\n\\nWhile routes help you build web applications that connect different parts of an application into a seamless user experience, it can also be a burden to manage them as the complexity of applications grow.\\n\\nNext.js has helped reduce complexity in developing with routes by introducing:\\n\\n- File-based routing system\\n- Dynamic routing\\n- API routes\\n- Middleware\\n- App router, and so on...\\n\\nWith language models becoming better at reasoning, we believe that there is a future where developers only write core application specific components while models take care of routing them based on the user\\'s state in an application.\\n\\nWith generative user interfaces, the language model decides which user interface to render based on the user\\'s state in the application, giving users the flexibility to interact with your application in a conversational manner instead of navigating through a series of predefined routes.\\n\\n### Routing by parameters\\n\\nFor routes like:\\n\\n- `/profile/[username]`\\n- `/search?q=[query]`\\n- `/media/[id]`\\n\\nthat have segments dependent on dynamic data, the language model can generate the correct parameters and render the user interface.\\n\\nFor example, when you\\'re in a search application, you can ask the language model to search for artworks from different artists. The language model will call the search function with the artist\\'s name as a parameter and render the search results.\\n\\n<div className=\"not-prose\">\\n  <CardPlayer\\n    type=\"media-search\"\\n    title=\"Media Search\"\\n    description=\"Let your users see more than words can say by rendering components directly within your search experience.\"\\n  />\\n</div>\\n\\n### Routing by sequence\\n\\nFor actions that require a sequence of steps to be completed by navigating through different routes, the language model can generate the correct sequence of routes to complete in order to fulfill the user\\'s request.\\n\\nFor example, when you\\'re in a calendar application, you can ask the language model to schedule a happy hour evening with your friends. The language model will then understand your request and will perform the right sequence of [tool calls](/docs/ai-sdk-core/tools-and-tool-calling) to:\\n\\n1. Lookup your calendar\\n2. Lookup your friends\\' calendars\\n3. Determine the best time for everyone\\n4. Search for nearby happy hour spots\\n5. Create an event and send out invites to your friends\\n\\n<div className=\"not-prose\">\\n  <CardPlayer\\n    type=\"event-planning\"\\n    title=\"Planning an Event\"\\n    description=\"The model calls functions and generates interfaces based on user intent, acting like a router.\"\\n  />\\n</div>\\n\\nJust by defining functions to lookup contacts, pull events from a calendar, and search for nearby locations, the model is able to sequentially navigate the routes for you.\\n\\nTo learn more, check out these [examples](/examples/next-app/interface) using the `streamUI` function to stream generative user interfaces to the client based on the response from the language model.\\n', children=[]), DocItem(origPath=Path('06-advanced/09-multistep-interfaces.mdx'), name='09-multistep-interfaces.mdx', displayName='09-multistep-interfaces.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Multistep Interfaces\\ndescription: Concepts behind building multistep interfaces\\n---\\n\\n# Multistep Interfaces\\n\\nMultistep interfaces refer to user interfaces that require multiple independent steps to be executed in order to complete a specific task.\\n\\nIn order to understand multistep interfaces, it is important to understand two concepts:\\n\\n- Tool composition\\n- Application context\\n\\n**Tool composition** is the process of combining multiple [tools](/docs/ai-sdk-core/tools-and-tool-calling) to create a new tool. This is a powerful concept that allows you to break down complex tasks into smaller, more manageable steps.\\n\\n**Application context** refers to the state of the application at any given point in time. This includes the user\\'s input, the output of the language model, and any other relevant information.\\n\\nWhen designing multistep interfaces, you need to consider how the tools in your application can be composed together to form a coherent user experience as well as how the application context changes as the user progresses through the interface.\\n\\n## Application Context\\n\\nThe application context can be thought of as the conversation history between the user and the language model. The richer the context, the more information the model has to generate relevant responses.\\n\\nIn the context of multistep interfaces, the application context becomes even more important. This is because **the user\\'s input in one step may affect the output of the model in the next step**.\\n\\nFor example, consider a meal logging application that helps users track their daily food intake. The language model is provided with the following tools:\\n\\n- `log_meal` takes in parameters like the name of the food, the quantity, and the time of consumption to log a meal.\\n- `delete_meal` takes in the name of the meal to be deleted.\\n\\nWhen the user logs a meal, the model generates a response confirming the meal has been logged.\\n\\n```txt highlight=\"2\"\\nUser: Log a chicken shawarma for lunch.\\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\\nModel: Chicken shawarma has been logged for lunch.\\n```\\n\\nNow when the user decides to delete the meal, the model should be able to reference the previous step to identify the meal to be deleted.\\n\\n```txt highlight=\"7\"\\nUser: Log a chicken shawarma for lunch.\\nTool: log_meal(\"chicken shawarma\", \"250g\", \"12:00 PM\")\\nModel: Chicken shawarma has been logged for lunch.\\n...\\n...\\nUser: I skipped lunch today, can you update my log?\\nTool: delete_meal(\"chicken shawarma\")\\nModel: Chicken shawarma has been deleted from your log.\\n```\\n\\nIn this example, managing the application context is important for the model to generate the correct response. The model needs to have information about the previous actions in order for it to use generate the parameters for the `delete_meal` tool.\\n\\n## Tool Composition\\n\\nTool composition is the process of combining multiple tools to create a new tool. This involves defining the inputs and outputs of each tool, as well as how they interact with each other.\\n\\nThe design of how these tools can be composed together to form a multistep interface is crucial to both the user experience of your application and the model\\'s ability to generate the correct output.\\n\\nFor example, consider a flight booking assistant that can help users book flights. The assistant can be designed to have the following tools:\\n\\n- `searchFlights`: Searches for flights based on the user\\'s query.\\n- `lookupFlight`: Looks up details of a specific flight based on the flight number.\\n- `bookFlight`: Books a flight based on the user\\'s selection.\\n\\nThe `searchFlights` tool is called when the user wants to lookup flights for a specific route. This would typically mean the tool should be able to take in parameters like the origin and destination of the flight.\\n\\nThe `lookupFlight` tool is called when the user wants to get more details about a specific flight. This would typically mean the tool should be able to take in parameters like the flight number and return information about seat availability.\\n\\nThe `bookFlight` tool is called when the user decides to book a flight. In order to identify the flight to book, the tool should be able to take in parameters like the flight number, trip date, and passenger details.\\n\\nSo the conversation between the user and the model could look like this:\\n\\n```txt highlight=\"8\"\\nUser: I want to book a flight from New York to London.\\nTool: searchFlights(\"New York\", \"London\")\\nModel: Here are the available flights from New York to London.\\nUser: I want to book flight number BA123 on 12th December for myself and my wife.\\nTool: lookupFlight(\"BA123\") -> \"4 seats available\"\\nModel: Sure, there are seats available! Can you provide the names of the passengers?\\nUser: John Doe and Jane Doe.\\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\\nModel: Your flight has been booked!\\n```\\n\\nIn the last tool call, the `bookFlight` tool does not include passenger details as it is not available in the application context. As a result, it requests the user to provide the passenger details before proceeding with the booking.\\n\\nLooking up passenger information could\\'ve been another tool that the model could\\'ve called before calling the `bookFlight` tool assuming that the user is logged into the application. This way, the model does not have to ask the user for the passenger details and can proceed with the booking.\\n\\n```txt highlight=\"5,6\"\\nUser: I want to book a flight from New York to London.\\nTool: searchFlights(\"New York\", \"London\")\\nModel: Here are the available flights from New York to London.\\nUser: I want to book flight number BA123 on 12th December for myself an my wife.\\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\\nTool: bookFlight(\"BA123\", \"12th December\", [\"John Doe\", \"Jane Doe\"])\\nModel: Your flight has been booked!\\n```\\n\\nThe `lookupContacts` tool is called before the `bookFlight` tool to ensure that the passenger details are available in the application context when booking the flight. This way, the model can reduce the number of steps required from the user and use its ability to call tools that populate its context and use that information to complete the booking process.\\n\\nNow, let\\'s introduce another tool called `lookupBooking` that can be used to show booking details by taking in the name of the passenger as parameter. This tool can be composed with the existing tools to provide a more complete user experience.\\n\\n```txt highlight=\"2-4\"\\nUser: What\\'s the status of my wife\\'s upcoming flight?\\nTool: lookupContacts() -> [\"John Doe\", \"Jane Doe\"]\\nTool: lookupBooking(\"Jane Doe\") -> \"BA123 confirmed\"\\nTool: lookupFlight(\"BA123\") -> \"Flight BA123 is scheduled to depart on 12th December.\"\\nModel: Your wife\\'s flight BA123 is confirmed and scheduled to depart on 12th December.\\n```\\n\\nIn this example, the `lookupBooking` tool is used to provide the user with the status of their wife\\'s upcoming flight. By composing this tool with the existing tools, the model is able to generate a response that includes the booking status and the departure date of the flight without requiring the user to provide additional information.\\n\\nAs a result, the more tools you design that can be composed together, the more complex and powerful your application can become.\\n', children=[]), DocItem(origPath=Path('06-advanced/09-sequential-generations.mdx'), name='09-sequential-generations.mdx', displayName='09-sequential-generations.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Sequential Generations\\ndescription: Learn how to implement sequential generations (\"chains\") with the AI SDK\\n---\\n\\n# Sequential Generations\\n\\nWhen working with the AI SDK, you may want to create sequences of generations (often referred to as \"chains\" or \"pipes\"), where the output of one becomes the input for the next. This can be useful for creating more complex AI-powered workflows or for breaking down larger tasks into smaller, more manageable steps.\\n\\n## Example\\n\\nIn a sequential chain, the output of one generation is directly used as input for the next generation. This allows you to create a series of dependent generations, where each step builds upon the previous one.\\n\\nHere\\'s an example of how you can implement sequential actions:\\n\\n```typescript\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nasync function sequentialActions() {\\n  // Generate blog post ideas\\n  const ideasGeneration = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'Generate 10 ideas for a blog post about making spaghetti.\\',\\n  });\\n\\n  console.log(\\'Generated Ideas:\\\\n\\', ideasGeneration);\\n\\n  // Pick the best idea\\n  const bestIdeaGeneration = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: `Here are some blog post ideas about making spaghetti:\\n${ideasGeneration}\\n\\nPick the best idea from the list above and explain why it\\'s the best.`,\\n  });\\n\\n  console.log(\\'\\\\nBest Idea:\\\\n\\', bestIdeaGeneration);\\n\\n  // Generate an outline\\n  const outlineGeneration = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: `We\\'ve chosen the following blog post idea about making spaghetti:\\n${bestIdeaGeneration}\\n\\nCreate a detailed outline for a blog post based on this idea.`,\\n  });\\n\\n  console.log(\\'\\\\nBlog Post Outline:\\\\n\\', outlineGeneration);\\n}\\n\\nsequentialActions().catch(console.error);\\n```\\n\\nIn this example, we first generate ideas for a blog post, then pick the best idea, and finally create an outline based on that idea. Each step uses the output from the previous step as input for the next generation.\\n', children=[]), DocItem(origPath=Path('06-advanced/10-vercel-deployment-guide.mdx'), name='10-vercel-deployment-guide.mdx', displayName='10-vercel-deployment-guide.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Vercel Deployment Guide\\ndescription: Learn how to deploy an AI application to production on Vercel\\n---\\n\\n# Vercel Deployment Guide\\n\\nIn this guide, you will deploy an AI application to [Vercel](https://vercel.com) using [Next.js](https://nextjs.org) (App Router).\\n\\nVercel is a platform for developers that provides the tools, workflows, and infrastructure you need to build and deploy your web apps faster, without the need for additional configuration.\\n\\nVercel allows for automatic deployments on every branch push and merges onto the production branch of your GitHub, GitLab, and Bitbucket projects. It is a great option for deploying your AI application.\\n\\n## Before You Begin\\n\\nTo follow along with this guide, you will need:\\n\\n- a Vercel account\\n- an account with a Git provider (this tutorial will use [Github](https://github.com))\\n- an OpenAI API key\\n\\nThis guide will teach you how to deploy the application you built in the Next.js (App Router) quickstart tutorial to Vercel. If you haven’t completed the quickstart guide, you can start with [this repo](https://github.com/vercel-labs/ai-sdk-deployment-guide).\\n\\n## Commit Changes\\n\\nVercel offers a powerful git-centered workflow that automatically deploys your application to production every time you push to your repository’s main branch.\\n\\nBefore committing your local changes, make sure that you have a `.gitignore`. Within your `.gitignore`, ensure that you are excluding your environment variables (`.env`) and your node modules (`node_modules`).\\n\\nIf you have any local changes, you can commit them by running the following commands:\\n\\n```bash\\ngit add .\\ngit commit -m \"init\"\\n```\\n\\n## Create Git Repo\\n\\nYou can create a GitHub repository from within your terminal, or on [github.com](https://github.com/). For this tutorial, you will use the GitHub CLI ([more info here](https://cli.github.com/)).\\n\\nTo create your GitHub repository:\\n\\n1. Navigate to [github.com](http://github.com/)\\n2. In the top right corner, click the \"plus\" icon and select \"New repository\"\\n3. Pick a name for your repository (this can be anything)\\n4. Click \"Create repository\"\\n\\nOnce you have created your repository, GitHub will redirect you to your new repository.\\n\\n1. Scroll down the page and copy the commands under the title \"...or push an existing repository from the command line\"\\n2. Go back to the terminal, paste and then run the commands\\n\\nNote: if you run into the error \"error: remote origin already exists.\", this is because your local repository is still linked to the repository you cloned. To \"unlink\", you can run the following command:\\n\\n```bash\\nrm -rf .git\\ngit init\\ngit add .\\ngit commit -m \"init\"\\n```\\n\\nRerun the code snippet from the previous step.\\n\\n## Import Project in Vercel\\n\\nOn the [New Project](https://vercel.com/new) page, under the **Import Git Repository** section, select the Git provider that you would like to import your project from. Follow the prompts to sign in to your GitHub account.\\n\\nOnce you have signed in, you should see your newly created repository from the previous step in the \"Import Git Repository\" section. Click the \"Import\" button next to that project.\\n\\n### Add Environment Variables\\n\\nYour application stores uses environment secrets to store your OpenAI API key using a `.env.local` file locally in development. To add this API key to your production deployment, expand the \"Environment Variables\" section and paste in your `.env.local` file. Vercel will automatically parse your variables and enter them in the appropriate `key:value` format.\\n\\n### Deploy\\n\\nPress the **Deploy** button. Vercel will create the Project and deploy it based on the chosen configurations.\\n\\n### Enjoy the confetti!\\n\\nTo view your deployment, select the Project in the dashboard and then select the\\xa0**Domain**. This page is now visible to anyone who has the URL.\\n\\n## Considerations\\n\\nWhen deploying an AI application, there are infrastructure-related considerations to be aware of.\\n\\n### Function Duration\\n\\nIn most cases, you will call the large language model (LLM) on the server. By default, Vercel serverless functions have a maximum duration of 10 seconds on the Hobby Tier. Depending on your prompt, it can take an LLM more than this limit to complete a response. If the response is not resolved within this limit, the server will throw an error.\\n\\nYou can specify the maximum duration of your Vercel function using [route segment config](https://nextjs.org/docs/app/api-reference/file-conventions/route-segment-config). To update your maximum duration, add the following route segment config to the top of your route handler or the page which is calling your server action.\\n\\n```ts\\nexport const maxDuration = 30;\\n```\\n\\nYou can increase the max duration to 60 seconds on the Hobby Tier. For other tiers, [see the documentation](https://vercel.com/docs/functions/runtimes#max-duration) for limits.\\n\\n## Security Considerations\\n\\nGiven the high cost of calling an LLM, it\\'s important to have measures in place that can protect your application from abuse.\\n\\n### Rate Limit\\n\\nRate limiting is a method used to regulate network traffic by defining a maximum number of requests that a client can send to a server within a given time frame.\\n\\nFollow [this guide](https://vercel.com/guides/securing-ai-app-rate-limiting) to add rate limiting to your application.\\n\\n### Firewall\\n\\nA\\xa0firewall helps protect your applications and websites from DDoS attacks and unauthorized access.\\n\\n[Vercel Firewall](https://vercel.com/docs/security/vercel-firewall) is a set of tools and infrastructure, created specifically with security in mind. It automatically mitigates DDoS attacks and Enterprise teams can get further customization for their site, including dedicated support and custom rules for IP blocking.\\n\\n## Troubleshooting\\n\\n- Streaming not working when [proxied](/docs/troubleshooting/streaming-not-working-when-proxied)\\n- Experiencing [Timeouts](/docs/troubleshooting/timeout-on-vercel)\\n', children=[]), DocItem(origPath=Path('06-advanced/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Advanced\\ndescription: Learn how to use advanced functionality within the AI SDK and RSC API.\\ncollapsed: true\\n---\\n\\n# Advanced\\n\\nThis section covers advanced topics and concepts for the AI SDK and RSC API. Working with LLMs often requires a different mental model compared to traditional software development.\\n\\nAfter these concepts, you should have a better understanding of the paradigms behind the AI SDK and RSC API, and how to use them to build more AI applications.\\n', children=[])]),\n",
       " DocItem(origPath=Path('07-reference'), name='07-reference', displayName='07-reference', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/01-ai-sdk-core'), name='01-ai-sdk-core', displayName='01-ai-sdk-core', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/01-ai-sdk-core/01-generate-text.mdx'), name='01-generate-text.mdx', displayName='01-generate-text.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateText\\ndescription: API Reference for generateText.\\n---\\n\\n# `generateText()`\\n\\nGenerates text and calls tools for a given prompt using a language model.\\n\\nIt is ideal for non-interactive use cases such as automation tasks where you need to write text (e.g. drafting email or summarizing web pages) and for agents that use tools.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText } from \\'ai\\';\\n\\nconst { text } = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n\\nconsole.log(text);\\n```\\n\\nTo see `generateText` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { generateText } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4o\\')\",\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Input (parameters) generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'output\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      description:\\n        \\'Tools that are accessible to and can be called by the model. The model needs to support calling tools.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n            },\\n            {\\n              name: \\'execute\\',\\n              isOptional: true,\\n              type: \\'async (parameters: T, options: ToolExecutionOptions) => RESULT\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolExecutionOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'ModelMessage[]\\',\\n                      description:\\n                        \\'Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'abortSignal\\',\\n                      type: \\'AbortSignal\\',\\n                      description:\\n                        \\'An optional abort signal that indicates that the overall operation should be aborted.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      isOptional: true,\\n      type: \\'\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\\',\\n      description:\\n        \\'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.\\',\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'activeTools\\',\\n      type: \\'Array<TOOLNAME>\\',\\n      isOptional: true,\\n      description:\\n        \\'Limits the tools that are available for the model to call without changing the tool call and result types in the result. All tools are active by default.\\',\\n    },\\n    {\\n      name: \\'stopWhen\\',\\n      type: \\'StopCondition<TOOLS> | Array<StopCondition<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Condition for stopping the generation when there are tool results in the last step. When the condition is an array, any of the conditions can be met to stop the generation. Default: stepCountIs(1).\\',\\n    },\\n    {\\n      name: \\'prepareStep\\',\\n      type: \\'(options: PrepareStepOptions) => PrepareStepResult<TOOLS> | Promise<PrepareStepResult<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional function that you can use to provide different settings for a step. You can modify the model, tool choices, active tools, system prompt, and input messages for each step.\\',\\n      properties: [\\n        {\\n          type: \\'PrepareStepFunction<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'options\\',\\n              type: \\'object\\',\\n              description: \\'The options for the step.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareStepOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'steps\\',\\n                      type: \\'Array<StepResult<TOOLS>>\\',\\n                      description: \\'The steps that have been executed so far.\\',\\n                    },\\n                    {\\n                      name: \\'stepNumber\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The number of the step that is being executed.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'LanguageModel\\',\\n                      description: \\'The model that is being used.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ModelMessage>\\',\\n                      description:\\n                        \\'The messages that will be sent to the model for the current step.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'PrepareStepResult<TOOLS>\\',\\n          description:\\n            \\'Return value that can modify settings for the current step.\\',\\n          parameters: [\\n            {\\n              name: \\'model\\',\\n              type: \\'LanguageModel\\',\\n              isOptional: true,\\n              description: \\'Change the model for this step.\\',\\n            },\\n            {\\n              name: \\'toolChoice\\',\\n              type: \\'ToolChoice<TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change the tool choice strategy for this step.\\',\\n            },\\n            {\\n              name: \\'activeTools\\',\\n              type: \\'Array<keyof TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change which tools are active for this step.\\',\\n            },\\n            {\\n              name: \\'system | SystemModelMessage\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'Change the system prompt for this step.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ModelMessage>\\',\\n              isOptional: true,\\n              description: \\'Modify the input messages for this step.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_context\\',\\n      type: \\'unknown\\',\\n      isOptional: true,\\n      description:\\n        \\'Context that is passed into tool execution. Experimental (can break in patch releases).\\',\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_repairToolCall\\',\\n      type: \\'(options: ToolCallRepairOptions) => Promise<LanguageModelV3ToolCall | null>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'ToolCallRepairOptions\\',\\n          parameters: [\\n            {\\n              name: \\'system\\',\\n              type: \\'string | SystemModelMessage | undefined\\',\\n              description: \\'The system prompt.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'ModelMessage[]\\',\\n              description: \\'The messages in the current generation step.\\',\\n            },\\n            {\\n              name: \\'toolCall\\',\\n              type: \\'LanguageModelV3ToolCall\\',\\n              description: \\'The tool call that failed to parse.\\',\\n            },\\n            {\\n              name: \\'tools\\',\\n              type: \\'TOOLS\\',\\n              description: \\'The tools that are available.\\',\\n            },\\n            {\\n              name: \\'parameterSchema\\',\\n              type: \\'(options: { toolName: string }) => JSONSchema7\\',\\n              description:\\n                \\'A function that returns the JSON Schema for a tool.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'NoSuchToolError | InvalidToolInputError\\',\\n              description:\\n                \\'The error that occurred while parsing the tool call.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description:\\n        \\'Specification for parsing structured outputs from the LLM response.\\',\\n      properties: [\\n        {\\n          type: \\'Output\\',\\n          parameters: [\\n            {\\n              name: \\'Output.text()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for text generation (default).\\',\\n            },\\n            {\\n              name: \\'Output.object()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for typed object generation using schemas. When the model generates a text response, it will return an object that matches the schema.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'schema\\',\\n                      type: \\'Schema<OBJECT>\\',\\n                      description: \\'The schema of the object to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.array()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for array generation. When the model generates a text response, it will return an array of elements.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'element\\',\\n                      type: \\'Schema<ELEMENT>\\',\\n                      description:\\n                        \\'The schema of the array elements to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.choice()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for choice generation. When the model generates a text response, it will return a one of the choice options.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'Array<string>\\',\\n                      description: \\'The available choices.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.json()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\' Output specification for unstructured JSON generation. When the model generates a text response, it will return a JSON object.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onStepFinish\\',\\n      type: \\'(result: OnStepFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description: \\'Callback that is called when a step is finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnStepFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description:\\n                \\'The reason the model finished generating the text for the step.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of last step.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'totalUsage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The total token usage from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'modelId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'body\\',\\n                      isOptional: true,\\n                      type: \\'unknown\\',\\n                      description: \\'Optional response body.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'isContinued\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when there will be a continuation step with a continuation text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response and all request tool executions (for tools that have an `execute` function) are finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,Record<string,JSONValue>> | undefined\\',\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'reasoningDetails\\',\\n              type: \\'Array<ReasoningDetail>\\',\\n              description:\\n                \\'The reasoning details of the model (only available for some models).\\',\\n              properties: [\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content (only for type \"text\").\\',\\n                    },\\n                    {\\n                      name: \\'signature\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'Optional signature (only for type \"text\").\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'redacted\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The redacted data content (only for type \"redacted\").\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description:\\n                \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'Files that were generated in the final step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'steps\\',\\n              type: \\'Array<StepResult>\\',\\n              description:\\n                \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'content\\',\\n      type: \\'Array<ContentPart<TOOLS>>\\',\\n      description: \\'The content that was generated in the last step.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      type: \\'string\\',\\n      description: \\'The generated text by the model.\\',\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: \\'Array<ReasoningOutput>\\',\\n      description:\\n        \\'The full reasoning that the model has generated in the last step.\\',\\n      properties: [\\n        {\\n          type: \\'ReasoningOutput\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning\\'\",\\n              description: \\'The type of the message part.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The reasoning text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'reasoningText\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \\'The reasoning text that the model has generated in the last step. Can be undefined if the model has only generated text.\\',\\n    },\\n    {\\n      name: \\'sources\\',\\n      type: \\'Array<Source>\\',\\n      description:\\n        \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n      properties: [\\n        {\\n          type: \\'Source\\',\\n          parameters: [\\n            {\\n              name: \\'sourceType\\',\\n              type: \"\\'url\\'\",\\n              description:\\n                \\'A URL source. This is return by web search RAG models.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'The ID of the source.\\',\\n            },\\n            {\\n              name: \\'url\\',\\n              type: \\'string\\',\\n              description: \\'The URL of the source.\\',\\n            },\\n            {\\n              name: \\'title\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The title of the source.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'files\\',\\n      type: \\'Array<GeneratedFile>\\',\\n      description: \\'Files that were generated in the final step.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'File as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'File as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the file.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolCalls\\',\\n      type: \\'ToolCallArray<TOOLS>\\',\\n      description: \\'The tool calls that were made in the last step.\\',\\n    },\\n    {\\n      name: \\'toolResults\\',\\n      type: \\'ToolResultArray<TOOLS>\\',\\n      description: \\'The results of the tool calls from the last step.\\',\\n    },\\n    {\\n      name: \\'finishReason\\',\\n      type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n      description: \\'The reason the model finished generating the text.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'LanguageModelUsage\\',\\n      description: \\'The token usage of the last step.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'totalUsage\\',\\n      type: \\'CompletionTokenUsage\\',\\n      description:\\n        \\'The total token usage of all steps. When there are multiple steps, the usage is the sum of all step usages.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'LanguageModelRequestMetadata\\',\\n      isOptional: true,\\n      description: \\'Request metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'LanguageModelResponseMetadata\\',\\n      isOptional: true,\\n      description: \\'Response metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'unknown\\',\\n              description: \\'Optional response body.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ResponseMessage>\\',\\n              description:\\n                \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description: \\'Experimental setting for generating structured outputs.\\',\\n    },\\n    {\\n      name: \\'steps\\',\\n      type: \\'Array<StepResult<TOOLS>>\\',\\n      description:\\n        \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n      properties: [\\n        {\\n          type: \\'StepResult\\',\\n          parameters: [\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ContentPart<TOOLS>>\\',\\n              description: \\'The content that was generated in the last step.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The generated text.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'Array<ReasoningPart>\\',\\n              description:\\n                \\'The reasoning that was generated during the generation.\\',\\n              properties: [\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'reasoningText\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text that was generated during the generation.\\',\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description:\\n                \\'The files that were generated during the generation.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description: \\'The sources that were used to generate the text.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCallArray<TOOLS>\\',\\n              description:\\n                \\'The tool calls that were made during the generation.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResultArray<TOOLS>\\',\\n              description: \\'The results of the tool calls.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason why the generation finished.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'request\\',\\n              type: \\'LanguageModelRequestMetadata\\',\\n              description: \\'Additional request information.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelRequestMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'body\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'LanguageModelResponseMetadata\\',\\n              description: \\'Additional response information.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelResponseMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'modelId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'body\\',\\n                      isOptional: true,\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'Response body (available only for providers that use HTTP requests).\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. Response messages can be either assistant messages or tool messages. They contain a generated id.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata | undefined\\',\\n              description:\\n                \\'Additional provider-specific metadata. They are passed through from the provider to the AI SDK and enable provider-specific results that can be fully encapsulated in the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to generate text using a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/generating-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate a chat completion using a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/generating-text\\',\\n    },\\n    {\\n      title: \\'Learn to call tools using a language model in Next.js\\',\\n      link: \\'/examples/next-app/tools/call-tool\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to render a React component as a tool call using a language model in Next.js\\',\\n      link: \\'/examples/next-app/tools/render-interface-during-tool-call\\',\\n    },\\n    {\\n      title: \\'Learn to generate text using a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/generate-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate chat completions using a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/generate-text-with-chat-prompt\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/02-stream-text.mdx'), name='02-stream-text.mdx', displayName='02-stream-text.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamText\\ndescription: API Reference for streamText.\\n---\\n\\n# `streamText()`\\n\\nStreams text generations from a language model.\\n\\nYou can use the streamText function for interactive use cases such as chat bots and other real-time applications. You can also generate UI components with tools.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamText } from \\'ai\\';\\n\\nconst { textStream } = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n\\nfor await (const textPart of textStream) {\\n  process.stdout.write(textPart);\\n}\\n```\\n\\nTo see `streamText` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { streamText } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4.1\\')\",\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The IANA media type of the image.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the reasoning part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      description:\\n        \\'Tools that are accessible to and can be called by the model. The model needs to support calling tools.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n            },\\n            {\\n              name: \\'execute\\',\\n              isOptional: true,\\n              type: \\'async (parameters: T, options: ToolExecutionOptions) => RESULT\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result. If not provided, the tool will not be executed automatically.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolExecutionOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'ModelMessage[]\\',\\n                      description:\\n                        \\'Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'abortSignal\\',\\n                      type: \\'AbortSignal\\',\\n                      description:\\n                        \\'An optional abort signal that indicates that the overall operation should be aborted.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      isOptional: true,\\n      type: \\'\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\\',\\n      description:\\n        \\'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.\\',\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_generateMessageId\\',\\n      type: \\'() => string\\',\\n      isOptional: true,\\n      description:\\n        \\'Function used to generate a unique ID for each message. This is an experimental feature.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_transform\\',\\n      type: \\'StreamTextTransform | Array<StreamTextTransform>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional stream transformations. They are applied in the order they are provided. The stream transformations must maintain the stream structure for streamText to work correctly.\\',\\n      properties: [\\n        {\\n          type: \\'StreamTextTransform\\',\\n          parameters: [\\n            {\\n              name: \\'transform\\',\\n              type: \\'(options: TransformOptions) => TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>\\',\\n              description: \\'A transformation that is applied to the stream.\\',\\n              properties: [\\n                {\\n                  type: \\'TransformOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'stopStream\\',\\n                      type: \\'() => void\\',\\n                      description: \\'A function that stops the stream.\\',\\n                    },\\n                    {\\n                      name: \\'tools\\',\\n                      type: \\'TOOLS\\',\\n                      description: \\'The tools that are available.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'includeRawChunks\\',\\n      type: \\'boolean\\',\\n      isOptional: true,\\n      description:\\n        \\'Whether to include raw chunks from the provider in the stream. When enabled, you will receive raw chunks with type \"raw\" that contain the unprocessed data from the provider. This allows access to cutting-edge provider features not yet wrapped by the AI SDK. Defaults to false.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'activeTools\\',\\n      type: \\'Array<TOOLNAME> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'The tools that are currently active. All tools are active by default.\\',\\n    },\\n    {\\n      name: \\'stopWhen\\',\\n      type: \\'StopCondition<TOOLS> | Array<StopCondition<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Condition for stopping the generation when there are tool results in the last step. When the condition is an array, any of the conditions can be met to stop the generation. Default: stepCountIs(1).\\',\\n    },\\n    {\\n      name: \\'prepareStep\\',\\n      type: \\'(options: PrepareStepOptions) => PrepareStepResult<TOOLS> | Promise<PrepareStepResult<TOOLS>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional function that you can use to provide different settings for a step. You can modify the model, tool choices, active tools, system prompt, and input messages for each step.\\',\\n      properties: [\\n        {\\n          type: \\'PrepareStepFunction<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'options\\',\\n              type: \\'object\\',\\n              description: \\'The options for the step.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareStepOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'steps\\',\\n                      type: \\'Array<StepResult<TOOLS>>\\',\\n                      description: \\'The steps that have been executed so far.\\',\\n                    },\\n                    {\\n                      name: \\'stepNumber\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The number of the step that is being executed.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'LanguageModel\\',\\n                      description: \\'The model that is being used.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ModelMessage>\\',\\n                      description:\\n                        \\'The messages that will be sent to the model for the current step.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'PrepareStepResult<TOOLS>\\',\\n          description:\\n            \\'Return value that can modify settings for the current step.\\',\\n          parameters: [\\n            {\\n              name: \\'model\\',\\n              type: \\'LanguageModel\\',\\n              isOptional: true,\\n              description: \\'Change the model for this step.\\',\\n            },\\n            {\\n              name: \\'toolChoice\\',\\n              type: \\'ToolChoice<TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change the tool choice strategy for this step.\\',\\n            },\\n            {\\n              name: \\'activeTools\\',\\n              type: \\'Array<keyof TOOLS>\\',\\n              isOptional: true,\\n              description: \\'Change which tools are active for this step.\\',\\n            },\\n            {\\n              name: \\'system\\',\\n              type: \\'string | SystemModelMessage\\',\\n              isOptional: true,\\n              description: \\'Change the system prompt for this step.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ModelMessage>\\',\\n              isOptional: true,\\n              description: \\'Modify the input messages for this step.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_context\\',\\n      type: \\'unknown\\',\\n      isOptional: true,\\n      description:\\n        \\'Context that is passed into tool execution. Experimental (can break in patch releases).\\',\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_repairToolCall\\',\\n      type: \\'(options: ToolCallRepairOptions) => Promise<LanguageModelV3ToolCall | null>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair a tool call that failed to parse. Return either a repaired tool call or null if the tool call cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'ToolCallRepairOptions\\',\\n          parameters: [\\n            {\\n              name: \\'system\\',\\n              type: \\'string | SystemModelMessage | undefined\\',\\n              description: \\'The system prompt.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'ModelMessage[]\\',\\n              description: \\'The messages in the current generation step.\\',\\n            },\\n            {\\n              name: \\'toolCall\\',\\n              type: \\'LanguageModelV3ToolCall\\',\\n              description: \\'The tool call that failed to parse.\\',\\n            },\\n            {\\n              name: \\'tools\\',\\n              type: \\'TOOLS\\',\\n              description: \\'The tools that are available.\\',\\n            },\\n            {\\n              name: \\'parameterSchema\\',\\n              type: \\'(options: { toolName: string }) => JSONSchema7\\',\\n              description:\\n                \\'A function that returns the JSON Schema for a tool.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'NoSuchToolError | InvalidToolInputError\\',\\n              description:\\n                \\'The error that occurred while parsing the tool call.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onChunk\\',\\n      type: \\'(event: OnChunkResult) => Promise<void> |void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.\\',\\n      properties: [\\n        {\\n          type: \\'OnChunkResult\\',\\n          parameters: [\\n            {\\n              name: \\'chunk\\',\\n              type: \\'TextStreamPart\\',\\n              description: \\'The chunk of the stream.\\',\\n              properties: [\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description:\\n                        \\'The type to identify the object as text delta.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text delta.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description:\\n                        \\'The type to identify the object as reasoning.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text delta.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'source\\'\",\\n                      description: \\'The type to identify the object as source.\\',\\n                    },\\n                    {\\n                      name: \\'source\\',\\n                      type: \\'Source\\',\\n                      description: \\'The source.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description:\\n                        \\'The type to identify the object as tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call-streaming-start\\'\",\\n                      description:\\n                        \\'Indicates the start of a tool call streaming. Only available when streaming tool calls.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call-delta\\'\",\\n                      description:\\n                        \\'The type to identify the object as tool call delta. Only available when streaming tool calls.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'argsTextDelta\\',\\n                      type: \\'string\\',\\n                      description: \\'The text delta of the tool call arguments.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'TextStreamPart\\',\\n                  description: \\'The result of a tool call execution.\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description:\\n                        \\'The type to identify the object as tool result.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'input\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                    {\\n                      name: \\'output\\',\\n                      type: \\'any\\',\\n                      description:\\n                        \\'The result returned by the tool after execution has completed.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(event: OnErrorResult) => Promise<void> |void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when an error occurs during streaming. You can use it to log errors.\\',\\n      properties: [\\n        {\\n          type: \\'OnErrorResult\\',\\n          parameters: [\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description: \\'The error that occurred.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description:\\n        \\'Specification for parsing structured outputs from the LLM response.\\',\\n      properties: [\\n        {\\n          type: \\'Output\\',\\n          parameters: [\\n            {\\n              name: \\'Output.text()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for text generation (default).\\',\\n            },\\n            {\\n              name: \\'Output.object()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for typed object generation using schemas. When the model generates a text response, it will return an object that matches the schema.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'schema\\',\\n                      type: \\'Schema<OBJECT>\\',\\n                      description: \\'The schema of the object to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.array()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for array generation. When the model generates a text response, it will return an array of elements.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'element\\',\\n                      type: \\'Schema<ELEMENT>\\',\\n                      description:\\n                        \\'The schema of the array elements to generate.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.choice()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\'Output specification for choice generation. When the model generates a text response, it will return a one of the choice options.\\',\\n              properties: [\\n                {\\n                  type: \\'Options\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'Array<string>\\',\\n                      description: \\'The available choices.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'Output.json()\\',\\n              type: \\'Output\\',\\n              description:\\n                \\' Output specification for unstructured JSON generation. When the model generates a text response, it will return a JSON object.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onStepFinish\\',\\n      type: \\'(result: onStepFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description: \\'Callback that is called when a step is finished.\\',\\n      properties: [\\n        {\\n          type: \\'onStepFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'stepType\\',\\n              type: \\'\"initial\" | \"continue\" | \"tool-result\"\\',\\n              description:\\n                \\'The type of step. The first step is always an \"initial\" step, and subsequent steps are either \"continue\" steps or \"tool-result\" steps.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description:\\n                \\'The reason the model finished generating the text for the step.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the step.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description:\\n                \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'All files that were generated in this step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'isContinued\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when there will be a continuation step with a continuation text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response and all request tool executions (for tools that have an `execute` function) are finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'\"stop\" | \"length\" | \"content-filter\" | \"tool-calls\" | \"error\" | \"other\" | \"unknown\"\\',\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of last step.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'totalUsage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The total token usage from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The full text that has been generated.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'reasoningDetails\\',\\n              type: \\'Array<ReasoningDetail>\\',\\n              description:\\n                \\'The reasoning details of the model (only available for some models).\\',\\n              properties: [\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content (only for type \"text\").\\',\\n                    },\\n                    {\\n                      name: \\'signature\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'Optional signature (only for type \"text\").\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningDetail\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'redacted\\'\",\\n                      description: \\'The type of the reasoning detail.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The redacted data content (only for type \"redacted\").\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description:\\n                \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'Files that were generated in the final step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'The tool calls that have been executed.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'ToolResult[]\\',\\n              description: \\'The tool results that have been generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'steps\\',\\n              type: \\'Array<StepResult>\\',\\n              description:\\n                \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onAbort\\',\\n      type: \\'(event: OnAbortResult) => Promise<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when a stream is aborted via AbortSignal. You can use it to perform cleanup operations.\\',\\n      properties: [\\n        {\\n          type: \\'OnAbortResult\\',\\n          parameters: [\\n            {\\n              name: \\'steps\\',\\n              type: \\'Array<StepResult>\\',\\n              description: \\'Details for all previously finished steps.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'content\\',\\n      type: \\'Promise<Array<ContentPart<TOOLS>>>\\',\\n      description: \\'The content that was generated in the last step. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'finishReason\\',\\n      type: \"Promise<\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'>\",\\n      description:\\n        \\'The reason why the generation finished. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'Promise<LanguageModelUsage>\\',\\n      description:\\n        \\'The token usage of the last step. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'totalUsage\\',\\n      type: \\'Promise<LanguageModelUsage>\\',\\n      description: \\'The total token usage of the generated response. When there are multiple steps, the usage is the sum of all step usages. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'Promise<ProviderMetadata | undefined>\\',\\n      description:\\n        \\'Additional provider-specific metadata from the last step. Metadata is passed through from the provider to the AI SDK and enables provider-specific results that can be fully encapsulated in the provider.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      type: \\'Promise<string>\\',\\n      description:\\n        \\'The full text that has been generated. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: \\'Promise<Array<ReasoningOutput>>\\',\\n      description:\\n        \\'The full reasoning that the model has generated in the last step. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'ReasoningOutput\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning\\'\",\\n              description: \\'The type of the message part.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The reasoning text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'reasoningText\\',\\n      type: \\'Promise<string | undefined>\\',\\n      description:\\n        \\'The reasoning text that the model has generated in the last step. Can be undefined if the model has only generated text. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'sources\\',\\n      type: \\'Promise<Array<Source>>\\',\\n      description:\\n        \\'Sources that have been used as input to generate the response. For multi-step generation, the sources are accumulated from all steps. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'Source\\',\\n          parameters: [\\n            {\\n              name: \\'sourceType\\',\\n              type: \"\\'url\\'\",\\n              description:\\n                \\'A URL source. This is return by web search RAG models.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'The ID of the source.\\',\\n            },\\n            {\\n              name: \\'url\\',\\n              type: \\'string\\',\\n              description: \\'The URL of the source.\\',\\n            },\\n            {\\n              name: \\'title\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The title of the source.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'SharedV2ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'files\\',\\n      type: \\'Promise<Array<GeneratedFile>>\\',\\n      description:\\n        \\'Files that were generated in the final step. Automatically consumes the stream.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'File as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'File as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the file.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolCalls\\',\\n      type: \\'Promise<TypedToolCall<TOOLS>[]>\\',\\n      description:\\n        \\'The tool calls that have been executed. Automatically consumes the stream.\\',\\n    },\\n    {\\n      name: \\'toolResults\\',\\n      type: \\'Promise<TypedToolResult<TOOLS>[]>\\',\\n      description:\\n        \\'The tool results that have been generated. Resolved when the all tool executions are finished.\\',\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'Promise<LanguageModelRequestMetadata>\\',\\n      description: \\'Additional request information from the last step.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Promise<LanguageModelResponseMetadata & { messages: Array<ResponseMessage>; }>\\',\\n      description: \\'Additional response information from the last step.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'model\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'Array<ResponseMessage>\\',\\n              description:\\n                \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Promise<Warning[] | undefined>\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings) for the first step.\\',\\n    },\\n    {\\n      name: \\'steps\\',\\n      type: \\'Promise<Array<StepResult>>\\',\\n      description:\\n        \\'Response information for every step. You can use this to get information about intermediate steps, such as the tool calls or the response headers.\\',\\n      properties: [\\n        {\\n          type: \\'StepResult\\',\\n          parameters: [\\n            {\\n              name: \\'stepType\\',\\n              type: \\'\"initial\" | \"continue\" | \"tool-result\"\\',\\n              description:\\n                \\'The type of step. The first step is always an \"initial\" step, and subsequent steps are either \"continue\" steps or \"tool-result\" steps.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The generated text by the model.\\',\\n            },\\n            {\\n              name: \\'reasoning\\',\\n              type: \\'string | undefined\\',\\n              description:\\n                \\'The reasoning text of the model (only available for some models).\\',\\n            },\\n            {\\n              name: \\'sources\\',\\n              type: \\'Array<Source>\\',\\n              description: \\'Sources that have been used as input.\\',\\n              properties: [\\n                {\\n                  type: \\'Source\\',\\n                  parameters: [\\n                    {\\n                      name: \\'sourceType\\',\\n                      type: \"\\'url\\'\",\\n                      description:\\n                        \\'A URL source. This is return by web search RAG models.\\',\\n                    },\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the source.\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'The URL of the source.\\',\\n                    },\\n                    {\\n                      name: \\'title\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description: \\'The title of the source.\\',\\n                    },\\n                    {\\n                      name: \\'providerMetadata\\',\\n                      type: \\'SharedV2ProviderMetadata\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional provider metadata for the source.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'files\\',\\n              type: \\'Array<GeneratedFile>\\',\\n              description: \\'Files that were generated in this step.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'toolCalls\\',\\n              type: \\'array\\',\\n              description: \\'A list of tool calls made by the model.\\',\\n            },\\n            {\\n              name: \\'toolResults\\',\\n              type: \\'array\\',\\n              description:\\n                \\'A list of tool results returned as responses to earlier tool calls.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'request\\',\\n              type: \\'RequestMetadata\\',\\n              isOptional: true,\\n              description: \\'Request metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'RequestMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'body\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'ResponseMetadata\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'ResponseMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'Array<ResponseMessage>\\',\\n                      description:\\n                        \\'The response messages that were generated during the call. It consists of an assistant message, potentially containing tool calls.  When there are tool results, there is an additional tool message with the tool results that are available. If there are tools that do not have execute functions, they are not included in the tool results and need to be added separately.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'isContinued\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when there will be a continuation step with a continuation text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'Record<string,JSONObject> | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'textStream\\',\\n      type: \\'AsyncIterableStream<string>\\',\\n      description:\\n        \\'A text stream that returns only the generated text deltas. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.\\',\\n    },\\n    {\\n      name: \\'fullStream\\',\\n      type: \\'AsyncIterable<TextStreamPart<TOOLS>> & ReadableStream<TextStreamPart<TOOLS>>\\',\\n      description:\\n        \\'A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. Only errors that stop the stream, such as network errors, are thrown.\\',\\n      properties: [\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Text content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'text\\'\",\\n              description: \\'The type to identify the object as text.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The text content.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Reasoning content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning\\'\",\\n              description: \\'The type to identify the object as reasoning.\\',\\n            },\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The reasoning text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Optional provider metadata for the reasoning.\\',\\n            },\\n          ],\\n        },\\n\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Source content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'source\\'\",\\n              description: \\'The type to identify the object as source.\\',\\n            },\\n            {\\n              name: \\'sourceType\\',\\n              type: \"\\'url\\'\",\\n              description: \\'A URL source. This is returned by web search RAG models.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'The ID of the source.\\',\\n            },\\n            {\\n              name: \\'url\\',\\n              type: \\'string\\',\\n              description: \\'The URL of the source.\\',\\n            },\\n            {\\n              name: \\'title\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The title of the source.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata\\',\\n              isOptional: true,\\n              description: \\'Additional provider metadata for the source.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'File content part from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'file\\'\",\\n              description: \\'The type to identify the object as file.\\',\\n            },\\n            {\\n              name: \\'file\\',\\n              type: \\'GeneratedFile\\',\\n              description: \\'The file.\\',\\n              properties: [\\n                {\\n                  type: \\'GeneratedFile\\',\\n                  parameters: [\\n                    {\\n                      name: \\'base64\\',\\n                      type: \\'string\\',\\n                      description: \\'File as a base64 encoded string.\\',\\n                    },\\n                    {\\n                      name: \\'uint8Array\\',\\n                      type: \\'Uint8Array\\',\\n                      description: \\'File as a Uint8Array.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Tool call from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call\\'\",\\n              description: \\'The type to identify the object as tool call.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'input\\',\\n              type: \\'object based on tool parameters\\',\\n              description:\\n                \\'Parameters generated by the model to be used by the tool. The type is inferred from the tool definition.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call-streaming-start\\'\",\\n              description:\\n                \\'Indicates the start of a tool call streaming. Only available when streaming tool calls.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call-delta\\'\",\\n              description:\\n                \\'The type to identify the object as tool call delta. Only available when streaming tool calls.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'argsTextDelta\\',\\n              type: \\'string\\',\\n              description: \\'The text delta of the tool call arguments.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          description: \\'Tool result from ContentPart<TOOLS>\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-result\\'\",\\n              description: \\'The type to identify the object as tool result.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'input\\',\\n              type: \\'object based on tool parameters\\',\\n              description:\\n                \\'Parameters that were passed to the tool. The type is inferred from the tool definition.\\',\\n            },\\n            {\\n              name: \\'output\\',\\n              type: \\'tool execution return type\\',\\n              description:\\n                \\'The result returned by the tool after execution has completed. The type is inferred from the tool execute function return type.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'start-step\\'\",\\n              description: \\'Indicates the start of a new step in the stream.\\',\\n            },\\n            {\\n              name: \\'request\\',\\n              type: \\'LanguageModelRequestMetadata\\',\\n              description:\\n                \\'Information about the request that was sent to the language model provider.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelRequestMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'body\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'Raw request HTTP body that was sent to the provider API as a string.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[]\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish-step\\'\",\\n              description:\\n                \\'Indicates the end of the current step in the stream.\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'LanguageModelResponseMetadata\\',\\n              description:\\n                \\'Response metadata from the language model provider.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelResponseMetadata\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'The response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata | undefined\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'start\\'\",\\n              description: \\'Indicates the start of the stream.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish\\'\",\\n              description: \\'The type to identify the object as finish.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'totalUsage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The total token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'reasoning-part-finish\\'\",\\n              description: \\'Indicates the end of a reasoning part.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'error\\'\",\\n              description: \\'The type to identify the object as error.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description:\\n                \\'Describes the error that may have occurred during execution.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextStreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'abort\\'\",\\n              description: \\'The type to identify the object as abort.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'partialOutputStream\\',\\n      type: \\'AsyncIterableStream<PARTIAL_OUTPUT>\\',\\n      description:\\n        \\'A stream of partial parsed outputs. It uses the `output` specification. AsyncIterableStream is defined as AsyncIterable<T> & ReadableStream<T>.\\',\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Promise<COMPLETE_OUTPUT>\\',\\n      description:\\n        \\'The complete parsed output. It uses the `output` specification.\\',\\n    },\\n    {\\n      name: \\'consumeStream\\',\\n      type: \\'(options?: ConsumeStreamOptions) => Promise<void>\\',\\n      description:\\n        \\'Consumes the stream without processing the parts. This is useful to force the stream to finish. If an error occurs, it is passed to the optional `onError` callback.\\',\\n      properties: [\\n        {\\n          type: \\'ConsumeStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onError\\',\\n              type: \\'(error: unknown) => void\\',\\n              isOptional: true,\\n              description: \\'The error callback.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toUIMessageStream\\',\\n      type: \\'(options?: UIMessageStreamOptions) => AsyncIterableStream<UIMessageChunk>\\',\\n      description:\\n        \\'Converts the result to a UI message stream. Returns an AsyncIterableStream that can be used as both an AsyncIterable and a ReadableStream.\\',\\n      properties: [\\n        {\\n          type: \\'UIMessageStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'originalMessages\\',\\n              type: \\'UIMessage[]\\',\\n              isOptional: true,\\n              description: \\'The original messages.\\',\\n            },\\n            {\\n              name: \\'onFinish\\',\\n              type: \\'(options: { messages: UIMessage[]; isContinuation: boolean; responseMessage: UIMessage; isAborted: boolean; }) => void\\',\\n              isOptional: true,\\n              description: \\'Callback function called when the stream finishes. Provides the updated list of UI messages, whether the response is a continuation, the response message, and whether the stream was aborted.\\',\\n            },\\n            {\\n              name: \\'messageMetadata\\',\\n              type: \\'(options: { part: TextStreamPart<TOOLS> & { type: \"start\" | \"finish\" | \"start-step\" | \"finish-step\"; }; }) => unknown\\',\\n              isOptional: true,\\n              description: \\'Extracts message metadata that will be sent to the client. Called on start and finish events.\\',\\n            },\\n            {\\n              name: \\'sendReasoning\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send reasoning parts to the client. Defaults to false.\\',\\n            },\\n            {\\n              name: \\'sendSources\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send source parts to the client. Defaults to false.\\',\\n            },\\n            {\\n              name: \\'sendFinish\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send the finish event to the client. Defaults to true.\\',\\n            },\\n            {\\n              name: \\'sendStart\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Send the message start event to the client. Set to false if you are using additional streamText calls and the message start event has already been sent. Defaults to true.\\',\\n            },\\n            {\\n              name: \\'onError\\',\\n              type: \\'(error: unknown) => string\\',\\n              isOptional: true,\\n              description:\\n                \\'Process an error, e.g. to log it. Returns error message to include in the data stream. Defaults to () => \"An error occurred.\"\\',\\n            },\\n            {\\n              name: \\'consumeSseStream\\',\\n              type: \\'(stream: ReadableStream) => Promise<void>\\',\\n              isOptional: true,\\n              description:\\n                \\'Function to consume the SSE stream. Required for proper abort handling in UI message streams. Use the `consumeStream` function from the AI SDK.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'pipeUIMessageStreamToResponse\\',\\n      type: \\'(response: ServerResponse, options?: ResponseInit & UIMessageStreamOptions) => void\\',\\n      description:\\n        \\'Writes UI message stream output to a Node.js response-like object.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit & UIMessageStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'HeadersInit\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'pipeTextStreamToResponse\\',\\n      type: \\'(response: ServerResponse, init?: ResponseInit) => void\\',\\n      description:\\n        \\'Writes text delta output to a Node.js response-like object. It sets a `Content-Type` header to `text/plain; charset=utf-8` and writes each text delta as a separate chunk.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toUIMessageStreamResponse\\',\\n      type: \\'(options?: ResponseInit & UIMessageStreamOptions) => Response\\',\\n      description:\\n        \\'Converts the result to a streamed response object with a UI message stream.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit & UIMessageStreamOptions\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'HeadersInit\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toTextStreamResponse\\',\\n      type: \\'(init?: ResponseInit) => Response\\',\\n      description:\\n        \\'Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n\\n]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to stream text generated by a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/streaming-text-generation\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream chat completions generated by a language model in Next.js\\',\\n      link: \\'/examples/next-app/chat/stream-chat-completion\\',\\n    },\\n    {\\n      title: \\'Learn to stream text generated by a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/stream-text\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to stream chat completions generated by a language model in Node.js\\',\\n      link: \\'/examples/node/generating-text/stream-text-with-chat-prompt\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/03-generate-object.mdx'), name='03-generate-object.mdx', displayName='03-generate-object.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateObject\\ndescription: API Reference for generateObject.\\n---\\n\\n# `generateObject()`\\n\\nGenerates a typed, structured object for a given prompt and schema using a language model.\\n\\nIt can be used to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\\n\\n#### Example: generate an object using a schema\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(z.string()),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n\\nconsole.log(JSON.stringify(object, null, 2));\\n```\\n\\n#### Example: generate an array using a schema\\n\\nFor arrays, you specify the schema of the array items.\\n\\n```ts highlight=\"7\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'array\\',\\n  schema: z.object({\\n    name: z.string(),\\n    class: z\\n      .string()\\n      .describe(\\'Character class, e.g. warrior, mage, or thief.\\'),\\n    description: z.string(),\\n  }),\\n  prompt: \\'Generate 3 hero descriptions for a fantasy role playing game.\\',\\n});\\n```\\n\\n#### Example: generate an enum\\n\\nWhen you want to generate a specific enum value, you can set the output strategy to `enum`\\nand provide the list of possible values in the `enum` parameter.\\n\\n```ts highlight=\"5-6\"\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'enum\\',\\n  enum: [\\'action\\', \\'comedy\\', \\'drama\\', \\'horror\\', \\'sci-fi\\'],\\n  prompt:\\n    \\'Classify the genre of this movie plot: \\' +\\n    \\'\"A group of astronauts travel through a wormhole in search of a \\' +\\n    \\'new habitable planet for humanity.\"\\',\\n});\\n```\\n\\n#### Example: generate JSON without a schema\\n\\n```ts highlight=\"6\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateObject } from \\'ai\\';\\n\\nconst { object } = await generateObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'no-schema\\',\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n```\\n\\nTo see `generateObject` in action, check out the [additional examples](#more-examples).\\n\\n## Import\\n\\n<Snippet text={`import { generateObject } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4.1\\')\",\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \"\\'object\\' | \\'array\\' | \\'enum\\' | \\'no-schema\\' | undefined\",\\n      description: \"The type of output to generate. Defaults to \\'object\\'.\",\\n    },\\n    {\\n      name: \\'mode\\',\\n      type: \"\\'auto\\' | \\'json\\' | \\'tool\\'\",\\n      description:\\n        \"The mode to use for object generation. Not every model supports all modes. \\\\\\n        Defaults to \\'auto\\' for \\'object\\' output and to \\'json\\' for \\'no-schema\\' output. \\\\\\n        Must be \\'json\\' for \\'no-schema\\' output.\",\\n    },\\n    {\\n      name: \\'schema\\',\\n      type: \\'Zod Schema | JSON Schema\\',\\n      description:\\n        \"The schema that describes the shape of the object to generate. \\\\\\n        It is sent to the model to generate the object and used to validate the output. \\\\\\n        You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). \\\\\\n        In \\'array\\' mode, the schema is used to describe an array element. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaName\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional name of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaDescription\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional description of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'enum\\',\\n      type: \\'string[]\\',\\n      description:\\n        \"List of possible values to generate. \\\\\\n        Only available with \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'args\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_repairText\\',\\n      type: \\'(options: RepairTextOptions) => Promise<string>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair the raw output of the model to enable JSON parsing. Should return the repaired text or null if the text cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'RepairTextOptions\\',\\n          parameters: [\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The text that was generated by the model.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'JSONParseError | TypeValidationError\\',\\n              description: \\'The error that occurred while parsing the text.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'object\\',\\n      type: \\'based on the schema\\',\\n      description:\\n        \\'The generated object, validated by the schema (if it supports validation).\\',\\n    },\\n    {\\n      name: \\'finishReason\\',\\n      type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n      description: \\'The reason the model finished generating the text.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'LanguageModelUsage\\',\\n      description: \\'The token usage of the generated text.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'LanguageModelRequestMetadata\\',\\n      isOptional: true,\\n      description: \\'Request metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'LanguageModelResponseMetadata\\',\\n      isOptional: true,\\n      description: \\'Response metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'unknown\\',\\n              description: \\'Optional response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \\'The reasoning that was used to generate the object. Concatenated from all reasoning parts.\\',\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'toJsonResponse\\',\\n      type: \\'(init?: ResponseInit) => Response\\',\\n      description:\\n        \\'Converts the object to a JSON response. The response will have a status code of 200 and a content type of `application/json; charset=utf-8`.\\',\\n    },\\n  ]}\\n/>\\n\\n## More Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title:\\n        \\'Learn to generate structured data using a language model in Next.js\\',\\n      link: \\'/examples/next-app/basics/generating-object\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to generate structured data using a language model in Node.js\\',\\n      link: \\'/examples/node/generating-structured-data/generate-object\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/04-stream-object.mdx'), name='04-stream-object.mdx', displayName='04-stream-object.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamObject\\ndescription: API Reference for streamObject\\n---\\n\\n# `streamObject()`\\n\\nStreams a typed, structured object for a given prompt and schema using a language model.\\n\\nIt can be used to force the language model to return structured data, e.g. for information extraction, synthetic data generation, or classification tasks.\\n\\n#### Example: stream an object using a schema\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  schema: z.object({\\n    recipe: z.object({\\n      name: z.string(),\\n      ingredients: z.array(z.string()),\\n      steps: z.array(z.string()),\\n    }),\\n  }),\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n\\nfor await (const partialObject of partialObjectStream) {\\n  console.clear();\\n  console.log(partialObject);\\n}\\n```\\n\\n#### Example: stream an array using a schema\\n\\nFor arrays, you specify the schema of the array items.\\nYou can use `elementStream` to get the stream of complete array elements.\\n\\n```ts highlight=\"7,18\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst { elementStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'array\\',\\n  schema: z.object({\\n    name: z.string(),\\n    class: z\\n      .string()\\n      .describe(\\'Character class, e.g. warrior, mage, or thief.\\'),\\n    description: z.string(),\\n  }),\\n  prompt: \\'Generate 3 hero descriptions for a fantasy role playing game.\\',\\n});\\n\\nfor await (const hero of elementStream) {\\n  console.log(hero);\\n}\\n```\\n\\n#### Example: generate JSON without a schema\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { streamObject } from \\'ai\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'no-schema\\',\\n  prompt: \\'Generate a lasagna recipe.\\',\\n});\\n\\nfor await (const partialObject of partialObjectStream) {\\n  console.clear();\\n  console.log(partialObject);\\n}\\n```\\n\\n#### Example: generate an enum\\n\\nWhen you want to generate a specific enum value, you can set the output strategy to `enum`\\nand provide the list of possible values in the `enum` parameter.\\n\\n```ts highlight=\"5-6\"\\nimport { streamObject } from \\'ai\\';\\n\\nconst { partialObjectStream } = streamObject({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: \\'enum\\',\\n  enum: [\\'action\\', \\'comedy\\', \\'drama\\', \\'horror\\', \\'sci-fi\\'],\\n  prompt:\\n    \\'Classify the genre of this movie plot: \\' +\\n    \\'\"A group of astronauts travel through a wormhole in search of a \\' +\\n    \\'new habitable planet for humanity.\"\\',\\n});\\n```\\n\\nTo see `streamObject` in action, check out the [additional examples](#more-examples).\\n\\n## Import\\n\\n<Snippet text={`import { streamObject } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \"The language model to use. Example: openai(\\'gpt-4.1\\')\",\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \"\\'object\\' | \\'array\\' | \\'enum\\' | \\'no-schema\\' | undefined\",\\n      description: \"The type of output to generate. Defaults to \\'object\\'.\",\\n    },\\n    {\\n      name: \\'mode\\',\\n      type: \"\\'auto\\' | \\'json\\' | \\'tool\\'\",\\n      description:\\n        \"The mode to use for object generation. Not every model supports all modes. \\\\\\n        Defaults to \\'auto\\' for \\'object\\' output and to \\'json\\' for \\'no-schema\\' output. \\\\\\n        Must be \\'json\\' for \\'no-schema\\' output.\",\\n    },\\n    {\\n      name: \\'schema\\',\\n      type: \\'Zod Schema | JSON Schema\\',\\n      description:\\n        \"The schema that describes the shape of the object to generate. \\\\\\n        It is sent to the model to generate the object and used to validate the output. \\\\\\n        You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function). \\\\\\n        In \\'array\\' mode, the schema is used to describe an array element. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaName\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional name of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'schemaDescription\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \"Optional description of the output that should be generated. \\\\\\n        Used by some providers for additional LLM guidance, e.g. via tool or schema name. \\\\\\n        Not available with \\'no-schema\\' or \\'enum\\' output.\",\\n    },\\n    {\\n      name: \\'system | SystemModelMessage\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemModelMessage | UserModelMessage | AssistantModelMessage | ToolModelMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'SystemModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | FilePart | ReasoningPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ReasoningPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'reasoning\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The reasoning text.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                    {\\n                      name: \\'filename\\',\\n                      type: \\'string\\',\\n                      description: \\'The name of the file.\\',\\n                      isOptional: true,\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'args\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolModelMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_repairText\\',\\n      type: \\'(options: RepairTextOptions) => Promise<string>\\',\\n      isOptional: true,\\n      description:\\n        \\'A function that attempts to repair the raw output of the model to enable JSON parsing. Should return the repaired text or null if the text cannot be repaired.\\',\\n      properties: [\\n        {\\n          type: \\'RepairTextOptions\\',\\n          parameters: [\\n            {\\n              name: \\'text\\',\\n              type: \\'string\\',\\n              description: \\'The text that was generated by the model.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'JSONParseError | TypeValidationError\\',\\n              description: \\'The error that occurred while parsing the text.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'(requestedDownloads: Array<{ url: URL; isUrlSupportedByModel: boolean }>) => Promise<Array<null | { data: Uint8Array; mediaType?: string }>>\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom download function to control how URLs are fetched when they appear in prompts. By default, files are downloaded if the model does not support the URL for the given media type. Experimental feature. Return null to pass the URL directly to the model (when supported), or return downloaded content with data and media type.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(event: OnErrorResult) => Promise<void> |void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when an error occurs during streaming. You can use it to log errors.\\',\\n      properties: [\\n        {\\n          type: \\'OnErrorResult\\',\\n          parameters: [\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description: \\'The error that occurred.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response has finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'usage\\',\\n              type: \\'LanguageModelUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'LanguageModelUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'inputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description: \\'The number of input (prompt) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'outputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The number of output (completion) tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number | undefined\\',\\n                      description:\\n                        \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n                    },\\n                    {\\n                      name: \\'reasoningTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of reasoning tokens used.\\',\\n                    },\\n                    {\\n                      name: \\'cachedInputTokens\\',\\n                      type: \\'number | undefined\\',\\n                      isOptional: true,\\n                      description: \\'The number of cached input tokens.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'providerMetadata\\',\\n              type: \\'ProviderMetadata | undefined\\',\\n              description:\\n                \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n            },\\n            {\\n              name: \\'object\\',\\n              type: \\'T | undefined\\',\\n              description:\\n                \\'The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown | undefined\\',\\n              description:\\n                \\'Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Optional response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'usage\\',\\n      type: \\'Promise<LanguageModelUsage>\\',\\n      description:\\n        \\'The token usage of the generated text. Resolved when the response is finished.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'inputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of input (prompt) tokens used.\\',\\n            },\\n            {\\n              name: \\'outputTokens\\',\\n              type: \\'number | undefined\\',\\n              description: \\'The number of output (completion) tokens used.\\',\\n            },\\n            {\\n              name: \\'totalTokens\\',\\n              type: \\'number | undefined\\',\\n              description:\\n                \\'The total number of tokens as reported by the provider. This number might be different from the sum of inputTokens and outputTokens and e.g. include reasoning tokens or other overhead.\\',\\n            },\\n            {\\n              name: \\'reasoningTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of reasoning tokens used.\\',\\n            },\\n            {\\n              name: \\'cachedInputTokens\\',\\n              type: \\'number | undefined\\',\\n              isOptional: true,\\n              description: \\'The number of cached input tokens.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'Promise<Record<string,JSONObject> | undefined>\\',\\n      description:\\n        \\'Optional metadata from the provider. Resolved whe the response is finished. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'object\\',\\n      type: \\'Promise<T>\\',\\n      description:\\n        \\'The generated object (typed according to the schema). Resolved when the response is finished.\\',\\n    },\\n    {\\n      name: \\'partialObjectStream\\',\\n      type: \\'AsyncIterableStream<DeepPartial<T>>\\',\\n      description:\\n        \\'Stream of partial objects. It gets more complete as the stream progresses. Note that the partial object is not validated. If you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.\\',\\n    },\\n    {\\n      name: \\'elementStream\\',\\n      type: \\'AsyncIterableStream<ELEMENT>\\',\\n      description: \\'Stream of array elements. Only available in \"array\" mode.\\',\\n    },\\n    {\\n      name: \\'textStream\\',\\n      type: \\'AsyncIterableStream<string>\\',\\n      description:\\n        \\'Text stream of the JSON representation of the generated object. It contains text chunks. When the stream is finished, the object is valid JSON that can be parsed.\\',\\n    },\\n    {\\n      name: \\'fullStream\\',\\n      type: \\'AsyncIterableStream<ObjectStreamPart<T>>\\',\\n      description:\\n        \\'Stream of different types of events, including partial objects, errors, and finish events. Only errors that stop the stream, such as network errors, are thrown.\\',\\n      properties: [\\n        {\\n          type: \\'ObjectPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'object\\'\",\\n            },\\n            {\\n              name: \\'object\\',\\n              type: \\'DeepPartial<T>\\',\\n              description: \\'The partial object that was generated.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'TextDeltaPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'text-delta\\'\",\\n            },\\n            {\\n              name: \\'textDelta\\',\\n              type: \\'string\\',\\n              description: \\'The text delta for the underlying raw JSON text.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ErrorPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'error\\'\",\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown\\',\\n              description: \\'The error that occurred.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'FinishPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish\\'\",\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \\'FinishReason\\',\\n            },\\n            {\\n              name: \\'logprobs\\',\\n              type: \\'Logprobs\\',\\n              isOptional: true,\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'Usage\\',\\n              description: \\'Token usage.\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              isOptional: true,\\n              description: \\'Response metadata.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'model\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n                    },\\n                    {\\n                      name: \\'timestamp\\',\\n                      type: \\'Date\\',\\n                      description:\\n                        \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'request\\',\\n      type: \\'Promise<LanguageModelRequestMetadata>\\',\\n      description: \\'Request metadata.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelRequestMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Raw request HTTP body that was sent to the provider API as a string (JSON should be stringified).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Promise<LanguageModelResponseMetadata>\\',\\n      description: \\'Response metadata. Resolved when the response is finished.\\',\\n      properties: [\\n        {\\n          type: \\'LanguageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The response identifier. The AI SDK uses the ID from the provider response when available, and generates an ID otherwise.\\',\\n            },\\n            {\\n              name: \\'model\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The model that was used to generate the response. The AI SDK uses the response model from the provider response when available, and the model from the function call otherwise.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description:\\n                \\'The timestamp of the response. The AI SDK uses the response timestamp from the provider response when available, and creates a timestamp otherwise.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Optional response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'CallWarning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'pipeTextStreamToResponse\\',\\n      type: \\'(response: ServerResponse, init?: ResponseInit => void\\',\\n      description:\\n        \\'Writes text delta output to a Node.js response-like object. It sets a `Content-Type` header to `text/plain; charset=utf-8` and writes each text delta as a separate chunk.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toTextStreamResponse\\',\\n      type: \\'(init?: ResponseInit) => Response\\',\\n      description:\\n        \\'Creates a simple text stream response. Each text delta is encoded as UTF-8 and sent as a separate chunk. Non-text-delta events are ignored.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description: \\'The response status code.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The response status text.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'The response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## More Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Streaming Object Generation with RSC\\',\\n      link: \\'/examples/next-app/basics/streaming-object-generation\\',\\n    },\\n    {\\n      title: \\'Streaming Object Generation with useObject\\',\\n      link: \\'/examples/next-pages/basics/streaming-object-generation\\',\\n    },\\n    {\\n      title: \\'Streaming Partial Objects\\',\\n      link: \\'/examples/node/streaming-structured-data/stream-object\\',\\n    },\\n    {\\n      title: \\'Recording Token Usage\\',\\n      link: \\'/examples/node/streaming-structured-data/token-usage\\',\\n    },\\n    {\\n      title: \\'Recording Final Object\\',\\n      link: \\'/examples/node/streaming-structured-data/object\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/05-embed.mdx'), name='05-embed.mdx', displayName='05-embed.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: embed\\ndescription: API Reference for embed.\\n---\\n\\n# `embed()`\\n\\nGenerate an embedding for a single value using an embedding model.\\n\\nThis is ideal for use cases where you need to embed a single value to e.g. retrieve similar items or to use the embedding in a downstream task.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embed } from \\'ai\\';\\n\\nconst { embedding } = await embed({\\n  model: \\'openai/text-embedding-3-small\\',\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { embed } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'EmbeddingModel\\',\\n      description:\\n        \"The embedding model to use. Example: openai.embeddingModel(\\'text-embedding-3-small\\')\",\\n    },\\n    {\\n      name: \\'value\\',\\n      type: \\'VALUE\\',\\n      description: \\'The value to embed. The type depends on the model.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n            {\\n              name: \\'tracer\\',\\n              type: \\'Tracer\\',\\n              isOptional: true,\\n              description: \\'A custom tracer to use for the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'VALUE\\',\\n      description: \\'The value that was embedded.\\',\\n    },\\n    {\\n      name: \\'embedding\\',\\n      type: \\'number[]\\',\\n      description: \\'The embedding of the value.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'EmbeddingModelUsage\\',\\n      description: \\'The token usage for generating the embeddings.\\',\\n      properties: [\\n        {\\n          type: \\'EmbeddingModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'tokens\\',\\n              type: \\'number\\',\\n              description: \\'The number of tokens used in the embedding.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      isOptional: true,\\n      description: \\'Optional response data.\\',\\n      properties: [\\n        {\\n          type: \\'Response\\',\\n          parameters: [\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/06-embed-many.mdx'), name='06-embed-many.mdx', displayName='06-embed-many.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: embedMany\\ndescription: API Reference for embedMany.\\n---\\n\\n# `embedMany()`\\n\\nEmbed several values using an embedding model. The type of the value is defined\\nby the embedding model.\\n\\n`embedMany` automatically splits large requests into smaller chunks if the model\\nhas a limit on how many embeddings can be generated in a single call.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { embedMany } from \\'ai\\';\\n\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { embedMany } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'EmbeddingModel\\',\\n      description:\\n        \"The embedding model to use. Example: openai.embeddingModel(\\'text-embedding-3-small\\')\",\\n    },\\n    {\\n      name: \\'values\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The values to embed. The type depends on the model.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n            {\\n              name: \\'tracer\\',\\n              type: \\'Tracer\\',\\n              isOptional: true,\\n              description: \\'A custom tracer to use for the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'values\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The values that were embedded.\\',\\n    },\\n    {\\n      name: \\'embeddings\\',\\n      type: \\'number[][]\\',\\n      description: \\'The embeddings. They are in the same order as the values.\\',\\n    },\\n    {\\n      name: \\'usage\\',\\n      type: \\'EmbeddingModelUsage\\',\\n      description: \\'The token usage for generating the embeddings.\\',\\n      properties: [\\n        {\\n          type: \\'EmbeddingModelUsage\\',\\n          parameters: [\\n            {\\n              name: \\'tokens\\',\\n              type: \\'number\\',\\n              description: \\'The total number of input tokens.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/06-rerank.mdx'), name='06-rerank.mdx', displayName='06-rerank.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: rerank\\ndescription: API Reference for rerank.\\n---\\n\\n# `rerank()`\\n\\nRerank a set of documents based on their relevance to a query using a reranking model.\\n\\nThis is ideal for improving search relevance by reordering documents, emails, or other content based on semantic understanding of the query and documents.\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { rerank } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'RerankingModel\\',\\n      description:\\n        \"The reranking model to use. Example: cohere.reranking(\\'rerank-v3.5\\')\",\\n    },\\n    {\\n      name: \\'documents\\',\\n      type: \\'Array<VALUE>\\',\\n      description:\\n        \\'The documents to rerank. Can be an array of strings or JSON objects.\\',\\n    },\\n    {\\n      name: \\'query\\',\\n      type: \\'string\\',\\n      description: \\'The search query to rank documents against.\\',\\n    },\\n    {\\n      name: \\'topN\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of top documents to return. If not specified, all documents are returned.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'ProviderOptions\\',\\n      isOptional: true,\\n      description: \\'Provider-specific options for the reranking request.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Telemetry configuration. Experimental feature.\\',\\n      properties: [\\n        {\\n          type: \\'TelemetrySettings\\',\\n          parameters: [\\n            {\\n              name: \\'isEnabled\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable telemetry. Disabled by default while experimental.\\',\\n            },\\n            {\\n              name: \\'recordInputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable input recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'recordOutputs\\',\\n              type: \\'boolean\\',\\n              isOptional: true,\\n              description:\\n                \\'Enable or disable output recording. Enabled by default.\\',\\n            },\\n            {\\n              name: \\'functionId\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'Identifier for this function. Used to group telemetry data by function.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              isOptional: true,\\n              type: \\'Record<string, string | number | boolean | Array<null | undefined | string> | Array<null | undefined | number> | Array<null | undefined | boolean>>\\',\\n              description:\\n                \\'Additional information to include in the telemetry data.\\',\\n            },\\n            {\\n              name: \\'tracer\\',\\n              type: \\'Tracer\\',\\n              isOptional: true,\\n              description: \\'A custom tracer to use for the telemetry data.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'originalDocuments\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The original documents array in their original order.\\',\\n    },\\n    {\\n      name: \\'rerankedDocuments\\',\\n      type: \\'Array<VALUE>\\',\\n      description: \\'The documents sorted by relevance score (descending).\\',\\n    },\\n    {\\n      name: \\'ranking\\',\\n      type: \\'Array<RankingItem<VALUE>>\\',\\n      description: \\'Array of ranking items with scores and indices.\\',\\n      properties: [\\n        {\\n          type: \\'RankingItem<VALUE>\\',\\n          parameters: [\\n            {\\n              name: \\'originalIndex\\',\\n              type: \\'number\\',\\n              description:\\n                \\'The index of the document in the original documents array.\\',\\n            },\\n            {\\n              name: \\'score\\',\\n              type: \\'number\\',\\n              description:\\n                \\'The relevance score for the document (typically 0-1, where higher is more relevant).\\',\\n            },\\n            {\\n              name: \\'document\\',\\n              type: \\'VALUE\\',\\n              description: \\'The document itself.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description: \\'Response data.\\',\\n      properties: [\\n        {\\n          type: \\'Response\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description: \\'The response ID from the provider.\\',\\n            },\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'The timestamp of the response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description: \\'The model ID used for reranking.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Response headers.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The raw response body.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ProviderMetadata | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n### String Documents\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking, rerankedDocuments } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\n    \\'sunny day at the beach\\',\\n    \\'rainy afternoon in the city\\',\\n    \\'snowy night in the mountains\\',\\n  ],\\n  query: \\'talk about rain\\',\\n  topN: 2,\\n});\\n\\nconsole.log(rerankedDocuments);\\n// [\\'rainy afternoon in the city\\', \\'sunny day at the beach\\']\\n\\nconsole.log(ranking);\\n// [\\n//   { originalIndex: 1, score: 0.9, document: \\'rainy afternoon...\\' },\\n//   { originalIndex: 0, score: 0.3, document: \\'sunny day...\\' }\\n// ]\\n```\\n\\n### Object Documents\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst documents = [\\n  {\\n    from: \\'Paul Doe\\',\\n    subject: \\'Follow-up\\',\\n    text: \\'We are happy to give you a discount of 20%.\\',\\n  },\\n  {\\n    from: \\'John McGill\\',\\n    subject: \\'Missing Info\\',\\n    text: \\'Here is the pricing from Oracle: $5000/month\\',\\n  },\\n];\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents,\\n  query: \\'Which pricing did we get from Oracle?\\',\\n  topN: 1,\\n});\\n\\nconsole.log(ranking[0].document);\\n// { from: \\'John McGill\\', subject: \\'Missing Info\\', ... }\\n```\\n\\n### With Provider Options\\n\\n```ts\\nimport { cohere } from \\'@ai-sdk/cohere\\';\\nimport { rerank } from \\'ai\\';\\n\\nconst { ranking } = await rerank({\\n  model: cohere.reranking(\\'rerank-v3.5\\'),\\n  documents: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n  query: \\'talk about rain\\',\\n  providerOptions: {\\n    cohere: {\\n      maxTokensPerDoc: 1000,\\n    },\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/10-generate-image.mdx'), name='10-generate-image.mdx', displayName='10-generate-image.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateImage\\ndescription: API Reference for generateImage.\\n---\\n\\n# `generateImage()`\\n\\n<Note type=\"warning\">`generateImage` is an experimental feature.</Note>\\n\\nGenerates images based on a given prompt using an image model.\\n\\nIt is ideal for use cases where you need to generate images programmatically,\\nsuch as creating visual content or generating images for data augmentation.\\n\\n```ts\\nimport { experimental_generateImage as generateImage } from \\'ai\\';\\n\\nconst { images } = await generateImage({\\n  model: openai.image(\\'dall-e-3\\'),\\n  prompt: \\'A futuristic cityscape at sunset\\',\\n  n: 3,\\n  size: \\'1024x1024\\',\\n});\\n\\nconsole.log(images);\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_generateImage as generateImage } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'ImageModelV3\\',\\n      description: \\'The image model to use.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string\\',\\n      description: \\'The input prompt to generate the image from.\\',\\n    },\\n    {\\n      name: \\'n\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Number of images to generate.\\',\\n    },\\n    {\\n      name: \\'size\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'Size of the images to generate. Format: `{width}x{height}`.\\',\\n    },\\n    {\\n      name: \\'aspectRatio\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'Aspect ratio of the images to generate. Format: `{width}:{height}`.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Seed for the image generation.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'ProviderOptions\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific options.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'An optional abort signal to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional HTTP headers for the request.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'image\\',\\n      type: \\'GeneratedFile\\',\\n      description: \\'The first image that was generated.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'Image as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'Image as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the image.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'images\\',\\n      type: \\'Array<GeneratedFile>\\',\\n      description: \\'All images that were generated.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'Image as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'Image as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mediaType\\',\\n              type: \\'string\\',\\n              description: \\'The IANA media type of the image.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[]\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'providerMetadata\\',\\n      type: \\'ImageModelProviderMetadata\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional metadata from the provider. The outer key is the provider name. The inner values are the metadata. An `images` key is always present in the metadata and is an array with the same length as the top level `images` key. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'responses\\',\\n      type: \\'Array<ImageModelResponseMetadata>\\',\\n      description:\\n        \\'Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.\\',\\n      properties: [\\n        {\\n          type: \\'ImageModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'Timestamp for the start of the generated response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the response model that was used to generate the response.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/11-transcribe.mdx'), name='11-transcribe.mdx', displayName='11-transcribe.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: transcribe\\ndescription: API Reference for transcribe.\\n---\\n\\n# `transcribe()`\\n\\n<Note type=\"warning\">`transcribe` is an experimental feature.</Note>\\n\\nGenerates a transcript from an audio file.\\n\\n```ts\\nimport { experimental_transcribe as transcribe } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { readFile } from \\'fs/promises\\';\\n\\nconst { text: transcript } = await transcribe({\\n  model: openai.transcription(\\'whisper-1\\'),\\n  audio: await readFile(\\'audio.mp3\\'),\\n});\\n\\nconsole.log(transcript);\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_transcribe as transcribe } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'TranscriptionModelV3\\',\\n      description: \\'The transcription model to use.\\',\\n    },\\n    {\\n      name: \\'audio\\',\\n      type: \\'DataContent (string | Uint8Array | ArrayBuffer | Buffer) | URL\\',\\n      description: \\'The audio file to generate the transcript from.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string, JSONObject>\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific options.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'An optional abort signal to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional HTTP headers for the request.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'text\\',\\n      type: \\'string\\',\\n      description: \\'The complete transcribed text from the audio input.\\',\\n    },\\n    {\\n      name: \\'segments\\',\\n      type: \\'Array<{ text: string; startSecond: number; endSecond: number }>\\',\\n      description:\\n        \\'An array of transcript segments, each containing a portion of the transcribed text along with its start and end times in seconds.\\',\\n    },\\n    {\\n      name: \\'language\\',\\n      type: \\'string | undefined\\',\\n      description:\\n        \\'The language of the transcript in ISO-639-1 format e.g. \"en\" for English.\\',\\n    },\\n    {\\n      name: \\'durationInSeconds\\',\\n      type: \\'number | undefined\\',\\n      description: \\'The duration of the transcript in seconds.\\',\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[]\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'responses\\',\\n      type: \\'Array<TranscriptionModelResponseMetadata>\\',\\n      description:\\n        \\'Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.\\',\\n      properties: [\\n        {\\n          type: \\'TranscriptionModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'Timestamp for the start of the generated response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the response model that was used to generate the response.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/12-generate-speech.mdx'), name='12-generate-speech.mdx', displayName='12-generate-speech.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateSpeech\\ndescription: API Reference for generateSpeech.\\n---\\n\\n# `generateSpeech()`\\n\\n<Note type=\"warning\">`generateSpeech` is an experimental feature.</Note>\\n\\nGenerates speech audio from text.\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { audio } = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello from the AI SDK!\\',\\n  voice: \\'alloy\\',\\n});\\n\\nconsole.log(audio);\\n```\\n\\n## Examples\\n\\n### OpenAI\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nconst { audio } = await generateSpeech({\\n  model: openai.speech(\\'tts-1\\'),\\n  text: \\'Hello from the AI SDK!\\',\\n  voice: \\'alloy\\',\\n});\\n```\\n\\n### ElevenLabs\\n\\n```ts\\nimport { experimental_generateSpeech as generateSpeech } from \\'ai\\';\\nimport { elevenlabs } from \\'@ai-sdk/elevenlabs\\';\\n\\nconst { audio } = await generateSpeech({\\n  model: elevenlabs.speech(\\'eleven_multilingual_v2\\'),\\n  text: \\'Hello from the AI SDK!\\',\\n  voice: \\'your-voice-id\\', // Required: get this from your ElevenLabs account\\n});\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_generateSpeech as generateSpeech } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'SpeechModelV3\\',\\n      description: \\'The speech model to use.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      type: \\'string\\',\\n      description: \\'The text to generate the speech from.\\',\\n    },\\n    {\\n      name: \\'voice\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'The voice to use for the speech.\\',\\n    },\\n    {\\n      name: \\'outputFormat\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'The output format to use for the speech e.g. \"mp3\", \"wav\", etc.\\',\\n    },\\n    {\\n      name: \\'instructions\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'Instructions for the speech generation.\\',\\n    },\\n    {\\n      name: \\'speed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'The speed of the speech generation.\\',\\n    },\\n    {\\n      name: \\'language\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'The language for speech generation. This should be an ISO 639-1 language code (e.g. \"en\", \"es\", \"fr\") or \"auto\" for automatic language detection. Provider support varies.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string, JSONObject>\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific options.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'An optional abort signal to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional HTTP headers for the request.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'audio\\',\\n      type: \\'GeneratedAudioFile\\',\\n      description: \\'The generated audio.\\',\\n      properties: [\\n        {\\n          type: \\'GeneratedAudioFile\\',\\n          parameters: [\\n            {\\n              name: \\'base64\\',\\n              type: \\'string\\',\\n              description: \\'Audio as a base64 encoded string.\\',\\n            },\\n            {\\n              name: \\'uint8Array\\',\\n              type: \\'Uint8Array\\',\\n              description: \\'Audio as a Uint8Array.\\',\\n            },\\n            {\\n              name: \\'mimeType\\',\\n              type: \\'string\\',\\n              description: \\'MIME type of the audio (e.g. \"audio/mpeg\").\\',\\n            },\\n            {\\n              name: \\'format\\',\\n              type: \\'string\\',\\n              description: \\'Format of the audio (e.g. \"mp3\").\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[]\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'responses\\',\\n      type: \\'Array<SpeechModelResponseMetadata>\\',\\n      description:\\n        \\'Response metadata from the provider. There may be multiple responses if we made multiple calls to the model.\\',\\n      properties: [\\n        {\\n          type: \\'SpeechModelResponseMetadata\\',\\n          parameters: [\\n            {\\n              name: \\'timestamp\\',\\n              type: \\'Date\\',\\n              description: \\'Timestamp for the start of the generated response.\\',\\n            },\\n            {\\n              name: \\'modelId\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the response model that was used to generate the response.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'unknown\\',\\n              description: \\'Optional response body.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/15-agent.mdx'), name='15-agent.mdx', displayName='15-agent.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Agent (Interface)\\ndescription: API Reference for the Agent interface.\\n---\\n\\n# `Agent` (interface)\\n\\nThe `Agent` interface defines a contract for agents that can generate or stream AI-generated responses in response to prompts. Agents may encapsulate advanced logic such as tool usage, multi-step workflows, or prompt handling, enabling both simple and autonomous AI agents.\\n\\nImplementations of the `Agent` interface—such as `ToolLoopAgent`—fulfill the same contract and integrate seamlessly with all SDK APIs and utilities that expect an agent. This design allows users to supply custom agent classes or wrappers for third-party chains, while maximizing compatibility with AI SDK features.\\n\\n## Interface Definition\\n\\n```ts\\nimport { ModelMessage } from \\'@ai-sdk/provider-utils\\';\\nimport { ToolSet } from \\'../generate-text/tool-set\\';\\nimport { Output } from \\'../generate-text/output\\';\\nimport { GenerateTextResult } from \\'../generate-text/generate-text-result\\';\\nimport { StreamTextResult } from \\'../generate-text/stream-text-result\\';\\n\\nexport type AgentCallParameters<CALL_OPTIONS> = ([CALL_OPTIONS] extends [never]\\n  ? { options?: never }\\n  : { options: CALL_OPTIONS }) &\\n  (\\n    | {\\n        /**\\n         * A prompt. It can be either a text prompt or a list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        prompt: string | Array<ModelMessage>;\\n\\n        /**\\n         * A list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        messages?: never;\\n      }\\n    | {\\n        /**\\n         * A list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        messages: Array<ModelMessage>;\\n\\n        /**\\n         * A prompt. It can be either a text prompt or a list of messages.\\n         *\\n         * You can either use `prompt` or `messages` but not both.\\n         */\\n        prompt?: never;\\n      }\\n  ) & {\\n    /**\\n     * Abort signal.\\n     */\\n    abortSignal?: AbortSignal;\\n  };\\n\\n/**\\n * An Agent receives a prompt (text or messages) and generates or streams an output\\n * that consists of steps, tool calls, data parts, etc.\\n *\\n * You can implement your own Agent by implementing the `Agent` interface,\\n * or use the `ToolLoopAgent` class.\\n */\\nexport interface Agent<\\n  CALL_OPTIONS = never,\\n  TOOLS extends ToolSet = {},\\n  OUTPUT extends Output = never,\\n> {\\n  /**\\n   * The specification version of the agent interface. This will enable\\n   * us to evolve the agent interface and retain backwards compatibility.\\n   */\\n  readonly version: \\'agent-v1\\';\\n\\n  /**\\n   * The id of the agent.\\n   */\\n  readonly id: string | undefined;\\n\\n  /**\\n   * The tools that the agent can use.\\n   */\\n  readonly tools: TOOLS;\\n\\n  /**\\n   * Generates an output from the agent (non-streaming).\\n   */\\n  generate(\\n    options: AgentCallParameters<CALL_OPTIONS>,\\n  ): PromiseLike<GenerateTextResult<TOOLS, OUTPUT>>;\\n\\n  /**\\n   * Streams an output from the agent (streaming).\\n   */\\n  stream(\\n    options: AgentCallParameters<CALL_OPTIONS>,\\n  ): PromiseLike<StreamTextResult<TOOLS, OUTPUT>>;\\n}\\n```\\n\\n## Core Properties & Methods\\n\\n| Name         | Type                                             | Description                                                         |\\n| ------------ | ------------------------------------------------ | ------------------------------------------------------------------- |\\n| `version`    | `\\'agent-v1\\'`                                     | Interface version for compatibility.                                |\\n| `id`         | `string \\\\| undefined`                            | Optional agent identifier.                                          |\\n| `tools`      | `ToolSet`                                        | The set of tools available to this agent.                           |\\n| `generate()` | `PromiseLike<GenerateTextResult<TOOLS, OUTPUT>>` | Generates full, non-streaming output for a text prompt or messages. |\\n| `stream()`   | `PromiseLike<StreamTextResult<TOOLS, OUTPUT>>`   | Streams output (chunks or steps) for a text prompt or messages.     |\\n\\n## Generic Parameters\\n\\n| Parameter      | Default | Description                                                                |\\n| -------------- | ------- | -------------------------------------------------------------------------- |\\n| `CALL_OPTIONS` | `never` | Optional type for additional call options that can be passed to the agent. |\\n| `TOOLS`        | `{}`    | The type of the tool set available to this agent.                          |\\n| `OUTPUT`       | `never` | The type of additional output data that the agent can produce.             |\\n\\n## Method Parameters\\n\\nBoth `generate()` and `stream()` accept an `AgentCallParameters<CALL_OPTIONS>` object with:\\n\\n- `prompt` (optional): A string prompt or array of `ModelMessage` objects\\n- `messages` (optional): An array of `ModelMessage` objects (mutually exclusive with `prompt`)\\n- `options` (optional): Additional call options when `CALL_OPTIONS` is not `never`\\n- `abortSignal` (optional): An `AbortSignal` to cancel the operation\\n\\n## Example: Custom Agent Implementation\\n\\nHere\\'s how you might implement your own Agent:\\n\\n```ts\\nimport { Agent, GenerateTextResult, StreamTextResult } from \\'ai\\';\\nimport type { ModelMessage } from \\'@ai-sdk/provider-utils\\';\\n\\nclass MyEchoAgent implements Agent {\\n  version = \\'agent-v1\\' as const;\\n  id = \\'echo\\';\\n  tools = {};\\n\\n  async generate({ prompt, messages, abortSignal }) {\\n    const text = prompt ?? JSON.stringify(messages);\\n    return { text, steps: [] };\\n  }\\n\\n  async stream({ prompt, messages, abortSignal }) {\\n    const text = prompt ?? JSON.stringify(messages);\\n    return {\\n      textStream: (async function* () {\\n        yield text;\\n      })(),\\n    };\\n  }\\n}\\n```\\n\\n## Usage: Interacting with Agents\\n\\nAll SDK utilities that accept an agent—including [`createAgentUIStream`](/docs/reference/ai-sdk-core/create-agent-ui-stream), [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response), and [`pipeAgentUIStreamToResponse`](/docs/reference/ai-sdk-core/pipe-agent-ui-stream-to-response)—expect an object adhering to the `Agent` interface.\\n\\nYou can use the official [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent) (recommended for multi-step AI workflows with tool use), or supply your own implementation:\\n\\n```ts\\nimport { ToolLoopAgent, createAgentUIStream } from \"ai\";\\n\\nconst agent = new ToolLoopAgent({ ... });\\n\\nconst stream = await createAgentUIStream({\\n  agent,\\n  messages: [{ role: \"user\", content: \"What is the weather in NYC?\" }]\\n});\\n\\nfor await (const chunk of stream) {\\n  console.log(chunk);\\n}\\n```\\n\\n## See Also\\n\\n- [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent) &mdash; Official multi-step agent implementation\\n- [`createAgentUIStream`](/docs/reference/ai-sdk-core/create-agent-ui-stream)\\n- [`GenerateTextResult`](/docs/reference/ai-sdk-core/generate-text)\\n- [`StreamTextResult`](/docs/reference/ai-sdk-core/stream-text)\\n\\n## Notes\\n\\n- Agents should define their `tools` property, even if empty (`{}`), for compatibility with SDK utilities.\\n- The interface accepts both plain prompts and message arrays as input, but only one at a time.\\n- The `CALL_OPTIONS` generic parameter allows agents to accept additional call-specific options when needed.\\n- The `abortSignal` parameter enables cancellation of agent operations.\\n- This design is extensible for both complex autonomous agents and simple LLM wrappers.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/16-tool-loop-agent.mdx'), name='16-tool-loop-agent.mdx', displayName='16-tool-loop-agent.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: ToolLoopAgent\\ndescription: API Reference for the ToolLoopAgent class.\\n---\\n\\n# `ToolLoopAgent`\\n\\nCreates a reusable AI agent capable of generating text, streaming responses, and using tools over multiple steps (a reasoning-and-acting loop). `ToolLoopAgent` is ideal for building autonomous, multi-step agents that can take actions, call tools, and reason over the results until a stop condition is reached.\\n\\nUnlike single-step calls like `generateText()`, an agent can iteratively invoke tools, collect tool results, and decide next actions until completion or user approval is required.\\n\\n```ts\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: {\\n    weather: weatherTool,\\n    calculator: calculatorTool,\\n  },\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'What is the weather in NYC?\\',\\n});\\n\\nconsole.log(result.text);\\n```\\n\\nTo see `ToolLoopAgent` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { ToolLoopAgent } from \"ai\"`} prompt={false} />\\n\\n## Constructor\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      isRequired: true,\\n      description:\\n        \\'The language model instance to use (e.g., from a provider).\\',\\n    },\\n    {\\n      name: \\'instructions\\',\\n      type: \\'string | SystemModelMessage\\',\\n      isOptional: true,\\n      description:\\n        \\'Instructions for the agent, usually used for system prompt/context.\\',\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'Record<string, Tool>\\',\\n      isOptional: true,\\n      description:\\n        \\'A set of tools the agent can call. Keys are tool names. Tools require the underlying model to support tool calling.\\',\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      type: \\'ToolChoice\\',\\n      isOptional: true,\\n      description:\\n        \"Tool call selection strategy. Options: \\'auto\\' | \\'none\\' | \\'required\\' | { type: \\'tool\\', toolName: string }. Default: \\'auto\\'.\",\\n    },\\n    {\\n      name: \\'stopWhen\\',\\n      type: \\'StopCondition | StopCondition[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Condition(s) for ending the agent loop. Default: stepCountIs(20).\\',\\n    },\\n    {\\n      name: \\'activeTools\\',\\n      type: \\'Array<string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Limits the subset of tools that are available in a specific call.\\',\\n    },\\n    {\\n      name: \\'output\\',\\n      type: \\'Output\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional structured output specification, for parsing responses into typesafe data.\\',\\n    },\\n    {\\n      name: \\'prepareStep\\',\\n      type: \\'PrepareStepFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional function to mutate step settings or inject state for each agent step.\\',\\n    },\\n    {\\n      name: \\'experimental_repairToolCall\\',\\n      type: \\'ToolCallRepairFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback to attempt automatic recovery when a tool call cannot be parsed.\\',\\n    },\\n    {\\n      name: \\'onStepFinish\\',\\n      type: \\'GenerateTextOnStepFinishCallback\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback invoked after each agent step (LLM/tool call) completes.\\',\\n    },\\n    {\\n      name: \\'experimental_context\\',\\n      type: \\'unknown\\',\\n      isOptional: true,\\n      description:\\n        \\'Experimental: Custom context object passed to each tool call.\\',\\n    },\\n    {\\n      name: \\'experimental_telemetry\\',\\n      type: \\'TelemetrySettings\\',\\n      isOptional: true,\\n      description: \\'Experimental: Optional telemetry configuration.\\',\\n    },\\n    {\\n      name: \\'experimental_download\\',\\n      type: \\'DownloadFunction | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Experimental: Custom download function for fetching files/URLs for tool or model use. By default, files are downloaded if the model does not support the URL for a given media type.\\',\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens the model is allowed to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Sampling temperature, controls randomness. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Top-p (nucleus) sampling parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Top-k sampling parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Presence penalty parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Frequency penalty parameter. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom token sequences which stop the model output. Passed through to the model.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Seed for deterministic generation (if supported).\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'How many times to retry on failure. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description: \\'Optional abort signal to cancel the ongoing request.\\',\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'ProviderOptions\\',\\n      isOptional: true,\\n      description: \\'Additional provider-specific configuration.\\',\\n    },\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'Custom agent identifier.\\',\\n    },\\n  ]}\\n/>\\n\\n## Methods\\n\\n### `generate()`\\n\\nGenerates a response and triggers tool calls as needed, running the agent loop and returning the final result. Returns a promise resolving to a `GenerateTextResult`.\\n\\n```ts\\nconst result = await agent.generate({\\n  prompt: \\'What is the weather like?\\',\\n});\\n```\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<ModelMessage>\\',\\n      description: \\'A text prompt or message array.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<ModelMessage>\\',\\n      description: \\'A full conversation history as a list of model messages.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n  ]}\\n/>\\n\\n#### Returns\\n\\nThe `generate()` method returns a `GenerateTextResult` object (see [`generateText`](/docs/reference/ai-sdk-core/generate-text#returns) for details).\\n\\n### `stream()`\\n\\nStreams a response from the agent, including agent reasoning and tool calls, as they occur. Returns a `StreamTextResult`.\\n\\n```ts\\nconst stream = agent.stream({\\n  prompt: \\'Tell me a story about a robot.\\',\\n});\\n\\nfor await (const chunk of stream.textStream) {\\n  console.log(chunk);\\n}\\n```\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string | Array<ModelMessage>\\',\\n      description: \\'A text prompt or message array.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<ModelMessage>\\',\\n      description: \\'A full conversation history as a list of model messages.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'experimental_transform\\',\\n      type: \\'StreamTextTransform | Array<StreamTextTransform>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional stream transformation(s). They are applied in the order provided and must maintain the stream structure. See `streamText` docs for details.\\',\\n    },\\n  ]}\\n/>\\n\\n#### Returns\\n\\nThe `stream()` method returns a `StreamTextResult` object (see [`streamText`](/docs/reference/ai-sdk-core/stream-text#returns) for details).\\n\\n## Types\\n\\n### `InferAgentUIMessage`\\n\\nInfers the UI message type for the given agent instance. Useful for type-safe UI and message exchanges.\\n\\n#### Basic Example\\n\\n```ts\\nimport { ToolLoopAgent, InferAgentUIMessage } from \\'ai\\';\\n\\nconst weatherAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: { weather: weatherTool },\\n});\\n\\ntype WeatherAgentUIMessage = InferAgentUIMessage<typeof weatherAgent>;\\n```\\n\\n#### Example with Message Metadata\\n\\nYou can provide a second type argument to customize the metadata for each message. This is useful for tracking rich metadata returned by the agent (such as createdAt, tokens, finish reason, etc.).\\n\\n```ts\\nimport { ToolLoopAgent, InferAgentUIMessage } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Example schema for message metadata\\nconst exampleMetadataSchema = z.object({\\n  createdAt: z.number().optional(),\\n  model: z.string().optional(),\\n  totalTokens: z.number().optional(),\\n  finishReason: z.string().optional(),\\n});\\ntype ExampleMetadata = z.infer<typeof exampleMetadataSchema>;\\n\\n// Define agent as usual\\nconst metadataAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  // ...other options\\n});\\n\\n// Type-safe UI message type with custom metadata\\ntype MetadataAgentUIMessage = InferAgentUIMessage<\\n  typeof metadataAgent,\\n  ExampleMetadata\\n>;\\n```\\n\\n## Examples\\n\\n### Basic Agent with Tools\\n\\n```ts\\nimport { ToolLoopAgent, stepCountIs } from \\'ai\\';\\nimport { weatherTool, calculatorTool } from \\'./tools\\';\\n\\nconst assistant = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: {\\n    weather: weatherTool,\\n    calculator: calculatorTool,\\n  },\\n  stopWhen: stepCountIs(3),\\n});\\n\\nconst result = await assistant.generate({\\n  prompt: \\'What is the weather in NYC and what is 100 * 25?\\',\\n});\\n\\nconsole.log(result.text);\\nconsole.log(result.steps); // Array of all steps taken by the agent\\n```\\n\\n### Streaming Agent Response\\n\\n```ts\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a creative storyteller.\\',\\n});\\n\\nconst stream = agent.stream({\\n  prompt: \\'Tell me a short story about a time traveler.\\',\\n});\\n\\nfor await (const chunk of stream.textStream) {\\n  process.stdout.write(chunk);\\n}\\n```\\n\\n### Agent with Output Parsing\\n\\n```ts\\nimport { z } from \\'zod\\';\\n\\nconst analysisAgent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  output: {\\n    schema: z.object({\\n      sentiment: z.enum([\\'positive\\', \\'negative\\', \\'neutral\\']),\\n      score: z.number(),\\n      summary: z.string(),\\n    }),\\n  },\\n});\\n\\nconst result = await analysisAgent.generate({\\n  prompt: \\'Analyze this review: \"The product exceeded my expectations!\"\\',\\n});\\n\\nconsole.log(result.output);\\n// Typed as { sentiment: \\'positive\\' | \\'negative\\' | \\'neutral\\', score: number, summary: string }\\n```\\n\\n### Example: Approved Tool Execution\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { ToolLoopAgent } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are an agent with access to a weather API.\\',\\n  tools: {\\n    weather: openai.tools.weather({\\n      /* ... */\\n    }),\\n  },\\n  // Optionally require approval, etc.\\n});\\n\\nconst result = await agent.generate({\\n  prompt: \\'Is it raining in Paris today?\\',\\n});\\nconsole.log(result.text);\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/17-create-agent-ui-stream.mdx'), name='17-create-agent-ui-stream.mdx', displayName='17-create-agent-ui-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createAgentUIStream\\ndescription: API Reference for the createAgentUIStream utility.\\n---\\n\\n# `createAgentUIStream`\\n\\nThe `createAgentUIStream` function runs an [Agent](/docs/reference/ai-sdk-core/agent) and returns a streaming UI message stream as an async iterable. This allows you to consume an agent\\'s reasoning and UI messages incrementally in your own server, edge function, or background job—ideal for building custom streaming interfaces or piping the data to different outputs.\\n\\n## Import\\n\\n<Snippet text={`import { createAgentUIStream } from \"ai\"`} prompt={false} />\\n\\n## Usage\\n\\n```ts\\nimport { ToolLoopAgent, createAgentUIStream } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: { weather: weatherTool, calculator: calculatorTool },\\n});\\n\\nexport async function* streamAgent(\\n  messages: unknown[],\\n  abortSignal?: AbortSignal,\\n) {\\n  const stream = await createAgentUIStream({\\n    agent,\\n    messages,\\n    abortSignal,\\n    // ...other options\\n  });\\n\\n  for await (const chunk of stream) {\\n    yield chunk; // UI message chunk object (see UIMessageStream)\\n  }\\n}\\n```\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'agent\\',\\n      type: \\'Agent\\',\\n      isRequired: true,\\n      description:\\n        \\'The agent instance to run. Must define its tools and implement `.stream({ prompt })`.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'unknown[]\\',\\n      isRequired: true,\\n      description:\\n        \\'Array of input UI messages sent to the agent (e.g., from user/assistant).\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isRequired: false,\\n      description:\\n        \\'Optional abort signal to cancel the agent streaming, e.g., in response to client disconnection.\\',\\n    },\\n    {\\n      name: \\'...options\\',\\n      type: \\'UIMessageStreamOptions\\',\\n      isRequired: false,\\n      description:\\n        \\'Additional options for customizing UI message streaming, such as source inclusion or error formatting.\\',\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\nA `Promise<AsyncIterableStream<UIMessageChunk>>`, where each yielded value is a UI message chunk representing incremental agent UI output. This stream can be piped to HTTP responses, processed for dashboards, or logged.\\n\\n## Example\\n\\n```ts\\nimport { createAgentUIStream } from \\'ai\\';\\n\\nconst controller = new AbortController();\\n\\nconst stream = await createAgentUIStream({\\n  agent,\\n  messages: [{ role: \\'user\\', content: \\'What is the weather in SF today?\\' }],\\n  abortSignal: controller.signal,\\n  sendStart: true,\\n});\\n\\nfor await (const chunk of stream) {\\n  // Process each UI message chunk (e.g., send to client)\\n  console.log(chunk);\\n}\\n\\n// Call controller.abort() to stop streaming early if needed.\\n```\\n\\n## How It Works\\n\\n1. **Message Validation:** The incoming array of `messages` is validated and normalized according to the agent\\'s tools and requirements. Invalid messages will cause an error.\\n2. **Model Message Conversion:** The validated UI messages are converted into the model message format the agent expects.\\n3. **Agent Streaming:** The agent\\'s `.stream({ prompt, abortSignal })` method is invoked to produce a low-level result stream. If an `abortSignal` is provided and triggered, streaming will be canceled promptly.\\n4. **UI Message Stream:** That result stream is exposed as a streaming async iterable of UI message chunks.\\n\\n## Notes\\n\\n- The agent **must** define its `tools` and a `.stream({ prompt })` method.\\n- This utility returns an async iterator for full streaming flexibility. To produce an HTTP response, see [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response) or [`pipeAgentUIStreamToResponse`](/docs/reference/ai-sdk-core/pipe-agent-ui-stream-to-response).\\n- You can provide UI message stream options (see [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)) for fine-grained control over the output.\\n- To cancel a streaming agent operation, supply an [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) via the `abortSignal` option.\\n\\n## See Also\\n\\n- [`Agent`](/docs/reference/ai-sdk-core/agent)\\n- [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent)\\n- [`UIMessage`](/docs/reference/ai-sdk-core/ui-message)\\n- [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)\\n- [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response)\\n- [`pipeAgentUIStreamToResponse`](/docs/reference/ai-sdk-core/pipe-agent-ui-stream-to-response)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/18-create-agent-ui-stream-response.mdx'), name='18-create-agent-ui-stream-response.mdx', displayName='18-create-agent-ui-stream-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createAgentUIStreamResponse\\ndescription: API Reference for the createAgentUIStreamResponse utility.\\n---\\n\\n# `createAgentUIStreamResponse`\\n\\nThe `createAgentUIStreamResponse` function executes an [Agent](/docs/reference/ai-sdk-core/agent) and streams its output as a UI message stream in an HTTP [Response](https://developer.mozilla.org/en-US/docs/Web/API/Response) body. This is designed for building API endpoints that deliver real-time streaming results from an agent (for example, chat or tool-use applications).\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createAgentUIStreamResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Usage\\n\\n```ts\\nimport { ToolLoopAgent, createAgentUIStreamResponse } from \\'ai\\';\\n\\nconst agent = new ToolLoopAgent({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  instructions: \\'You are a helpful assistant.\\',\\n  tools: { weather: weatherTool, calculator: calculatorTool },\\n});\\n\\n// Typical usage with streaming options\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  // Optional: Use abortSignal for cancellation support (client-side disconnects)\\n  const abortController = new AbortController();\\n\\n  return createAgentUIStreamResponse({\\n    agent,\\n    messages,\\n    abortSignal: abortController.signal, // optional\\n    // ...other UIMessageStreamOptions like sendSources, includeUsage, experimental_transform, etc.\\n  });\\n}\\n```\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'agent\\',\\n      type: \\'Agent\\',\\n      isRequired: true,\\n      description:\\n        \\'The agent instance to use for streaming responses. Must implement `.stream({ prompt })` and define tools.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'unknown[]\\',\\n      isRequired: true,\\n      description:\\n        \\'Array of input UI messages sent to the agent (typically user and assistant message objects).\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isRequired: false,\\n      description:\\n        \\'Optional abort signal to cancel streaming, e.g., when client disconnects. Useful for long-running or cancelable requests.\\',\\n    },\\n    {\\n      name: \\'...options\\',\\n      type: \\'UIMessageStreamOptions\\',\\n      isRequired: false,\\n      description:\\n        \\'Additional UI message streaming options, such as `sendSources`, `includeUsage`, `experimental_transform`, etc. See [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options) for details.\\',\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\nA `Promise<Response>` whose body is a stream of UI messages from the agent—suitable as an HTTP response in server-side API routes (Next.js, Express, serverless, or edge handlers).\\n\\n## Example: Next.js API Route Handler\\n\\n```ts\\nimport { createAgentUIStreamResponse } from \\'ai\\';\\nimport { MyCustomAgent } from \\'@/agent/my-custom-agent\\';\\n\\nexport async function POST(request: Request) {\\n  const { messages } = await request.json();\\n\\n  return createAgentUIStreamResponse({\\n    agent: MyCustomAgent,\\n    messages,\\n    sendSources: true, // optionally include sources in the UI message stream\\n    includeUsage: true, // include token usage details if enabled by the agent\\n    // Optionally, provide abortSignal for cancellation and other stream options\\n  });\\n}\\n```\\n\\n## How It Works\\n\\nUnder the hood, this function uses the internal `createAgentUIStream` utility and wraps its result as an HTTP `Response` readable stream:\\n\\n1. **Message Validation:** The incoming array of `messages` is validated and normalized according to the agent\\'s tools and requirements. Messages not meeting spec will trigger an error.\\n2. **Conversion:** Validated messages are transformed to the internal model message format expected by the agent.\\n3. **Streaming:** The agent\\'s `.stream({ prompt })` method is called, producing a stream of UI message chunks representing the agent\\'s process and outputs.\\n4. **HTTP Response:** The stream of UI message chunks is returned as a streaming `Response` object suitable for consumption by server-side API clients (such as a chat UI or live tool interface).\\n\\n## Notes\\n\\n- The agent **must** define its `tools` and implement `.stream({ prompt })`.\\n- **Do not use in the browser**; call this from backend/API/server code only.\\n- You can provide additional UI message streaming options (see [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)) to customize the response, including experimental stream transforms.\\n- The returned `Response` leverages [Readable Streams](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream). Make sure your client or framework can consume streamed HTTP responses.\\n\\n## See Also\\n\\n- [`Agent`](/docs/reference/ai-sdk-core/agent)\\n- [`ToolLoopAgent`](/docs/reference/ai-sdk-core/tool-loop-agent)\\n- [`UIMessage`](/docs/reference/ai-sdk-core/ui-message)\\n- [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)\\n- [`createAgentUIStream`](/docs/reference/ai-sdk-core/create-agent-ui-stream)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/18-pipe-agent-ui-stream-to-response.mdx'), name='18-pipe-agent-ui-stream-to-response.mdx', displayName='18-pipe-agent-ui-stream-to-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: pipeAgentUIStreamToResponse\\ndescription: API Reference for the pipeAgentUIStreamToResponse utility.\\n---\\n\\n# `pipeAgentUIStreamToResponse`\\n\\nThe `pipeAgentUIStreamToResponse` function executes an [Agent](/docs/reference/ai-sdk-core/agent) and streams its output as a UI message stream directly to a Node.js [`ServerResponse`](https://nodejs.org/api/http.html#class-httpserverresponse) object. This is ideal for building API endpoints in Node.js servers (such as Express, Hono, or custom servers) that require low-latency, real-time UI message streaming from an Agent (e.g., for chat- or tool-use-based applications).\\n\\n## Import\\n\\n<Snippet\\n  text={`import { pipeAgentUIStreamToResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Usage\\n\\n```ts\\nimport { pipeAgentUIStreamToResponse } from \\'ai\\';\\nimport { MyCustomAgent } from \\'./agent\\';\\n\\nexport async function handler(req, res) {\\n  const { messages } = JSON.parse(req.body);\\n\\n  // Optional: Use abortSignal for request cancellation support\\n  const abortController = new AbortController();\\n\\n  await pipeAgentUIStreamToResponse({\\n    response: res, // Node.js ServerResponse\\n    agent: MyCustomAgent,\\n    messages,\\n    abortSignal: abortController.signal, // optional, see notes\\n    // ...other optional streaming options\\n  });\\n}\\n```\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'ServerResponse\\',\\n      isRequired: true,\\n      description:\\n        \\'The Node.js ServerResponse object to which the UI message stream will be piped.\\',\\n    },\\n    {\\n      name: \\'agent\\',\\n      type: \\'Agent\\',\\n      isRequired: true,\\n      description:\\n        \\'The agent instance to use for streaming responses. Must implement `.stream({ prompt })` and define tools.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'unknown[]\\',\\n      isRequired: true,\\n      description:\\n        \\'Array of input UI messages sent to the agent (typically user and assistant message objects).\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isRequired: false,\\n      description:\\n        \\'Optional abort signal to cancel streaming (e.g., when the client disconnects). Useful for enabling cancellation of long-running or streaming agent responses. Provide an instance of [`AbortSignal`](https://developer.mozilla.org/en-US/docs/Web/API/AbortSignal) (for example, from an `AbortController`).\\',\\n    },\\n    {\\n      name: \\'...options\\',\\n      type: \\'UIMessageStreamResponseInit & UIMessageStreamOptions\\',\\n      isRequired: false,\\n      description:\\n        \\'Options for response headers, status, and additional streaming configuration.\\',\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\nA `Promise<void>`. This function returns a promise that resolves when piping the UI message stream to the ServerResponse is complete.\\n\\n## Example: Hono/Express Route Handler\\n\\n```ts\\nimport { pipeAgentUIStreamToResponse } from \\'ai\\';\\nimport { openaiWebSearchAgent } from \\'./openai-web-search-agent\\';\\n\\napp.post(\\'/chat\\', async (req, res) => {\\n  const { messages } = await getJsonBody(req);\\n\\n  // Optionally use abortSignal for cancellation on disconnect, etc.\\n  const abortController = new AbortController();\\n  // e.g., tie abortController to request lifecycle/disconnect detection as needed\\n\\n  await pipeAgentUIStreamToResponse({\\n    response: res,\\n    agent: openaiWebSearchAgent,\\n    messages,\\n    abortSignal: abortController.signal, // optional\\n    // status: 200,\\n    // headers: { \\'X-Custom\\': \\'foo\\' },\\n    // ...additional streaming options\\n  });\\n});\\n```\\n\\n## How It Works\\n\\n1. **Streams Output:** The function creates a UI message stream from the agent and efficiently pipes it to the provided Node.js `ServerResponse`, setting appropriate HTTP headers (including content type and streaming-friendly headers) and status.\\n2. **Abort Support:** If you provide an `abortSignal`, you can cancel the streaming response (for example, when a client disconnects or a timeout occurs), improving resource usage and responsiveness.\\n3. **No Response Object:** Unlike serverless `Response`-returning APIs, this function does _not_ return a Response object. It writes streaming bytes directly to the Node.js response. This is more memory- and latency-efficient for Node.js server frameworks.\\n\\n## Notes\\n\\n- **abortSignal for Cancellation:** Use `abortSignal` to stop agent and stream processing early, improving robustness for long-running connections. In frameworks like Express or Hono, tie this to your server\\'s disconnect or timeout events when possible.\\n- **Only for Node.js:** This function is intended for use in Node.js environments with access to `ServerResponse` objects, not for Edge/serverless/server-side frameworks using web `Response` objects.\\n- **Streaming Support:** Ensure your client and reverse proxy/server infrastructure support streaming HTTP responses.\\n- Supports both Hono (`@hono/node-server`), Express, and similar Node.js frameworks.\\n\\n## See Also\\n\\n- [`createAgentUIStreamResponse`](/docs/reference/ai-sdk-core/create-agent-ui-stream-response)\\n- [`Agent`](/docs/reference/ai-sdk-core/agent)\\n- [`UIMessageStreamOptions`](/docs/reference/ai-sdk-core/ui-message-stream-options)\\n- [`UIMessage`](/docs/reference/ai-sdk-core/ui-message)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/20-tool.mdx'), name='20-tool.mdx', displayName='20-tool.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: tool\\ndescription: Helper function for tool type inference\\n---\\n\\n# `tool()`\\n\\nTool is a helper function that infers the tool input for its `execute` method.\\n\\nIt does not have any runtime behavior, but it helps TypeScript infer the types of the input for the `execute` method.\\n\\nWithout this helper function, TypeScript is unable to connect the `inputSchema` property to the `execute` method,\\nand the argument types of `execute` cannot be inferred.\\n\\n```ts highlight={\"1,4,9,10\"}\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const weatherTool = tool({\\n  description: \\'Get the weather in a location\\',\\n  inputSchema: z.object({\\n    location: z.string().describe(\\'The location to get the weather for\\'),\\n  }),\\n  // location below is inferred to be a string:\\n  execute: async ({ location }) => ({\\n    location,\\n    temperature: 72 + Math.floor(Math.random() * 21) - 10,\\n  }),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { tool } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tool\\',\\n      type: \\'Tool\\',\\n      description: \\'The tool definition.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the input that the tool expects. The language model will use this to generate the input. It is also used to validate the output of the language model. Use descriptions to make the input understandable for the language model. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n            },\\n            {\\n              name: \\'execute\\',\\n              isOptional: true,\\n              type: \\'async (input: INPUT, options: ToolCallOptions) => RESULT | Promise<RESULT> | AsyncIterable<RESULT>\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result or a results iterable. If an iterable is provided, all results but the last one are considered preliminary. If not provided, the tool will not be executed automatically.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolCallOptions\\',\\n                  parameters: [\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The ID of the tool call. You can use it e.g. when sending tool-call related information with stream data.\\',\\n                    },\\n                    {\\n                      name: \\'messages\\',\\n                      type: \\'ModelMessage[]\\',\\n                      description:\\n                        \\'Messages that were sent to the language model to initiate the response that contained the tool call. The messages do not include the system prompt nor the assistant response that contained the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'abortSignal\\',\\n                      type: \\'AbortSignal\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'An optional abort signal that indicates that the overall operation should be aborted.\\',\\n                    },\\n                    {\\n                      name: \\'experimental_context\\',\\n                      type: \\'unknown\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Context that is passed into tool execution. Experimental (can break in patch releases).\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'outputSchema\\',\\n              isOptional: true,\\n              type: \\'Zod Schema | JSON Schema\\',\\n              description:\\n                \\'The schema of the output that the tool produces. Used for validation and type inference.\\',\\n            },\\n            {\\n              name: \\'toModelOutput\\',\\n              isOptional: true,\\n              type: \"(output: RESULT) => LanguageModelV3ToolResultPart[\\'output\\']\",\\n              description:\\n                \\'Optional conversion function that maps the tool result to an output that can be used by the language model. If not provided, the tool result will be sent as a JSON object.\\',\\n            },\\n            {\\n              name: \\'onInputStart\\',\\n              isOptional: true,\\n              type: \\'(options: ToolCallOptions) => void | PromiseLike<void>\\',\\n              description:\\n                \\'Optional function that is called when the argument streaming starts. Only called when the tool is used in a streaming context.\\',\\n            },\\n            {\\n              name: \\'onInputDelta\\',\\n              isOptional: true,\\n              type: \\'(options: { inputTextDelta: string } & ToolCallOptions) => void | PromiseLike<void>\\',\\n              description:\\n                \\'Optional function that is called when an argument streaming delta is available. Only called when the tool is used in a streaming context.\\',\\n            },\\n            {\\n              name: \\'onInputAvailable\\',\\n              isOptional: true,\\n              type: \\'(options: { input: INPUT } & ToolCallOptions) => void | PromiseLike<void>\\',\\n              description:\\n                \\'Optional function that is called when a tool call can be started, even if the execute function is not provided.\\',\\n            },\\n            {\\n              name: \\'providerOptions\\',\\n              isOptional: true,\\n              type: \\'ProviderOptions\\',\\n              description:\\n                \\'Additional provider-specific metadata. They are passed through to the provider from the AI SDK and enable provider-specific functionality that can be fully encapsulated in the provider.\\',\\n            },\\n            {\\n              name: \\'type\\',\\n              isOptional: true,\\n              type: \"\\'function\\' | \\'provider-defined\\'\",\\n              description:\\n                \\'The type of the tool. Defaults to \"function\" for regular tools. Use \"provider-defined\" for provider-specific tools.\\',\\n            },\\n            {\\n              name: \\'id\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'The ID of the tool for provider-defined tools. Should follow the format `<provider-name>.<unique-tool-name>`. Required when type is \"provider-defined\".\\',\\n            },\\n            {\\n              name: \\'name\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool that the user must use in the tool set. Required when type is \"provider-defined\".\\',\\n            },\\n            {\\n              name: \\'args\\',\\n              isOptional: true,\\n              type: \\'Record<string, unknown>\\',\\n              description:\\n                \\'The arguments for configuring the tool. Must match the expected arguments defined by the provider for this tool. Required when type is \"provider-defined\".\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe tool that was passed in.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/22-dynamic-tool.mdx'), name='22-dynamic-tool.mdx', displayName='22-dynamic-tool.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: dynamicTool\\ndescription: Helper function for creating dynamic tools with unknown types\\n---\\n\\n# `dynamicTool()`\\n\\nThe `dynamicTool` function creates tools where the input and output types are not known at compile time. This is useful for scenarios such as:\\n\\n- MCP (Model Context Protocol) tools without schemas\\n- User-defined functions loaded at runtime\\n- Tools loaded from external sources or databases\\n- Dynamic tool generation based on user input\\n\\nUnlike the regular `tool` function, `dynamicTool` accepts and returns `unknown` types, allowing you to work with tools that have runtime-determined schemas.\\n\\n```ts highlight={\"1,4,9,10,11\"}\\nimport { dynamicTool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nexport const customTool = dynamicTool({\\n  description: \\'Execute a custom user-defined function\\',\\n  inputSchema: z.object({}),\\n  // input is typed as \\'unknown\\'\\n  execute: async input => {\\n    const { action, parameters } = input as any;\\n\\n    // Execute your dynamic logic\\n    return {\\n      result: `Executed ${action} with ${JSON.stringify(parameters)}`,\\n    };\\n  },\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { dynamicTool } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tool\\',\\n      type: \\'Object\\',\\n      description: \\'The dynamic tool definition.\\',\\n      properties: [\\n        {\\n          type: \\'Object\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\'\\n            },\\n            {\\n              name: \\'inputSchema\\',\\n              type: \\'FlexibleSchema<unknown>\\',\\n              description:\\n                \\'The schema of the input that the tool expects. While the type is unknown, a schema is still required for validation. You can use Zod schemas with z.unknown() or z.any() for fully dynamic inputs.\\'\\n            },\\n            {\\n              name: \\'execute\\',\\n              type: \\'ToolExecuteFunction<unknown, unknown>\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call. The input is typed as unknown and must be validated/cast at runtime.\\',\\n                properties: [\\n                  {\\n                    type: \"ToolCallOptions\",\\n                    parameters: [\\n                      {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The ID of the tool call.\\',\\n                    },\\n                    {\\n                        name: \"messages\",\\n                        type: \"ModelMessage[]\",\\n                        description: \"Messages that were sent to the language model.\"\\n                      },\\n                      {\\n                        name: \"abortSignal\",\\n                        type: \"AbortSignal\",\\n                        isOptional: true,\\n                        description: \"An optional abort signal.\"\\n                      }\\n                    ]\\n                  }\\n                ]\\n            },\\n            {\\n              name: \\'toModelOutput\\',\\n              isOptional: true,\\n              type: \\'(output: unknown) => LanguageModelV3ToolResultPart[\\\\\\'output\\\\\\']\\',\\n              description: \\'Optional conversion function that maps the tool result to an output that can be used by the language model.\\'\\n            },\\n            {\\n              name: \\'providerOptions\\',\\n              isOptional: true,\\n              type: \\'ProviderOptions\\',\\n              description: \\'Additional provider-specific metadata.\\'\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n\\n]}\\n/>\\n\\n### Returns\\n\\nA `Tool<unknown, unknown>` with `type: \\'dynamic\\'` that can be used with `generateText`, `streamText`, and other AI SDK functions.\\n\\n## Type-Safe Usage\\n\\nWhen using dynamic tools alongside static tools, you need to check the `dynamic` flag for proper type narrowing:\\n\\n```ts\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // Static tool with known types\\n    weather: weatherTool,\\n    // Dynamic tool with unknown types\\n    custom: dynamicTool({\\n      /* ... */\\n    }),\\n  },\\n  onStepFinish: ({ toolCalls, toolResults }) => {\\n    for (const toolCall of toolCalls) {\\n      if (toolCall.dynamic) {\\n        // Dynamic tool: input/output are \\'unknown\\'\\n        console.log(\\'Dynamic tool:\\', toolCall.toolName);\\n        console.log(\\'Input:\\', toolCall.input);\\n        continue;\\n      }\\n\\n      // Static tools have full type inference\\n      switch (toolCall.toolName) {\\n        case \\'weather\\':\\n          // TypeScript knows the exact types\\n          console.log(toolCall.input.location); // string\\n          break;\\n      }\\n    }\\n  },\\n});\\n```\\n\\n## Usage with `useChat`\\n\\nWhen used with useChat (`UIMessage` format), dynamic tools appear as `dynamic-tool` parts:\\n\\n```tsx\\n{\\n  message.parts.map(part => {\\n    switch (part.type) {\\n      case \\'dynamic-tool\\':\\n        return (\\n          <div>\\n            <h4>Tool: {part.toolName}</h4>\\n            <pre>{JSON.stringify(part.input, null, 2)}</pre>\\n          </div>\\n        );\\n      // ... handle other part types\\n    }\\n  });\\n}\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/23-create-mcp-client.mdx'), name='23-create-mcp-client.mdx', displayName='23-create-mcp-client.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: experimental_createMCPClient\\ndescription: Create a client for connecting to MCP servers\\n---\\n\\n# `experimental_createMCPClient()`\\n\\nCreates a lightweight Model Context Protocol (MCP) client that connects to an MCP server. The client provides:\\n\\n- **Tools**: Automatic conversion between MCP tools and AI SDK tools\\n- **Resources**: Methods to list, read, and discover resource templates from MCP servers\\n- **Prompts**: Methods to list available prompts and retrieve prompt messages\\n- **Elicitation**: Support for handling server requests for additional input during tool execution\\n\\nIt currently does not support accepting notifications from an MCP server, and custom configuration of the client.\\n\\nThis feature is experimental and may change or be removed in the future.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { experimental_createMCPClient } from \"@ai-sdk/mcp\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'config\\',\\n      type: \\'MCPClientConfig\\',\\n      description: \\'Configuration for the MCP client.\\',\\n      properties: [\\n        {\\n          type: \\'MCPClientConfig\\',\\n          parameters: [\\n            {\\n              name: \\'transport\\',\\n              type: \\'TransportConfig = MCPTransport | McpSSEServerConfig\\',\\n              description: \\'Configuration for the message transport layer.\\',\\n              properties: [\\n                {\\n                  type: \\'MCPTransport\\',\\n                  description:\\n                    \\'A client transport instance, used explicitly for stdio or custom transports\\',\\n                  parameters: [\\n                    {\\n                      name: \\'start\\',\\n                      type: \\'() => Promise<void>\\',\\n                      description: \\'A method that starts the transport\\',\\n                    },\\n                    {\\n                      name: \\'send\\',\\n                      type: \\'(message: JSONRPCMessage) => Promise<void>\\',\\n                      description:\\n                        \\'A method that sends a message through the transport\\',\\n                    },\\n                    {\\n                      name: \\'close\\',\\n                      type: \\'() => Promise<void>\\',\\n                      description: \\'A method that closes the transport\\',\\n                    },\\n                    {\\n                      name: \\'onclose\\',\\n                      type: \\'() => void\\',\\n                      description:\\n                        \\'A method that is called when the transport is closed\\',\\n                    },\\n                    {\\n                      name: \\'onerror\\',\\n                      type: \\'(error: Error) => void\\',\\n                      description:\\n                        \\'A method that is called when the transport encounters an error\\',\\n                    },\\n                    {\\n                      name: \\'onmessage\\',\\n                      type: \\'(message: JSONRPCMessage) => void\\',\\n                      description:\\n                        \\'A method that is called when the transport receives a message\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'MCPTransportConfig\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'sse\\' | \\'http\",\\n                      description: \\'Use Server-Sent Events for communication\\',\\n                    },\\n                    {\\n                      name: \\'url\\',\\n                      type: \\'string\\',\\n                      description: \\'URL of the MCP server\\',\\n                    },\\n                    {\\n                      name: \\'headers\\',\\n                      type: \\'Record<string, string>\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Additional HTTP headers to be sent with requests.\\',\\n                    },\\n                    {\\n                      name: \\'authProvider\\',\\n                      type: \\'OAuthClientProvider\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Optional OAuth provider for authorization to access protected remote MCP servers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'name\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'Client name. Defaults to \"ai-sdk-mcp-client\"\\',\\n            },\\n            {\\n              name: \\'onUncaughtError\\',\\n              type: \\'(error: unknown) => void\\',\\n              isOptional: true,\\n              description: \\'Handler for uncaught errors\\',\\n            },\\n            {\\n              name: \\'capabilities\\',\\n              type: \\'ClientCapabilities\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional client capabilities to advertise during initialization. For example, set { elicitation: {} } to enable handling elicitation requests from the server.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a Promise that resolves to an `MCPClient` with the following methods:\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tools\\',\\n      type: `async (options?: {\\n        schemas?: TOOL_SCHEMAS\\n      }) => Promise<McpToolSet<TOOL_SCHEMAS>>`,\\n      description: \\'Gets the tools available from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'schemas\\',\\n              type: \\'TOOL_SCHEMAS\\',\\n              isOptional: true,\\n              description:\\n                \\'Schema definitions for compile-time type checking. When not provided, schemas are inferred from the server.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'listResources\\',\\n      type: `async (options?: {\\n        params?: PaginatedRequest[\\'params\\'];\\n        options?: RequestOptions;\\n      }) => Promise<ListResourcesResult>`,\\n      description: \\'Lists all available resources from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'params\\',\\n              type: \"PaginatedRequest[\\'params\\']\",\\n              isOptional: true,\\n              description: \\'Optional pagination parameters including cursor.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'readResource\\',\\n      type: `async (args: {\\n        uri: string;\\n        options?: RequestOptions;\\n      }) => Promise<ReadResourceResult>`,\\n      description: \\'Reads the contents of a specific resource by URI.\\',\\n      properties: [\\n        {\\n          type: \\'args\\',\\n          parameters: [\\n            {\\n              name: \\'uri\\',\\n              type: \\'string\\',\\n              description: \\'The URI of the resource to read.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'listResourceTemplates\\',\\n      type: `async (options?: {\\n        options?: RequestOptions;\\n      }) => Promise<ListResourceTemplatesResult>`,\\n      description:\\n        \\'Lists all available resource templates from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'listPrompts\\',\\n      type: `async (options?: {\\n        params?: PaginatedRequest[\\'params\\'];\\n        options?: RequestOptions;\\n      }) => Promise<ListPromptsResult>`,\\n      description: \\'Lists available prompts from the MCP server.\\',\\n      properties: [\\n        {\\n          type: \\'options\\',\\n          parameters: [\\n            {\\n              name: \\'params\\',\\n              type: \"PaginatedRequest[\\'params\\']\",\\n              isOptional: true,\\n              description: \\'Optional pagination parameters including cursor.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'getPrompt\\',\\n      type: `async (args: {\\n        name: string;\\n        arguments?: Record<string, unknown>;\\n        options?: RequestOptions;\\n      }) => Promise<GetPromptResult>`,\\n      description: \\'Retrieves a prompt by name, optionally passing arguments.\\',\\n      properties: [\\n        {\\n          type: \\'args\\',\\n          parameters: [\\n            {\\n              name: \\'name\\',\\n              type: \\'string\\',\\n              description: \\'Prompt name to retrieve.\\',\\n            },\\n            {\\n              name: \\'arguments\\',\\n              type: \\'Record<string, unknown>\\',\\n              isOptional: true,\\n              description: \\'Optional arguments to fill into the prompt.\\',\\n            },\\n            {\\n              name: \\'options\\',\\n              type: \\'RequestOptions\\',\\n              isOptional: true,\\n              description:\\n                \\'Optional request options including signal and timeout.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onElicitationRequest\\',\\n      type: `(\\n        schema: typeof ElicitationRequestSchema,\\n        handler: (request: ElicitationRequest) => Promise<ElicitResult> | ElicitResult\\n      ) => void`,\\n      description:\\n        \\'Registers a handler for elicitation requests from the MCP server. The handler receives requests when the server needs additional input during tool execution.\\',\\n      properties: [\\n        {\\n          type: \\'parameters\\',\\n          parameters: [\\n            {\\n              name: \\'schema\\',\\n              type: \\'typeof ElicitationRequestSchema\\',\\n              description:\\n                \\'The schema to validate requests against. Must be ElicitationRequestSchema.\\',\\n            },\\n            {\\n              name: \\'handler\\',\\n              type: \\'(request: ElicitationRequest) => Promise<ElicitResult> | ElicitResult\\',\\n              description:\\n                \\'A function that handles the elicitation request. The request contains a message and requestedSchema. The handler must return an object with an action (\"accept\", \"decline\", or \"cancel\") and optionally content when accepting.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'close\\',\\n      type: \\'async () => void\\',\\n      description:\\n        \\'Closes the connection to the MCP server and cleans up resources.\\',\\n    },\\n  ]}\\n/>\\n\\n## Example\\n\\n```typescript\\nimport {\\n  experimental_createMCPClient as createMCPClient,\\n  generateText,\\n} from \\'@ai-sdk/mcp\\';\\nimport { Experimental_StdioMCPTransport } from \\'@ai-sdk/mcp/mcp-stdio\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\n\\nlet client;\\n\\ntry {\\n  client = await createMCPClient({\\n    transport: new Experimental_StdioMCPTransport({\\n      command: \\'node server.js\\',\\n    }),\\n  });\\n\\n  const tools = await client.tools();\\n\\n  const response = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    tools,\\n    messages: [{ role: \\'user\\', content: \\'Query the data\\' }],\\n  });\\n\\n  console.log(response);\\n} catch (error) {\\n  console.error(\\'Error:\\', error);\\n} finally {\\n  // ensure the client is closed even if an error occurs\\n  if (client) {\\n    await client.close();\\n  }\\n}\\n```\\n\\n## Error Handling\\n\\nThe client throws `MCPClientError` for:\\n\\n- Client initialization failures\\n- Protocol version mismatches\\n- Missing server capabilities\\n- Connection failures\\n\\nFor tool execution, errors are propagated as `CallToolError` errors.\\n\\nFor unknown errors, the client exposes an `onUncaughtError` callback that can be used to manually log or handle errors that are not covered by known error types.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/24-mcp-stdio-transport.mdx'), name='24-mcp-stdio-transport.mdx', displayName='24-mcp-stdio-transport.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Experimental_StdioMCPTransport\\ndescription: Create a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams\\n---\\n\\n# `Experimental_StdioMCPTransport`\\n\\nCreates a transport for Model Context Protocol (MCP) clients to communicate with MCP servers using standard input and output streams. This transport is only supported in Node.js environments.\\n\\nThis feature is experimental and may change or be removed in the future.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { Experimental_StdioMCPTransport } from \"ai/mcp-stdio\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'config\\',\\n      type: \\'StdioConfig\\',\\n      description: \\'Configuration for the MCP client.\\',\\n      properties: [\\n        {\\n          type: \\'StdioConfig\\',\\n          parameters: [\\n            {\\n              name: \\'command\\',\\n              type: \\'string\\',\\n              description: \\'The command to run the MCP server.\\',\\n            },\\n            {\\n              name: \\'args\\',\\n              type: \\'string[]\\',\\n              isOptional: true,\\n              description: \\'The arguments to pass to the MCP server.\\',\\n            },\\n            {\\n              name: \\'env\\',\\n              type: \\'Record<string, string>\\',\\n              isOptional: true,\\n              description:\\n                \\'The environment variables to set for the MCP server.\\',\\n            },\\n            {\\n              name: \\'stderr\\',\\n              type: \\'IOType | Stream | number\\',\\n              isOptional: true,\\n              description: \"The stream to write the MCP server\\'s stderr to.\",\\n            },\\n            {\\n              name: \\'cwd\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description: \\'The current working directory for the MCP server.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/25-json-schema.mdx'), name='25-json-schema.mdx', displayName='25-json-schema.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: jsonSchema\\ndescription: Helper function for creating JSON schemas\\n---\\n\\n# `jsonSchema()`\\n\\n`jsonSchema` is a helper function that creates a JSON schema object that is compatible with the AI SDK.\\nIt takes the JSON schema and an optional validation function as inputs, and can be typed.\\n\\nYou can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n`jsonSchema` is an alternative to using Zod schemas that provides you with flexibility in dynamic situations\\n(e.g. when using OpenAPI definitions) or for using other validation libraries.\\n\\n```ts\\nimport { jsonSchema } from \\'ai\\';\\n\\nconst mySchema = jsonSchema<{\\n  recipe: {\\n    name: string;\\n    ingredients: { name: string; amount: string }[];\\n    steps: string[];\\n  };\\n}>({\\n  type: \\'object\\',\\n  properties: {\\n    recipe: {\\n      type: \\'object\\',\\n      properties: {\\n        name: { type: \\'string\\' },\\n        ingredients: {\\n          type: \\'array\\',\\n          items: {\\n            type: \\'object\\',\\n            properties: {\\n              name: { type: \\'string\\' },\\n              amount: { type: \\'string\\' },\\n            },\\n            required: [\\'name\\', \\'amount\\'],\\n          },\\n        },\\n        steps: {\\n          type: \\'array\\',\\n          items: { type: \\'string\\' },\\n        },\\n      },\\n      required: [\\'name\\', \\'ingredients\\', \\'steps\\'],\\n    },\\n  },\\n  required: [\\'recipe\\'],\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { jsonSchema } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'schema\\',\\n      type: \\'JSONSchema7\\',\\n      description: \\'The JSON schema definition.\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'SchemaOptions\\',\\n      description: \\'Additional options for the JSON schema.\\',\\n      properties: [\\n        {\\n          type: \\'SchemaOptions\\',\\n          parameters: [\\n            {\\n              name: \\'validate\\',\\n              isOptional: true,\\n              type: \\'(value: unknown) => { success: true; value: OBJECT } | { success: false; error: Error };\\',\\n              description:\\n                \\'A function that validates the value against the JSON schema. If the value is valid, the function should return an object with a `success` property set to `true` and a `value` property set to the validated value. If the value is invalid, the function should return an object with a `success` property set to `false` and an `error` property set to the error.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA JSON schema object that is compatible with the AI SDK.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/26-zod-schema.mdx'), name='26-zod-schema.mdx', displayName='26-zod-schema.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: zodSchema\\ndescription: Helper function for creating Zod schemas\\n---\\n\\n# `zodSchema()`\\n\\n`zodSchema` is a helper function that converts a Zod schema into a JSON schema object that is compatible with the AI SDK.\\nIt takes a Zod schema and optional configuration as inputs, and returns a typed schema.\\n\\nYou can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n<Note>\\n  You can also pass Zod objects directly to the AI SDK functions. Internally,\\n  the AI SDK will convert the Zod schema to a JSON schema using `zodSchema()`.\\n  However, if you want to specify options such as `useReferences`, you can pass\\n  the `zodSchema()` helper function instead.\\n</Note>\\n\\n<Note type=\"warning\">\\n  When using `.meta()` or `.describe()` to add metadata to your Zod schemas,\\n  make sure these methods are called **at the end** of the schema chain.\\n  \\n  metadata is attached to a specific schema\\n  instance, and most schema methods (`.min()`, `.optional()`, `.extend()`, etc.)\\n  return a new schema instance that does not inherit metadata from the previous one.\\n  Due to Zod\\'s immutability, metadata is only included in the JSON schema output\\n  if `.meta()` or `.describe()` is the last method in the chain.\\n\\n```ts\\n// ❌ Metadata will be lost - .min() returns a new instance without metadata\\nz.string().meta({ describe: \\'first name\\' }).min(1);\\n\\n// ✅ Metadata is preserved - .meta() is the final method\\nz.string().min(1).meta({ describe: \\'first name\\' });\\n```\\n\\n</Note>\\n\\n## Example with recursive schemas\\n\\n```ts\\nimport { zodSchema } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\n// Define a base category schema\\nconst baseCategorySchema = z.object({\\n  name: z.string(),\\n});\\n\\n// Define the recursive Category type\\ntype Category = z.infer<typeof baseCategorySchema> & {\\n  subcategories: Category[];\\n};\\n\\n// Create the recursive schema using z.lazy\\nconst categorySchema: z.ZodType<Category> = baseCategorySchema.extend({\\n  subcategories: z.lazy(() => categorySchema.array()),\\n});\\n\\n// Create the final schema with useReferences enabled for recursive support\\nconst mySchema = zodSchema(\\n  z.object({\\n    category: categorySchema,\\n  }),\\n  { useReferences: true },\\n);\\n```\\n\\n## Import\\n\\n<Snippet text={`import { zodSchema } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'zodSchema\\',\\n      type: \\'z.Schema\\',\\n      description: \\'The Zod schema definition.\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'object\\',\\n      description: \\'Additional options for the schema conversion.\\',\\n      properties: [\\n        {\\n          type: \\'object\\',\\n          parameters: [\\n            {\\n              name: \\'useReferences\\',\\n              isOptional: true,\\n              type: \\'boolean\\',\\n              description:\\n                \\'Enables support for references in the schema. This is required for recursive schemas, e.g. with `z.lazy`. However, not all language models and providers support such references. Defaults to `false`.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/27-valibot-schema.mdx'), name='27-valibot-schema.mdx', displayName='27-valibot-schema.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: valibotSchema\\ndescription: Helper function for creating Valibot schemas\\n---\\n\\n# `valibotSchema()`\\n\\n<Note type=\"warning\">`valibotSchema` is currently experimental.</Note>\\n\\n`valibotSchema` is a helper function that converts a Valibot schema into a JSON schema object that is compatible with the AI SDK.\\nIt takes a Valibot schema as input, and returns a typed schema.\\n\\nYou can use it to [generate structured data](/docs/ai-sdk-core/generating-structured-data) and in [tools](/docs/ai-sdk-core/tools-and-tool-calling).\\n\\n## Example\\n\\n```ts\\nimport { valibotSchema } from \\'@ai-sdk/valibot\\';\\nimport { object, string, array } from \\'valibot\\';\\n\\nconst recipeSchema = valibotSchema(\\n  object({\\n    name: string(),\\n    ingredients: array(\\n      object({\\n        name: string(),\\n        amount: string(),\\n      }),\\n    ),\\n    steps: array(string()),\\n  }),\\n);\\n```\\n\\n## Import\\n\\n<Snippet text={`import { valibotSchema } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'valibotSchema\\',\\n      type: \\'GenericSchema<unknown, T>\\',\\n      description: \\'The Valibot schema definition.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA Schema object that is compatible with the AI SDK, containing both the JSON schema representation and validation functionality.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/30-model-message.mdx'), name='30-model-message.mdx', displayName='30-model-message.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: ModelMessage\\ndescription: Message types for AI SDK Core (API Reference)\\n---\\n\\n# `ModelMessage`\\n\\n`ModelMessage` represents the fundamental message structure used with AI SDK Core functions.\\nIt encompasses various message types that can be used in the `messages` field of any AI SDK Core functions.\\n\\nYou can access the Zod schema for `ModelMessage` with the `modelMessageSchema` export.\\n\\n## `ModelMessage` Types\\n\\n### `SystemModelMessage`\\n\\nA system message that can contain system information.\\n\\n```typescript\\ntype SystemModelMessage = {\\n  role: \\'system\\';\\n  content: string;\\n};\\n```\\n\\nYou can access the Zod schema for `SystemModelMessage` with the `systemModelMessageSchema` export.\\n\\n<Note>\\n  Using the \"system\" property instead of a system message is recommended to\\n  enhance resilience against prompt injection attacks.\\n</Note>\\n\\n### `UserModelMessage`\\n\\nA user message that can contain text or a combination of text, images, and files.\\n\\n```typescript\\ntype UserModelMessage = {\\n  role: \\'user\\';\\n  content: UserContent;\\n};\\n\\ntype UserContent = string | Array<TextPart | ImagePart | FilePart>;\\n```\\n\\nYou can access the Zod schema for `UserModelMessage` with the `userModelMessageSchema` export.\\n\\n### `AssistantModelMessage`\\n\\nAn assistant message that can contain text, tool calls, or a combination of both.\\n\\n```typescript\\ntype AssistantModelMessage = {\\n  role: \\'assistant\\';\\n  content: AssistantContent;\\n};\\n\\ntype AssistantContent = string | Array<TextPart | ToolCallPart>;\\n```\\n\\nYou can access the Zod schema for `AssistantModelMessage` with the `assistantModelMessageSchema` export.\\n\\n### `ToolModelMessage`\\n\\nA tool message that contains the result of one or more tool calls.\\n\\n```typescript\\ntype ToolModelMessage = {\\n  role: \\'tool\\';\\n  content: ToolContent;\\n};\\n\\ntype ToolContent = Array<ToolResultPart>;\\n```\\n\\nYou can access the Zod schema for `ToolModelMessage` with the `toolModelMessageSchema` export.\\n\\n## `ModelMessage` Parts\\n\\n### `TextPart`\\n\\nRepresents a text content part of a prompt. It contains a string of text.\\n\\n```typescript\\nexport interface TextPart {\\n  type: \\'text\\';\\n  /**\\n   * The text content.\\n   */\\n  text: string;\\n}\\n```\\n\\n### `ImagePart`\\n\\nRepresents an image part in a user message.\\n\\n```typescript\\nexport interface ImagePart {\\n  type: \\'image\\';\\n\\n  /**\\n   * Image data. Can either be:\\n   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\\n   * - URL: a URL that points to the image\\n   */\\n  image: DataContent | URL;\\n\\n  /**\\n   * Optional IANA media type of the image.\\n   * We recommend leaving this out as it will be detected automatically.\\n   */\\n  mediaType?: string;\\n}\\n```\\n\\n### `FilePart`\\n\\nRepresents an file part in a user message.\\n\\n```typescript\\nexport interface FilePart {\\n  type: \\'file\\';\\n\\n  /**\\n   * File data. Can either be:\\n   * - data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\\n   * - URL: a URL that points to the file\\n   */\\n  data: DataContent | URL;\\n\\n  /**\\n   * Optional filename of the file.\\n   */\\n  filename?: string;\\n\\n  /**\\n   * IANA media type of the file.\\n   */\\n  mediaType: string;\\n}\\n```\\n\\n### `ToolCallPart`\\n\\nRepresents a tool call content part of a prompt, typically generated by the AI model.\\n\\n```typescript\\nexport interface ToolCallPart {\\n  type: \\'tool-call\\';\\n\\n  /**\\n   * ID of the tool call. This ID is used to match the tool call with the tool result.\\n   */\\n  toolCallId: string;\\n\\n  /**\\n   * Name of the tool that is being called.\\n   */\\n  toolName: string;\\n\\n  /**\\n   * Arguments of the tool call. This is a JSON-serializable object that matches the tool\\'s input schema.\\n   */\\n  args: unknown;\\n}\\n```\\n\\n### `ToolResultPart`\\n\\nRepresents the result of a tool call in a tool message.\\n\\n```typescript\\nexport interface ToolResultPart {\\n  type: \\'tool-result\\';\\n\\n  /**\\n   * ID of the tool call that this result is associated with.\\n   */\\n  toolCallId: string;\\n\\n  /**\\n   * Name of the tool that generated this result.\\n   */\\n  toolName: string;\\n\\n  /**\\n   * Result of the tool call. This is a JSON-serializable object.\\n   */\\n  output: LanguageModelV3ToolResultOutput;\\n\\n  /**\\n  Additional provider-specific metadata. They are passed through\\n  to the provider from the AI SDK and enable provider-specific\\n  functionality that can be fully encapsulated in the provider.\\n  */\\n  providerOptions?: ProviderOptions;\\n}\\n```\\n\\n### `LanguageModelV3ToolResultOutput`\\n\\n```ts\\n/**\\n * Output of a tool result.\\n */\\nexport type ToolResultOutput =\\n  | {\\n      /**\\n       * Text tool output that should be directly sent to the API.\\n       */\\n      type: \\'text\\';\\n      value: string;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'json\\';\\n      value: JSONValue;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      /**\\n       * Type when the user has denied the execution of the tool call.\\n       */\\n      type: \\'execution-denied\\';\\n\\n      /**\\n       * Optional reason for the execution denial.\\n       */\\n      reason?: string;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'error-text\\';\\n      value: string;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'error-json\\';\\n      value: JSONValue;\\n\\n      /**\\n       * Provider-specific options.\\n       */\\n      providerOptions?: ProviderOptions;\\n    }\\n  | {\\n      type: \\'content\\';\\n      value: Array<\\n        | {\\n            type: \\'text\\';\\n\\n            /**\\nText content.\\n*/\\n            text: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * @deprecated Use image-data or file-data instead.\\n             */\\n            type: \\'media\\';\\n            data: string;\\n            mediaType: string;\\n          }\\n        | {\\n            type: \\'file-data\\';\\n\\n            /**\\nBase-64 encoded media data.\\n*/\\n            data: string;\\n\\n            /**\\nIANA media type.\\n@see https://www.iana.org/assignments/media-types/media-types.xhtml\\n*/\\n            mediaType: string;\\n\\n            /**\\n             * Optional filename of the file.\\n             */\\n            filename?: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            type: \\'file-url\\';\\n\\n            /**\\n             * URL of the file.\\n             */\\n            url: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            type: \\'file-id\\';\\n\\n            /**\\n             * ID of the file.\\n             *\\n             * If you use multiple providers, you need to\\n             * specify the provider specific ids using\\n             * the Record option. The key is the provider\\n             * name, e.g. \\'openai\\' or \\'anthropic\\'.\\n             */\\n            fileId: string | Record<string, string>;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Images that are referenced using base64 encoded data.\\n             */\\n            type: \\'image-data\\';\\n\\n            /**\\nBase-64 encoded image data.\\n*/\\n            data: string;\\n\\n            /**\\nIANA media type.\\n@see https://www.iana.org/assignments/media-types/media-types.xhtml\\n*/\\n            mediaType: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Images that are referenced using a URL.\\n             */\\n            type: \\'image-url\\';\\n\\n            /**\\n             * URL of the image.\\n             */\\n            url: string;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Images that are referenced using a provider file id.\\n             */\\n            type: \\'image-file-id\\';\\n\\n            /**\\n             * Image that is referenced using a provider file id.\\n             *\\n             * If you use multiple providers, you need to\\n             * specify the provider specific ids using\\n             * the Record option. The key is the provider\\n             * name, e.g. \\'openai\\' or \\'anthropic\\'.\\n             */\\n            fileId: string | Record<string, string>;\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n        | {\\n            /**\\n             * Custom content part. This can be used to implement\\n             * provider-specific content parts.\\n             */\\n            type: \\'custom\\';\\n\\n            /**\\n             * Provider-specific options.\\n             */\\n            providerOptions?: ProviderOptions;\\n          }\\n      >;\\n    };\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/31-ui-message.mdx'), name='31-ui-message.mdx', displayName='31-ui-message.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: UIMessage\\ndescription: API Reference for UIMessage\\n---\\n\\n# `UIMessage`\\n\\n`UIMessage` serves as the source of truth for your application's state, representing the complete message history including metadata, data parts, and all contextual information. In contrast to `ModelMessage`, which represents the state or context passed to the model, `UIMessage` contains the full application state needed for UI rendering and client-side functionality.\\n\\n## Type Safety\\n\\n`UIMessage` is designed to be type-safe and accepts three generic parameters to ensure proper typing throughout your application:\\n\\n1. **`METADATA`** - Custom metadata type for additional message information\\n2. **`DATA_PARTS`** - Custom data part types for structured data components\\n3. **`TOOLS`** - Tool definitions for type-safe tool interactions\\n\\n## Creating Your Own UIMessage Type\\n\\nHere's an example of how to create a custom typed UIMessage for your application:\\n\\n```typescript\\nimport { InferUITools, ToolSet, UIMessage, tool } from 'ai';\\nimport z from 'zod';\\n\\nconst metadataSchema = z.object({\\n  someMetadata: z.string().datetime(),\\n});\\n\\ntype MyMetadata = z.infer<typeof metadataSchema>;\\n\\nconst dataPartSchema = z.object({\\n  someDataPart: z.object({}),\\n  anotherDataPart: z.object({}),\\n});\\n\\ntype MyDataPart = z.infer<typeof dataPartSchema>;\\n\\nconst tools = {\\n  someTool: tool({}),\\n} satisfies ToolSet;\\n\\ntype MyTools = InferUITools<typeof tools>;\\n\\nexport type MyUIMessage = UIMessage<MyMetadata, MyDataPart, MyTools>;\\n```\\n\\n## `UIMessage` Interface\\n\\n```typescript\\ninterface UIMessage<\\n  METADATA = unknown,\\n  DATA_PARTS extends UIDataTypes = UIDataTypes,\\n  TOOLS extends UITools = UITools,\\n> {\\n  /**\\n   * A unique identifier for the message.\\n   */\\n  id: string;\\n\\n  /**\\n   * The role of the message.\\n   */\\n  role: 'system' | 'user' | 'assistant';\\n\\n  /**\\n   * The metadata of the message.\\n   */\\n  metadata?: METADATA;\\n\\n  /**\\n   * The parts of the message. Use this for rendering the message in the UI.\\n   */\\n  parts: Array<UIMessagePart<DATA_PARTS, TOOLS>>;\\n}\\n```\\n\\n## `UIMessagePart` Types\\n\\n### `TextUIPart`\\n\\nA text part of a message.\\n\\n```typescript\\ntype TextUIPart = {\\n  type: 'text';\\n  /**\\n   * The text content.\\n   */\\n  text: string;\\n  /**\\n   * The state of the text part.\\n   */\\n  state?: 'streaming' | 'done';\\n};\\n```\\n\\n### `ReasoningUIPart`\\n\\nA reasoning part of a message.\\n\\n```typescript\\ntype ReasoningUIPart = {\\n  type: 'reasoning';\\n  /**\\n   * The reasoning text.\\n   */\\n  text: string;\\n  /**\\n   * The state of the reasoning part.\\n   */\\n  state?: 'streaming' | 'done';\\n  /**\\n   * The provider metadata.\\n   */\\n  providerMetadata?: Record<string, any>;\\n};\\n```\\n\\n### `ToolUIPart`\\n\\nA tool part of a message that represents tool invocations and their results.\\n\\n<Note>\\n  The type is based on the name of the tool (e.g., `tool-someTool` for a tool\\n  named `someTool`).\\n</Note>\\n\\n```typescript\\ntype ToolUIPart<TOOLS extends UITools = UITools> = ValueOf<{\\n  [NAME in keyof TOOLS & string]: {\\n    type: `tool-${NAME}`;\\n    toolCallId: string;\\n  } & (\\n    | {\\n        state: 'input-streaming';\\n        input: DeepPartial<TOOLS[NAME]['input']> | undefined;\\n        providerExecuted?: boolean;\\n        output?: never;\\n        errorText?: never;\\n      }\\n    | {\\n        state: 'input-available';\\n        input: TOOLS[NAME]['input'];\\n        providerExecuted?: boolean;\\n        output?: never;\\n        errorText?: never;\\n      }\\n    | {\\n        state: 'output-available';\\n        input: TOOLS[NAME]['input'];\\n        output: TOOLS[NAME]['output'];\\n        errorText?: never;\\n        providerExecuted?: boolean;\\n      }\\n    | {\\n        state: 'output-error';\\n        input: TOOLS[NAME]['input'];\\n        output?: never;\\n        errorText: string;\\n        providerExecuted?: boolean;\\n      }\\n  );\\n}>;\\n```\\n\\n### `SourceUrlUIPart`\\n\\nA source URL part of a message.\\n\\n```typescript\\ntype SourceUrlUIPart = {\\n  type: 'source-url';\\n  sourceId: string;\\n  url: string;\\n  title?: string;\\n  providerMetadata?: Record<string, any>;\\n};\\n```\\n\\n### `SourceDocumentUIPart`\\n\\nA document source part of a message.\\n\\n```typescript\\ntype SourceDocumentUIPart = {\\n  type: 'source-document';\\n  sourceId: string;\\n  mediaType: string;\\n  title: string;\\n  filename?: string;\\n  providerMetadata?: Record<string, any>;\\n};\\n```\\n\\n### `FileUIPart`\\n\\nA file part of a message.\\n\\n```typescript\\ntype FileUIPart = {\\n  type: 'file';\\n  /**\\n   * IANA media type of the file.\\n   */\\n  mediaType: string;\\n  /**\\n   * Optional filename of the file.\\n   */\\n  filename?: string;\\n  /**\\n   * The URL of the file.\\n   * It can either be a URL to a hosted file or a Data URL.\\n   */\\n  url: string;\\n};\\n```\\n\\n### `DataUIPart`\\n\\nA data part of a message for custom data types.\\n\\n<Note>\\n  The type is based on the name of the data part (e.g., `data-someDataPart` for\\n  a data part named `someDataPart`).\\n</Note>\\n\\n```typescript\\ntype DataUIPart<DATA_TYPES extends UIDataTypes> = ValueOf<{\\n  [NAME in keyof DATA_TYPES & string]: {\\n    type: `data-${NAME}`;\\n    id?: string;\\n    data: DATA_TYPES[NAME];\\n  };\\n}>;\\n```\\n\\n### `StepStartUIPart`\\n\\nA step boundary part of a message.\\n\\n```typescript\\ntype StepStartUIPart = {\\n  type: 'step-start';\\n};\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/32-validate-ui-messages.mdx'), name='32-validate-ui-messages.mdx', displayName='32-validate-ui-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: validateUIMessages\\ndescription: API Reference for validateUIMessages\\n---\\n\\n# `validateUIMessages`\\n\\n`validateUIMessages` is an async function that validates UI messages against schemas for metadata, data parts, and tools. It ensures type safety and data integrity for your message arrays before processing or rendering.\\n\\n## Basic Usage\\n\\nSimple validation without custom schemas:\\n\\n```typescript\\nimport { validateUIMessages } from 'ai';\\n\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    parts: [{ type: 'text', text: 'Hello!' }],\\n  },\\n];\\n\\nconst validatedMessages = await validateUIMessages({\\n  messages,\\n});\\n```\\n\\n## Advanced Usage\\n\\nComprehensive validation with custom metadata, data parts, and tools:\\n\\n```typescript\\nimport { validateUIMessages, tool } from 'ai';\\nimport { z } from 'zod';\\n\\n// Define schemas\\nconst metadataSchema = z.object({\\n  timestamp: z.string().datetime(),\\n  userId: z.string(),\\n});\\n\\nconst dataSchemas = {\\n  chart: z.object({\\n    data: z.array(z.number()),\\n    labels: z.array(z.string()),\\n  }),\\n  image: z.object({\\n    url: z.string().url(),\\n    caption: z.string(),\\n  }),\\n};\\n\\nconst tools = {\\n  weather: tool({\\n    description: 'Get weather info',\\n    parameters: z.object({\\n      location: z.string(),\\n    }),\\n    execute: async ({ location }) => `Weather in ${location}: sunny`,\\n  }),\\n};\\n\\n// Messages with custom parts\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    metadata: { timestamp: '2024-01-01T00:00:00Z', userId: 'user123' },\\n    parts: [\\n      { type: 'text', text: 'Show me a chart' },\\n      {\\n        type: 'data-chart',\\n        data: { data: [1, 2, 3], labels: ['A', 'B', 'C'] },\\n      },\\n    ],\\n  },\\n  {\\n    id: '2',\\n    role: 'assistant',\\n    parts: [\\n      {\\n        type: 'tool-weather',\\n        toolCallId: 'call_123',\\n        state: 'output-available',\\n        input: { location: 'San Francisco' },\\n        output: 'Weather in San Francisco: sunny',\\n      },\\n    ],\\n  },\\n];\\n\\n// Validate with all schemas\\nconst validatedMessages = await validateUIMessages({\\n  messages,\\n  metadataSchema,\\n  dataSchemas,\\n  tools,\\n});\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/33-safe-validate-ui-messages.mdx'), name='33-safe-validate-ui-messages.mdx', displayName='33-safe-validate-ui-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: safeValidateUIMessages\\ndescription: API Reference for safeValidateUIMessages\\n---\\n\\n# `safeValidateUIMessages`\\n\\n`safeValidateUIMessages` is an async function that validates UI messages like [`validateUIMessages`](https://ai-sdk.dev/docs/reference/ai-sdk-core/validate-ui-messages), but instead of throwing it returns an object with a `success` key and either `data` or `error`.\\n\\n## Basic Usage\\n\\nSimple validation without custom schemas:\\n\\n```typescript\\nimport { safeValidateUIMessages } from 'ai';\\n\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    parts: [{ type: 'text', text: 'Hello!' }],\\n  },\\n];\\n\\nconst result = await safeValidateUIMessages({\\n  messages,\\n});\\n\\nif (!result.success) {\\n  console.error(result.error.message);\\n} else {\\n  const validatedMessages = result.data;\\n}\\n```\\n\\n## Advanced Usage\\n\\nComprehensive validation with custom metadata, data parts, and tools:\\n\\n```typescript\\nimport { safeValidateUIMessages, tool } from 'ai';\\nimport { z } from 'zod';\\n\\n// Define schemas\\nconst metadataSchema = z.object({\\n  timestamp: z.string().datetime(),\\n  userId: z.string(),\\n});\\n\\nconst dataSchemas = {\\n  chart: z.object({\\n    data: z.array(z.number()),\\n    labels: z.array(z.string()),\\n  }),\\n  image: z.object({\\n    url: z.string().url(),\\n    caption: z.string(),\\n  }),\\n};\\n\\nconst tools = {\\n  weather: tool({\\n    description: 'Get weather info',\\n    parameters: z.object({\\n      location: z.string(),\\n    }),\\n    execute: async ({ location }) => `Weather in ${location}: sunny`,\\n  }),\\n};\\n\\n// Messages with custom parts\\nconst messages = [\\n  {\\n    id: '1',\\n    role: 'user',\\n    metadata: { timestamp: '2024-01-01T00:00:00Z', userId: 'user123' },\\n    parts: [\\n      { type: 'text', text: 'Show me a chart' },\\n      {\\n        type: 'data-chart',\\n        data: { data: [1, 2, 3], labels: ['A', 'B', 'C'] },\\n      },\\n    ],\\n  },\\n  {\\n    id: '2',\\n    role: 'assistant',\\n    parts: [\\n      {\\n        type: 'tool-weather',\\n        toolCallId: 'call_123',\\n        state: 'output-available',\\n        input: { location: 'San Francisco' },\\n        output: 'Weather in San Francisco: sunny',\\n      },\\n    ],\\n  },\\n];\\n\\n// Validate with all schemas\\nconst result = await safeValidateUIMessages({\\n  messages,\\n  metadataSchema,\\n  dataSchemas,\\n  tools,\\n});\\n\\nif (!result.success) {\\n  console.error(result.error.message);\\n} else {\\n  const validatedMessages = result.data;\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/40-provider-registry.mdx'), name='40-provider-registry.mdx', displayName='40-provider-registry.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createProviderRegistry\\ndescription: Registry for managing multiple providers and models (API Reference)\\n---\\n\\n# `createProviderRegistry()`\\n\\nWhen you work with multiple providers and models, it is often desirable to manage them\\nin a central place and access the models through simple string ids.\\n\\n`createProviderRegistry` lets you create a registry with multiple providers that you\\ncan access by their ids in the format `providerId:modelId`.\\n\\n### Setup\\n\\nYou can create a registry with multiple providers and models using `createProviderRegistry`.\\n\\n```ts\\nimport { anthropic } from \\'@ai-sdk/anthropic\\';\\nimport { createOpenAI } from \\'@ai-sdk/openai\\';\\nimport { createProviderRegistry } from \\'ai\\';\\n\\nexport const registry = createProviderRegistry({\\n  // register provider with prefix and default setup:\\n  anthropic,\\n\\n  // register provider with prefix and custom setup:\\n  openai: createOpenAI({\\n    apiKey: process.env.OPENAI_API_KEY,\\n  }),\\n});\\n```\\n\\n### Custom Separator\\n\\nBy default, the registry uses `:` as the separator between provider and model IDs. You can customize this separator by passing a `separator` option:\\n\\n```ts\\nconst registry = createProviderRegistry(\\n  {\\n    anthropic,\\n    openai,\\n  },\\n  { separator: \\' > \\' },\\n);\\n\\n// Now you can use the custom separator\\nconst model = registry.languageModel(\\'anthropic > claude-3-opus-20240229\\');\\n```\\n\\n### Language models\\n\\nYou can access language models by using the `languageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateText } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { text } = await generateText({\\n  model: registry.languageModel(\\'openai:gpt-4.1\\'),\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n});\\n```\\n\\n### Text embedding models\\n\\nYou can access text embedding models by using the `.embeddingModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { embed } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { embedding } = await embed({\\n  model: registry.embeddingModel(\\'openai:text-embedding-3-small\\'),\\n  value: \\'sunny day at the beach\\',\\n});\\n```\\n\\n### Image models\\n\\nYou can access image models by using the `imageModel` method on the registry.\\nThe provider id will become the prefix of the model id: `providerId:modelId`.\\n\\n```ts highlight={\"5\"}\\nimport { generateImage } from \\'ai\\';\\nimport { registry } from \\'./registry\\';\\n\\nconst { image } = await generateImage({\\n  model: registry.imageModel(\\'openai:dall-e-3\\'),\\n  prompt: \\'A beautiful sunset over a calm ocean\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { createProviderRegistry } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'providers\\',\\n      type: \\'Record<string, Provider>\\',\\n      description:\\n        \\'The unique identifier for the provider. It should be unique within the registry.\\',\\n      properties: [\\n        {\\n          type: \\'Provider\\',\\n          parameters: [\\n            {\\n              name: \\'languageModel\\',\\n              type: \\'(id: string) => LanguageModel\\',\\n              description:\\n                \\'A function that returns a language model by its id.\\',\\n            },\\n            {\\n              name: \\'embeddingModel\\',,\\n              type: \\'(id: string) => EmbeddingModel<string>\\',\\n              description:\\n                \\'A function that returns a text embedding model by its id.\\',\\n            },\\n            {\\n              name: \\'imageModel\\',\\n              type: \\'(id: string) => ImageModel\\',\\n              description: \\'A function that returns an image model by its id.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'object\\',\\n      description: \\'Optional configuration for the registry.\\',\\n      properties: [\\n        {\\n          type: \\'Options\\',\\n          parameters: [\\n            {\\n              name: \\'separator\\',\\n              type: \\'string\\',\\n              description:\\n                \\'Custom separator between provider and model IDs. Defaults to \":\".\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe `createProviderRegistry` function returns a `Provider` instance. It has the following methods:\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'languageModel\\',\\n      type: \\'(id: string) => LanguageModel\\',\\n      description:\\n        \\'A function that returns a language model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'embeddingModel\\',,\\n      type: \\'(id: string) => EmbeddingModel<string>\\',\\n      description:\\n        \\'A function that returns a text embedding model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'imageModel\\',\\n      type: \\'(id: string) => ImageModel\\',\\n      description:\\n        \\'A function that returns an image model by its id (format: providerId:modelId)\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/42-custom-provider.mdx'), name='42-custom-provider.mdx', displayName='42-custom-provider.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: customProvider\\ndescription: Custom provider that uses models from a different provider (API Reference)\\n---\\n\\n# `customProvider()`\\n\\nWith a custom provider, you can map ids to any model.\\nThis allows you to set up custom model configurations, alias names, and more.\\nThe custom provider also supports a fallback provider, which is useful for\\nwrapping existing providers and adding additional functionality.\\n\\n### Example: custom model settings\\n\\nYou can create a custom provider using `customProvider`.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { customProvider } from \\'ai\\';\\n\\n// custom provider with different model settings:\\nexport const myOpenAI = customProvider({\\n  languageModels: {\\n    // replacement model with custom settings:\\n    \\'gpt-4\\': wrapLanguageModel({\\n      model: openai(\\'gpt-4\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n    // alias model with custom settings:\\n    \\'gpt-4o-reasoning-high\\': wrapLanguageModel({\\n      model: openai(\\'gpt-4o\\'),\\n      middleware: defaultSettingsMiddleware({\\n        settings: {\\n          providerOptions: {\\n            openai: {\\n              reasoningEffort: \\'high\\',\\n            },\\n          },\\n        },\\n      }),\\n    }),\\n  },\\n  fallbackProvider: openai,\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import {  customProvider } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'languageModels\\',\\n      type: \\'Record<string, LanguageModel>\\',\\n      isOptional: true,\\n      description:\\n        \\'A record of language models, where keys are model IDs and values are LanguageModel instances.\\',\\n    },\\n    {\\n      name: \\'.embeddingModels\\',\\n      type: \\'Record<string, EmbeddingModel<string>>\\',\\n      isOptional: true,\\n      description:\\n        \\'A record of text embedding models, where keys are model IDs and values are EmbeddingModel<string> instances.\\',\\n    },\\n    {\\n      name: \\'imageModels\\',\\n      type: \\'Record<string, ImageModel>\\',\\n      isOptional: true,\\n      description:\\n        \\'A record of image models, where keys are model IDs and values are image model instances.\\',\\n    },\\n    {\\n      name: \\'fallbackProvider\\',\\n      type: \\'Provider\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional fallback provider to use when a requested model is not found in the custom provider.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe `customProvider` function returns a `Provider` instance. It has the following methods:\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'languageModel\\',\\n      type: \\'(id: string) => LanguageModel\\',\\n      description:\\n        \\'A function that returns a language model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'embeddingModel\\',,\\n      type: \\'(id: string) => EmbeddingModel<string>\\',\\n      description:\\n        \\'A function that returns a text embedding model by its id (format: providerId:modelId)\\',\\n    },\\n    {\\n      name: \\'imageModel\\',\\n      type: \\'(id: string) => ImageModel\\',\\n      description:\\n        \\'A function that returns an image model by its id (format: providerId:modelId)\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/50-cosine-similarity.mdx'), name='50-cosine-similarity.mdx', displayName='50-cosine-similarity.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: cosineSimilarity\\ndescription: Calculate the cosine similarity between two vectors (API Reference)\\n---\\n\\n# `cosineSimilarity()`\\n\\nWhen you want to compare the similarity of embeddings, standard vector similarity metrics\\nlike cosine similarity are often used.\\n\\n`cosineSimilarity` calculates the cosine similarity between two vectors.\\nA high value (close to 1) indicates that the vectors are very similar, while a low value (close to -1) indicates that they are different.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { cosineSimilarity, embedMany } from \\'ai\\';\\n\\nconst { embeddings } = await embedMany({\\n  model: \\'openai/text-embedding-3-small\\',\\n  values: [\\'sunny day at the beach\\', \\'rainy afternoon in the city\\'],\\n});\\n\\nconsole.log(\\n  `cosine similarity: ${cosineSimilarity(embeddings[0], embeddings[1])}`,\\n);\\n```\\n\\n## Import\\n\\n<Snippet text={`import { cosineSimilarity } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'vector1\\',\\n      type: \\'number[]\\',\\n      description: \\'The first vector to compare\\',\\n    },\\n    {\\n      name: \\'vector2\\',\\n      type: \\'number[]\\',\\n      description: \\'The second vector to compare\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA number between -1 and 1 representing the cosine similarity between the two vectors.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/60-wrap-language-model.mdx'), name='60-wrap-language-model.mdx', displayName='60-wrap-language-model.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: wrapLanguageModel\\ndescription: Function for wrapping a language model with middleware (API Reference)\\n---\\n\\n# `wrapLanguageModel()`\\n\\nThe `wrapLanguageModel` function provides a way to enhance the behavior of language models\\nby wrapping them with middleware.\\nSee [Language Model Middleware](/docs/ai-sdk-core/middleware) for more information on middleware.\\n\\n```ts\\nimport { wrapLanguageModel } from \\'ai\\';\\n\\nconst wrappedLanguageModel = wrapLanguageModel({\\n  model: \\'openai/gpt-4.1\\',\\n  middleware: yourLanguageModelMiddleware,\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { wrapLanguageModel } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModelV3\\',\\n      description: \\'The original LanguageModelV3 instance to be wrapped.\\',\\n    },\\n    {\\n      name: \\'middleware\\',\\n      type: \\'LanguageModelV3Middleware | LanguageModelV3Middleware[]\\',\\n      description:\\n        \\'The middleware to be applied to the language model. When multiple middlewares are provided, the first middleware will transform the input first, and the last middleware will be wrapped directly around the model.\\',\\n    },\\n    {\\n      name: \\'modelId\\',\\n      type: \\'string\\',\\n      description:\\n        \"Optional custom model ID to override the original model\\'s ID.\",\\n    },\\n    {\\n      name: \\'providerId\\',\\n      type: \\'string\\',\\n      description:\\n        \"Optional custom provider ID to override the original model\\'s provider.\",\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA new `LanguageModelV3` instance with middleware applied.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/65-language-model-v2-middleware.mdx'), name='65-language-model-v2-middleware.mdx', displayName='65-language-model-v2-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: LanguageModelV3Middleware\\ndescription: Middleware for enhancing language model behavior (API Reference)\\n---\\n\\n# `LanguageModelV3Middleware`\\n\\n<Note type=\"warning\">\\n  Language model middleware is an experimental feature.\\n</Note>\\n\\nLanguage model middleware provides a way to enhance the behavior of language models\\nby intercepting and modifying the calls to the language model. It can be used to add\\nfeatures like guardrails, RAG, caching, and logging in a language model agnostic way.\\n\\nSee [Language Model Middleware](/docs/ai-sdk-core/middleware) for more information.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { LanguageModelV3Middleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'transformParams\\',\\n      type: \\'({ type: \"generate\" | \"stream\", params: LanguageModelV3CallOptions }) => Promise<LanguageModelV3CallOptions>\\',\\n      description:\\n        \\'Transforms the parameters before they are passed to the language model.\\',\\n    },\\n    {\\n      name: \\'wrapGenerate\\',\\n      type: \\'({ doGenerate: DoGenerateFunction, params: LanguageModelV3CallOptions, model: LanguageModelV3 }) => Promise<DoGenerateResult>\\',\\n      description: \\'Wraps the generate operation of the language model.\\',\\n    },\\n    {\\n      name: \\'wrapStream\\',\\n      type: \\'({ doStream: DoStreamFunction, params: LanguageModelV3CallOptions, model: LanguageModelV3 }) => Promise<DoStreamResult>\\',\\n      description: \\'Wraps the stream operation of the language model.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/66-extract-reasoning-middleware.mdx'), name='66-extract-reasoning-middleware.mdx', displayName='66-extract-reasoning-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: extractReasoningMiddleware\\ndescription: Middleware that extracts XML-tagged reasoning sections from generated text\\n---\\n\\n# `extractReasoningMiddleware()`\\n\\n`extractReasoningMiddleware` is a middleware function that extracts XML-tagged reasoning sections from generated text and exposes them separately from the main text content. This is particularly useful when you want to separate an AI model\\'s reasoning process from its final output.\\n\\n```ts\\nimport { extractReasoningMiddleware } from \\'ai\\';\\n\\nconst middleware = extractReasoningMiddleware({\\n  tagName: \\'reasoning\\',\\n  separator: \\'\\\\n\\',\\n});\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { extractReasoningMiddleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'tagName\\',\\n      type: \\'string\\',\\n      isOptional: false,\\n      description:\\n        \\'The name of the XML tag to extract reasoning from (without angle brackets)\\',\\n    },\\n    {\\n      name: \\'separator\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'The separator to use between reasoning and text sections. Defaults to \"\\\\\\\\n\"\\',\\n    },\\n    {\\n      name: \\'startWithReasoning\\',\\n      type: \\'boolean\\',\\n      isOptional: true,\\n      description:\\n        \\'Starts with reasoning tokens. Set to true when the response always starts with reasoning and the initial tag is omitted. Defaults to false.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a middleware object that:\\n\\n- Processes both streaming and non-streaming responses\\n- Extracts content between specified XML tags as reasoning\\n- Removes the XML tags and reasoning from the main text\\n- Adds a `reasoning` property to the result containing the extracted content\\n- Maintains proper separation between text sections using the specified separator\\n\\n### Type Parameters\\n\\nThe middleware works with the `LanguageModelV3StreamPart` type for streaming responses.\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx'), name='67-simulate-streaming-middleware.mdx', displayName='67-simulate-streaming-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: simulateStreamingMiddleware\\ndescription: Middleware that simulates streaming for non-streaming language models\\n---\\n\\n# `simulateStreamingMiddleware()`\\n\\n`simulateStreamingMiddleware` is a middleware function that simulates streaming behavior with responses from non-streaming language models. This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\\n\\n```ts\\nimport { simulateStreamingMiddleware } from \\'ai\\';\\n\\nconst middleware = simulateStreamingMiddleware();\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { simulateStreamingMiddleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\nThis middleware doesn\\'t accept any parameters.\\n\\n### Returns\\n\\nReturns a middleware object that:\\n\\n- Takes a complete response from a language model\\n- Converts it into a simulated stream of chunks\\n- Properly handles various response components including:\\n  - Text content\\n  - Reasoning (as string or array of objects)\\n  - Tool calls\\n  - Metadata and usage information\\n  - Warnings\\n\\n### Usage Example\\n\\n```ts\\nimport { streamText } from \\'ai\\';\\nimport { wrapLanguageModel } from \\'ai\\';\\nimport { simulateStreamingMiddleware } from \\'ai\\';\\n\\n// Example with a non-streaming model\\nconst result = streamText({\\n  model: wrapLanguageModel({\\n    model: nonStreamingModel,\\n    middleware: simulateStreamingMiddleware(),\\n  }),\\n  prompt: \\'Your prompt here\\',\\n});\\n\\n// Now you can use the streaming interface\\nfor await (const chunk of result.fullStream) {\\n  // Process streaming chunks\\n}\\n```\\n\\n## How It Works\\n\\nThe middleware:\\n\\n1. Awaits the complete response from the language model\\n2. Creates a `ReadableStream` that emits chunks in the correct sequence\\n3. Simulates streaming by breaking down the response into appropriate chunk types\\n4. Preserves all metadata, reasoning, tool calls, and other response properties\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/68-default-settings-middleware.mdx'), name='68-default-settings-middleware.mdx', displayName='68-default-settings-middleware.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: defaultSettingsMiddleware\\ndescription: Middleware that applies default settings for language models\\n---\\n\\n# `defaultSettingsMiddleware()`\\n\\n`defaultSettingsMiddleware` is a middleware function that applies default settings to language model calls. This is useful when you want to establish consistent default parameters across multiple model invocations.\\n\\n```ts\\nimport { defaultSettingsMiddleware } from \\'ai\\';\\n\\nconst middleware = defaultSettingsMiddleware({\\n  settings: {\\n    temperature: 0.7,\\n    maxOutputTokens: 1000,\\n    // other settings...\\n  },\\n});\\n```\\n\\n## Import\\n\\n<Snippet\\n  text={`import { defaultSettingsMiddleware } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\nThe middleware accepts a configuration object with the following properties:\\n\\n- `settings`: An object containing default parameter values to apply to language model calls. These can include any valid `LanguageModelV3CallOptions` properties and optional provider metadata.\\n\\n### Returns\\n\\nReturns a middleware object that:\\n\\n- Merges the default settings with the parameters provided in each model call\\n- Ensures that explicitly provided parameters take precedence over defaults\\n- Merges provider metadata objects\\n\\n### Usage Example\\n\\n```ts\\nimport { streamText, wrapLanguageModel, defaultSettingsMiddleware } from \\'ai\\';\\n\\n// Create a model with default settings\\nconst modelWithDefaults = wrapLanguageModel({\\n  model: gateway(\\'anthropic/claude-sonnet-4.5\\'),\\n  middleware: defaultSettingsMiddleware({\\n    settings: {\\n      providerOptions: {\\n        openai: {\\n          reasoningEffort: \\'high\\',\\n        },\\n      },\\n    },\\n  }),\\n});\\n\\n// Use the model - default settings will be applied\\nconst result = await streamText({\\n  model: modelWithDefaults,\\n  prompt: \\'Your prompt here\\',\\n  // These parameters will override the defaults\\n  temperature: 0.8,\\n});\\n```\\n\\n## How It Works\\n\\nThe middleware:\\n\\n1. Takes a set of default settings as configuration\\n2. Merges these defaults with the parameters provided in each model call\\n3. Ensures that explicitly provided parameters take precedence over defaults\\n4. Merges provider metadata objects from both sources\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/70-step-count-is.mdx'), name='70-step-count-is.mdx', displayName='70-step-count-is.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: stepCountIs\\ndescription: API Reference for stepCountIs.\\n---\\n\\n# `stepCountIs()`\\n\\nCreates a stop condition that stops when the number of steps reaches a specified count.\\n\\nThis function is used with `stopWhen` in `generateText` and `streamText` to control when a tool-calling loop should stop based on the number of steps executed.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText, stepCountIs } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    // your tools\\n  },\\n  // Stop after 5 steps\\n  stopWhen: stepCountIs(5),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { stepCountIs } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'count\\',\\n      type: \\'number\\',\\n      description:\\n        \\'The maximum number of steps to execute before stopping the tool-calling loop.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `StopCondition` function that returns `true` when the step count reaches the specified number. The function can be used with the `stopWhen` parameter in `generateText` and `streamText`.\\n\\n## Examples\\n\\n### Basic Usage\\n\\nStop after 3 steps:\\n\\n```ts\\nimport { generateText, stepCountIs } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: yourTools,\\n  stopWhen: stepCountIs(3),\\n});\\n```\\n\\n### Combining with Other Conditions\\n\\nYou can combine multiple stop conditions in an array:\\n\\n```ts\\nimport { generateText, stepCountIs, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: yourTools,\\n  // Stop after 10 steps OR when finalAnswer tool is called\\n  stopWhen: [stepCountIs(10), hasToolCall(\\'finalAnswer\\')],\\n});\\n```\\n\\n## See also\\n\\n- [`hasToolCall()`](/docs/reference/ai-sdk-core/has-tool-call)\\n- [`generateText()`](/docs/reference/ai-sdk-core/generate-text)\\n- [`streamText()`](/docs/reference/ai-sdk-core/stream-text)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/71-has-tool-call.mdx'), name='71-has-tool-call.mdx', displayName='71-has-tool-call.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: hasToolCall\\ndescription: API Reference for hasToolCall.\\n---\\n\\n# `hasToolCall()`\\n\\nCreates a stop condition that stops when a specific tool is called.\\n\\nThis function is used with `stopWhen` in `generateText` and `streamText` to control when a tool-calling loop should stop based on whether a particular tool has been invoked.\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { generateText, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  tools: {\\n    weather: weatherTool,\\n    finalAnswer: finalAnswerTool,\\n  },\\n  // Stop when the finalAnswer tool is called\\n  stopWhen: hasToolCall(\\'finalAnswer\\'),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { hasToolCall } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'toolName\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The name of the tool that should trigger the stop condition when called.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `StopCondition` function that returns `true` when the specified tool is called in the current step. The function can be used with the `stopWhen` parameter in `generateText` and `streamText`.\\n\\n## Examples\\n\\n### Basic Usage\\n\\nStop when a specific tool is called:\\n\\n```ts\\nimport { generateText, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: {\\n    submitAnswer: submitAnswerTool,\\n    search: searchTool,\\n  },\\n  stopWhen: hasToolCall(\\'submitAnswer\\'),\\n});\\n```\\n\\n### Combining with Other Conditions\\n\\nYou can combine multiple stop conditions in an array:\\n\\n```ts\\nimport { generateText, hasToolCall, stepCountIs } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: {\\n    weather: weatherTool,\\n    search: searchTool,\\n    finalAnswer: finalAnswerTool,\\n  },\\n  // Stop when weather tool is called OR finalAnswer is called OR after 5 steps\\n  stopWhen: [\\n    hasToolCall(\\'weather\\'),\\n    hasToolCall(\\'finalAnswer\\'),\\n    stepCountIs(5),\\n  ],\\n});\\n```\\n\\n### Agent Pattern\\n\\nCommon pattern for agents that run until they provide a final answer:\\n\\n```ts\\nimport { generateText, hasToolCall } from \\'ai\\';\\n\\nconst result = await generateText({\\n  model: yourModel,\\n  tools: {\\n    search: searchTool,\\n    calculate: calculateTool,\\n    finalAnswer: {\\n      description: \\'Provide the final answer to the user\\',\\n      parameters: z.object({\\n        answer: z.string(),\\n      }),\\n      execute: async ({ answer }) => answer,\\n    },\\n  },\\n  stopWhen: hasToolCall(\\'finalAnswer\\'),\\n});\\n```\\n\\n## See also\\n\\n- [`stepCountIs()`](/docs/reference/ai-sdk-core/step-count-is)\\n- [`generateText()`](/docs/reference/ai-sdk-core/generate-text)\\n- [`streamText()`](/docs/reference/ai-sdk-core/stream-text)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx'), name='75-simulate-readable-stream.mdx', displayName='75-simulate-readable-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: simulateReadableStream\\ndescription: Create a ReadableStream that emits values with configurable delays\\n---\\n\\n# `simulateReadableStream()`\\n\\n`simulateReadableStream` is a utility function that creates a ReadableStream which emits provided values sequentially with configurable delays. This is particularly useful for testing streaming functionality or simulating time-delayed data streams.\\n\\n```ts\\nimport { simulateReadableStream } from \\'ai\\';\\n\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n  initialDelayInMs: 100,\\n  chunkDelayInMs: 50,\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { simulateReadableStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'chunks\\',\\n      type: \\'T[]\\',\\n      isOptional: false,\\n      description: \\'Array of values to be emitted by the stream\\',\\n    },\\n    {\\n      name: \\'initialDelayInMs\\',\\n      type: \\'number | null\\',\\n      isOptional: true,\\n      description:\\n        \\'Initial delay in milliseconds before emitting the first value. Defaults to 0. Set to null to skip the initial delay entirely.\\',\\n    },\\n    {\\n      name: \\'chunkDelayInMs\\',\\n      type: \\'number | null\\',\\n      isOptional: true,\\n      description:\\n        \\'Delay in milliseconds between emitting each value. Defaults to 0. Set to null to skip delays between chunks.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a `ReadableStream<T>` that:\\n\\n- Emits each value from the provided `chunks` array sequentially\\n- Waits for `initialDelayInMs` before emitting the first value (if not `null`)\\n- Waits for `chunkDelayInMs` between emitting subsequent values (if not `null`)\\n- Closes automatically after all chunks have been emitted\\n\\n### Type Parameters\\n\\n- `T`: The type of values contained in the chunks array and emitted by the stream\\n\\n## Examples\\n\\n### Basic Usage\\n\\n```ts\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n});\\n```\\n\\n### With Delays\\n\\n```ts\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n  initialDelayInMs: 1000, // Wait 1 second before first chunk\\n  chunkDelayInMs: 500, // Wait 0.5 seconds between chunks\\n});\\n```\\n\\n### Without Delays\\n\\n```ts\\nconst stream = simulateReadableStream({\\n  chunks: [\\'Hello\\', \\' \\', \\'World\\'],\\n  initialDelayInMs: null, // No initial delay\\n  chunkDelayInMs: null, // No delay between chunks\\n});\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/80-smooth-stream.mdx'), name='80-smooth-stream.mdx', displayName='80-smooth-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: smoothStream\\ndescription: Stream transformer for smoothing text output\\n---\\n\\n# `smoothStream()`\\n\\n`smoothStream` is a utility function that creates a TransformStream\\nfor the `streamText` `transform` option\\nto smooth out text streaming by buffering and releasing complete words with configurable delays.\\nThis creates a more natural reading experience when streaming text responses.\\n\\n```ts highlight={\"6-9\"}\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: smoothStream({\\n    delayInMs: 20, // optional: defaults to 10ms\\n    chunking: \\'line\\', // optional: defaults to \\'word\\'\\n  }),\\n});\\n```\\n\\n## Import\\n\\n<Snippet text={`import { smoothStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'delayInMs\\',\\n      type: \\'number | null\\',\\n      isOptional: true,\\n      description:\\n        \\'The delay in milliseconds between outputting each chunk. Defaults to 10ms. Set to `null` to disable delays.\\',\\n    },\\n    {\\n      name: \\'chunking\\',\\n      type: \\'\"word\" | \"line\" | RegExp | (buffer: string) => string | undefined | null\\',\\n      isOptional: true,\\n      description:\\n        \\'Controls how the text is chunked for streaming. Use \"word\" to stream word by word (default), \"line\" to stream line by line, or provide a custom callback or RegExp pattern for custom chunking.\\',\\n    },\\n  ]}\\n/>\\n\\n#### Word chunking caveats with non-latin languages\\n\\nThe word based chunking **does not work well** with the following languages that do not delimit words with spaces:\\n\\nFor these languages we recommend using a custom regex, like the following:\\n\\n- Chinese - `/[\\\\u4E00-\\\\u9FFF]|\\\\S+\\\\s+/`\\n- Japanese - `/[\\\\u3040-\\\\u309F\\\\u30A0-\\\\u30FF]|\\\\S+\\\\s+/`\\n\\n```tsx filename=\"Japanese example\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Your prompt here\\',\\n  experimental_transform: smoothStream({\\n    chunking: /[\\\\u3040-\\\\u309F\\\\u30A0-\\\\u30FF]|\\\\S+\\\\s+/,\\n  }),\\n});\\n```\\n\\n```tsx filename=\"Chinese example\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Your prompt here\\',\\n  experimental_transform: smoothStream({\\n    chunking: /[\\\\u4E00-\\\\u9FFF]|\\\\S+\\\\s+/,\\n  }),\\n});\\n```\\n\\nFor these languages you could pass your own language aware chunking function:\\n\\n- Vietnamese\\n- Thai\\n- Javanese (Aksara Jawa)\\n\\n#### Regex based chunking\\n\\nTo use regex based chunking, pass a `RegExp` to the `chunking` option.\\n\\n```ts\\n// To split on underscores:\\nsmoothStream({\\n  chunking: /_+/,\\n});\\n\\n// Also can do it like this, same behavior\\nsmoothStream({\\n  chunking: /[^_]*_/,\\n});\\n```\\n\\n#### Custom callback chunking\\n\\nTo use a custom callback for chunking, pass a function to the `chunking` option.\\n\\n```ts\\nsmoothStream({\\n  chunking: text => {\\n    const findString = \\'some string\\';\\n    const index = text.indexOf(findString);\\n\\n    if (index === -1) {\\n      return null;\\n    }\\n\\n    return text.slice(0, index) + findString;\\n  },\\n});\\n```\\n\\n### Returns\\n\\nReturns a `TransformStream` that:\\n\\n- Buffers incoming text chunks\\n- Releases text when the chunking pattern is encountered\\n- Adds configurable delays between chunks for smooth output\\n- Passes through non-text chunks (like step-finish events) immediately\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/90-generate-id.mdx'), name='90-generate-id.mdx', displayName='90-generate-id.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: generateId\\ndescription: Generate a unique identifier (API Reference)\\n---\\n\\n# `generateId()`\\n\\nGenerates a unique identifier. You can optionally provide the length of the ID.\\n\\nThis is the same id generator used by the AI SDK.\\n\\n```ts\\nimport { generateId } from \\'ai\\';\\n\\nconst id = generateId();\\n```\\n\\n## Import\\n\\n<Snippet text={`import { generateId } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'size\\',\\n      type: \\'number\\',\\n      description:\\n        \\'The length of the generated ID. It defaults to 16. This parameter is deprecated and will be removed in the next major version.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA string representing the generated ID.\\n\\n## See also\\n\\n- [`createIdGenerator()`](/docs/reference/ai-sdk-core/create-id-generator)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/91-create-id-generator.mdx'), name='91-create-id-generator.mdx', displayName='91-create-id-generator.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createIdGenerator\\ndescription: Create a customizable unique identifier generator (API Reference)\\n---\\n\\n# `createIdGenerator()`\\n\\nCreates a customizable ID generator function. You can configure the alphabet, prefix, separator, and default size of the generated IDs.\\n\\n```ts\\nimport { createIdGenerator } from \\'ai\\';\\n\\nconst generateCustomId = createIdGenerator({\\n  prefix: \\'user\\',\\n  separator: \\'_\\',\\n});\\n\\nconst id = generateCustomId(); // Example: \"user_1a2b3c4d5e6f7g8h\"\\n```\\n\\n## Import\\n\\n<Snippet text={`import { createIdGenerator } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'options\\',\\n      type: \\'object\\',\\n      description:\\n        \\'Optional configuration object with the following properties:\\',\\n    },\\n    {\\n      name: \\'options.alphabet\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The characters to use for generating the random part of the ID. Defaults to alphanumeric characters (0-9, A-Z, a-z).\\',\\n    },\\n    {\\n      name: \\'options.prefix\\',\\n      type: \\'string\\',\\n      description:\\n        \\'A string to prepend to all generated IDs. Defaults to none.\\',\\n    },\\n    {\\n      name: \\'options.separator\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The character(s) to use between the prefix and the random part. Defaults to \"-\".\\',\\n    },\\n    {\\n      name: \\'options.size\\',\\n      type: \\'number\\',\\n      description:\\n        \\'The default length of the random part of the ID. Defaults to 16.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nReturns a function that generates IDs based on the configured options.\\n\\n### Notes\\n\\n- The generator uses non-secure random generation and should not be used for security-critical purposes.\\n- The separator character must not be part of the alphabet to ensure reliable prefix checking.\\n\\n## Example\\n\\n```ts\\n// Create a custom ID generator for user IDs\\nconst generateUserId = createIdGenerator({\\n  prefix: \\'user\\',\\n  separator: \\'_\\',\\n  size: 8,\\n});\\n\\n// Generate IDs\\nconst id1 = generateUserId(); // e.g., \"user_1a2b3c4d\"\\n```\\n\\n## See also\\n\\n- [`generateId()`](/docs/reference/ai-sdk-core/generate-id)\\n', children=[]), DocItem(origPath=Path('07-reference/01-ai-sdk-core/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK Core\\ndescription: Reference documentation for the AI SDK Core\\ncollapsed: true\\n---\\n\\n# AI SDK Core\\n\\n[AI SDK Core](/docs/ai-sdk-core) is a set of functions that allow you to interact with language models and other AI models.\\nThese functions are designed to be easy-to-use and flexible, allowing you to generate text, structured data,\\nand embeddings from language models and other AI models.\\n\\nAI SDK Core contains the following main functions:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'generateText()',\\n      description: 'Generate text and call tools from a language model.',\\n      href: '/docs/reference/ai-sdk-core/generate-text',\\n    },\\n    {\\n      title: 'streamText()',\\n      description: 'Stream text and call tools from a language model.',\\n      href: '/docs/reference/ai-sdk-core/stream-text',\\n    },\\n    {\\n      title: 'generateObject()',\\n      description: 'Generate structured data from a language model.',\\n      href: '/docs/reference/ai-sdk-core/generate-object',\\n    },\\n    {\\n      title: 'streamObject()',\\n      description: 'Stream structured data from a language model.',\\n      href: '/docs/reference/ai-sdk-core/stream-object',\\n    },\\n    {\\n      title: 'embed()',\\n      description:\\n        'Generate an embedding for a single value using an embedding model.',\\n      href: '/docs/reference/ai-sdk-core/embed',\\n    },\\n    {\\n      title: 'embedMany()',\\n      description:\\n        'Generate embeddings for several values using an embedding model (batch embedding).',\\n      href: '/docs/reference/ai-sdk-core/embed-many',\\n    },\\n    {\\n      title: 'experimental_generateImage()',\\n      description:\\n        'Generate images based on a given prompt using an image model.',\\n      href: '/docs/reference/ai-sdk-core/generate-image',\\n    },\\n    {\\n      title: 'experimental_transcribe()',\\n      description: 'Generate a transcript from an audio file.',\\n      href: '/docs/reference/ai-sdk-core/transcribe',\\n    },\\n    {\\n      title: 'experimental_generateSpeech()',\\n      description: 'Generate speech audio from text.',\\n      href: '/docs/reference/ai-sdk-core/generate-speech',\\n    },\\n  ]}\\n/>\\n\\nIt also contains the following helper functions:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'tool()',\\n      description: 'Type inference helper function for tools.',\\n      href: '/docs/reference/ai-sdk-core/tool',\\n    },\\n    {\\n      title: 'experimental_createMCPClient()',\\n      description: 'Creates a client for connecting to MCP servers.',\\n      href: '/docs/reference/ai-sdk-core/create-mcp-client',\\n    },\\n    {\\n      title: 'jsonSchema()',\\n      description: 'Creates AI SDK compatible JSON schema objects.',\\n      href: '/docs/reference/ai-sdk-core/json-schema',\\n    },\\n    {\\n      title: 'zodSchema()',\\n      description: 'Creates AI SDK compatible Zod schema objects.',\\n      href: '/docs/reference/ai-sdk-core/zod-schema',\\n    },\\n    {\\n      title: 'createProviderRegistry()',\\n      description:\\n        'Creates a registry for using models from multiple providers.',\\n      href: '/docs/reference/ai-sdk-core/provider-registry',\\n    },\\n    {\\n      title: 'cosineSimilarity()',\\n      description:\\n        'Calculates the cosine similarity between two vectors, e.g. embeddings.',\\n      href: '/docs/reference/ai-sdk-core/cosine-similarity',\\n    },\\n    {\\n      title: 'simulateReadableStream()',\\n      description:\\n        'Creates a ReadableStream that emits values with configurable delays.',\\n      href: '/docs/reference/ai-sdk-core/simulate-readable-stream',\\n    },\\n    {\\n      title: 'wrapLanguageModel()',\\n      description: 'Wraps a language model with middleware.',\\n      href: '/docs/reference/ai-sdk-core/wrap-language-model',\\n    },\\n    {\\n      title: 'extractReasoningMiddleware()',\\n      description:\\n        'Extracts reasoning from the generated text and exposes it as a `reasoning` property on the result.',\\n      href: '/docs/reference/ai-sdk-core/extract-reasoning-middleware',\\n    },\\n    {\\n      title: 'simulateStreamingMiddleware()',\\n      description:\\n        'Simulates streaming behavior with responses from non-streaming language models.',\\n      href: '/docs/reference/ai-sdk-core/simulate-streaming-middleware',\\n    },\\n    {\\n      title: 'defaultSettingsMiddleware()',\\n      description: 'Applies default settings to a language model.',\\n      href: '/docs/reference/ai-sdk-core/default-settings-middleware',\\n    },\\n    {\\n      title: 'smoothStream()',\\n      description: 'Smooths text streaming output.',\\n      href: '/docs/reference/ai-sdk-core/smooth-stream',\\n    },\\n    {\\n      title: 'generateId()',\\n      description: 'Helper function for generating unique IDs',\\n      href: '/docs/reference/ai-sdk-core/generate-id',\\n    },\\n    {\\n      title: 'createIdGenerator()',\\n      description: 'Creates an ID generator',\\n      href: '/docs/reference/ai-sdk-core/create-id-generator',\\n    },\\n  ]}\\n/>\\n\", children=[])]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui'), name='02-ai-sdk-ui', displayName='02-ai-sdk-ui', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/02-ai-sdk-ui/01-use-chat.mdx'), name='01-use-chat.mdx', displayName='01-use-chat.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat\\ndescription: API reference for the useChat hook.\\n---\\n\\n# `useChat()`\\n\\nAllows you to easily create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages are received.\\n\\n<Note>\\n  The `useChat` API has been significantly updated in AI SDK 5.0. It now uses a\\n  transport-based architecture and no longer manages input state internally. See\\n  the [migration\\n  guide](/docs/migration-guides/migration-guide-5-0#usechat-changes) for\\n  details.\\n</Note>\\n\\n## Import\\n\\n<Tabs items={[\\'React\\', \\'Svelte\\', \\'Vue\\']}>\\n  <Tab>\\n    <Snippet\\n      text=\"import { useChat } from \\'@ai-sdk/react\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"import { Chat } from \\'@ai-sdk/svelte\\'\" dark prompt={false} />\\n  </Tab>\\n  <Tab>\\n    <Snippet text=\"import { Chat } from \\'@ai-sdk/vue\\'\" dark prompt={false} />\\n  </Tab>\\n</Tabs>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'chat\\',\\n      type: \\'Chat<UIMessage>\\',\\n      isOptional: true,\\n      description:\\n        \\'An existing Chat instance to use. If provided, other parameters are ignored.\\',\\n    },\\n    {\\n      name: \\'transport\\',\\n      type: \\'ChatTransport\\',\\n      isOptional: true,\\n      description:\\n        \\'The transport to use for sending messages. Defaults to DefaultChatTransport with `/api/chat` endpoint.\\',\\n      properties: [\\n        {\\n          type: \\'DefaultChatTransport\\',\\n          parameters: [\\n            {\\n              name: \\'api\\',\\n              type: \"string = \\'/api/chat\\'\",\\n              isOptional: true,\\n              description: \\'The API endpoint for chat requests.\\',\\n            },\\n            {\\n              name: \\'credentials\\',\\n              type: \\'RequestCredentials\\',\\n              isOptional: true,\\n              description: \\'The credentials mode for fetch requests.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string> | Headers\\',\\n              isOptional: true,\\n              description: \\'HTTP headers to send with requests.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'object\\',\\n              isOptional: true,\\n              description: \\'Extra body object to send with requests.\\',\\n            },\\n            {\\n              name: \\'prepareSendMessagesRequest\\',\\n              type: \\'PrepareSendMessagesRequest\\',\\n              isOptional: true,\\n              description:\\n                \\'A function to customize the request before chat API calls.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareSendMessagesRequest\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'PrepareSendMessageRequestOptions\\',\\n                      description: \\'Options for preparing the request\\',\\n                      properties: [\\n                        {\\n                          type: \\'PrepareSendMessageRequestOptions\\',\\n                          parameters: [\\n                            {\\n                              name: \\'id\\',\\n                              type: \\'string\\',\\n                              description: \\'The chat ID\\',\\n                            },\\n                            {\\n                              name: \\'messages\\',\\n                              type: \\'UIMessage[]\\',\\n                              description: \\'Current messages in the chat\\',\\n                            },\\n                            {\\n                              name: \\'requestMetadata\\',\\n                              type: \\'unknown\\',\\n                              description: \\'The request metadata\\',\\n                            },\\n                            {\\n                              name: \\'body\\',\\n                              type: \\'Record<string, any> | undefined\\',\\n                              description: \\'The request body\\',\\n                            },\\n                            {\\n                              name: \\'credentials\\',\\n                              type: \\'RequestCredentials | undefined\\',\\n                              description: \\'The request credentials\\',\\n                            },\\n                            {\\n                              name: \\'headers\\',\\n                              type: \\'HeadersInit | undefined\\',\\n                              description: \\'The request headers\\',\\n                            },\\n                            {\\n                              name: \\'api\\',\\n                              type: \\'string\\',\\n                              description: `The API endpoint to use for the request. If not specified, it defaults to the transport’s API endpoint: /api/chat.`,\\n                            },\\n                            {\\n                              name: \\'trigger\\',\\n                              type: \"\\'submit-message\\' | \\'regenerate-message\\'\",\\n                              description: \\'The trigger for the request\\',\\n                            },\\n                            {\\n                              name: \\'messageId\\',\\n                              type: \\'string | undefined\\',\\n                              description: \\'The message ID if applicable\\',\\n                            },\\n                          ],\\n                        },\\n                      ],\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'prepareReconnectToStreamRequest\\',\\n              type: \\'PrepareReconnectToStreamRequest\\',\\n              isOptional: true,\\n              description:\\n                \\'A function to customize the request before reconnect API call.\\',\\n              properties: [\\n                {\\n                  type: \\'PrepareReconnectToStreamRequest\\',\\n                  parameters: [\\n                    {\\n                      name: \\'options\\',\\n                      type: \\'PrepareReconnectToStreamRequestOptions\\',\\n                      description:\\n                        \\'Options for preparing the reconnect request\\',\\n                      properties: [\\n                        {\\n                          type: \\'PrepareReconnectToStreamRequestOptions\\',\\n                          parameters: [\\n                            {\\n                              name: \\'id\\',\\n                              type: \\'string\\',\\n                              description: \\'The chat ID\\',\\n                            },\\n                            {\\n                              name: \\'requestMetadata\\',\\n                              type: \\'unknown\\',\\n                              description: \\'The request metadata\\',\\n                            },\\n                            {\\n                              name: \\'body\\',\\n                              type: \\'Record<string, any> | undefined\\',\\n                              description: \\'The request body\\',\\n                            },\\n                            {\\n                              name: \\'credentials\\',\\n                              type: \\'RequestCredentials | undefined\\',\\n                              description: \\'The request credentials\\',\\n                            },\\n                            {\\n                              name: \\'headers\\',\\n                              type: \\'HeadersInit | undefined\\',\\n                              description: \\'The request headers\\',\\n                            },\\n                            {\\n                              name: \\'api\\',\\n                              type: \\'string\\',\\n                              description: `The API endpoint to use for the request. If not specified, it defaults to the transport’s API endpoint combined with the chat ID: /api/chat/{chatId}/stream.`,\\n                            },\\n                          ],\\n                        },\\n                      ],\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \\'A unique identifier for the chat. If not provided, a random one will be generated.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'UIMessage[]\\',\\n      isOptional: true,\\n      description: \\'Initial chat messages to populate the conversation with.\\',\\n    },\\n    {\\n      name: \\'onToolCall\\',\\n      type: \\'({toolCall: ToolCall}) => void | Promise<void>\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback function that is invoked when a tool call is received. You must call addToolOutput to provide the tool result.\\',\\n    },\\n    {\\n      name: \\'sendAutomaticallyWhen\\',\\n      type: \\'(options: { messages: UIMessage[] }) => boolean | PromiseLike<boolean>\\',\\n      isOptional: true,\\n      description:\\n        \\'When provided, this function will be called when the stream is finished or a tool call is added to determine if the current messages should be resubmitted. You can use the lastAssistantMessageIsCompleteWithToolCalls helper for common scenarios.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(options: OnFinishOptions) => void\\',\\n      isOptional: true,\\n      description: \\'Called when the assistant response has finished streaming.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishOptions\\',\\n          parameters: [\\n            {\\n              name: \\'message\\',\\n              type: \\'UIMessage\\',\\n              description: \\'The response message.\\',\\n            },\\n            {\\n              name: \\'messages\\',\\n              type: \\'UIMessage[]\\',\\n              description: \\'All messages including the response message\\',\\n            },\\n            {\\n              name: \\'isAbort\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True when the request has been aborted by the client.\\',\\n            },\\n            {\\n              name: \\'isDisconnect\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'True if the server has been disconnected, e.g. because of a network error.\\',\\n            },\\n            {\\n              name: \\'isError\\',\\n              type: \\'boolean\\',\\n              description: `True if errors during streaming caused the response to stop early.`,\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              isOptional: true,\\n              description:\\n                \\'The reason why the model finished generating the response. Undefined if the finish reason was not provided by the model.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: Error) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback function to be called when an error is encountered.\\',\\n    },\\n    {\\n      name: \\'onData\\',\\n      type: \\'(dataPart: DataUIPart) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback function that is called when a data part is received.\\',\\n    },\\n    {\\n      name: \\'experimental_throttle\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Custom throttle wait in ms for the chat messages and data updates. Default is undefined, which disables throttling.\\',\\n    },\\n    {\\n      name: \\'resume\\',\\n      type: \\'boolean\\',\\n      isOptional: true,\\n      description:\\n        \\'Whether to resume an ongoing chat generation stream. Defaults to false.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      description: \\'The id of the chat.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'UIMessage[]\\',\\n      description: \\'The current array of chat messages.\\',\\n      properties: [\\n        {\\n          type: \\'UIMessage\\',\\n          parameters: [\\n            {\\n              name: \\'id\\',\\n              type: \\'string\\',\\n              description: \\'A unique identifier for the message.\\',\\n            },\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\' | \\'user\\' | \\'assistant\\'\",\\n              description: \\'The role of the message.\\',\\n            },\\n            {\\n              name: \\'parts\\',\\n              type: \\'UIMessagePart[]\\',\\n              description:\\n                \\'The parts of the message. Use this for rendering the message in the UI.\\',\\n            },\\n            {\\n              name: \\'metadata\\',\\n              type: \\'unknown\\',\\n              isOptional: true,\\n              description: \\'The metadata of the message.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'status\\',\\n      type: \"\\'submitted\\' | \\'streaming\\' | \\'ready\\' | \\'error\\'\",\\n      description:\\n        \\'The current status of the chat: \"ready\" (idle), \"submitted\" (request sent), \"streaming\" (receiving response), or \"error\" (request failed).\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'Error | undefined\\',\\n      description: \\'The error object if an error occurred.\\',\\n    },\\n    {\\n      name: \\'sendMessage\\',\\n      type: \\'(message: CreateUIMessage | string, options?: ChatRequestOptions) => void\\',\\n      description:\\n        \\'Function to send a new message to the chat. This will trigger an API call to generate the assistant response.\\',\\n      properties: [\\n        {\\n          type: \\'ChatRequestOptions\\',\\n          parameters: [\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string> | Headers\\',\\n              description:\\n                \\'Additional headers that should be to be passed to the API endpoint.\\',\\n            },\\n            {\\n              name: \\'body\\',\\n              type: \\'object\\',\\n              description:\\n                \\'Additional body JSON properties that should be sent to the API endpoint.\\',\\n            },\\n            {\\n              name: \\'data\\',\\n              type: \\'JSONValue\\',\\n              description: \\'Additional data to be sent to the API endpoint.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'regenerate\\',\\n      type: \\'(options?: { messageId?: string }) => void\\',\\n      description:\\n        \\'Function to regenerate the last assistant message or a specific message. If no messageId is provided, regenerates the last assistant message.\\',\\n    },\\n    {\\n      name: \\'stop\\',\\n      type: \\'() => void\\',\\n      description:\\n        \\'Function to abort the current streaming response from the assistant.\\',\\n    },\\n    {\\n      name: \\'clearError\\',\\n      type: \\'() => void\\',\\n      description: \\'Clears the error state.\\',\\n    },\\n    {\\n      name: \\'resumeStream\\',\\n      type: \\'() => void\\',\\n      description:\\n        \\'Function to resume an interrupted streaming response. Useful when a network error occurs during streaming.\\',\\n    },\\n    {\\n      name: \\'addToolOutput\\',\\n      type: \\'(options: { tool: string; toolCallId: string; output: unknown } | { tool: string; toolCallId: string; state: \"output-error\", errorText: string }) => void\\',\\n      description:\\n        \\'Function to add a tool result to the chat. This will update the chat messages with the tool result. If sendAutomaticallyWhen is configured, it may trigger an automatic submission.\\',\\n    },\\n    {\\n      name: \\'setMessages\\',\\n      type: \\'(messages: UIMessage[] | ((messages: UIMessage[]) => UIMessage[])) => void\\',\\n      description:\\n        \\'Function to update the messages state locally without triggering an API call. Useful for optimistic updates.\\',\\n    },\\n  ]}\\n/>\\n\\n## Learn more\\n\\n- [Chatbot](/docs/ai-sdk-ui/chatbot)\\n- [Chatbot with Tools](/docs/ai-sdk-ui/chatbot-with-tool-calling)\\n- [UIMessage](/docs/reference/ai-sdk-core/ui-message)\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/02-use-completion.mdx'), name='02-use-completion.mdx', displayName='02-use-completion.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useCompletion\\ndescription: API reference for the useCompletion hook.\\n---\\n\\n# `useCompletion()`\\n\\nAllows you to create text completion based capabilities for your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\\n\\n## Import\\n\\n<Tabs items={[\\'React\\', \\'Svelte\\', \\'Vue\\']}>\\n  <Tab>\\n    <Snippet\\n      text=\"import { useCompletion } from \\'@ai-sdk/react\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n  <Tab>\\n    <Snippet\\n      text=\"import { Completion } from \\'@ai-sdk/svelte\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n  <Tab>\\n    <Snippet\\n      text=\"import { useCompletion } from \\'@ai-sdk/vue\\'\"\\n      dark\\n      prompt={false}\\n    />\\n  </Tab>\\n\\n</Tabs>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'api\\',\\n      type: \"string = \\'/api/completion\\'\",\\n      description:\\n        \\'The API endpoint that is called to generate text. It can be a relative path (starting with `/`) or an absolute URL.\\',\\n    },\\n    {\\n      name: \\'id\\',\\n      type: \\'string\\',\\n      description:\\n        \\'An unique identifier for the completion. If not provided, a random one will be generated. When provided, the `useCompletion` hook with the same `id` will have shared states across components. This is useful when you have multiple components showing the same chat stream\\',\\n    },\\n    {\\n      name: \\'initialInput\\',\\n      type: \\'string\\',\\n      description: \\'An optional string for the initial prompt input.\\',\\n    },\\n    {\\n      name: \\'initialCompletion\\',\\n      type: \\'string\\',\\n      description: \\'An optional string for the initial completion result.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(prompt: string, completion: string) => void\\',\\n      description:\\n        \\'An optional callback function that is called when the completion stream ends.\\',\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: Error) => void\\',\\n      description:\\n        \\'An optional callback that will be called when the chat stream encounters an error.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string> | Headers\\',\\n      description:\\n        \\'An optional object of headers to be passed to the API endpoint.\\',\\n    },\\n    {\\n      name: \\'body\\',\\n      type: \\'any\\',\\n      description:\\n        \\'An optional, additional body object to be passed to the API endpoint.\\',\\n    },\\n    {\\n      name: \\'credentials\\',\\n      type: \"\\'omit\\' | \\'same-origin\\' | \\'include\\'\",\\n      description:\\n        \\'An optional literal that sets the mode of credentials to be used on the request. Defaults to same-origin.\\',\\n    },\\n    {\\n      name: \\'streamProtocol\\',\\n      type: \"\\'text\\' | \\'data\\'\",\\n      isOptional: true,\\n      description:\\n        \\'An optional literal that sets the type of stream to be used. Defaults to `data`. If set to `text`, the stream will be treated as a text stream.\\',\\n    },\\n    {\\n      name: \\'fetch\\',\\n      type: \\'FetchFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional. A custom fetch function to be used for the API call. Defaults to the global fetch function.\\',\\n    },\\n    {\\n      name: \\'experimental_throttle\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'React only. Custom throttle wait time in milliseconds for the completion and data updates. When specified, throttles how often the UI updates during streaming. Default is undefined, which disables throttling.\\',\\n    },\\n\\n]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'completion\\',\\n      type: \\'string\\',\\n      description: \\'The current text completion.\\',\\n    },\\n    {\\n      name: \\'complete\\',\\n      type: \\'(prompt: string, options: { headers, body }) => void\\',\\n      description:\\n        \\'Function to execute text completion based on the provided prompt.\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'undefined | Error\\',\\n      description: \\'The error thrown during the completion process, if any.\\',\\n    },\\n    {\\n      name: \\'setCompletion\\',\\n      type: \\'(completion: string) => void\\',\\n      description: \\'Function to update the `completion` state.\\',\\n    },\\n    {\\n      name: \\'stop\\',\\n      type: \\'() => void\\',\\n      description: \\'Function to abort the current API request.\\',\\n    },\\n    {\\n      name: \\'input\\',\\n      type: \\'string\\',\\n      description: \\'The current value of the input field.\\',\\n    },\\n    {\\n      name: \\'setInput\\',\\n      type: \\'React.Dispatch<React.SetStateAction<string>>\\',\\n      description: \\'The current value of the input field.\\',\\n    },\\n    {\\n      name: \\'handleInputChange\\',\\n      type: \\'(event: any) => void\\',\\n      description:\\n        \"Handler for the `onChange` event of the input field to control the input\\'s value.\",\\n    },\\n    {\\n      name: \\'handleSubmit\\',\\n      type: \\'(event?: { preventDefault?: () => void }) => void\\',\\n      description:\\n        \\'Form submission handler that automatically resets the input field and appends a user message.\\',\\n    },\\n    {\\n      name: \\'isLoading\\',\\n      type: \\'boolean\\',\\n      description:\\n        \\'Boolean flag indicating whether a fetch operation is currently in progress.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/03-use-object.mdx'), name='03-use-object.mdx', displayName='03-use-object.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useObject\\ndescription: API reference for the useObject hook.\\n---\\n\\n# `experimental_useObject()`\\n\\n<Note>\\n  `useObject` is an experimental feature and only available in React, Svelte,\\n  and Vue.\\n</Note>\\n\\nAllows you to consume text streams that represent a JSON object and parse them into a complete object based on a schema.\\nYou can use it together with [`streamObject`](/docs/reference/ai-sdk-core/stream-object) in the backend.\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { experimental_useObject as useObject } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { object, submit } = useObject({\\n    api: \\'/api/use-object\\',\\n    schema: z.object({ content: z.string() }),\\n  });\\n\\n  return (\\n    <div>\\n      <button onClick={() => submit(\\'example input\\')}>Generate</button>\\n      {object?.content && <p>{object.content}</p>}\\n    </div>\\n  );\\n}\\n```\\n\\n## Import\\n\\n<Snippet\\n  text=\"import { experimental_useObject as useObject } from \\'@ai-sdk/react\\'\"\\n  dark\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'api\\',\\n      type: \\'string\\',\\n      description:\\n        \\'The API endpoint that is called to generate objects. It should stream JSON that matches the schema as chunked text. It can be a relative path (starting with `/`) or an absolute URL.\\',\\n    },\\n    {\\n      name: \\'schema\\',\\n      type: \\'Zod Schema | JSON Schema\\',\\n      description:\\n        \\'A schema that defines the shape of the complete object. You can either pass in a Zod schema or a JSON schema (using the `jsonSchema` function).\\',\\n    },\\n    {\\n      name: \\'id?\\',\\n      type: \\'string\\',\\n      description:\\n        \\'A unique identifier. If not provided, a random one will be generated. When provided, the `useObject` hook with the same `id` will have shared states across components.\\',\\n    },\\n    {\\n      name: \\'initialValue\\',\\n      type: \\'DeepPartial<RESULT> | undefined\\',\\n      isOptional: true,\\n      description: \\'An value for the initial object. Optional.\\',\\n    },\\n    {\\n      name: \\'fetch\\',\\n      type: \\'FetchFunction\\',\\n      isOptional: true,\\n      description:\\n        \\'A custom fetch function to be used for the API call. Defaults to the global fetch function. Optional.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string> | Headers\\',\\n      isOptional: true,\\n      description:\\n        \\'A headers object to be passed to the API endpoint. Optional.\\',\\n    },\\n    {\\n      name: \\'credentials\\',\\n      type: \\'RequestCredentials\\',\\n      isOptional: true,\\n      description:\\n        \\'The credentials mode to be used for the fetch request. Possible values are: \"omit\", \"same-origin\", \"include\". Optional.\\',\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: Error) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback function to be called when an error is encountered. Optional.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => void\\',\\n      isOptional: true,\\n      description: \\'Called when the streaming response has finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'object\\',\\n              type: \\'T | undefined\\',\\n              description:\\n                \\'The generated object (typed according to the schema). Can be undefined if the final object does not match the schema.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'unknown | undefined\\',\\n              description:\\n                \\'Optional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'submit\\',\\n      type: \\'(input: INPUT) => void\\',\\n      description: \\'Calls the API with the provided input as JSON body.\\',\\n    },\\n    {\\n      name: \\'object\\',\\n      type: \\'DeepPartial<RESULT> | undefined\\',\\n      description:\\n        \\'The current value for the generated object. Updated as the API streams JSON chunks.\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'Error | unknown\\',\\n      description: \\'The error object if the API call fails.\\',\\n    },\\n    {\\n      name: \\'isLoading\\',\\n      type: \\'boolean\\',\\n      description:\\n        \\'Boolean flag indicating whether a request is currently in progress.\\',\\n    },\\n    {\\n      name: \\'stop\\',\\n      type: \\'() => void\\',\\n      description: \\'Function to abort the current API request.\\',\\n    },\\n    {\\n      name: \\'clear\\',\\n      type: \\'() => void\\',\\n      description: \\'Function to clear the object state.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Streaming Object Generation with useObject\\',\\n      link: \\'/examples/next-pages/basics/streaming-object-generation\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/31-convert-to-model-messages.mdx'), name='31-convert-to-model-messages.mdx', displayName='31-convert-to-model-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: convertToModelMessages\\ndescription: Convert useChat messages to ModelMessages for AI functions (API Reference)\\n---\\n\\n# `convertToModelMessages()`\\n\\nThe `convertToModelMessages` function is used to transform an array of UI messages from the `useChat` hook into an array of `ModelMessage` objects. These `ModelMessage` objects are compatible with AI core functions like `streamText`.\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Import\\n\\n<Snippet text={`import { convertToModelMessages } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'messages\\',\\n      type: \\'Message[]\\',\\n      description:\\n        \\'An array of UI messages from the useChat hook to be converted\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'{ tools?: ToolSet, convertDataPart?: (part: DataUIPart) => TextPart | FilePart | null }\\',\\n      description:\\n        \\'Optional configuration object. Provide tools to enable multi-modal tool responses, and convertDataPart to transform custom data parts into model-compatible content.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn array of [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) objects.\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'ModelMessage[]\\',\\n      type: \\'Array\\',\\n      description: \\'An array of ModelMessage objects\\',\\n    },\\n  ]}\\n/>\\n\\n## Multi-modal Tool Responses\\n\\nThe `convertToModelMessages` function supports tools that can return multi-modal content. This is useful when tools need to return non-text content like images.\\n\\n```ts\\nimport { tool } from \\'ai\\';\\nimport { z } from \\'zod\\';\\n\\nconst screenshotTool = tool({\\n  parameters: z.object({}),\\n  execute: async () => \\'imgbase64\\',\\n  toModelOutput: result => [{ type: \\'image\\', data: result }],\\n});\\n\\nconst result = streamText({\\n  model: openai(\\'gpt-4\\'),\\n  messages: convertToModelMessages(messages, {\\n    tools: {\\n      screenshot: screenshotTool,\\n    },\\n  }),\\n});\\n```\\n\\nTools can implement the optional `toModelOutput` method to transform their results into multi-modal content. The content is an array of content parts, where each part has a `type` (e.g., \\'text\\', \\'image\\') and corresponding data.\\n\\n## Custom Data Part Conversion\\n\\nThe `convertToModelMessages` function supports converting custom data parts attached to user messages. This is useful when users need to include additional context (URLs, code files, JSON configs) with their messages.\\n\\n### Basic Usage\\n\\nBy default, data parts in user messages are filtered out during conversion. To include them, provide a `convertDataPart` callback that transforms data parts into text or file parts that the model can understand:\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\n\\ntype CustomUIMessage = UIMessage<\\n  never,\\n  {\\n    url: { url: string; title: string; content: string };\\n    \\'code-file\\': { filename: string; code: string; language: string };\\n  }\\n>;\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages<CustomUIMessage>(messages, {\\n      convertDataPart: part => {\\n        // Convert URL attachments to text\\n        if (part.type === \\'data-url\\') {\\n          return {\\n            type: \\'text\\',\\n            text: `[Reference: ${part.data.title}](${part.data.url})\\\\n\\\\n${part.data.content}`,\\n          };\\n        }\\n\\n        // Convert code file attachments\\n        if (part.type === \\'data-code-file\\') {\\n          return {\\n            type: \\'text\\',\\n            text: `\\\\`\\\\`\\\\`${part.data.language}\\\\n// ${part.data.filename}\\\\n${part.data.code}\\\\n\\\\`\\\\`\\\\``,\\n          };\\n        }\\n\\n        // Other data parts are ignored\\n      },\\n    }),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n### Use Cases\\n\\n**Attaching URL Content**\\nAllow users to attach URLs to their messages, with the content fetched and formatted for the model:\\n\\n```ts\\n// Client side\\nsendMessage({\\n  parts: [\\n    { type: \\'text\\', text: \\'Analyze this article\\' },\\n    {\\n      type: \\'data-url\\',\\n      data: {\\n        url: \\'https://example.com/article\\',\\n        title: \\'Important Article\\',\\n        content: \\'...\\',\\n      },\\n    },\\n  ],\\n});\\n```\\n\\n**Including Code Files as Context**\\nLet users reference code files in their conversations:\\n\\n```ts\\nconvertDataPart: part => {\\n  if (part.type === \\'data-code-file\\') {\\n    return {\\n      type: \\'text\\',\\n      text: `\\\\`\\\\`\\\\`${part.data.language}\\\\n${part.data.code}\\\\n\\\\`\\\\`\\\\``,\\n    };\\n  }\\n};\\n```\\n\\n**Selective Inclusion**\\nOnly data parts for which you return a text or file model message part are included,\\nall other data parts are ignored.\\n\\n```ts\\nconst result = convertToModelMessages<\\n  UIMessage<\\n    unknown,\\n    {\\n      url: { url: string; title: string };\\n      code: { code: string; language: string };\\n      note: { text: string };\\n    }\\n  >\\n>(messages, {\\n  convertDataPart: part => {\\n    if (part.type === \\'data-url\\') {\\n      return {\\n        type: \\'text\\',\\n        text: `[${part.data.title}](${part.data.url})`,\\n      };\\n    }\\n\\n    // data-code and data-node are ignored\\n  },\\n});\\n```\\n\\n### Type Safety\\n\\nThe generic parameter ensures full type safety for your custom data parts:\\n\\n```ts\\ntype MyUIMessage = UIMessage<\\n  unknown,\\n  {\\n    url: { url: string; content: string };\\n    config: { key: string; value: string };\\n  }\\n>;\\n\\n// TypeScript knows the exact shape of part.data\\nconvertToModelMessages<MyUIMessage>(messages, {\\n  convertDataPart: part => {\\n    if (part.type === \\'data-url\\') {\\n      // part.data is typed as { url: string; content: string }\\n      return { type: \\'text\\', text: part.data.url };\\n    }\\n    return null;\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/32-prune-messages.mdx'), name='32-prune-messages.mdx', displayName='32-prune-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: pruneMessages\\ndescription: API Reference for pruneMessages.\\n---\\n\\n# `pruneMessages()`\\n\\nThe `pruneMessages` function is used to prune or filter an array of `ModelMessage` objects. This is useful for reducing message context (to save tokens), removing intermediate reasoning, or trimming tool calls and empty messages before sending to an LLM.\\n\\n```ts filename=\"app/api/chat/route.ts\"\\nimport { pruneMessages, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const prunedMessages = pruneMessages({\\n    messages,\\n    reasoning: \\'before-last-message\\',\\n    toolCalls: \\'before-last-2-messages\\',\\n    emptyMessages: \\'remove\\',\\n  });\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: prunedMessages,\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Import\\n\\n<Snippet text={`import { pruneMessages } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'messages\\',\\n      type: \\'ModelMessage[]\\',\\n      description: \\'An array of ModelMessage objects to prune.\\',\\n    },\\n    {\\n      name: \\'reasoning\\',\\n      type: `\\'all\\' | \\'before-last-message\\' | \\'none\\'`,\\n      description:\\n        \\'How to remove reasoning content from assistant messages. Default: \"none\".\\',\\n    },\\n    {\\n      name: \\'toolCalls\\',\\n      type: `\\'all\\' | \\'before-last-message\\' | \\'before-last-\\\\${number}-messages\\\\\\' | \\'none\\' | PruneToolCallsOption[]`,\\n      description:\\n        \\'How to prune tool call/results/approval content. Can specify strategy or a list with tools.\\',\\n    },\\n    {\\n      name: \\'emptyMessages\\',\\n      type: `\\'keep\\' | \\'remove\\'`,\\n      description:\\n        \\'Whether to keep or remove messages whose content is empty after pruning. Default: \"remove\".\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn array of [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) objects, pruned according to the provided options.\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'ModelMessage[]\\',\\n      type: \\'Array\\',\\n      description: \\'The pruned list of ModelMessage objects\\',\\n    },\\n  ]}\\n/>\\n\\n## Example Usage\\n\\n```ts\\nimport { pruneMessages } from \\'ai\\';\\n\\nconst pruned = pruneMessages({\\n  messages,\\n  reasoning: \\'all\\', // Remove all reasoning parts\\n  toolCalls: \\'before-last-message\\', // Remove tool calls except those in the last message\\n});\\n```\\n\\n## Pruning Options\\n\\n- **reasoning:** Removes reasoning parts from assistant messages. Use `\\'all\\'` to remove all, `\\'before-last-message\\'` to keep reasoning in the last message, or `\\'none\\'` to retain all reasoning.\\n- **toolCalls:** Prune tool-call, tool-result, and tool-approval chunks from assistant/tool messages. Options include:\\n  - `\\'all\\'`: Prune all such content.\\n  - `\\'before-last-message\\'`: Prune except in the last message.\\n  - `before-last-N-messages`: Prune except in the last N messages.\\n  - `\\'none\\'`: Do not prune.\\n  - Or provide an array for per-tool fine control.\\n- **emptyMessages:** Set to `\\'remove\\'` (default) to exclude messages that have no content after pruning.\\n\\n> **Tip**: `pruneMessages` is typically used prior to sending a context window to an LLM to reduce message/token count, especially after a series of tool-calls and approvals.\\n\\nFor advanced usage and the full list of possible message parts, see [`ModelMessage`](/docs/reference/ai-sdk-core/model-message) and [`pruneMessages` implementation](https://github.com/vercel/ai/blob/main/packages/ai/src/generate-text/prune-messages.ts).\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/40-create-ui-message-stream.mdx'), name='40-create-ui-message-stream.mdx', displayName='40-create-ui-message-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createUIMessageStream\\ndescription: API Reference for createUIMessageStream.\\n---\\n\\n# `createUIMessageStream`\\n\\nThe `createUIMessageStream` function allows you to create a readable stream for UI messages with advanced features like message merging, error handling, and finish callbacks.\\n\\n## Import\\n\\n<Snippet text={`import { createUIMessageStream } from \"ai\"`} prompt={false} />\\n\\n## Example\\n\\n```tsx\\nconst existingMessages: UIMessage[] = [\\n  /* ... */\\n];\\n\\nconst stream = createUIMessageStream({\\n  async execute({ writer }) {\\n    // Start a text message\\n    // Note: The id must be consistent across text-start, text-delta, and text-end steps\\n    // This allows the system to correctly identify they belong to the same text block\\n    writer.write({\\n      type: \\'text-start\\',\\n      id: \\'example-text\\',\\n    });\\n\\n    // Write a message chunk\\n    writer.write({\\n      type: \\'text-delta\\',\\n      id: \\'example-text\\',\\n      delta: \\'Hello\\',\\n    });\\n\\n    // End the text message\\n    writer.write({\\n      type: \\'text-end\\',\\n      id: \\'example-text\\',\\n    });\\n\\n    // Merge another stream from streamText\\n    const result = streamText({\\n      model: \\'anthropic/claude-sonnet-4.5\\',\\n      prompt: \\'Write a haiku about AI\\',\\n    });\\n\\n    writer.merge(result.toUIMessageStream());\\n  },\\n  onError: error => `Custom error: ${error.message}`,\\n  originalMessages: existingMessages,\\n  onFinish: ({ messages, isContinuation, responseMessage }) => {\\n    console.log(\\'Stream finished with messages:\\', messages);\\n  },\\n});\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'execute\\',\\n      type: \\'(options: { writer: UIMessageStreamWriter }) => Promise<void> | void\\',\\n      description:\\n        \\'A function that receives a writer instance and can use it to write UI message chunks to the stream.\\',\\n      properties: [\\n        {\\n          type: \\'UIMessageStreamWriter\\',\\n          parameters: [\\n            {\\n              name: \\'write\\',\\n              type: \\'(part: UIMessageChunk) => void\\',\\n              description: \\'Writes a UI message chunk to the stream.\\',\\n            },\\n            {\\n              name: \\'merge\\',\\n              type: \\'(stream: ReadableStream<UIMessageChunk>) => void\\',\\n              description:\\n                \\'Merges the contents of another UI message stream into this stream.\\',\\n            },\\n            {\\n              name: \\'onError\\',\\n              type: \\'(error: unknown) => string\\',\\n              description:\\n                \\'Error handler that is used by the stream writer for handling errors in merged streams.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'onError\\',\\n      type: \\'(error: unknown) => string\\',\\n      description:\\n        \\'A function that handles errors and returns an error message string. By default, it returns the error message.\\',\\n    },\\n    {\\n      name: \\'originalMessages\\',\\n      type: \\'UIMessage[] | undefined\\',\\n      description:\\n        \\'The original messages. If provided, persistence mode is assumed and a message ID is provided for the response message.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(options: { messages: UIMessage[]; isContinuation: boolean; responseMessage: UIMessage }) => void | undefined\\',\\n      description:\\n        \\'A callback function that is called when the stream finishes.\\',\\n      properties: [\\n        {\\n          type: \\'FinishOptions\\',\\n          parameters: [\\n            {\\n              name: \\'messages\\',\\n              type: \\'UIMessage[]\\',\\n              description: \\'The updated list of UI messages.\\',\\n            },\\n            {\\n              name: \\'isContinuation\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'Indicates whether the response message is a continuation of the last original message, or if a new message was created.\\',\\n            },\\n            {\\n              name: \\'responseMessage\\',\\n              type: \\'UIMessage\\',\\n              description:\\n                \\'The message that was sent to the client as a response (including the original message if it was extended).\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'generateId\\',\\n      type: \\'IdGenerator | undefined\\',\\n      description:\\n        \\'A function to generate unique IDs for messages. Uses the default ID generator if not provided.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n`ReadableStream<UIMessageChunk>`\\n\\nA readable stream that emits UI message chunks. The stream automatically handles error propagation, merging of multiple streams, and proper cleanup when all operations are complete.\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/41-create-ui-message-stream-response.mdx'), name='41-create-ui-message-stream-response.mdx', displayName='41-create-ui-message-stream-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createUIMessageStreamResponse\\ndescription: API Reference for createUIMessageStreamResponse.\\n---\\n\\n# `createUIMessageStreamResponse`\\n\\nThe `createUIMessageStreamResponse` function creates a Response object that streams UI messages to the client.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createUIMessageStreamResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\n```tsx\\nimport {\\n  createUIMessageStream,\\n  createUIMessageStreamResponse,\\n  streamText,\\n} from \\'ai\\';\\n\\nconst response = createUIMessageStreamResponse({\\n  status: 200,\\n  statusText: \\'OK\\',\\n  headers: {\\n    \\'Custom-Header\\': \\'value\\',\\n  },\\n  stream: createUIMessageStream({\\n    execute({ writer }) {\\n      // Write custom data\\n      writer.write({\\n        type: \\'data\\',\\n        value: { message: \\'Hello\\' },\\n      });\\n\\n      // Write text content\\n      writer.write({\\n        type: \\'text\\',\\n        value: \\'Hello, world!\\',\\n      });\\n\\n      // Write source information\\n      writer.write({\\n        type: \\'source-url\\',\\n        value: {\\n          type: \\'source\\',\\n          id: \\'source-1\\',\\n          url: \\'https://example.com\\',\\n          title: \\'Example Source\\',\\n        },\\n      });\\n\\n      // Merge with LLM stream\\n      const result = streamText({\\n        model: \\'anthropic/claude-sonnet-4.5\\',\\n        prompt: \\'Say hello\\',\\n      });\\n\\n      writer.merge(result.toUIMessageStream());\\n    },\\n  }),\\n});\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream<UIMessageChunk>\\',\\n      description: \\'The UI message stream to send to the client.\\',\\n    },\\n    {\\n      name: \\'status\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'The status code for the response. Defaults to 200.\\',\\n    },\\n    {\\n      name: \\'statusText\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description: \\'The status text for the response.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Headers | Record<string, string>\\',\\n      isOptional: true,\\n      description: \\'Additional headers for the response.\\',\\n    },\\n    {\\n      name: \\'consumeSseStream\\',\\n      type: \\'(options: { stream: ReadableStream<string> }) => PromiseLike<void> | void\\',\\n      isOptional: true,\\n      description:\\n        \\'Optional callback to consume the Server-Sent Events stream.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n`Response`\\n\\nA Response object that streams UI message chunks with the specified status, headers, and content.\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/42-pipe-ui-message-stream-to-response.mdx'), name='42-pipe-ui-message-stream-to-response.mdx', displayName='42-pipe-ui-message-stream-to-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: pipeUIMessageStreamToResponse\\ndescription: Learn to use pipeUIMessageStreamToResponse helper function to pipe streaming data to a ServerResponse object.\\n---\\n\\n# `pipeUIMessageStreamToResponse`\\n\\nThe `pipeUIMessageStreamToResponse` function pipes streaming data to a Node.js ServerResponse object (see [Streaming Data](/docs/ai-sdk-ui/streaming-data)).\\n\\n## Import\\n\\n<Snippet\\n  text={`import { pipeUIMessageStreamToResponse } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\n```tsx\\npipeUIMessageStreamToResponse({\\n  response: serverResponse,\\n  status: 200,\\n  statusText: \\'OK\\',\\n  headers: {\\n    \\'Custom-Header\\': \\'value\\',\\n  },\\n  stream: myUIMessageStream,\\n  consumeSseStream: ({ stream }) => {\\n    // Optional: consume the SSE stream independently\\n    console.log(\\'Consuming SSE stream:\\', stream);\\n  },\\n});\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'ServerResponse\\',\\n      description: \\'The Node.js ServerResponse object to pipe the data to.\\',\\n    },\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream<UIMessageChunk>\\',\\n      description: \\'The UI message stream to pipe to the response.\\',\\n    },\\n    {\\n      name: \\'status\\',\\n      type: \\'number\\',\\n      description: \\'The status code for the response.\\',\\n    },\\n    {\\n      name: \\'statusText\\',\\n      type: \\'string\\',\\n      description: \\'The status text for the response.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Headers | Record<string, string>\\',\\n      description: \\'Additional headers for the response.\\',\\n    },\\n    {\\n      name: \\'consumeSseStream\\',\\n      type: \\'({ stream }: { stream: ReadableStream }) => void\\',\\n      description:\\n        \\'Optional function to consume the SSE stream independently. The stream is teed and this function receives a copy.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/43-read-ui-message-stream.mdx'), name='43-read-ui-message-stream.mdx', displayName='43-read-ui-message-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: readUIMessageStream\\ndescription: API Reference for readUIMessageStream.\\n---\\n\\n# readUIMessageStream\\n\\nTransforms a stream of `UIMessageChunk`s into an `AsyncIterableStream` of `UIMessage`s.\\n\\nUI message streams are useful outside of Chat use cases, e.g. for terminal UIs, custom stream consumption on the client, or RSC (React Server Components).\\n\\n## Import\\n\\n```tsx\\nimport { readUIMessageStream } from 'ai';\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: 'message',\\n      type: 'UIMessage',\\n      isOptional: true,\\n      description:\\n        'The last assistant message to use as a starting point when the conversation is resumed. Otherwise undefined.',\\n    },\\n    {\\n      name: 'stream',\\n      type: 'ReadableStream<UIMessageChunk>',\\n      description: 'The stream of UIMessageChunk objects to read.',\\n    },\\n    {\\n      name: 'onError',\\n      type: '(error: unknown) => void',\\n      isOptional: true,\\n      description:\\n        'A function that is called when an error occurs during stream processing.',\\n    },\\n    {\\n      name: 'terminateOnError',\\n      type: 'boolean',\\n      isOptional: true,\\n      description:\\n        'Whether to terminate the stream if an error occurs. Defaults to false.',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn `AsyncIterableStream` of `UIMessage`s. Each stream part represents a different state of the same message as it is being completed.\\n\\nFor comprehensive examples and use cases, see [Reading UI Message Streams](/docs/ai-sdk-ui/reading-ui-message-streams).\\n\", children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/46-infer-ui-tools.mdx'), name='46-infer-ui-tools.mdx', displayName='46-infer-ui-tools.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: InferUITools\\ndescription: API Reference for InferUITools.\\n---\\n\\n# InferUITools\\n\\nInfers the input and output types of a `ToolSet`.\\n\\nThis type helper is useful when working with tools in TypeScript to ensure type safety for your tool inputs and outputs in `UIMessage`s.\\n\\n## Import\\n\\n```tsx\\nimport { InferUITools } from 'ai';\\n```\\n\\n## API Signature\\n\\n### Type Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: 'TOOLS',\\n      type: 'ToolSet',\\n      description: 'The tool set to infer types from.',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA type that maps each tool in the tool set to its inferred input and output types.\\n\\nThe resulting type has the shape:\\n\\n```typescript\\n{\\n  [NAME in keyof TOOLS & string]: {\\n    input: InferToolInput<TOOLS[NAME]>;\\n    output: InferToolOutput<TOOLS[NAME]>;\\n  };\\n}\\n```\\n\\n## Examples\\n\\n### Basic Usage\\n\\n```tsx\\nimport { InferUITools } from 'ai';\\nimport { z } from 'zod';\\n\\nconst tools = {\\n  weather: {\\n    description: 'Get the current weather',\\n    parameters: z.object({\\n      location: z.string().describe('The city and state'),\\n    }),\\n    execute: async ({ location }) => {\\n      return `The weather in ${location} is sunny.`;\\n    },\\n  },\\n  calculator: {\\n    description: 'Perform basic arithmetic',\\n    parameters: z.object({\\n      operation: z.enum(['add', 'subtract', 'multiply', 'divide']),\\n      a: z.number(),\\n      b: z.number(),\\n    }),\\n    execute: async ({ operation, a, b }) => {\\n      switch (operation) {\\n        case 'add':\\n          return a + b;\\n        case 'subtract':\\n          return a - b;\\n        case 'multiply':\\n          return a * b;\\n        case 'divide':\\n          return a / b;\\n      }\\n    },\\n  },\\n};\\n\\n// Infer the types from the tool set\\ntype MyUITools = InferUITools<typeof tools>;\\n// This creates a type with:\\n// {\\n//   weather: { input: { location: string }; output: string };\\n//   calculator: { input: { operation: 'add' | 'subtract' | 'multiply' | 'divide'; a: number; b: number }; output: number };\\n// }\\n```\\n\\n## Related\\n\\n- [`InferUITool`](/docs/reference/ai-sdk-ui/infer-ui-tool) - Infer types for a single tool\\n- [`useChat`](/docs/reference/ai-sdk-ui/use-chat) - Chat hook that supports typed tools\\n\", children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/47-infer-ui-tool.mdx'), name='47-infer-ui-tool.mdx', displayName='47-infer-ui-tool.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: InferUITool\\ndescription: API Reference for InferUITool.\\n---\\n\\n# InferUITool\\n\\nInfers the input and output types of a tool.\\n\\nThis type helper is useful when working with individual tools to ensure type safety for your tool inputs and outputs in `UIMessage`s.\\n\\n## Import\\n\\n```tsx\\nimport { InferUITool } from 'ai';\\n```\\n\\n## API Signature\\n\\n### Type Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: 'TOOL',\\n      type: 'Tool',\\n      description: 'The tool to infer types from.',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA type that contains the inferred input and output types of the tool.\\n\\nThe resulting type has the shape:\\n\\n```typescript\\n{\\n  input: InferToolInput<TOOL>;\\n  output: InferToolOutput<TOOL>;\\n}\\n```\\n\\n## Examples\\n\\n### Basic Usage\\n\\n```tsx\\nimport { InferUITool } from 'ai';\\nimport { z } from 'zod';\\n\\nconst weatherTool = {\\n  description: 'Get the current weather',\\n  parameters: z.object({\\n    location: z.string().describe('The city and state'),\\n  }),\\n  execute: async ({ location }) => {\\n    return `The weather in ${location} is sunny.`;\\n  },\\n};\\n\\n// Infer the types from the tool\\ntype WeatherUITool = InferUITool<typeof weatherTool>;\\n// This creates a type with:\\n// {\\n//   input: { location: string };\\n//   output: string;\\n// }\\n```\\n\\n## Related\\n\\n- [`InferUITools`](/docs/reference/ai-sdk-ui/infer-ui-tools) - Infer types for a tool set\\n- [`ToolUIPart`](/docs/reference/ai-sdk-ui/tool-ui-part) - Tool part type for UI messages\\n\", children=[]), DocItem(origPath=Path('07-reference/02-ai-sdk-ui/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI SDK UI\\ndescription: Reference documentation for the AI SDK UI\\ncollapsed: true\\n---\\n\\n# AI SDK UI\\n\\n[AI SDK UI](/docs/ai-sdk-ui) is designed to help you build interactive chat, completion, and assistant applications with ease.\\nIt is framework-agnostic toolkit, streamlining the integration of advanced AI functionalities into your applications.\\n\\nAI SDK UI contains the following hooks:\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'useChat',\\n      description:\\n        'Use a hook to interact with language models in a chat interface.',\\n      href: '/docs/reference/ai-sdk-ui/use-chat',\\n    },\\n    {\\n      title: 'useCompletion',\\n      description:\\n        'Use a hook to interact with language models in a completion interface.',\\n      href: '/docs/reference/ai-sdk-ui/use-completion',\\n    },\\n    {\\n      title: 'useObject',\\n      description: 'Use a hook for consuming a streamed JSON objects.',\\n      href: '/docs/reference/ai-sdk-ui/use-object',\\n    },\\n    {\\n      title: 'convertToModelMessages',\\n      description:\\n        'Convert useChat messages to ModelMessages for AI functions.',\\n      href: '/docs/reference/ai-sdk-ui/convert-to-model-messages',\\n    },\\n    {\\n      title: 'pruneMessages',\\n      description: 'Prunes model messages from a list of model messages.',\\n      href: '/docs/reference/ai-sdk-ui/prune-messages',\\n    },\\n    {\\n      title: 'createUIMessageStream',\\n      description:\\n        'Create a UI message stream to stream additional data to the client.',\\n      href: '/docs/reference/ai-sdk-ui/create-ui-message-stream',\\n    },\\n    {\\n      title: 'createUIMessageStreamResponse',\\n      description:\\n        'Create a response object to stream UI messages to the client.',\\n      href: '/docs/reference/ai-sdk-ui/create-ui-message-stream-response',\\n    },\\n    {\\n      title: 'pipeUIMessageStreamToResponse',\\n      description:\\n        'Pipe a UI message stream to a Node.js ServerResponse object.',\\n      href: '/docs/reference/ai-sdk-ui/pipe-ui-message-stream-to-response',\\n    },\\n    {\\n      title: 'readUIMessageStream',\\n      description:\\n        'Transform a stream of UIMessageChunk objects into an AsyncIterableStream of UIMessage objects.',\\n      href: '/docs/reference/ai-sdk-ui/read-ui-message-stream',\\n    },\\n  ]}\\n/>\\n\\n## UI Framework Support\\n\\nAI SDK UI supports the following frameworks: [React](https://react.dev/), [Svelte](https://svelte.dev/), and [Vue.js](https://vuejs.org/).\\nHere is a comparison of the supported functions across these frameworks:\\n\\n| Function                                                  | React               | Svelte                               | Vue.js              |\\n| --------------------------------------------------------- | ------------------- | ------------------------------------ | ------------------- |\\n| [useChat](/docs/reference/ai-sdk-ui/use-chat)             | <Check size={18} /> | <Check size={18} /> Chat             | <Check size={18} /> |\\n| [useCompletion](/docs/reference/ai-sdk-ui/use-completion) | <Check size={18} /> | <Check size={18} /> Completion       | <Check size={18} /> |\\n| [useObject](/docs/reference/ai-sdk-ui/use-object)         | <Check size={18} /> | <Check size={18} /> StructuredObject | <Cross size={18} /> |\\n\\n<Note>\\n  [Contributions](https://github.com/vercel/ai/blob/main/CONTRIBUTING.md) are\\n  welcome to implement missing features for non-React frameworks.\\n</Note>\\n\", children=[])]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc'), name='03-ai-sdk-rsc', displayName='03-ai-sdk-rsc', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/01-stream-ui.mdx'), name='01-stream-ui.mdx', displayName='01-stream-ui.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamUI\\ndescription: Reference for the streamUI function from the AI SDK RSC\\n---\\n\\n# `streamUI`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nA helper function to create a streamable UI from LLM providers. This function is similar to AI SDK Core APIs and supports the same model interfaces.\\n\\nTo see `streamUI` in action, check out [these examples](#examples).\\n\\n## Import\\n\\n<Snippet text={`import { streamUI } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'LanguageModel\\',\\n      description: \\'The language model to use. Example: openai(\"gpt-4.1\")\\',\\n    },\\n    {\\n      name: \\'initial\\',\\n      isOptional: true,\\n      type: \\'ReactNode\\',\\n      description: \\'The initial UI to render.\\',\\n    },\\n    {\\n      name: \\'system\\',\\n      type: \\'string | SystemModelMessage\\',\\n      description:\\n        \\'The system prompt to use that specifies the behavior of the model.\\',\\n    },\\n    {\\n      name: \\'prompt\\',\\n      type: \\'string\\',\\n      description: \\'The input prompt to generate the text from.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<CoreSystemMessage | CoreUserMessage | CoreAssistantMessage | CoreToolMessage> | Array<UIMessage>\\',\\n      description:\\n        \\'A list of messages that represent a conversation. Automatically converts UI messages from the useChat hook.\\',\\n      properties: [\\n        {\\n          type: \\'CoreSystemMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'CoreUserMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ImagePart | FilePart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ImagePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'image\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'image\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The image content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'The IANA media type of the image. Optional.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'FilePart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'file\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'data\\',\\n                      type: \\'string | Uint8Array | Buffer | ArrayBuffer | URL\\',\\n                      description:\\n                        \\'The file content of the message part. String are either base64 encoded content, base64 data URLs, or http(s) URLs.\\',\\n                    },\\n                    {\\n                      name: \\'mediaType\\',\\n                      type: \\'string\\',\\n                      description: \\'The IANA media type of the file.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'CoreAssistantMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string | Array<TextPart | ToolCallPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'TextPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'text\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'text\\',\\n                      type: \\'string\\',\\n                      description: \\'The text content of the message part.\\',\\n                    },\\n                  ],\\n                },\\n                {\\n                  type: \\'ToolCallPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-call\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool, which typically would be the name of the function.\\',\\n                    },\\n                    {\\n                      name: \\'args\\',\\n                      type: \\'object based on zod schema\\',\\n                      description:\\n                        \\'Parameters generated by the model to be used by the tool.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'CoreToolMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'Array<ToolResultPart>\\',\\n              description: \\'The content of the message.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolResultPart\\',\\n                  parameters: [\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'tool-result\\'\",\\n                      description: \\'The type of the message part.\\',\\n                    },\\n                    {\\n                      name: \\'toolCallId\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The id of the tool call the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'toolName\\',\\n                      type: \\'string\\',\\n                      description:\\n                        \\'The name of the tool the result corresponds to.\\',\\n                    },\\n                    {\\n                      name: \\'result\\',\\n                      type: \\'unknown\\',\\n                      description:\\n                        \\'The result returned by the tool after execution.\\',\\n                    },\\n                    {\\n                      name: \\'isError\\',\\n                      type: \\'boolean\\',\\n                      isOptional: true,\\n                      description:\\n                        \\'Whether the result is an error or an error message.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'maxOutputTokens\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description: \\'Maximum number of tokens to generate.\\',\\n    },\\n    {\\n      name: \\'temperature\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topP\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.\\',\\n    },\\n    {\\n      name: \\'topK\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.\\',\\n    },\\n    {\\n      name: \\'presencePenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'frequencyPenalty\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.\\',\\n    },\\n    {\\n      name: \\'stopSequences\\',\\n      type: \\'string[]\\',\\n      isOptional: true,\\n      description:\\n        \\'Sequences that will stop the generation of the text. If the model generates any of these sequences, it will stop generating further text.\\',\\n    },\\n    {\\n      name: \\'seed\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.\\',\\n    },\\n    {\\n      name: \\'maxRetries\\',\\n      type: \\'number\\',\\n      isOptional: true,\\n      description:\\n        \\'Maximum number of retries. Set to 0 to disable retries. Default: 2.\\',\\n    },\\n    {\\n      name: \\'abortSignal\\',\\n      type: \\'AbortSignal\\',\\n      isOptional: true,\\n      description:\\n        \\'An optional abort signal that can be used to cancel the call.\\',\\n    },\\n    {\\n      name: \\'headers\\',\\n      type: \\'Record<string, string>\\',\\n      isOptional: true,\\n      description:\\n        \\'Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\\',\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      description:\\n        \\'Tools that are accessible to and can be called by the model.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'parameters\\',\\n              type: \\'zod schema\\',\\n              description:\\n                \\'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\\',\\n            },\\n            {\\n              name: \\'generate\\',\\n              isOptional: true,\\n              type: \\'(async (parameters) => ReactNode) | AsyncGenerator<ReactNode, ReactNode, void>\\',\\n              description:\\n                \\'A function or a generator function that is called with the arguments from the tool call and yields React nodes as the UI.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'toolChoice\\',\\n      isOptional: true,\\n      type: \\'\"auto\" | \"none\" | \"required\" | { \"type\": \"tool\", \"toolName\": string }\\',\\n      description:\\n        \\'The tool choice setting. It specifies how tools are selected for execution. The default is \"auto\". \"none\" disables tool execution. \"required\" requires tools to be executed. { \"type\": \"tool\", \"toolName\": string } specifies a specific tool to execute.\\',\\n    },\\n    {\\n      name: \\'text\\',\\n      isOptional: true,\\n      type: \\'(Text) => ReactNode\\',\\n      description: \\'Callback to handle the generated tokens from the model.\\',\\n      properties: [\\n        {\\n          type: \\'Text\\',\\n          parameters: [\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The full content of the completion.\\',\\n            },\\n            { name: \\'delta\\', type: \\'string\\', description: \\'The delta.\\' },\\n            { name: \\'done\\', type: \\'boolean\\', description: \\'Is it done?\\' },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'providerOptions\\',\\n      type: \\'Record<string,JSONObject> | undefined\\',\\n      isOptional: true,\\n      description:\\n        \\'Provider-specific options. The outer key is the provider name. The inner values are the metadata. Details depend on the provider.\\',\\n    },\\n    {\\n      name: \\'onFinish\\',\\n      type: \\'(result: OnFinishResult) => void\\',\\n      isOptional: true,\\n      description:\\n        \\'Callback that is called when the LLM response and all request tool executions (for tools that have a `generate` function) are finished.\\',\\n      properties: [\\n        {\\n          type: \\'OnFinishResult\\',\\n          parameters: [\\n            {\\n              name: \\'usage\\',\\n              type: \\'TokenUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'TokenUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'promptTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens in the prompt.\\',\\n                    },\\n                    {\\n                      name: \\'completionTokens\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The total number of tokens in the completion.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens generated.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n            {\\n              name: \\'value\\',\\n              type: \\'ReactNode\\',\\n              description: \\'The final ui node that was generated.\\',\\n            },\\n            {\\n              name: \\'warnings\\',\\n              type: \\'Warning[] | undefined\\',\\n              description:\\n                \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n            },\\n            {\\n              name: \\'response\\',\\n              type: \\'Response\\',\\n              description: \\'Optional response data.\\',\\n              properties: [\\n                {\\n                  type: \\'Response\\',\\n                  parameters: [\\n                    {\\n                      name: \\'headers\\',\\n                      isOptional: true,\\n                      type: \\'Record<string, string>\\',\\n                      description: \\'Response headers.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'ReactNode\\',\\n      description: \\'The user interface based on the stream output.\\',\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      isOptional: true,\\n      description: \\'Optional response data.\\',\\n      properties: [\\n        {\\n          type: \\'Response\\',\\n          parameters: [\\n            {\\n              name: \\'headers\\',\\n              isOptional: true,\\n              type: \\'Record<string, string>\\',\\n              description: \\'Response headers.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'warnings\\',\\n      type: \\'Warning[] | undefined\\',\\n      description:\\n        \\'Warnings from the model provider (e.g. unsupported settings).\\',\\n    },\\n    {\\n      name: \\'stream\\',\\n      type: \\'AsyncIterable<StreamPart> & ReadableStream<StreamPart>\\',\\n      description:\\n        \\'A stream with all events, including text deltas, tool calls, tool results, and errors. You can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the stream will throw the error.\\',\\n      properties: [\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'text-delta\\'\",\\n              description: \\'The type to identify the object as text delta.\\',\\n            },\\n            {\\n              name: \\'textDelta\\',\\n              type: \\'string\\',\\n              description: \\'The text delta.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'tool-call\\'\",\\n              description: \\'The type to identify the object as tool call.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n            {\\n              name: \\'toolName\\',\\n              type: \\'string\\',\\n              description:\\n                \\'The name of the tool, which typically would be the name of the function.\\',\\n            },\\n            {\\n              name: \\'args\\',\\n              type: \\'object based on zod schema\\',\\n              description:\\n                \\'Parameters generated by the model to be used by the tool.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'error\\'\",\\n              description: \\'The type to identify the object as error.\\',\\n            },\\n            {\\n              name: \\'error\\',\\n              type: \\'Error\\',\\n              description:\\n                \\'Describes the error that may have occurred during execution.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'StreamPart\\',\\n          parameters: [\\n            {\\n              name: \\'type\\',\\n              type: \"\\'finish\\'\",\\n              description: \\'The type to identify the object as finish.\\',\\n            },\\n            {\\n              name: \\'finishReason\\',\\n              type: \"\\'stop\\' | \\'length\\' | \\'content-filter\\' | \\'tool-calls\\' | \\'error\\' | \\'other\\' | \\'unknown\\'\",\\n              description: \\'The reason the model finished generating the text.\\',\\n            },\\n            {\\n              name: \\'usage\\',\\n              type: \\'TokenUsage\\',\\n              description: \\'The token usage of the generated text.\\',\\n              properties: [\\n                {\\n                  type: \\'TokenUsage\\',\\n                  parameters: [\\n                    {\\n                      name: \\'promptTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens in the prompt.\\',\\n                    },\\n                    {\\n                      name: \\'completionTokens\\',\\n                      type: \\'number\\',\\n                      description:\\n                        \\'The total number of tokens in the completion.\\',\\n                    },\\n                    {\\n                      name: \\'totalTokens\\',\\n                      type: \\'number\\',\\n                      description: \\'The total number of tokens generated.\\',\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title:\\n        \\'Learn to render a React component as a function call using a language model in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n    {\\n      title: \\'Learn to persist and restore states UI/AI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/save-and-restore-states\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to route React components using a language model in Next.js\\',\\n      link: \\'/examples/next-app/interface/route-components\\',\\n    },\\n    {\\n      title: \\'Learn to stream component updates to the client in Next.js\\',\\n      link: \\'/examples/next-app/interface/stream-component-updates\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/02-create-ai.mdx'), name='02-create-ai.mdx', displayName='02-create-ai.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createAI\\ndescription: Reference for the createAI function from the AI SDK RSC\\n---\\n\\n# `createAI`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nCreates a client-server context provider that can be used to wrap parts of your application tree to easily manage both UI and AI states of your application.\\n\\n## Import\\n\\n<Snippet text={`import { createAI } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'actions\\',\\n      type: \\'Record<string, Action>\\',\\n      description: \\'Server side actions that can be called from the client.\\',\\n    },\\n    {\\n      name: \\'initialAIState\\',\\n      type: \\'any\\',\\n      description: \\'Initial AI state to be used in the client.\\',\\n    },\\n    {\\n      name: \\'initialUIState\\',\\n      type: \\'any\\',\\n      description: \\'Initial UI state to be used in the client.\\',\\n    },\\n    {\\n      name: \\'onGetUIState\\',\\n      type: \\'() => UIState\\',\\n      description: \\'is called during SSR to compare and update UI state.\\',\\n    },\\n    {\\n      name: \\'onSetAIState\\',\\n      type: \\'(Event) => void\\',\\n      description:\\n        \\'is triggered whenever an update() or done() is called by the mutable AI state in your action, so you can safely store your AI state in the database.\\',\\n      properties: [\\n        {\\n          type: \\'Event\\',\\n          parameters: [\\n            {\\n              name: \\'state\\',\\n              type: \\'AIState\\',\\n              description: \\'The resulting AI state after the update.\\',\\n            },\\n            {\\n              name: \\'done\\',\\n              type: \\'boolean\\',\\n              description:\\n                \\'Whether the AI state updates have been finalized or not.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nIt returns an `<AI/>` context provider.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to manage AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n    {\\n      title: \\'Learn to persist and restore states UI/AI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/save-and-restore-states\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/03-create-streamable-ui.mdx'), name='03-create-streamable-ui.mdx', displayName='03-create-streamable-ui.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createStreamableUI\\ndescription: Reference for the createStreamableUI function from the AI SDK RSC\\n---\\n\\n# `createStreamableUI`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nCreate a stream that sends UI from the server to the client. On the client side, it can be rendered as a normal React node.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createStreamableUI } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'initialValue\\',\\n      type: \\'ReactNode\\',\\n      isOptional: true,\\n      description: \\'The initial value of the streamable UI.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'ReactNode\\',\\n      description:\\n        \\'The value of the streamable UI. This can be returned from a Server Action and received by the client.\\',\\n    },\\n  ]}\\n/>\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'update\\',\\n      type: \\'(ReactNode) => void\\',\\n      description:\\n        \\'Updates the current UI node. It takes a new UI node and replaces the old one.\\',\\n    },\\n    {\\n      name: \\'append\\',\\n      type: \\'(ReactNode) => void\\',\\n      description:\\n        \\'Appends a new UI node to the end of the old one. Once appended a new UI node, the previous UI node cannot be updated anymore.\\',\\n    },\\n    {\\n      name: \\'done\\',\\n      type: \\'(ReactNode | null) => void\\',\\n      description:\\n        \\'Marks the UI node as finalized and closes the stream. Once called, the UI node cannot be updated or appended anymore. This method is always required to be called, otherwise the response will be stuck in a loading state.\\',\\n    },\\n    {\\n      name: \\'error\\',\\n      type: \\'(Error) => void\\',\\n      description:\\n        \\'Signals that there is an error in the UI stream. It will be thrown on the client side and caught by the nearest error boundary component.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Render a React component during a tool call\\',\\n      link: \\'/examples/next-app/tools/render-interface-during-tool-call\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/04-create-streamable-value.mdx'), name='04-create-streamable-value.mdx', displayName='04-create-streamable-value.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: createStreamableValue\\ndescription: Reference for the createStreamableValue function from the AI SDK RSC\\n---\\n\\n# `createStreamableValue`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nCreate a stream that sends values from the server to the client. The value can be any serializable data.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { createStreamableValue } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'any\\',\\n      description: \\'Any data that RSC supports. Example, JSON.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'value\\',\\n      type: \\'streamable\\',\\n      description:\\n        \\'This creates a special value that can be returned from Actions to the client. It holds the data inside and can be updated via the update method.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx'), name='05-read-streamable-value.mdx', displayName='05-read-streamable-value.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: readStreamableValue\\ndescription: Reference for the readStreamableValue function from the AI SDK RSC\\n---\\n\\n# `readStreamableValue`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a function that helps you read the streamable value from the client that was originally created using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) on the server.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { readStreamableValue } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\n```ts filename=\"app/actions.ts\"\\nasync function generate() {\\n  \\'use server\\';\\n  const streamable = createStreamableValue();\\n\\n  streamable.update(1);\\n  streamable.update(2);\\n  streamable.done(3);\\n\\n  return streamable.value;\\n}\\n```\\n\\n```tsx filename=\"app/page.tsx\" highlight=\"12\"\\nimport { readStreamableValue } from \\'@ai-sdk/rsc\\';\\n\\nexport default function Page() {\\n  const [generation, setGeneration] = useState(\\'\\');\\n\\n  return (\\n    <div>\\n      <button\\n        onClick={async () => {\\n          const stream = await generate();\\n\\n          for await (const delta of readStreamableValue(stream)) {\\n            setGeneration(generation => generation + delta);\\n          }\\n        }}\\n      >\\n        Generate\\n      </button>\\n    </div>\\n  );\\n}\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'StreamableValue\\',\\n      description: \\'The streamable value to read from.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nIt returns an async iterator that contains the values emitted by the streamable value.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/06-get-ai-state.mdx'), name='06-get-ai-state.mdx', displayName='06-get-ai-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: getAIState\\ndescription: Reference for the getAIState function from the AI SDK RSC\\n---\\n\\n# `getAIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nGet the current AI state.\\n\\n## Import\\n\\n<Snippet text={`import { getAIState } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'key\\',\\n      type: \\'string\\',\\n      isOptional: true,\\n      description:\\n        \"Returns the value of the specified key in the AI state, if it\\'s an object.\",\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe AI state.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title:\\n        \\'Learn to render a React component during a tool call made by a language model in Next.js\\',\\n      link: \\'/examples/next-app/tools/render-interface-during-tool-call\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/07-get-mutable-ai-state.mdx'), name='07-get-mutable-ai-state.mdx', displayName='07-get-mutable-ai-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: getMutableAIState\\ndescription: Reference for the getMutableAIState function from the AI SDK RSC\\n---\\n\\n# `getMutableAIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nGet a mutable copy of the AI state. You can use this to update the state in the server.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { getMutableAIState } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'key\\',\\n      isOptional: true,\\n      type: \\'string\\',\\n      description:\\n        \"Returns the value of the specified key in the AI state, if it\\'s an object.\",\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nThe mutable AI state.\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'update\\',\\n      type: \\'(newState: any) => void\\',\\n      description: \\'Updates the AI state with the new state.\\',\\n    },\\n    {\\n      name: \\'done\\',\\n      type: \\'(newState: any) => void\\',\\n      description:\\n        \\'Updates the AI state with the new state, marks it as finalized and closes the stream.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to persist and restore states AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/save-and-restore-states\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/08-use-ai-state.mdx'), name='08-use-ai-state.mdx', displayName='08-use-ai-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useAIState\\ndescription: Reference for the useAIState function from the AI SDK RSC\\n---\\n\\n# `useAIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a hook that enables you to read and update the AI state. The AI state is shared globally between all `useAIState` hooks under the same `<AI/>` provider.\\n\\nThe AI state is intended to contain context and information shared with the AI model, such as system messages, function responses, and other relevant data.\\n\\n## Import\\n\\n<Snippet text={`import { useAIState } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Returns\\n\\nA single element array where the first element is the current AI state.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/09-use-actions.mdx'), name='09-use-actions.mdx', displayName='09-use-actions.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useActions\\ndescription: Reference for the useActions function from the AI SDK RSC\\n---\\n\\n# `useActions`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a hook to help you access your Server Actions from the client. This is particularly useful for building interfaces that require user interactions with the server.\\n\\nIt is required to access these server actions via this hook because they are patched when passed through the context. Accessing them directly may result in a [Cannot find Client Component error](/docs/troubleshooting/common-issues/server-actions-in-client-components).\\n\\n## Import\\n\\n<Snippet text={`import { useActions } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Returns\\n\\n`Record<string, Action>`, a dictionary of server actions.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to manage AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n    {\\n      title:\\n        \\'Learn to route React components using a language model in Next.js\\',\\n      link: \\'/examples/next-app/interface/route-components\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/10-use-ui-state.mdx'), name='10-use-ui-state.mdx', displayName='10-use-ui-state.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useUIState\\ndescription: Reference for the useUIState function from the AI SDK RSC\\n---\\n\\n# `useUIState`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a hook that enables you to read and update the UI State. The state is client-side and can contain functions, React nodes, and other data. UIState is the visual representation of the AI state.\\n\\n## Import\\n\\n<Snippet text={`import { useUIState } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Returns\\n\\nSimilar to useState, it is an array, where the first element is the current UI state and the second element is the function that updates the state.\\n\\n## Examples\\n\\n<ExampleLinks\\n  examples={[\\n    {\\n      title: \\'Learn to manage AI and UI states in Next.js\\',\\n      link: \\'/examples/next-app/state-management/ai-ui-states\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx'), name='11-use-streamable-value.mdx', displayName='11-use-streamable-value.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useStreamableValue\\ndescription: Reference for the useStreamableValue function from the AI SDK RSC\\n---\\n\\n# `useStreamableValue`\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nIt is a React hook that takes a streamable value created using [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) and returns the current value, error, and pending state.\\n\\n## Import\\n\\n<Snippet\\n  text={`import { useStreamableValue } from \"@ai-sdk/rsc\"`}\\n  prompt={false}\\n/>\\n\\n## Example\\n\\nThis is useful for consuming streamable values received from a component\\'s props.\\n\\n```tsx\\nfunction MyComponent({ streamableValue }) {\\n  const [data, error, pending] = useStreamableValue(streamableValue);\\n\\n  if (pending) return <div>Loading...</div>;\\n  if (error) return <div>Error: {error.message}</div>;\\n\\n  return <div>Data: {data}</div>;\\n}\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\nIt accepts a streamable value created using `createStreamableValue`.\\n\\n### Returns\\n\\nIt is an array, where the first element contains the data, the second element contains an error if it is thrown anytime during the stream, and the third is a boolean indicating if the value is pending.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/20-render.mdx'), name='20-render.mdx', displayName='20-render.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: render (Removed)\\ndescription: Reference for the render function from the AI SDK RSC\\n---\\n\\n# `render` (Removed)\\n\\n<Note type=\"warning\">\"render\" has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\nA helper function to create a streamable UI from LLM providers. This function is similar to AI SDK Core APIs and supports the same model interfaces.\\n\\n> **Note**: `render` has been deprecated in favor of [`streamUI`](/docs/reference/ai-sdk-rsc/stream-ui). During migration, please ensure that the `messages` parameter follows the updated [specification](/docs/reference/ai-sdk-rsc/stream-ui#messages).\\n\\n## Import\\n\\n<Snippet text={`import { render } from \"@ai-sdk/rsc\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'model\\',\\n      type: \\'string\\',\\n      description: \\'Model identifier, must be OpenAI SDK compatible.\\',\\n    },\\n    {\\n      name: \\'provider\\',\\n      type: \\'provider client\\',\\n      description:\\n        \\'Currently the only provider available is OpenAI. This needs to match the model name.\\',\\n    },\\n    {\\n      name: \\'initial\\',\\n      isOptional: true,\\n      type: \\'ReactNode\\',\\n      description: \\'The initial UI to render.\\',\\n    },\\n    {\\n      name: \\'messages\\',\\n      type: \\'Array<SystemMessage | UserMessage | AssistantMessage | ToolMessage>\\',\\n      description: \\'A list of messages that represent a conversation.\\',\\n      properties: [\\n        {\\n          type: \\'SystemMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'system\\'\",\\n              description: \\'The role for the system message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'UserMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'user\\'\",\\n              description: \\'The role for the user message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'AssistantMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'assistant\\'\",\\n              description: \\'The role for the assistant message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n            {\\n              name: \\'tool_calls\\',\\n              type: \\'ToolCall[]\\',\\n              description: \\'A list of tool calls made by the model.\\',\\n              properties: [\\n                {\\n                  type: \\'ToolCall\\',\\n                  parameters: [\\n                    {\\n                      name: \\'id\\',\\n                      type: \\'string\\',\\n                      description: \\'The id of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'type\\',\\n                      type: \"\\'function\\'\",\\n                      description: \\'The type of the tool call.\\',\\n                    },\\n                    {\\n                      name: \\'function\\',\\n                      type: \\'Function\\',\\n                      description: \\'The function to call.\\',\\n                      properties: [\\n                        {\\n                          type: \\'Function\\',\\n                          parameters: [\\n                            {\\n                              name: \\'name\\',\\n                              type: \\'string\\',\\n                              description: \\'The name of the function.\\',\\n                            },\\n                            {\\n                              name: \\'arguments\\',\\n                              type: \\'string\\',\\n                              description: \\'The arguments of the function.\\',\\n                            },\\n                          ],\\n                        },\\n                      ],\\n                    },\\n                  ],\\n                },\\n              ],\\n            },\\n          ],\\n        },\\n        {\\n          type: \\'ToolMessage\\',\\n          parameters: [\\n            {\\n              name: \\'role\\',\\n              type: \"\\'tool\\'\",\\n              description: \\'The role for the tool message.\\',\\n            },\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The content of the message.\\',\\n            },\\n            {\\n              name: \\'toolCallId\\',\\n              type: \\'string\\',\\n              description: \\'The id of the tool call.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'functions\\',\\n      type: \\'ToolSet\\',\\n      isOptional: true,\\n      description:\\n        \\'Tools that are accessible to and can be called by the model.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'parameters\\',\\n              type: \\'zod schema\\',\\n              description:\\n                \\'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\\',\\n            },\\n            {\\n              name: \\'render\\',\\n              isOptional: true,\\n              type: \\'async (parameters) => any\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'tools\\',\\n      type: \\'ToolSet\\',\\n      isOptional: true,\\n      description:\\n        \\'Tools that are accessible to and can be called by the model.\\',\\n      properties: [\\n        {\\n          type: \\'Tool\\',\\n          parameters: [\\n            {\\n              name: \\'description\\',\\n              isOptional: true,\\n              type: \\'string\\',\\n              description:\\n                \\'Information about the purpose of the tool including details on how and when it can be used by the model.\\',\\n            },\\n            {\\n              name: \\'parameters\\',\\n              type: \\'zod schema\\',\\n              description:\\n                \\'The typed schema that describes the parameters of the tool that can also be used to validation and error handling.\\',\\n            },\\n            {\\n              name: \\'render\\',\\n              isOptional: true,\\n              type: \\'async (parameters) => any\\',\\n              description:\\n                \\'An async function that is called with the arguments from the tool call and produces a result.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'text\\',\\n      isOptional: true,\\n      type: \\'(Text) => ReactNode\\',\\n      description: \\'Callback to handle the generated tokens from the model.\\',\\n      properties: [\\n        {\\n          type: \\'Text\\',\\n          parameters: [\\n            {\\n              name: \\'content\\',\\n              type: \\'string\\',\\n              description: \\'The full content of the completion.\\',\\n            },\\n            { name: \\'delta\\', type: \\'string\\', description: \\'The delta.\\' },\\n            { name: \\'done\\', type: \\'boolean\\', description: \\'Is it done?\\' },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'temperature\\',\\n      isOptional: true,\\n      type: \\'number\\',\\n      description: \\'The temperature to use for the model.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nIt can return any valid ReactNode.\\n', children=[]), DocItem(origPath=Path('07-reference/03-ai-sdk-rsc/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK RSC\\ndescription: Reference documentation for the AI SDK UI\\ncollapsed: true\\n---\\n\\n# AI SDK RSC\\n\\n<Note type=\"warning\">\\n  AI SDK RSC is currently experimental. We recommend using [AI SDK\\n  UI](/docs/ai-sdk-ui/overview) for production. For guidance on migrating from\\n  RSC to UI, see our [migration guide](/docs/ai-sdk-rsc/migrating-to-ui).\\n</Note>\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: \\'streamUI\\',\\n      description:\\n        \\'Use a helper function that streams React Server Components on tool execution.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/stream-ui\\',\\n    },\\n    {\\n      title: \\'createAI\\',\\n      description:\\n        \\'Create a context provider that wraps your application and shares state between the client and language model on the server.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/create-ai\\',\\n    },\\n    {\\n      title: \\'createStreamableUI\\',\\n      description:\\n        \\'Create a streamable UI component that can be rendered on the server and streamed to the client.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/create-streamable-ui\\',\\n    },\\n    {\\n      title: \\'createStreamableValue\\',\\n      description:\\n        \\'Create a streamable value that can be rendered on the server and streamed to the client.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/create-streamable-value\\',\\n    },\\n    {\\n      title: \\'getAIState\\',\\n      description: \\'Read the AI state on the server.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/get-ai-state\\',\\n    },\\n    {\\n      title: \\'getMutableAIState\\',\\n      description: \\'Read and update the AI state on the server.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/get-mutable-ai-state\\',\\n    },\\n    {\\n      title: \\'useAIState\\',\\n      description: \\'Get the AI state on the client from the context provider.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/use-ai-state\\',\\n    },\\n    {\\n      title: \\'useUIState\\',\\n      description: \\'Get the UI state on the client from the context provider.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/use-ui-state\\',\\n    },\\n    {\\n      title: \\'useActions\\',\\n      description: \\'Call server actions from the client.\\',\\n      href: \\'/docs/reference/ai-sdk-rsc/use-actions\\',\\n    },\\n  ]}\\n/>\\n', children=[])]), DocItem(origPath=Path('07-reference/04-stream-helpers'), name='04-stream-helpers', displayName='04-stream-helpers', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/04-stream-helpers/01-ai-stream.mdx'), name='01-ai-stream.mdx', displayName='01-ai-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AIStream\\ndescription: Learn to use AIStream helper function in your application.\\n---\\n\\n# `AIStream`\\n\\n<Note type=\"warning\">\\n  AIStream has been removed in AI SDK 4.0. Use\\n  `streamText.toDataStreamResponse()` instead.\\n</Note>\\n\\nCreates a readable stream for AI responses. This is based on the responses returned\\nby fetch and serves as the basis for the OpenAIStream and AnthropicStream. It allows\\nyou to handle AI response streams in a controlled and customized manner that will\\nwork with useChat and useCompletion.\\n\\nAIStream will throw an error if response doesn\\'t have a 2xx status code. This is to ensure that the stream is only created for successful responses.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AIStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \"This is the response object returned by fetch. It\\'s used as the source of the readable stream.\",\\n    },\\n    {\\n      name: \\'customParser\\',\\n      type: \\'(AIStreamParser) => void\\',\\n      description:\\n        \\'This is a function that is used to parse the events in the stream. It should return a function that receives a stringified chunk from the LLM and extracts the message content. The function is expected to return nothing (void) or a string.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamParser\\',\\n          parameters: [\\n            {\\n              name: \\'\\',\\n              type: \\'(data: string) => string | void\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/02-streaming-text-response.mdx'), name='02-streaming-text-response.mdx', displayName='02-streaming-text-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: StreamingTextResponse\\ndescription: Learn to use StreamingTextResponse helper function in your application.\\n---\\n\\n# `StreamingTextResponse`\\n\\n<Note type=\"warning\">\\n  `StreamingTextResponse` has been removed in AI SDK 4.0. Use\\n  [`streamText.toDataStreamResponse()`](/docs/reference/ai-sdk-core/stream-text)\\n  instead.\\n</Note>\\n\\nIt is a utility class that simplifies the process of returning a ReadableStream of text in HTTP responses.\\nIt is a lightweight wrapper around the native Response class, automatically setting the status code to 200 and the Content-Type header to \\'text/plain; charset=utf-8\\'.\\n\\n## Import\\n\\n<Snippet text={`import { StreamingTextResponse } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n## Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream\\',\\n      description: \\'The stream of content which represents the HTTP response.\\',\\n    },\\n    {\\n      name: \\'init\\',\\n      isOptional: true,\\n      type: \\'ResponseInit\\',\\n      description:\\n        \\'It can be used to customize the properties of the HTTP response. It is an object that corresponds to the ResponseInit object used in the Response constructor.\\',\\n      properties: [\\n        {\\n          type: \\'ResponseInit\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              isOptional: true,\\n              description:\\n                \\'The status code for the response. StreamingTextResponse will overwrite this value with 200.\\',\\n            },\\n            {\\n              name: \\'statusText\\',\\n              type: \\'string\\',\\n              isOptional: true,\\n              description:\\n                \\'The status message associated with the status code.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'HeadersInit\\',\\n              isOptional: true,\\n              description:\\n                \"Any headers you want to add to your response. StreamingTextResponse will add \\'Content-Type\\': \\'text/plain; charset=utf-8\\' to these headers.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'data\\',\\n      isOptional: true,\\n      type: \\'StreamData\\',\\n      description:\\n        \\'StreamData object that you are using to generate additional data for the response.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nAn instance of Response with the provided ReadableStream as the body, the status set to 200, and the Content-Type header set to \\'text/plain; charset=utf-8\\'. Additional headers and properties can be added using the init parameter\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/05-stream-to-response.mdx'), name='05-stream-to-response.mdx', displayName='05-stream-to-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamToResponse\\ndescription: Learn to use streamToResponse helper function in your application.\\n---\\n\\n# `streamToResponse`\\n\\n<Note type=\"warning\">\\n  `streamToResponse` has been removed in AI SDK 4.0. Use\\n  `pipeDataStreamToResponse` from\\n  [streamText](/docs/reference/ai-sdk-core/stream-text) instead.\\n</Note>\\n\\n`streamToResponse` pipes a data stream to a Node.js `ServerResponse` object and sets the status code and headers.\\n\\nThis is useful to create data stream responses in environments that use `ServerResponse` objects, such as Node.js HTTP servers.\\n\\nThe status code and headers can be configured using the `options` parameter.\\nBy default, the status code is set to 200 and the Content-Type header is set to `text/plain; charset=utf-8`.\\n\\n## Import\\n\\n<Snippet text={`import { streamToResponse } from \"ai\"`} prompt={false} />\\n\\n## Example\\n\\nYou can e.g. use `streamToResponse` to pipe a data stream to a Node.js HTTP server response:\\n\\n```ts\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { StreamData, streamText, streamToResponse } from \\'ai\\';\\nimport { createServer } from \\'http\\';\\n\\ncreateServer(async (req, res) => {\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: \\'What is the weather in San Francisco?\\',\\n  });\\n\\n  // use stream data\\n  const data = new StreamData();\\n\\n  data.append(\\'initialized call\\');\\n\\n  streamToResponse(\\n    result.toAIStream({\\n      onFinal() {\\n        data.append(\\'call completed\\');\\n        data.close();\\n      },\\n    }),\\n    res,\\n    {},\\n    data,\\n  );\\n}).listen(8080);\\n```\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'stream\\',\\n      type: \\'ReadableStream\\',\\n      description:\\n        \\'The Web Stream to pipe to the response. It can be the return value of OpenAIStream, HuggingFaceStream, AnthropicStream, or an AIStream instance.\\',\\n    },\\n    {\\n      name: \\'response\\',\\n      type: \\'ServerResponse\\',\\n      description:\\n        \\'The Node.js ServerResponse object to pipe the stream to. This is usually the second argument of a Node.js HTTP request handler.\\',\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'Options\\',\\n      description: \\'Configure the response\\',\\n      properties: [\\n        {\\n          type: \\'Options\\',\\n          parameters: [\\n            {\\n              name: \\'status\\',\\n              type: \\'number\\',\\n              description:\\n                \\'The status code to set on the response. Defaults to `200`.\\',\\n            },\\n            {\\n              name: \\'headers\\',\\n              type: \\'Record<string, string>\\',\\n              description:\\n                \"Additional headers to set on the response. Defaults to `{ \\'Content-Type\\': \\'text/plain; charset=utf-8\\' }`.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'data\\',\\n      type: \\'StreamData\\',\\n      description:\\n        \\'StreamData object for forwarding additional data to the client.\\',\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/07-openai-stream.mdx'), name='07-openai-stream.mdx', displayName='07-openai-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: OpenAIStream\\ndescription: Learn to use OpenAIStream helper function in your application.\\n---\\n\\n# `OpenAIStream`\\n\\n<Note type=\"warning\">OpenAIStream has been removed in AI SDK 4.0</Note>\\n\\n<Note type=\"warning\">\\n  OpenAIStream is part of the legacy OpenAI integration. It is not compatible\\n  with the AI SDK 3.1 functions. It is recommended to use the [AI SDK OpenAI\\n  Provider](/providers/ai-sdk-providers/openai) instead.\\n</Note>\\n\\nTransforms the response from OpenAI\\'s language models into a ReadableStream.\\n\\nNote: Prior to v4, the official OpenAI API SDK does not support the Edge Runtime and only works in serverless environments. The openai-edge package is based on fetch instead of axios (and thus works in the Edge Runtime) so we recommend using openai v4+ or openai-edge.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { OpenAIStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/08-anthropic-stream.mdx'), name='08-anthropic-stream.mdx', displayName='08-anthropic-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AnthropicStream\\ndescription: Learn to use AnthropicStream helper function in your application.\\n---\\n\\n# `AnthropicStream`\\n\\n<Note type=\"warning\">AnthropicStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  AnthropicStream is part of the legacy Anthropic integration. It is not\\n  compatible with the AI SDK 3.1 functions. It is recommended to use the [AI SDK\\n  Anthropic Provider](/providers/ai-sdk-providers/anthropic) instead.\\n</Note>\\n\\nIt is a utility function that transforms the output from Anthropic\\'s SDK into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Anthropic\\'s response data structure.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AnthropicStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/09-aws-bedrock-stream.mdx'), name='09-aws-bedrock-stream.mdx', displayName='09-aws-bedrock-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockStream\\ndescription: Learn to use AWSBedrockStream helper function in your application.\\n---\\n\\n# `AWSBedrockStream`\\n\\n<Note type=\"warning\">AWSBedrockStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockStream is part of the legacy AWS Bedrock integration. It is not\\n  compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AWSBedrockStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/10-aws-bedrock-anthropic-stream.mdx'), name='10-aws-bedrock-anthropic-stream.mdx', displayName='10-aws-bedrock-anthropic-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockAnthropicStream\\ndescription: Learn to use AWSBedrockAnthropicStream helper function in your application.\\n---\\n\\n# `AWSBedrockAnthropicStream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicStream is part of the legacy AWS Bedrock integration. It is\\n  not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet\\n  text={`import { AWSBedrockAnthropicStream } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/10-aws-bedrock-messages-stream.mdx'), name='10-aws-bedrock-messages-stream.mdx', displayName='10-aws-bedrock-messages-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockAnthropicMessagesStream\\ndescription: Learn to use AWSBedrockAnthropicMessagesStream helper function in your application.\\n---\\n\\n# `AWSBedrockAnthropicMessagesStream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicMessagesStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockAnthropicMessagesStream is part of the legacy AWS Bedrock\\n  integration. It is not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet\\n  text={`import { AWSBedrockAnthropicMessagesStream } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/11-aws-bedrock-cohere-stream.mdx'), name='11-aws-bedrock-cohere-stream.mdx', displayName='11-aws-bedrock-cohere-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockCohereStream\\ndescription: Learn to use AWSBedrockCohereStream helper function in your application.\\n---\\n\\n# `AWSBedrockCohereStream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockCohereStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockCohereStream is part of the legacy AWS Bedrock integration. It is\\n  not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\n## Import\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handles parsing Bedrock\\'s response.\\n\\n### React\\n\\n<Snippet text={`import { AWSBedrockCohereStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/12-aws-bedrock-llama-2-stream.mdx'), name='12-aws-bedrock-llama-2-stream.mdx', displayName='12-aws-bedrock-llama-2-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AWSBedrockLlama2Stream\\ndescription: Learn to use AWSBedrockLlama2Stream helper function in your application.\\n---\\n\\n# `AWSBedrockLlama2Stream`\\n\\n<Note type=\"warning\">\\n  AWSBedrockLlama2Stream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  AWSBedrockLlama2Stream is part of the legacy AWS Bedrock integration. It is\\n  not compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe AWS Bedrock stream functions are utilties that transform the outputs from the AWS Bedrock API into a ReadableStream. It uses AIStream under the hood and handle parsing Bedrock\\'s response.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { AWSBedrockLlama2Stream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'AWSBedrockResponse\\',\\n      description: \\'The response object returned from AWS Bedrock.\\',\\n      properties: [\\n        {\\n          type: \\'AWSBedrockResponse\\',\\n          parameters: [\\n            {\\n              name: \\'body\\',\\n              isOptional: true,\\n              type: \\'AsyncIterable<{ chunk?: { bytes?: Uint8Array } }>\\',\\n              description:\\n                \\'An optional async iterable of objects containing optional binary data chunks.\\',\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/13-cohere-stream.mdx'), name='13-cohere-stream.mdx', displayName='13-cohere-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: CohereStream\\ndescription: Learn to use CohereStream helper function in your application.\\n---\\n\\n# `CohereStream`\\n\\n<Note type=\"warning\">CohereStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  CohereStream is part of the legacy Cohere integration. It is not compatible\\n  with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe CohereStream function is a utility that transforms the output from Cohere\\'s API into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Cohere\\'s response data structure. This works with the official Cohere API, and it\\'s supported in both Node.js, the Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { CohereStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/14-google-generative-ai-stream.mdx'), name='14-google-generative-ai-stream.mdx', displayName='14-google-generative-ai-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: GoogleGenerativeAIStream\\ndescription: Learn to use GoogleGenerativeAIStream helper function in your application.\\n---\\n\\n# `GoogleGenerativeAIStream`\\n\\n<Note type=\"warning\">\\n  GoogleGenerativeAIStream has been removed in AI SDK 4.0.\\n</Note>\\n\\n<Note type=\"warning\">\\n  GoogleGenerativeAIStream is part of the legacy Google Generative AI\\n  integration. It is not compatible with the AI SDK 3.1 functions. It is\\n  recommended to use the [AI SDK Google Generative AI\\n  Provider](/providers/ai-sdk-providers/google-generative-ai) instead.\\n</Note>\\n\\nThe GoogleGenerativeAIStream function is a utility that transforms the output from Google\\'s Generative AI SDK into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Google\\'s response data structure. This works with the official Generative AI SDK, and it\\'s supported in both Node.js, Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet\\n  text={`import { GoogleGenerativeAIStream } from \"ai\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'{ stream: AsyncIterable<GenerateContentResponse> }\\',\\n      description:\\n        \\'The response object returned by the Google Generative AI API.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/15-hugging-face-stream.mdx'), name='15-hugging-face-stream.mdx', displayName='15-hugging-face-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: HuggingFaceStream\\ndescription: Learn to use HuggingFaceStream helper function in your application.\\n---\\n\\n# `HuggingFaceStream`\\n\\n<Note type=\"warning\">HuggingFaceStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  HuggingFaceStream is part of the legacy Hugging Face integration. It is not\\n  compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nConverts the output from language models hosted on Hugging Face into a ReadableStream.\\n\\nWhile HuggingFaceStream is compatible with most Hugging Face language models, the rapidly evolving landscape of models may result in certain new or niche models not being supported. If you encounter a model that isn\\'t supported, we encourage you to open an issue.\\n\\nTo ensure that AI responses are comprised purely of text without any delimiters that could pose issues when rendering in chat or completion modes, we standardize and remove special end-of-response tokens. If your use case requires a different handling of responses, you can fork and modify this stream to meet your specific needs.\\n\\nCurrently, `</s>` and `<|endoftext|>` are recognized as end-of-stream tokens.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { HuggingFaceStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'iter\\',\\n      type: \\'AsyncGenerator<any>\\',\\n      description:\\n        \\'This parameter should be the generator function returned by the hf.textGenerationStream method in the Hugging Face Inference SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/16-langchain-adapter.mdx'), name='16-langchain-adapter.mdx', displayName='16-langchain-adapter.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: \\'@ai-sdk/langchain Adapter\\'\\ndescription: API Reference for the LangChain Adapter.\\n---\\n\\n# `@ai-sdk/langchain`\\n\\nThe `@ai-sdk/langchain` module provides helper functions to transform LangChain output streams into data streams and data stream responses.\\nSee the [LangChain Adapter documentation](/providers/adapters/langchain) for more information.\\n\\nIt supports:\\n\\n- LangChain StringOutputParser streams\\n- LangChain AIMessageChunk streams\\n- LangChain StreamEvents v2 streams\\n\\n## Import\\n\\n<Snippet\\n  text={`import { toDataStreamResponse } from \"@ai-sdk/langchain\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'toDataStream\\',\\n      type: \\'(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, AIStreamCallbacksAndOptions) => AIStream\\',\\n      description: \\'Converts LangChain output streams to data stream.\\',\\n    },\\n    {\\n      name: \\'toDataStreamResponse\\',\\n      type: \\'(stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}) => Response\\',\\n      description: \\'Converts LangChain output streams to data stream response.\\',\\n    },\\n    {\\n      name: \\'mergeIntoDataStream\\',\\n      type: \\'(stream: ReadableStream<LangChainStreamEvent> | ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }) => void\\',\\n      description:\\n        \\'Merges LangChain output streams into an existing data stream.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n### Convert LangChain Expression Language Stream\\n\\n```tsx filename=\"app/api/completion/route.ts\" highlight={\"13\"}\\nimport { toUIMessageStream } from \\'@ai-sdk/langchain\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\nimport { createUIMessageStreamResponse } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const model = new ChatOpenAI({\\n    model: \\'gpt-3.5-turbo-0125\\',\\n    temperature: 0,\\n  });\\n\\n  const stream = await model.stream(prompt);\\n\\n  return createUIMessageStreamResponse({\\n    stream: toUIMessageStream(stream),\\n  });\\n}\\n```\\n\\n### Convert StringOutputParser Stream\\n\\n```tsx filename=\"app/api/completion/route.ts\" highlight={\"16\"}\\nimport { toUIMessageStream } from \\'@ai-sdk/langchain\\';\\nimport { StringOutputParser } from \\'@langchain/core/output_parsers\\';\\nimport { ChatOpenAI } from \\'@langchain/openai\\';\\nimport { createUIMessageStreamResponse } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const model = new ChatOpenAI({\\n    model: \\'gpt-3.5-turbo-0125\\',\\n    temperature: 0,\\n  });\\n\\n  const parser = new StringOutputParser();\\n\\n  const stream = await model.pipe(parser).stream(prompt);\\n\\n  return createUIMessageStreamResponse({\\n    stream: toUIMessageStream(stream),\\n  });\\n}\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/16-llamaindex-adapter.mdx'), name='16-llamaindex-adapter.mdx', displayName='16-llamaindex-adapter.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: \\'@ai-sdk/llamaindex Adapter\\'\\ndescription: API Reference for the LlamaIndex Adapter.\\n---\\n\\n# `@ai-sdk/llamaindex`\\n\\nThe `@ai-sdk/llamaindex` package provides helper functions to transform LlamaIndex output streams into data streams and data stream responses.\\nSee the [LlamaIndex Adapter documentation](/providers/adapters/llamaindex) for more information.\\n\\nIt supports:\\n\\n- LlamaIndex ChatEngine streams\\n- LlamaIndex QueryEngine streams\\n\\n## Import\\n\\n<Snippet\\n  text={`import { toDataResponse } from \"@ai-sdk/llamaindex\"`}\\n  prompt={false}\\n/>\\n\\n## API Signature\\n\\n### Methods\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'toDataStream\\',\\n      type: \\'(stream: AsyncIterable<EngineResponse>, AIStreamCallbacksAndOptions) => AIStream\\',\\n      description: \\'Converts LlamaIndex output streams to data stream.\\',\\n    },\\n    {\\n      name: \\'toDataStreamResponse\\',\\n      type: \\'(stream: AsyncIterable<EngineResponse>, options?: {init?: ResponseInit, data?: StreamData, callbacks?: AIStreamCallbacksAndOptions}) => Response\\',\\n      description:\\n        \\'Converts LlamaIndex output streams to data stream response.\\',\\n    },\\n    {\\n      name: \\'mergeIntoDataStream\\',\\n      type: \\'(stream: AsyncIterable<EngineResponse>, options: { dataStream: DataStreamWriter; callbacks?: StreamCallbacks }) => void\\',\\n      description:\\n        \\'Merges LlamaIndex output streams into an existing data stream.\\',\\n    },\\n  ]}\\n/>\\n\\n## Examples\\n\\n### Convert LlamaIndex ChatEngine Stream\\n\\n```tsx filename=\"app/api/completion/route.ts\" highlight=\"15\"\\nimport { OpenAI, SimpleChatEngine } from \\'llamaindex\\';\\nimport { toDataStreamResponse } from \\'@ai-sdk/llamaindex\\';\\n\\nexport async function POST(req: Request) {\\n  const { prompt } = await req.json();\\n\\n  const llm = new OpenAI({ model: \\'gpt-4o\\' });\\n  const chatEngine = new SimpleChatEngine({ llm });\\n\\n  const stream = await chatEngine.chat({\\n    message: prompt,\\n    stream: true,\\n  });\\n\\n  return toDataStreamResponse(stream);\\n}\\n```\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/17-mistral-stream.mdx'), name='17-mistral-stream.mdx', displayName='17-mistral-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: MistralStream\\ndescription: Learn to use MistralStream helper function in your application.\\n---\\n\\n# `MistralStream`\\n\\n<Note type=\"warning\">MistralStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  MistralStream is part of the legacy Mistral integration. It is not compatible\\n  with the AI SDK 3.1 functions. It is recommended to use the [AI SDK Mistral\\n  Provider](/providers/ai-sdk-providers/mistral) instead.\\n</Note>\\n\\nTransforms the output from Mistral\\'s language models into a ReadableStream.\\n\\nThis works with the official Mistral API, and it\\'s supported in both Node.js, the Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { MistralStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/18-replicate-stream.mdx'), name='18-replicate-stream.mdx', displayName='18-replicate-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: ReplicateStream\\ndescription: Learn to use ReplicateStream helper function in your application.\\n---\\n\\n# `ReplicateStream`\\n\\n<Note type=\"warning\">ReplicateStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  ReplicateStream is part of the legacy Replicate integration. It is not\\n  compatible with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe ReplicateStream function is a utility that handles extracting the stream from the output of [Replicate](https://replicate.com)\\'s API. It expects a Prediction object as returned by the [Replicate JavaScript SDK](https://github.com/replicate/replicate-javascript), and returns a ReadableStream. Unlike other wrappers, ReplicateStream returns a Promise because it makes a fetch call to the [Replicate streaming API](https://github.com/replicate/replicate-javascript#streaming) under the hood.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { ReplicateStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'pre\\',\\n      type: \\'Prediction\\',\\n      description: \\'Object returned by the Replicate JavaScript SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n    {\\n      name: \\'options\\',\\n      type: \\'{ headers?: Record<string, string> }\\',\\n      isOptiona: true,\\n      description: \\'An optional parameter for passing additional headers.\\',\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream` wrapped in a promise.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/19-inkeep-stream.mdx'), name='19-inkeep-stream.mdx', displayName='19-inkeep-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: InkeepStream\\ndescription: Learn to use InkeepStream helper function in your application.\\n---\\n\\n# `InkeepStream`\\n\\n<Note type=\"warning\">InkeepStream has been removed in AI SDK 4.0.</Note>\\n\\n<Note type=\"warning\">\\n  InkeepStream is part of the legacy Inkeep integration. It is not compatible\\n  with the AI SDK 3.1 functions.\\n</Note>\\n\\nThe InkeepStream function is a utility that transforms the output from [Inkeep](https://inkeep.com)\\'s API into a ReadableStream. It uses AIStream under the hood, applying a specific parser for the Inkeep\\'s response data structure.\\n\\nThis works with the official Inkeep API, and it\\'s supported in both Node.js, the Edge Runtime, and browser environments.\\n\\n## Import\\n\\n### React\\n\\n<Snippet text={`import { InkeepStream } from \"ai\"`} prompt={false} />\\n\\n## API Signature\\n\\n### Parameters\\n\\n<PropertiesTable\\n  content={[\\n    {\\n      name: \\'response\\',\\n      type: \\'Response\\',\\n      description:\\n        \\'The response object returned by a call made by the Provider SDK.\\',\\n    },\\n    {\\n      name: \\'callbacks\\',\\n      type: \\'AIStreamCallbacksAndOptions\\',\\n      isOptional: true,\\n      description:\\n        \\'An object containing callback functions to handle the start, each token, and completion of the AI response. In the absence of this parameter, default behavior is implemented.\\',\\n      properties: [\\n        {\\n          type: \\'AIStreamCallbacksAndOptions\\',\\n          parameters: [\\n            {\\n              name: \\'onStart\\',\\n              type: \\'() => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called at the start of the stream processing.\\',\\n            },\\n            {\\n              name: \\'onCompletion\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for every completion. It\\'s passed the completion as a string.\",\\n            },\\n            {\\n              name: \\'onFinal\\',\\n              type: \\'(completion: string) => Promise<void>\\',\\n              description:\\n                \\'An optional function that is called once when the stream is closed with the final completion message.\\',\\n            },\\n            {\\n              name: \\'onToken\\',\\n              type: \\'(token: string) => Promise<void>\\',\\n              description:\\n                \"An optional function that is called for each token in the stream. It\\'s passed the token as a string.\",\\n            },\\n          ],\\n        },\\n      ],\\n    },\\n  ]}\\n/>\\n\\n### Returns\\n\\nA `ReadableStream`.\\n', children=[]), DocItem(origPath=Path('07-reference/04-stream-helpers/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stream Helpers\\ndescription: Learn to use help functions that help stream generations from different providers.\\ncollapsed: true\\n---\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: \\'AIStream\\',\\n      description: \\'Create a readable stream for AI responses.\\',\\n      href: \\'/docs/reference/stream-helpers/ai-stream\\',\\n    },\\n    {\\n      title: \\'StreamingTextResponse\\',\\n      description: \\'Create a streaming response for text generations.\\',\\n      href: \\'/docs/reference/stream-helpers/streaming-text-response\\',\\n    },\\n    {\\n      title: \\'streamtoResponse\\',\\n      description: \\'Pipe a ReadableStream to a Node.js ServerResponse object.\\',\\n      href: \\'/docs/reference/stream-helpers/stream-to-response\\',\\n    },\\n    {\\n      title: \\'OpenAIStream\\',\\n      description:\\n        \"Transforms the response from OpenAI\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/openai-stream\\',\\n    },\\n    {\\n      title: \\'AnthropicStream\\',\\n      description:\\n        \"Transforms the response from Anthropic\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/anthropic-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockStream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockMessagesStream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock Message\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-messages-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockCohereStream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock Cohere\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-cohere-stream\\',\\n    },\\n    {\\n      title: \\'AWSBedrockLlama-2Stream\\',\\n      description:\\n        \"Transforms the response from AWS Bedrock Llama-2\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/aws-bedrock-llama-2-stream\\',\\n    },\\n    {\\n      title: \\'CohereStream\\',\\n      description:\\n        \"Transforms the response from Cohere\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/cohere-stream\\',\\n    },\\n    {\\n      title: \\'GoogleGenerativeAIStream\\',\\n      description:\\n        \"Transforms the response from Google\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/google-generative-ai-stream\\',\\n    },\\n    {\\n      title: \\'HuggingFaceStream\\',\\n      description:\\n        \"Transforms the response from Hugging Face\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/hugging-face-stream\\',\\n    },\\n    {\\n      title: \\'LangChainStream\\',\\n      description:\\n        \"Transforms the response from LangChain\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/langchain-adapter\\',\\n    },\\n    {\\n      title: \\'MistralStream\\',\\n      description:\\n        \"Transforms the response from Mistral\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/mistral-stream\\',\\n    },\\n    {\\n      title: \\'ReplicateStream\\',\\n      description:\\n        \"Transforms the response from Replicate\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/replicate-stream\\',\\n    },\\n    {\\n      title: \\'InkeepsStream\\',\\n      description:\\n        \"Transforms the response from Inkeeps\\'s language models into a readable stream.\",\\n      href: \\'/docs/reference/stream-helpers/inkeep-stream\\',\\n    },\\n  ]}\\n/>\\n', children=[])]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors'), name='05-ai-sdk-errors', displayName='05-ai-sdk-errors', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-api-call-error.mdx'), name='ai-api-call-error.mdx', displayName='ai-api-call-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_APICallError\\ndescription: Learn how to fix AI_APICallError\\n---\\n\\n# AI_APICallError\\n\\nThis error occurs when an API call fails.\\n\\n## Properties\\n\\n- `url`: The URL of the API request that failed\\n- `requestBodyValues`: The request body values sent to the API\\n- `statusCode`: The HTTP status code returned by the API\\n- `responseHeaders`: The response headers returned by the API\\n- `responseBody`: The response body returned by the API\\n- `isRetryable`: Whether the request can be retried based on the status code\\n- `data`: Any additional data associated with the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_APICallError` using:\\n\\n```typescript\\nimport { APICallError } from 'ai';\\n\\nif (APICallError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-download-error.mdx'), name='ai-download-error.mdx', displayName='ai-download-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_DownloadError\\ndescription: Learn how to fix AI_DownloadError\\n---\\n\\n# AI_DownloadError\\n\\nThis error occurs when a download fails.\\n\\n## Properties\\n\\n- `url`: The URL that failed to download\\n- `statusCode`: The HTTP status code returned by the server\\n- `statusText`: The HTTP status text returned by the server\\n- `message`: The error message containing details about the download failure\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_DownloadError` using:\\n\\n```typescript\\nimport { DownloadError } from 'ai';\\n\\nif (DownloadError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-empty-response-body-error.mdx'), name='ai-empty-response-body-error.mdx', displayName='ai-empty-response-body-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_EmptyResponseBodyError\\ndescription: Learn how to fix AI_EmptyResponseBodyError\\n---\\n\\n# AI_EmptyResponseBodyError\\n\\nThis error occurs when the server returns an empty response body.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_EmptyResponseBodyError` using:\\n\\n```typescript\\nimport { EmptyResponseBodyError } from 'ai';\\n\\nif (EmptyResponseBodyError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-argument-error.mdx'), name='ai-invalid-argument-error.mdx', displayName='ai-invalid-argument-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidArgumentError\\ndescription: Learn how to fix AI_InvalidArgumentError\\n---\\n\\n# AI_InvalidArgumentError\\n\\nThis error occurs when an invalid argument was provided.\\n\\n## Properties\\n\\n- `parameter`: The name of the parameter that is invalid\\n- `value`: The invalid value\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidArgumentError` using:\\n\\n```typescript\\nimport { InvalidArgumentError } from 'ai';\\n\\nif (InvalidArgumentError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-data-content-error.mdx'), name='ai-invalid-data-content-error.mdx', displayName='ai-invalid-data-content-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidDataContentError\\ndescription: How to fix AI_InvalidDataContentError\\n---\\n\\n# AI_InvalidDataContentError\\n\\nThis error occurs when the data content provided in a multi-modal message part is invalid. Check out the [ prompt examples for multi-modal messages ](/docs/foundations/prompts#message-prompts).\\n\\n## Properties\\n\\n- `content`: The invalid content value\\n- `message`: The error message describing the expected and received content types\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidDataContentError` using:\\n\\n```typescript\\nimport { InvalidDataContentError } from 'ai';\\n\\nif (InvalidDataContentError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-data-content.mdx'), name='ai-invalid-data-content.mdx', displayName='ai-invalid-data-content.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidDataContent\\ndescription: Learn how to fix AI_InvalidDataContent\\n---\\n\\n# AI_InvalidDataContent\\n\\nThis error occurs when invalid data content is provided.\\n\\n## Properties\\n\\n- `content`: The invalid content value\\n- `message`: The error message\\n- `cause`: The cause of the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidDataContent` using:\\n\\n```typescript\\nimport { InvalidDataContent } from 'ai';\\n\\nif (InvalidDataContent.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-message-role-error.mdx'), name='ai-invalid-message-role-error.mdx', displayName='ai-invalid-message-role-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidMessageRoleError\\ndescription: Learn how to fix AI_InvalidMessageRoleError\\n---\\n\\n# AI_InvalidMessageRoleError\\n\\nThis error occurs when an invalid message role is provided.\\n\\n## Properties\\n\\n- `role`: The invalid role value\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidMessageRoleError` using:\\n\\n```typescript\\nimport { InvalidMessageRoleError } from 'ai';\\n\\nif (InvalidMessageRoleError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-prompt-error.mdx'), name='ai-invalid-prompt-error.mdx', displayName='ai-invalid-prompt-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidPromptError\\ndescription: Learn how to fix AI_InvalidPromptError\\n---\\n\\n# AI_InvalidPromptError\\n\\nThis error occurs when the prompt provided is invalid.\\n\\n## Potential Causes\\n\\n### UI Messages\\n\\nYou are passing a `UIMessage[]` as messages into e.g. `streamText`.\\n\\nYou need to first convert them to a `ModelMessage[]` using `convertToModelMessages()`.\\n\\n```typescript\\nimport { type UIMessage, generateText, convertToModelMessages } from 'ai';\\n\\nconst messages: UIMessage[] = [\\n  /* ... */\\n];\\n\\nconst result = await generateText({\\n  // ...\\n  messages: convertToModelMessages(messages),\\n});\\n```\\n\\n## Properties\\n\\n- `prompt`: The invalid prompt value\\n- `message`: The error message\\n- `cause`: The cause of the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidPromptError` using:\\n\\n```typescript\\nimport { InvalidPromptError } from 'ai';\\n\\nif (InvalidPromptError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-response-data-error.mdx'), name='ai-invalid-response-data-error.mdx', displayName='ai-invalid-response-data-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidResponseDataError\\ndescription: Learn how to fix AI_InvalidResponseDataError\\n---\\n\\n# AI_InvalidResponseDataError\\n\\nThis error occurs when the server returns a response with invalid data content.\\n\\n## Properties\\n\\n- `data`: The invalid response data value\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidResponseDataError` using:\\n\\n```typescript\\nimport { InvalidResponseDataError } from 'ai';\\n\\nif (InvalidResponseDataError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-invalid-tool-input-error.mdx'), name='ai-invalid-tool-input-error.mdx', displayName='ai-invalid-tool-input-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_InvalidToolInputError\\ndescription: Learn how to fix AI_InvalidToolInputError\\n---\\n\\n# AI_InvalidToolInputError\\n\\nThis error occurs when invalid tool input was provided.\\n\\n## Properties\\n\\n- `toolName`: The name of the tool with invalid inputs\\n- `toolInput`: The invalid tool inputs\\n- `message`: The error message\\n- `cause`: The cause of the error\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_InvalidToolInputError` using:\\n\\n```typescript\\nimport { InvalidToolInputError } from 'ai';\\n\\nif (InvalidToolInputError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-json-parse-error.mdx'), name='ai-json-parse-error.mdx', displayName='ai-json-parse-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_JSONParseError\\ndescription: Learn how to fix AI_JSONParseError\\n---\\n\\n# AI_JSONParseError\\n\\nThis error occurs when JSON fails to parse.\\n\\n## Properties\\n\\n- `text`: The text value that could not be parsed\\n- `message`: The error message including parse error details\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_JSONParseError` using:\\n\\n```typescript\\nimport { JSONParseError } from 'ai';\\n\\nif (JSONParseError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-load-api-key-error.mdx'), name='ai-load-api-key-error.mdx', displayName='ai-load-api-key-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_LoadAPIKeyError\\ndescription: Learn how to fix AI_LoadAPIKeyError\\n---\\n\\n# AI_LoadAPIKeyError\\n\\nThis error occurs when API key is not loaded successfully.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_LoadAPIKeyError` using:\\n\\n```typescript\\nimport { LoadAPIKeyError } from 'ai';\\n\\nif (LoadAPIKeyError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-load-setting-error.mdx'), name='ai-load-setting-error.mdx', displayName='ai-load-setting-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_LoadSettingError\\ndescription: Learn how to fix AI_LoadSettingError\\n---\\n\\n# AI_LoadSettingError\\n\\nThis error occurs when a setting is not loaded successfully.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_LoadSettingError` using:\\n\\n```typescript\\nimport { LoadSettingError } from 'ai';\\n\\nif (LoadSettingError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-message-conversion-error.mdx'), name='ai-message-conversion-error.mdx', displayName='ai-message-conversion-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_MessageConversionError\\ndescription: Learn how to fix AI_MessageConversionError\\n---\\n\\n# AI_MessageConversionError\\n\\nThis error occurs when message conversion fails.\\n\\n## Properties\\n\\n- `originalMessage`: The original message that failed conversion\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_MessageConversionError` using:\\n\\n```typescript\\nimport { MessageConversionError } from 'ai';\\n\\nif (MessageConversionError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-content-generated-error.mdx'), name='ai-no-content-generated-error.mdx', displayName='ai-no-content-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoContentGeneratedError\\ndescription: Learn how to fix AI_NoContentGeneratedError\\n---\\n\\n# AI_NoContentGeneratedError\\n\\nThis error occurs when the AI provider fails to generate content.\\n\\n## Properties\\n\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoContentGeneratedError` using:\\n\\n```typescript\\nimport { NoContentGeneratedError } from 'ai';\\n\\nif (NoContentGeneratedError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-image-generated-error.mdx'), name='ai-no-image-generated-error.mdx', displayName='ai-no-image-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoImageGeneratedError\\ndescription: Learn how to fix AI_NoImageGeneratedError\\n---\\n\\n# AI_NoImageGeneratedError\\n\\nThis error occurs when the AI provider fails to generate an image.\\nIt can arise due to the following reasons:\\n\\n- The model failed to generate a response.\\n- The model generated an invalid response.\\n\\n## Properties\\n\\n- `message`: The error message.\\n- `responses`: Metadata about the image model responses, including timestamp, model, and headers.\\n- `cause`: The cause of the error. You can use this for more detailed error handling.\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoImageGeneratedError` using:\\n\\n```typescript\\nimport { generateImage, NoImageGeneratedError } from 'ai';\\n\\ntry {\\n  await generateImage({ model, prompt });\\n} catch (error) {\\n  if (NoImageGeneratedError.isInstance(error)) {\\n    console.log('NoImageGeneratedError');\\n    console.log('Cause:', error.cause);\\n    console.log('Responses:', error.responses);\\n  }\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-object-generated-error.mdx'), name='ai-no-object-generated-error.mdx', displayName='ai-no-object-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoObjectGeneratedError\\ndescription: Learn how to fix AI_NoObjectGeneratedError\\n---\\n\\n# AI_NoObjectGeneratedError\\n\\nThis error occurs when the AI provider fails to generate a parsable object that conforms to the schema.\\nIt can arise due to the following reasons:\\n\\n- The model failed to generate a response.\\n- The model generated a response that could not be parsed.\\n- The model generated a response that could not be validated against the schema.\\n\\n## Properties\\n\\n- `message`: The error message.\\n- `text`: The text that was generated by the model. This can be the raw text or the tool call text, depending on the object generation mode.\\n- `response`: Metadata about the language model response, including response id, timestamp, and model.\\n- `usage`: Request token usage.\\n- `finishReason`: Request finish reason. For example 'length' if model generated maximum number of tokens, this could result in a JSON parsing error.\\n- `cause`: The cause of the error (e.g. a JSON parsing error). You can use this for more detailed error handling.\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoObjectGeneratedError` using:\\n\\n```typescript\\nimport { generateObject, NoObjectGeneratedError } from 'ai';\\n\\ntry {\\n  await generateObject({ model, schema, prompt });\\n} catch (error) {\\n  if (NoObjectGeneratedError.isInstance(error)) {\\n    console.log('NoObjectGeneratedError');\\n    console.log('Cause:', error.cause);\\n    console.log('Text:', error.text);\\n    console.log('Response:', error.response);\\n    console.log('Usage:', error.usage);\\n    console.log('Finish Reason:', error.finishReason);\\n  }\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-speech-generated-error.mdx'), name='ai-no-speech-generated-error.mdx', displayName='ai-no-speech-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSpeechGeneratedError\\ndescription: Learn how to fix AI_NoSpeechGeneratedError\\n---\\n\\n# AI_NoSpeechGeneratedError\\n\\nThis error occurs when no audio could be generated from the input.\\n\\n## Properties\\n\\n- `responses`: Array of responses\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSpeechGeneratedError` using:\\n\\n```typescript\\nimport { NoSpeechGeneratedError } from 'ai';\\n\\nif (NoSpeechGeneratedError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-such-model-error.mdx'), name='ai-no-such-model-error.mdx', displayName='ai-no-such-model-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSuchModelError\\ndescription: Learn how to fix AI_NoSuchModelError\\n---\\n\\n# AI_NoSuchModelError\\n\\nThis error occurs when a model ID is not found.\\n\\n## Properties\\n\\n- `modelId`: The ID of the model that was not found\\n- `modelType`: The type of model\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSuchModelError` using:\\n\\n```typescript\\nimport { NoSuchModelError } from 'ai';\\n\\nif (NoSuchModelError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-such-provider-error.mdx'), name='ai-no-such-provider-error.mdx', displayName='ai-no-such-provider-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSuchProviderError\\ndescription: Learn how to fix AI_NoSuchProviderError\\n---\\n\\n# AI_NoSuchProviderError\\n\\nThis error occurs when a provider ID is not found.\\n\\n## Properties\\n\\n- `providerId`: The ID of the provider that was not found\\n- `availableProviders`: Array of available provider IDs\\n- `modelId`: The ID of the model\\n- `modelType`: The type of model\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSuchProviderError` using:\\n\\n```typescript\\nimport { NoSuchProviderError } from 'ai';\\n\\nif (NoSuchProviderError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-such-tool-error.mdx'), name='ai-no-such-tool-error.mdx', displayName='ai-no-such-tool-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoSuchToolError\\ndescription: Learn how to fix AI_NoSuchToolError\\n---\\n\\n# AI_NoSuchToolError\\n\\nThis error occurs when a model tries to call an unavailable tool.\\n\\n## Properties\\n\\n- `toolName`: The name of the tool that was not found\\n- `availableTools`: Array of available tool names\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoSuchToolError` using:\\n\\n```typescript\\nimport { NoSuchToolError } from 'ai';\\n\\nif (NoSuchToolError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-no-transcript-generated-error.mdx'), name='ai-no-transcript-generated-error.mdx', displayName='ai-no-transcript-generated-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_NoTranscriptGeneratedError\\ndescription: Learn how to fix AI_NoTranscriptGeneratedError\\n---\\n\\n# AI_NoTranscriptGeneratedError\\n\\nThis error occurs when no transcript could be generated from the input.\\n\\n## Properties\\n\\n- `responses`: Array of responses\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_NoTranscriptGeneratedError` using:\\n\\n```typescript\\nimport { NoTranscriptGeneratedError } from 'ai';\\n\\nif (NoTranscriptGeneratedError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-retry-error.mdx'), name='ai-retry-error.mdx', displayName='ai-retry-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_RetryError\\ndescription: Learn how to fix AI_RetryError\\n---\\n\\n# AI_RetryError\\n\\nThis error occurs when a retry operation fails.\\n\\n## Properties\\n\\n- `reason`: The reason for the retry failure\\n- `lastError`: The most recent error that occurred during retries\\n- `errors`: Array of all errors that occurred during retry attempts\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_RetryError` using:\\n\\n```typescript\\nimport { RetryError } from 'ai';\\n\\nif (RetryError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-too-many-embedding-values-for-call-error.mdx'), name='ai-too-many-embedding-values-for-call-error.mdx', displayName='ai-too-many-embedding-values-for-call-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_TooManyEmbeddingValuesForCallError\\ndescription: Learn how to fix AI_TooManyEmbeddingValuesForCallError\\n---\\n\\n# AI_TooManyEmbeddingValuesForCallError\\n\\nThis error occurs when too many values are provided in a single embedding call.\\n\\n## Properties\\n\\n- `provider`: The AI provider name\\n- `modelId`: The ID of the embedding model\\n- `maxEmbeddingsPerCall`: The maximum number of embeddings allowed per call\\n- `values`: The array of values that was provided\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_TooManyEmbeddingValuesForCallError` using:\\n\\n```typescript\\nimport { TooManyEmbeddingValuesForCallError } from 'ai';\\n\\nif (TooManyEmbeddingValuesForCallError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-tool-call-repair-error.mdx'), name='ai-tool-call-repair-error.mdx', displayName='ai-tool-call-repair-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: ToolCallRepairError\\ndescription: Learn how to fix AI SDK ToolCallRepairError\\n---\\n\\n# ToolCallRepairError\\n\\nThis error occurs when there is a failure while attempting to repair an invalid tool call.\\nThis typically happens when the AI attempts to fix either\\na `NoSuchToolError` or `InvalidToolInputError`.\\n\\n## Properties\\n\\n- `originalError`: The original error that triggered the repair attempt (either `NoSuchToolError` or `InvalidToolInputError`)\\n- `message`: The error message\\n- `cause`: The underlying error that caused the repair to fail\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `ToolCallRepairError` using:\\n\\n```typescript\\nimport { ToolCallRepairError } from 'ai';\\n\\nif (ToolCallRepairError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-type-validation-error.mdx'), name='ai-type-validation-error.mdx', displayName='ai-type-validation-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_TypeValidationError\\ndescription: Learn how to fix AI_TypeValidationError\\n---\\n\\n# AI_TypeValidationError\\n\\nThis error occurs when type validation fails.\\n\\n## Properties\\n\\n- `value`: The value that failed validation\\n- `message`: The error message including validation details\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_TypeValidationError` using:\\n\\n```typescript\\nimport { TypeValidationError } from 'ai';\\n\\nif (TypeValidationError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/ai-unsupported-functionality-error.mdx'), name='ai-unsupported-functionality-error.mdx', displayName='ai-unsupported-functionality-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: AI_UnsupportedFunctionalityError\\ndescription: Learn how to fix AI_UnsupportedFunctionalityError\\n---\\n\\n# AI_UnsupportedFunctionalityError\\n\\nThis error occurs when functionality is not unsupported.\\n\\n## Properties\\n\\n- `functionality`: The name of the unsupported functionality\\n- `message`: The error message\\n\\n## Checking for this Error\\n\\nYou can check if an error is an instance of `AI_UnsupportedFunctionalityError` using:\\n\\n```typescript\\nimport { UnsupportedFunctionalityError } from 'ai';\\n\\nif (UnsupportedFunctionalityError.isInstance(error)) {\\n  // Handle the error\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('07-reference/05-ai-sdk-errors/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: AI SDK Errors\\ndescription: Troubleshooting information for common AI SDK errors.\\ncollapsed: true\\n---\\n\\n# AI SDK Errors\\n\\n- [AI_APICallError](/docs/reference/ai-sdk-errors/ai-api-call-error)\\n- [AI_DownloadError](/docs/reference/ai-sdk-errors/ai-download-error)\\n- [AI_EmptyResponseBodyError](/docs/reference/ai-sdk-errors/ai-empty-response-body-error)\\n- [AI_InvalidArgumentError](/docs/reference/ai-sdk-errors/ai-invalid-argument-error)\\n- [AI_InvalidDataContent](/docs/reference/ai-sdk-errors/ai-invalid-data-content)\\n- [AI_InvalidDataContentError](/docs/reference/ai-sdk-errors/ai-invalid-data-content-error)\\n- [AI_InvalidMessageRoleError](/docs/reference/ai-sdk-errors/ai-invalid-message-role-error)\\n- [AI_InvalidPromptError](/docs/reference/ai-sdk-errors/ai-invalid-prompt-error)\\n- [AI_InvalidResponseDataError](/docs/reference/ai-sdk-errors/ai-invalid-response-data-error)\\n- [AI_InvalidToolInputError](/docs/reference/ai-sdk-errors/ai-invalid-tool-input-error)\\n- [AI_JSONParseError](/docs/reference/ai-sdk-errors/ai-json-parse-error)\\n- [AI_LoadAPIKeyError](/docs/reference/ai-sdk-errors/ai-load-api-key-error)\\n- [AI_LoadSettingError](/docs/reference/ai-sdk-errors/ai-load-setting-error)\\n- [AI_MessageConversionError](/docs/reference/ai-sdk-errors/ai-message-conversion-error)\\n- [AI_NoSpeechGeneratedError](/docs/reference/ai-sdk-errors/ai-no-speech-generated-error)\\n- [AI_NoContentGeneratedError](/docs/reference/ai-sdk-errors/ai-no-content-generated-error)\\n- [AI_NoImageGeneratedError](/docs/reference/ai-sdk-errors/ai-no-image-generated-error)\\n- [AI_NoTranscriptGeneratedError](/docs/reference/ai-sdk-errors/ai-no-transcript-generated-error)\\n- [AI_NoObjectGeneratedError](/docs/reference/ai-sdk-errors/ai-no-object-generated-error)\\n- [AI_NoOutputSpecifiedError](/docs/reference/ai-sdk-errors/ai-no-output-specified-error)\\n- [AI_NoSuchModelError](/docs/reference/ai-sdk-errors/ai-no-such-model-error)\\n- [AI_NoSuchProviderError](/docs/reference/ai-sdk-errors/ai-no-such-provider-error)\\n- [AI_NoSuchToolError](/docs/reference/ai-sdk-errors/ai-no-such-tool-error)\\n- [AI_RetryError](/docs/reference/ai-sdk-errors/ai-retry-error)\\n- [AI_ToolCallRepairError](/docs/reference/ai-sdk-errors/ai-tool-call-repair-error)\\n- [AI_TooManyEmbeddingValuesForCallError](/docs/reference/ai-sdk-errors/ai-too-many-embedding-values-for-call-error)\\n- [AI_TypeValidationError](/docs/reference/ai-sdk-errors/ai-type-validation-error)\\n- [AI_UnsupportedFunctionalityError](/docs/reference/ai-sdk-errors/ai-unsupported-functionality-error)\\n', children=[])]), DocItem(origPath=Path('07-reference/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Reference\\ndescription: Reference documentation for the AI SDK\\n---\\n\\n# API Reference\\n\\n<IndexCards\\n  cards={[\\n    {\\n      title: 'AI SDK Core',\\n      description: 'Switch between model providers without changing your code.',\\n      href: '/docs/reference/ai-sdk-core',\\n    },\\n    {\\n      title: 'AI SDK RSC',\\n      description:\\n        'Use React Server Components to stream user interfaces to the client.',\\n      href: '/docs/reference/ai-sdk-rsc',\\n    },\\n    {\\n      title: 'AI SDK UI',\\n      description:\\n        'Use hooks to integrate user interfaces that interact with language models.',\\n      href: '/docs/reference/ai-sdk-ui',\\n    },\\n    {\\n      title: 'Stream Helpers',\\n      description:\\n        'Use special functions that help stream model generations from various providers.',\\n      href: '/docs/reference/stream-helpers',\\n    },\\n  ]}\\n/>\\n\", children=[])]),\n",
       " DocItem(origPath=Path('09-troubleshooting'), name='09-troubleshooting', displayName='09-troubleshooting', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='', children=[DocItem(origPath=Path('09-troubleshooting/01-azure-stream-slow.mdx'), name='01-azure-stream-slow.mdx', displayName='01-azure-stream-slow.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Azure OpenAI Slow to Stream\\ndescription: Learn to troubleshoot Azure OpenAI slow to stream issues.\\n---\\n\\n# Azure OpenAI Slow To Stream\\n\\n## Issue\\n\\nWhen using OpenAI hosted on Azure, streaming is slow and in big chunks.\\n\\n## Cause\\n\\nThis is a Microsoft Azure issue. Some users have reported the following solutions:\\n\\n- **Update Content Filtering Settings**:\\n  Inside [Azure AI Studio](https://ai.azure.com/), within \"Shared resources\" > \"Content filters\", create a new\\n  content filter and set the \"Streaming mode (Preview)\" under \"Output filter\" from \"Default\"\\n  to \"Asynchronous Filter\".\\n\\n## Solution\\n\\nYou can use the [`smoothStream` transformation](/docs/ai-sdk-core/generating-text#smoothing-streams) to stream each word individually.\\n\\n```tsx highlight=\"6\"\\nimport { smoothStream, streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model,\\n  prompt,\\n  experimental_transform: smoothStream(),\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/02-client-side-function-calls-not-invoked.mdx'), name='02-client-side-function-calls-not-invoked.mdx', displayName='02-client-side-function-calls-not-invoked.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Client-Side Function Calls Not Invoked\\ndescription: Troubleshooting client-side function calls not being invoked.\\n---\\n\\n# Client-Side Function Calls Not Invoked\\n\\n## Issue\\n\\nI upgraded the AI SDK to v3.0.20 or newer. I am using [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream). Client-side function calls are no longer invoked.\\n\\n## Solution\\n\\nYou will need to add a stub for `experimental_onFunctionCall` to [`OpenAIStream`](/docs/reference/stream-helpers/openai-stream) to enable the correct forwarding of the function calls to the client.\\n\\n```tsx\\nconst stream = OpenAIStream(response, {\\n  async experimental_onFunctionCall() {\\n    return;\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/03-server-actions-in-client-components.mdx'), name='03-server-actions-in-client-components.mdx', displayName='03-server-actions-in-client-components.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Server Actions in Client Components\\ndescription: Troubleshooting errors related to server actions in client components.\\n---\\n\\n# Server Actions in Client Components\\n\\nYou may use Server Actions in client components, but sometimes you may encounter the following issues.\\n\\n## Issue\\n\\nIt is not allowed to define inline `\"use server\"` annotated Server Actions in Client Components.\\n\\n## Solution\\n\\nTo use Server Actions in a Client Component, you can either:\\n\\n- Export them from a separate file with `\"use server\"` at the top.\\n- Pass them down through props from a Server Component.\\n- Implement a combination of [`createAI`](/docs/reference/ai-sdk-rsc/create-ai) and [`useActions`](/docs/reference/ai-sdk-rsc/use-actions) hooks to access them.\\n\\nLearn more about [Server Actions and Mutations](https://nextjs.org/docs/app/api-reference/functions/server-actions#with-client-components).\\n\\n```ts file=\\'actions.ts\\'\\n\\'use server\\';\\n\\nimport { generateText } from \\'ai\\';\\n\\nexport async function getAnswer(question: string) {\\n  \\'use server\\';\\n\\n  const { text } = await generateText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    prompt: question,\\n  });\\n\\n  return { answer: text };\\n}\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/04-strange-stream-output.mdx'), name='04-strange-stream-output.mdx', displayName='04-strange-stream-output.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat/useCompletion stream output contains 0:... instead of text\\ndescription: How to fix strange stream output in the UI\\n---\\n\\n# useChat/useCompletion stream output contains 0:... instead of text\\n\\n## Issue\\n\\nI am using custom client code to process a server response that is sent using [`StreamingTextResponse`](/docs/reference/stream-helpers/streaming-text-response). I am using version `3.0.20` or newer of the AI SDK. When I send a query, the UI streams text such as `0: \"Je\"`, `0: \" suis\"`, `0: \"des\"...` instead of the text that I’m looking for.\\n\\n## Background\\n\\nThe AI SDK has switched to the stream data protocol in version `3.0.20`. It sends different stream parts to support data, tool calls, etc. What you see is the raw stream data protocol response.\\n\\n## Solution\\n\\nYou have several options:\\n\\n1. Use the AI Core [`streamText`](/docs/reference/ai-sdk-core/stream-text) function to send a raw text stream:\\n\\n   ```tsx\\n   export async function POST(req: Request) {\\n     const { prompt } = await req.json();\\n\\n     const result = streamText({\\n       model: openai.completion(\\'gpt-3.5-turbo-instruct\\'),\\n       maxOutputTokens: 2000,\\n       prompt,\\n     });\\n\\n     return result.toTextStreamResponse();\\n   }\\n   ```\\n\\n2. Pin the AI SDK version to `3.0.19` . This will keep the raw text stream.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/05-streamable-ui-errors.mdx'), name='05-streamable-ui-errors.mdx', displayName='05-streamable-ui-errors.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streamable UI Errors\\ndescription: Troubleshooting errors related to streamable UI.\\n---\\n\\n# Streamable UI Component Error\\n\\n## Issue\\n\\n- Variable Not Found\\n- Cannot find `div`\\n- `Component` refers to a value, but is being used as a type\\n\\n## Solution\\n\\nIf you encounter these errors when working with streamable UIs within server actions, it is likely because the file ends in `.ts` instead of `.tsx`.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/05-tool-invocation-missing-result.mdx'), name='05-tool-invocation-missing-result.mdx', displayName='05-tool-invocation-missing-result.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Tool Invocation Missing Result Error\\ndescription: How to fix the \"ToolInvocation must have a result\" error when using tools without execute functions\\n---\\n\\n# Tool Invocation Missing Result Error\\n\\n## Issue\\n\\nWhen using `generateText()` or `streamText()`, you may encounter the error \"ToolInvocation must have a result\" when a tool without an `execute` function is called.\\n\\n## Cause\\n\\nThe error occurs when you define a tool without an `execute` function and don\\'t provide the result through other means (like `useChat`\\'s `onToolCall` or `addToolOutput` functions).\\n\\nEach time a tool is invoked, the model expects to receive a result before continuing the conversation. Without a result, the model cannot determine if the tool call succeeded or failed and the conversation state becomes invalid.\\n\\n## Solution\\n\\nYou have two options for handling tool results:\\n\\n1. Server-side execution using tools with an `execute` function:\\n\\n```tsx\\nconst tools = {\\n  weather: tool({\\n    description: \\'Get the weather in a location\\',\\n    parameters: z.object({\\n      location: z\\n        .string()\\n        .describe(\\'The city and state, e.g. \"San Francisco, CA\"\\'),\\n    }),\\n    execute: async ({ location }) => {\\n      // Fetch and return weather data\\n      return { temperature: 72, conditions: \\'sunny\\', location };\\n    },\\n  }),\\n};\\n```\\n\\n2. Client-side execution with `useChat` (omitting the `execute` function), you must provide results using `addToolOutput`:\\n\\n```tsx\\nimport { useChat } from \\'@ai-sdk/react\\';\\nimport {\\n  DefaultChatTransport,\\n  lastAssistantMessageIsCompleteWithToolCalls,\\n} from \\'ai\\';\\n\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  // Automatically submit when all tool results are available\\n  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n\\n  // Handle tool calls in onToolCall\\n  onToolCall: async ({ toolCall }) => {\\n    if (toolCall.toolName === \\'getLocation\\') {\\n      try {\\n        const result = await getLocationData();\\n\\n        // Important: Don\\'t await inside onToolCall to avoid deadlocks\\n        addToolOutput({\\n          tool: \\'getLocation\\',\\n          toolCallId: toolCall.toolCallId,\\n          output: result,\\n        });\\n      } catch (err) {\\n        // Important: Don\\'t await inside onToolCall to avoid deadlocks\\n        addToolOutput({\\n          tool: \\'getLocation\\',\\n          toolCallId: toolCall.toolCallId,\\n          state: \\'output-error\\',\\n          errorText: \\'Failed to get location\\',\\n        });\\n      }\\n    }\\n  },\\n});\\n```\\n\\n```tsx\\n// For interactive UI elements:\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  transport: new DefaultChatTransport({ api: \\'/api/chat\\' }),\\n  sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\\n});\\n\\n// Inside your JSX, when rendering tool calls:\\n<button\\n  onClick={() =>\\n    addToolOutput({\\n      tool: \\'myTool\\',\\n      toolCallId, // must provide tool call ID\\n      output: {\\n        /* your tool result */\\n      },\\n    })\\n  }\\n>\\n  Confirm\\n</button>;\\n```\\n\\n<Note type=\"warning\">\\n  Whether handling tools on the server or client, each tool call must have a\\n  corresponding result before the conversation can continue.\\n</Note>\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/06-streaming-not-working-when-deployed.mdx'), name='06-streaming-not-working-when-deployed.mdx', displayName='06-streaming-not-working-when-deployed.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Streaming Not Working When Deployed\\ndescription: Troubleshooting streaming issues in deployed apps.\\n---\\n\\n# Streaming Not Working When Deployed\\n\\n## Issue\\n\\nStreaming with the AI SDK works in my local development environment.\\nHowever, when deploying, streaming does not work in the deployed app.\\nInstead of streaming, only the full response is returned after a while.\\n\\n## Cause\\n\\nThe causes of this issue are varied and depend on the deployment environment.\\n\\n## Solution\\n\\nYou can try the following:\\n\\n- add `'Transfer-Encoding': 'chunked'` and/or `Connection: 'keep-alive'` headers\\n\\n  ```tsx\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      'Transfer-Encoding': 'chunked',\\n      Connection: 'keep-alive',\\n    },\\n  });\\n  ```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/06-streaming-not-working-when-proxied.mdx'), name='06-streaming-not-working-when-proxied.mdx', displayName='06-streaming-not-working-when-proxied.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Streaming Not Working When Proxied\\ndescription: Troubleshooting streaming issues in proxied apps.\\n---\\n\\n# Streaming Not Working When Proxied\\n\\n## Issue\\n\\nStreaming with the AI SDK doesn't work in local development environment, or deployed in some proxy environments.\\nInstead of streaming, only the full response is returned after a while.\\n\\n## Cause\\n\\nThe causes of this issue are caused by the proxy middleware.\\n\\nIf the middleware is configured to compress the response, it will cause the streaming to fail.\\n\\n## Solution\\n\\nYou can try the following, the solution only affects the streaming API:\\n\\n- add `'Content-Encoding': 'none'` headers\\n\\n  ```tsx\\n  return result.toUIMessageStreamResponse({\\n    headers: {\\n      'Content-Encoding': 'none',\\n    },\\n  });\\n  ```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/06-timeout-on-vercel.mdx'), name='06-timeout-on-vercel.mdx', displayName='06-timeout-on-vercel.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Getting Timeouts When Deploying on Vercel\\ndescription: Learn how to fix timeouts and cut off responses when deploying to Vercel.\\n---\\n\\n# Getting Timeouts When Deploying on Vercel\\n\\n## Issue\\n\\nStreaming with the AI SDK works in my local development environment.\\nHowever, when I\\'m deploying to Vercel, longer responses get chopped off in the UI and I\\'m seeing timeouts in the Vercel logs or I\\'m seeing the error: `Uncaught (in promise) Error: Connection closed`.\\n\\n## Solution\\n\\nWith Vercel\\'s [Fluid Compute](https://vercel.com/docs/fluid-compute), the default function duration is now **5 minutes (300 seconds)** across all plans. This should be sufficient for most streaming applications.\\n\\nIf you need to extend the timeout for longer-running processes, you can increase the `maxDuration` setting:\\n\\n### Next.js (App Router)\\n\\nAdd the following to your route file or the page you are calling your Server Action from:\\n\\n```tsx\\nexport const maxDuration = 600;\\n```\\n\\n<Note>\\n  Setting `maxDuration` above 300 seconds requires a Pro or Enterprise plan.\\n</Note>\\n\\n### Other Frameworks\\n\\nFor other frameworks, you can set timeouts in your `vercel.json` file:\\n\\n```json\\n{\\n  \"functions\": {\\n    \"api/chat/route.ts\": {\\n      \"maxDuration\": 600\\n    }\\n  }\\n}\\n```\\n\\n<Note>\\n  Setting `maxDuration` above 300 seconds requires a Pro or Enterprise plan.\\n</Note>\\n\\n### Maximum Duration Limits\\n\\nThe maximum duration you can set depends on your Vercel plan:\\n\\n- **Hobby**: Up to 300 seconds (5 minutes)\\n- **Pro**: Up to 800 seconds (~13 minutes)\\n- **Enterprise**: Up to 800 seconds (~13 minutes)\\n\\n## Learn more\\n\\n- [Fluid Compute Default Settings](https://vercel.com/docs/fluid-compute#default-settings-by-plan)\\n- [Configuring Maximum Duration for Vercel Functions](https://vercel.com/docs/functions/configuring-functions/duration)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/07-unclosed-streams.mdx'), name='07-unclosed-streams.mdx', displayName='07-unclosed-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Unclosed Streams\\ndescription: Troubleshooting errors related to unclosed streams.\\n---\\n\\n# Unclosed Streams\\n\\nSometimes streams are not closed properly, which can lead to unexpected behavior. The following are some common issues that can occur when streams are not closed properly.\\n\\n## Issue\\n\\nThe streamable UI has been slow to update.\\n\\n## Solution\\n\\nThis happens when you create a streamable UI using [`createStreamableUI`](/docs/reference/ai-sdk-rsc/create-streamable-ui) and fail to close the stream.\\nIn order to fix this, you must ensure you close the stream by calling the [`.done()`](/docs/reference/ai-sdk-rsc/create-streamable-ui#done) method.\\nThis will ensure the stream is closed.\\n\\n```tsx file='app/actions.tsx'\\nimport { createStreamableUI } from '@ai-sdk/rsc';\\n\\nconst submitMessage = async () => {\\n  'use server';\\n\\n  const stream = createStreamableUI('1');\\n\\n  stream.update('2');\\n  stream.append('3');\\n  stream.done('4'); // [!code ++]\\n\\n  return stream.value;\\n};\\n```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/08-use-chat-failed-to-parse-stream.mdx'), name='08-use-chat-failed-to-parse-stream.mdx', displayName='08-use-chat-failed-to-parse-stream.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat Failed to Parse Stream\\ndescription: Troubleshooting errors related to the Use Chat Failed to Parse Stream error.\\n---\\n\\n# `useChat` \"Failed to Parse Stream String\" Error\\n\\n## Issue\\n\\nI am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) or [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion), and I am getting a `\"Failed to parse stream string. Invalid code\"` error. I am using version `3.0.20` or newer of the AI SDK.\\n\\n## Background\\n\\nThe AI SDK has switched to the stream data protocol in version `3.0.20`.\\n[`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) expect stream parts that support data, tool calls, etc.\\nWhat you see is a failure to parse the stream.\\nThis can be caused by using an older version of the AI SDK in the backend, by providing a text stream using a custom provider, or by using a raw LangChain stream result.\\n\\n## Solution\\n\\nYou can switch [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and [`useCompletion`](/docs/reference/ai-sdk-ui/use-completion) to raw text stream processing with the [`streamProtocol`](/docs/reference/ai-sdk-ui/use-completion#stream-protocol) parameter.\\nSet it to `text` as follows:\\n\\n```tsx\\nconst { messages, append } = useChat({ streamProtocol: \\'text\\' });\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/09-client-stream-error.mdx'), name='09-client-stream-error.mdx', displayName='09-client-stream-error.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Server Action Plain Objects Error\\ndescription: Troubleshooting errors related to using AI SDK Core functions with Server Actions.\\n---\\n\\n# \"Only plain objects can be passed from client components\" Server Action Error\\n\\n## Issue\\n\\nI am using [`streamText`](/docs/reference/ai-sdk-core/stream-text) or [`streamObject`](/docs/reference/ai-sdk-core/stream-object) with Server Actions, and I am getting a `\"only plain objects and a few built ins can be passed from client components\"` error.\\n\\n## Background\\n\\nThis error occurs when you\\'re trying to return a non-serializable object from a Server Action to a Client Component. The streamText function likely returns an object with methods or complex structures that can\\'t be directly serialized and passed to the client.\\n\\n## Solution\\n\\nTo fix this issue, you need to ensure that you\\'re only returning serializable data from your Server Action. Here\\'s how you can modify your approach:\\n\\n1. Instead of returning the entire result object from streamText, extract only the necessary serializable data.\\n2. Use the [`createStreamableValue`](/docs/reference/ai-sdk-rsc/create-streamable-value) function to create a streamable value that can be safely passed to the client.\\n\\nHere\\'s an example that demonstrates how to implement this solution: [Streaming Text Generation](/examples/next-app/basics/streaming-text-generation).\\n\\nThis approach ensures that only serializable data (the text) is passed to the client, avoiding the \"only plain objects\" error.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/10-use-chat-tools-no-response.mdx'), name='10-use-chat-tools-no-response.mdx', displayName='10-use-chat-tools-no-response.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat No Response\\ndescription: Troubleshooting errors related to the Use Chat Failed to Parse Stream error.\\n---\\n\\n# `useChat` No Response\\n\\n## Issue\\n\\nI am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat).\\nWhen I log the incoming messages on the server, I can see the tool call and the tool result, but the model does not respond with anything.\\n\\n## Solution\\n\\nTo resolve this issue, convert the incoming messages to the `ModelMessage` format using the [`convertToModelMessages`](/docs/reference/ai-sdk-ui/convert-to-model-messages) function.\\n\\n```tsx highlight=\"9\"\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { convertToModelMessages, streamText } from \\'ai\\';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'anthropic/claude-sonnet-4.5\\',\\n    messages: convertToModelMessages(messages),\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/11-use-chat-custom-request-options.mdx'), name='11-use-chat-custom-request-options.mdx', displayName='11-use-chat-custom-request-options.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Custom headers, body, and credentials not working with useChat\\ndescription: Troubleshooting errors related to custom request configuration in useChat hook\\n---\\n\\n# Custom headers, body, and credentials not working with useChat\\n\\n## Issue\\n\\nWhen using the `useChat` hook, custom request options like headers, body fields, and credentials configured directly on the hook are not being sent with the request:\\n\\n```tsx\\n// These options are not sent with the request\\nconst { messages, sendMessage } = useChat({\\n  headers: {\\n    Authorization: 'Bearer token123',\\n  },\\n  body: {\\n    user_id: '123',\\n  },\\n  credentials: 'include',\\n});\\n```\\n\\n## Background\\n\\nThe `useChat` hook has changed its API for configuring request options. Direct options like `headers`, `body`, and `credentials` on the hook itself are no longer supported. Instead, you need to use the `transport` configuration with `DefaultChatTransport` or pass options at the request level.\\n\\n## Solution\\n\\nThere are three ways to properly configure request options with `useChat`:\\n\\n### Option 1: Request-Level Configuration (Recommended for Dynamic Values)\\n\\nFor dynamic values that change over time, the recommended approach is to pass options when calling `sendMessage`:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat();\\n\\n// Send options with each message\\nsendMessage(\\n  { text: input },\\n  {\\n    headers: {\\n      Authorization: `Bearer ${getAuthToken()}`, // Dynamic auth token\\n      'X-Request-ID': generateRequestId(),\\n    },\\n    body: {\\n      temperature: 0.7,\\n      max_tokens: 100,\\n      user_id: getCurrentUserId(), // Dynamic user ID\\n      sessionId: getCurrentSessionId(), // Dynamic session\\n    },\\n  },\\n);\\n```\\n\\nThis approach ensures that the most up-to-date values are always sent with each request.\\n\\n### Option 2: Hook-Level Configuration with Static Values\\n\\nFor static values that don't change during the component lifecycle, use the `DefaultChatTransport`:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: {\\n      'X-API-Version': 'v1', // Static API version\\n      'X-App-ID': 'my-app', // Static app identifier\\n    },\\n    body: {\\n      model: 'gpt-5.1', // Default model\\n      stream: true, // Static configuration\\n    },\\n    credentials: 'include', // Static credentials policy\\n  }),\\n});\\n```\\n\\n### Option 3: Hook-Level Configuration with Resolvable Functions\\n\\nIf you need dynamic values at the hook level, you can use functions that return configuration values. However, request-level configuration is generally preferred for better reliability:\\n\\n```tsx\\nimport { useChat } from '@ai-sdk/react';\\nimport { DefaultChatTransport } from 'ai';\\n\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: () => ({\\n      Authorization: `Bearer ${getAuthToken()}`,\\n      'X-User-ID': getCurrentUserId(),\\n    }),\\n    body: () => ({\\n      sessionId: getCurrentSessionId(),\\n      preferences: getUserPreferences(),\\n    }),\\n    credentials: () => (isAuthenticated() ? 'include' : 'same-origin'),\\n  }),\\n});\\n```\\n\\n<Note>\\n  For component state that changes over time, request-level configuration\\n  (Option 1) is recommended. If using hook-level functions, consider using\\n  `useRef` to store current values and reference `ref.current` in your\\n  configuration function.\\n</Note>\\n\\n### Combining Hook and Request Level Options\\n\\nRequest-level options take precedence over hook-level options:\\n\\n```tsx\\n// Hook-level default configuration\\nconst { messages, sendMessage } = useChat({\\n  transport: new DefaultChatTransport({\\n    api: '/api/chat',\\n    headers: {\\n      'X-API-Version': 'v1',\\n    },\\n    body: {\\n      model: 'gpt-5.1',\\n    },\\n  }),\\n});\\n\\n// Override or add options per request\\nsendMessage(\\n  { text: input },\\n  {\\n    headers: {\\n      'X-API-Version': 'v2', // This overrides the hook-level header\\n      'X-Request-ID': '123', // This is added\\n    },\\n    body: {\\n      model: 'gpt-5-mini', // This overrides the hook-level body field\\n      temperature: 0.5, // This is added\\n    },\\n  },\\n);\\n```\\n\\nFor more details on request configuration, see the [Request Configuration](/docs/ai-sdk-ui/chatbot#request-configuration) documentation.\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/12-typescript-performance-zod.mdx'), name='12-typescript-performance-zod.mdx', displayName='12-typescript-performance-zod.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: TypeScript performance issues with Zod and AI SDK 5\\ndescription: Troubleshooting TypeScript server crashes and slow performance when using Zod with AI SDK 5\\n---\\n\\n# TypeScript performance issues with Zod and AI SDK 5\\n\\n## Issue\\n\\nWhen using the AI SDK 5 with Zod, you may experience:\\n\\n- TypeScript server crashes or hangs\\n- Extremely slow type checking in files that import AI SDK functions\\n- Error messages like \"Type instantiation is excessively deep and possibly infinite\"\\n- IDE becoming unresponsive when working with AI SDK code\\n\\n## Background\\n\\nThe AI SDK 5 has specific compatibility requirements with Zod versions. When importing Zod using the standard import path (`import { z } from \\'zod\\'`), TypeScript\\'s type inference can become excessively complex, leading to performance degradation or crashes.\\n\\n## Solution\\n\\n### Upgrade Zod to 4.1.8 or Later\\n\\nThe primary solution is to upgrade to Zod version 4.1.8 or later, which includes a fix for this module resolution issue:\\n\\n```bash\\npnpm add zod@^4.1.8\\n```\\n\\nThis version resolves the underlying problem where different module resolution settings were causing TypeScript to load the same Zod declarations twice, leading to expensive structural comparisons.\\n\\n### Alternative: Update TypeScript Configuration\\n\\nIf upgrading Zod isn\\'t possible, you can update your `tsconfig.json` to use `moduleResolution: \"nodenext\"`:\\n\\n```json\\n{\\n  \"compilerOptions\": {\\n    \"moduleResolution\": \"nodenext\"\\n    // ... other options\\n  }\\n}\\n```\\n\\nThis resolves the TypeScript performance issues while allowing you to continue using the standard Zod import.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/12-use-chat-an-error-occurred.mdx'), name='12-use-chat-an-error-occurred.mdx', displayName='12-use-chat-an-error-occurred.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: useChat \"An error occurred\"\\ndescription: Troubleshooting errors related to the \"An error occurred\" error in useChat.\\n---\\n\\n# `useChat` \"An error occurred\"\\n\\n## Issue\\n\\nI am using [`useChat`](/docs/reference/ai-sdk-ui/use-chat) and I get the error \"An error occurred\".\\n\\n## Background\\n\\nError messages from `streamText` are masked by default when using `toDataStreamResponse` for security reasons (secure-by-default).\\nThis prevents leaking sensitive information to the client.\\n\\n## Solution\\n\\nTo forward error details to the client or to log errors, use the `getErrorMessage` function when calling `toDataStreamResponse`.\\n\\n```tsx\\nexport function errorHandler(error: unknown) {\\n  if (error == null) {\\n    return \\'unknown error\\';\\n  }\\n\\n  if (typeof error === \\'string\\') {\\n    return error;\\n  }\\n\\n  if (error instanceof Error) {\\n    return error.message;\\n  }\\n\\n  return JSON.stringify(error);\\n}\\n```\\n\\n```tsx\\nconst result = streamText({\\n  // ...\\n});\\n\\nreturn result.toUIMessageStreamResponse({\\n  getErrorMessage: errorHandler,\\n});\\n```\\n\\nIn case you are using `createDataStreamResponse`, you can use the `onError` function when calling `toDataStreamResponse`:\\n\\n```tsx\\nconst response = createDataStreamResponse({\\n  // ...\\n  async execute(dataStream) {\\n    // ...\\n  },\\n  onError: errorHandler,\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/13-repeated-assistant-messages.mdx'), name='13-repeated-assistant-messages.mdx', displayName='13-repeated-assistant-messages.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Repeated assistant messages in useChat\\ndescription: Troubleshooting duplicate assistant messages when using useChat with streamText\\n---\\n\\n# Repeated assistant messages in useChat\\n\\n## Issue\\n\\nWhen using `useChat` with `streamText` on the server, the assistant's messages appear duplicated in the UI - showing both the previous message and the new message, or showing the same message multiple times. This can occur when using tool calls or complex message flows.\\n\\n```tsx\\n// Server-side code that may experience assistant message duplication on the client\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'openai/gpt-5-mini',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: {\\n        description: 'Get the weather for a location',\\n        parameters: z.object({\\n          location: z.string(),\\n        }),\\n        execute: async ({ location }) => {\\n          return { temperature: 72, condition: 'sunny' };\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\n## Background\\n\\nThe duplication occurs because `toUIMessageStreamResponse` generates new message IDs for each new message.\\n\\n## Solution\\n\\nPass the original messages array to `toUIMessageStreamResponse` using the `originalMessages` option. By passing `originalMessages`, the method can reuse existing message IDs instead of generating new ones, ensuring the client properly updates existing messages rather than creating duplicates.\\n\\n```tsx\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'openai/gpt-5-mini',\\n    messages: convertToModelMessages(messages),\\n    tools: {\\n      weather: {\\n        description: 'Get the weather for a location',\\n        parameters: z.object({\\n          location: z.string(),\\n        }),\\n        execute: async ({ location }) => {\\n          return { temperature: 72, condition: 'sunny' };\\n        },\\n      },\\n    },\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    originalMessages: messages, // Pass the original messages here\\n    generateMessageId: generateId,\\n    onFinish: ({ messages }) => {\\n      saveChat({ id, messages });\\n    },\\n  });\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/14-stream-abort-handling.mdx'), name='14-stream-abort-handling.mdx', displayName='14-stream-abort-handling.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: onFinish not called when stream is aborted\\ndescription: Troubleshooting onFinish callback not executing when streams are aborted with toUIMessageStreamResponse\\n---\\n\\n# onFinish not called when stream is aborted\\n\\n## Issue\\n\\nWhen using `toUIMessageStreamResponse` with an `onFinish` callback, the callback may not execute when the stream is aborted. This happens because the abort handler immediately terminates the response, preventing the `onFinish` callback from being triggered.\\n\\n```tsx\\n// Server-side code where onFinish isn't called on abort\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages: convertToModelMessages(messages),\\n    abortSignal: req.signal,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onFinish: async ({ isAborted }) => {\\n      // This isn't called when the stream is aborted!\\n      if (isAborted) {\\n        console.log('Stream was aborted');\\n        // Handle abort-specific cleanup\\n      } else {\\n        console.log('Stream completed normally');\\n        // Handle normal completion\\n      }\\n    },\\n  });\\n}\\n```\\n\\n## Background\\n\\nWhen a stream is aborted, the response is immediately terminated. Without proper handling, the `onFinish` callback has no chance to execute, preventing important cleanup operations like saving partial results or logging abort events.\\n\\n## Solution\\n\\nAdd `consumeStream` to the `toUIMessageStreamResponse` configuration. This ensures that abort events are properly captured and forwarded to the `onFinish` callback, allowing it to execute even when the stream is aborted.\\n\\n```tsx\\n// other imports...\\nimport { consumeStream } from 'ai';\\n\\nexport async function POST(req: Request) {\\n  const { messages } = await req.json();\\n\\n  const result = streamText({\\n    model: 'anthropic/claude-sonnet-4.5',\\n    messages: convertToModelMessages(messages),\\n    abortSignal: req.signal,\\n  });\\n\\n  return result.toUIMessageStreamResponse({\\n    onFinish: async ({ isAborted }) => {\\n      // Now this WILL be called even when aborted!\\n      if (isAborted) {\\n        console.log('Stream was aborted');\\n        // Handle abort-specific cleanup\\n      } else {\\n        console.log('Stream completed normally');\\n        // Handle normal completion\\n      }\\n    },\\n    consumeSseStream: consumeStream, // This enables onFinish to be called on abort\\n  });\\n}\\n```\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/14-tool-calling-with-structured-outputs.mdx'), name='14-tool-calling-with-structured-outputs.mdx', displayName='14-tool-calling-with-structured-outputs.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Tool calling with generateObject and streamObject\\ndescription: Troubleshooting tool calling when combined with generateObject and streamObject\\n---\\n\\n# Tool calling with generateObject and streamObject (structured outputs)\\n\\n## Issue\\n\\nYou may want to combine tool calling with structured output generation. While `generateObject` and `streamObject` are designed specifically for structured outputs, they don't support tool calling.\\n\\n## Background\\n\\nTo use tool calling with structured outputs, use `generateText` or `streamText` with the `output` option.\\n\\n**Important**: When using `output` with tool calling, the structured output generation counts as an additional step in the execution flow.\\n\\n## Solution\\n\\nWhen using `output` with tool calling, adjust your `stopWhen` condition to account for the additional step required for structured output generation:\\n\\n```tsx\\nconst result = await generateText({\\n  model: 'anthropic/claude-sonnet-4.5',\\n  output: Output.object({\\n    schema: z.object({\\n      summary: z.string(),\\n      sentiment: z.enum(['positive', 'neutral', 'negative']),\\n    }),\\n  }),\\n  tools: {\\n    analyze: tool({\\n      description: 'Analyze data',\\n      inputSchema: z.object({\\n        data: z.string(),\\n      }),\\n      execute: async ({ data }) => {\\n        return { result: 'analyzed' };\\n      }),\\n    },\\n  },\\n  // Add at least 1 to your intended step count to account for structured output\\n  stopWhen: stepCountIs(3), // Now accounts for: tool call + tool result + structured output\\n  prompt: 'Analyze the data and provide a summary',\\n});\\n```\\n\\nFor more information about using structured outputs with `generateText` and `streamText` see [Generating Structured Data](/docs/ai-sdk-core/generating-structured-data#structured-outputs-with-generatetext-and-streamtext).\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/15-abort-breaks-resumable-streams.mdx'), name='15-abort-breaks-resumable-streams.mdx', displayName='15-abort-breaks-resumable-streams.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext=\"---\\ntitle: Abort breaks resumable streams\\ndescription: Troubleshooting stream resumption failures when using abort functionality\\n---\\n\\n# Abort breaks resumable streams\\n\\n## Issue\\n\\nWhen using `useChat` with `resume: true` for stream resumption, the abort functionality breaks. Closing a tab, refreshing the page, or calling the `stop()` function will trigger an abort signal that interferes with the resumption mechanism, preventing streams from being properly resumed.\\n\\n```tsx\\n// This configuration will cause conflicts\\nconst { messages, stop } = useChat({\\n  id: chatId,\\n  resume: true, // Stream resumption enabled\\n});\\n\\n// Closing the tab will trigger abort and stop resumption\\n```\\n\\n## Background\\n\\nWhen a page is closed or refreshed, the browser automatically sends an abort signal, which breaks the resumption flow.\\n\\n## Current limitations\\n\\nWe're aware of this incompatibility and are exploring solutions. **In the meantime, please choose either stream resumption or abort functionality based on your application's requirements**, but not both.\\n\\n### Option 1: Use stream resumption without abort\\n\\nIf you need to support long-running generations that persist across page reloads:\\n\\n```tsx\\nconst { messages, sendMessage } = useChat({\\n  id: chatId,\\n  resume: true,\\n});\\n```\\n\\n### Option 2: Use abort without stream resumption\\n\\nIf you need to allow users to stop streams manually:\\n\\n```tsx\\nconst { messages, sendMessage, stop } = useChat({\\n  id: chatId,\\n  resume: false, // Disable stream resumption (default behaviour)\\n});\\n```\\n\\n## Related\\n\\n- [Chatbot Resume Streams](/docs/ai-sdk-ui/chatbot-resume-streams)\\n- [Stopping Streams](/docs/advanced/stopping-streams)\\n\", children=[]), DocItem(origPath=Path('09-troubleshooting/15-stream-text-not-working.mdx'), name='15-stream-text-not-working.mdx', displayName='15-stream-text-not-working.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: streamText fails silently\\ndescription: Troubleshooting errors related to the streamText function not working.\\n---\\n\\n# `streamText` is not working\\n\\n## Issue\\n\\nI am using [`streamText`](/docs/reference/ai-sdk-core/stream-text) function, and it does not work.\\nIt does not throw any errors and the stream is only containing error parts.\\n\\n## Background\\n\\n`streamText` immediately starts streaming to enable sending data without waiting for the model.\\nErrors become part of the stream and are not thrown to prevent e.g. servers from crashing.\\n\\n## Solution\\n\\nTo log errors, you can provide an `onError` callback that is triggered when an error occurs.\\n\\n```tsx highlight=\"6-8\"\\nimport { streamText } from \\'ai\\';\\n\\nconst result = streamText({\\n  model: \\'anthropic/claude-sonnet-4.5\\',\\n  prompt: \\'Invent a new holiday and describe its traditions.\\',\\n  onError({ error }) {\\n    console.error(error); // your error logging logic here\\n  },\\n});\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/16-streaming-status-delay.mdx'), name='16-streaming-status-delay.mdx', displayName='16-streaming-status-delay.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Streaming Status Shows But No Text Appears\\ndescription: Why useChat shows \"streaming\" status without any visible content\\n---\\n\\n# Streaming Status Shows But No Text Appears\\n\\n## Issue\\n\\nWhen using `useChat`, the status changes to \"streaming\" immediately, but no text appears for several seconds.\\n\\n## Background\\n\\nThe status changes to \"streaming\" as soon as the connection to the server is established and streaming begins - this includes metadata streaming, not just the LLM\\'s generated tokens.\\n\\n## Solution\\n\\nCreate a custom loading state that checks if the last assistant message actually contains content:\\n\\n```tsx\\n\\'use client\\';\\n\\nimport { useChat } from \\'@ai-sdk/react\\';\\n\\nexport default function Page() {\\n  const { messages, status } = useChat();\\n\\n  const lastMessage = messages.at(-1);\\n\\n  const showLoader =\\n    status === \\'streaming\\' &&\\n    lastMessage?.role === \\'assistant\\' &&\\n    lastMessage?.parts?.length === 0;\\n\\n  return (\\n    <>\\n      {messages.map(message => (\\n        <div key={message.id}>\\n          {message.role === \\'user\\' ? \\'User: \\' : \\'AI: \\'}\\n          {message.parts.map((part, index) =>\\n            part.type === \\'text\\' ? <span key={index}>{part.text}</span> : null,\\n          )}\\n        </div>\\n      ))}\\n\\n      {showLoader && <div>Loading...</div>}\\n    </>\\n  );\\n}\\n```\\n\\nYou can also check for specific part types if you\\'re waiting for something specific:\\n\\n```tsx\\nconst showLoader =\\n  status === \\'streaming\\' &&\\n  lastMessage?.role === \\'assistant\\' &&\\n  !lastMessage?.parts?.some(part => part.type === \\'text\\');\\n```\\n\\n## Related Issues\\n\\n- [GitHub Issue #7586](https://github.com/vercel/ai/issues/7586)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/17-use-chat-stale-body-data.mdx'), name='17-use-chat-stale-body-data.mdx', displayName='17-use-chat-stale-body-data.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Stale body values with useChat\\ndescription: Troubleshooting stale values when passing information via the body parameter of useChat\\n---\\n\\n# Stale body values with useChat\\n\\n## Issue\\n\\nWhen using `useChat` and passing dynamic information via the `body` parameter at the hook level, the data remains stale and only reflects the value from the initial component render. This occurs because the body configuration is captured once when the hook is initialized and doesn\\'t update with subsequent component re-renders.\\n\\n```tsx\\n// Problematic code - body data will be stale\\nexport default function Chat() {\\n  const [temperature, setTemperature] = useState(0.7);\\n  const [userId, setUserId] = useState(\\'user123\\');\\n\\n  // This body configuration is captured once and won\\'t update\\n  const { messages, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n      body: {\\n        temperature, // Always the initial value (0.7)\\n        userId, // Always the initial value (\\'user123\\')\\n      },\\n    }),\\n  });\\n\\n  // Even if temperature or userId change, the body in requests will still use initial values\\n  return (\\n    <div>\\n      <input\\n        type=\"range\"\\n        value={temperature}\\n        onChange={e => setTemperature(parseFloat(e.target.value))}\\n      />\\n      {/* Chat UI */}\\n    </div>\\n  );\\n}\\n```\\n\\n## Background\\n\\nThe hook-level body configuration is evaluated once during the initial render and doesn\\'t re-evaluate when component state changes.\\n\\n## Solution\\n\\nPass dynamic variables via the second argument of the `sendMessage` function instead of at the hook level. Request-level options are evaluated on each call and take precedence over hook-level options.\\n\\n```tsx\\nexport default function Chat() {\\n  const [temperature, setTemperature] = useState(0.7);\\n  const [userId, setUserId] = useState(\\'user123\\');\\n  const [input, setInput] = useState(\\'\\');\\n\\n  const { messages, sendMessage } = useChat({\\n    // Static configuration only\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n    }),\\n  });\\n\\n  return (\\n    <div>\\n      <input\\n        type=\"range\"\\n        value={temperature}\\n        onChange={e => setTemperature(parseFloat(e.target.value))}\\n      />\\n\\n      <form\\n        onSubmit={event => {\\n          event.preventDefault();\\n          if (input.trim()) {\\n            // Pass dynamic values as request-level options\\n            sendMessage(\\n              { text: input },\\n              {\\n                body: {\\n                  temperature, // Current value at request time\\n                  userId, // Current value at request time\\n                },\\n              },\\n            );\\n            setInput(\\'\\');\\n          }\\n        }}\\n      >\\n        <input value={input} onChange={e => setInput(e.target.value)} />\\n      </form>\\n    </div>\\n  );\\n}\\n```\\n\\n### Alternative: Dynamic Hook-Level Configuration\\n\\nIf you need hook-level configuration that responds to changes, you can use functions that return configuration values. However, for component state, you\\'ll need to use `useRef` to access current values:\\n\\n```tsx\\nexport default function Chat() {\\n  const temperatureRef = useRef(0.7);\\n\\n  const { messages, sendMessage } = useChat({\\n    transport: new DefaultChatTransport({\\n      api: \\'/api/chat\\',\\n      body: () => ({\\n        temperature: temperatureRef.current, // Access via ref.current\\n        sessionId: getCurrentSessionId(), // Function calls work directly\\n      }),\\n    }),\\n  });\\n\\n  // ...\\n}\\n```\\n\\n**Recommendation:** Request-level configuration is simpler and more reliable for component state. Use it whenever you need to pass dynamic values that change during the component lifecycle.\\n\\n### Server-side handling\\n\\nOn your server side, retrieve the custom fields by destructuring the request body:\\n\\n```tsx\\n// app/api/chat/route.ts\\nexport async function POST(req: Request) {\\n  const { messages, temperature, userId } = await req.json();\\n\\n  const result = streamText({\\n    model: \\'openai/gpt-5-mini\\',\\n    messages: convertToModelMessages(messages),\\n    temperature, // Use the dynamic temperature from the request\\n    // ... other configuration\\n  });\\n\\n  return result.toUIMessageStreamResponse();\\n}\\n```\\n\\nFor more information, see [chatbot request configuration documentation](/docs/ai-sdk-ui/chatbot#request-configuration).\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/18-ontoolcall-type-narrowing.mdx'), name='18-ontoolcall-type-narrowing.mdx', displayName='18-ontoolcall-type-narrowing.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Type Error with onToolCall\\ndescription: How to handle TypeScript type errors when using the onToolCall callback\\n---\\n\\n# Type Error with onToolCall\\n\\nWhen using the `onToolCall` callback with TypeScript, you may encounter type errors when trying to pass tool properties directly to `addToolOutput`.\\n\\n## Problem\\n\\nTypeScript cannot automatically narrow the type of `toolCall.toolName` when you have both static and dynamic tools, leading to type errors:\\n\\n```tsx\\n// ❌ This causes a TypeScript error\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  async onToolCall({ toolCall }) {\\n    addToolOutput({\\n      tool: toolCall.toolName, // Type \\'string\\' is not assignable to type \\'\"yourTool\" | \"yourOtherTool\"\\'\\n      toolCallId: toolCall.toolCallId,\\n      output: someOutput,\\n    });\\n  },\\n});\\n```\\n\\nThe error occurs because:\\n\\n- Static tools have specific literal types for their names (e.g., `\"getWeatherInformation\"`)\\n- Dynamic tools have `toolName` as a generic `string`\\n- TypeScript can\\'t guarantee that `toolCall.toolName` matches your specific tool names\\n\\n## Solution\\n\\nCheck if the tool is dynamic first to enable proper type narrowing:\\n\\n```tsx\\n// ✅ Correct approach with type narrowing\\nconst { messages, sendMessage, addToolOutput } = useChat({\\n  async onToolCall({ toolCall }) {\\n    // Check if it\\'s a dynamic tool first\\n    if (toolCall.dynamic) {\\n      return;\\n    }\\n\\n    // Now TypeScript knows this is a static tool with the correct type\\n    addToolOutput({\\n      tool: toolCall.toolName, // No type error!\\n      toolCallId: toolCall.toolCallId,\\n      output: someOutput,\\n    });\\n  },\\n});\\n```\\n\\n<Note>\\n  If you\\'re still using the deprecated `addToolResult` method, this solution\\n  applies the same way. Consider migrating to `addToolOutput` for consistency\\n  with the latest API.\\n</Note>\\n\\n## Related\\n\\n- [Chatbot Tool Usage](/docs/ai-sdk-ui/chatbot-tool-usage)\\n- [Dynamic Tools](/docs/reference/ai-sdk-core/dynamic-tool)\\n- [useChat Reference](/docs/reference/ai-sdk-ui/use-chat)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/19-unsupported-model-version.mdx'), name='19-unsupported-model-version.mdx', displayName='19-unsupported-model-version.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Unsupported model version error\\ndescription: Troubleshooting the AI_UnsupportedModelVersionError when migrating to AI SDK 5\\n---\\n\\n# Unsupported model version error\\n\\n## Issue\\n\\nWhen migrating to AI SDK 5, you might encounter an error stating that your model uses an unsupported version:\\n\\n```\\nAI_UnsupportedModelVersionError: Unsupported model version v1 for provider \"ollama.chat\" and model \"gamma3:4b\".\\nAI SDK 5 only supports models that implement specification version \"v2\".\\n```\\n\\nThis error occurs because the version of the provider package you\\'re using implements the older (v1) model specification.\\n\\n## Background\\n\\nAI SDK 5 requires all provider packages to implement specification version \"v2\". When you upgrade to AI SDK 5 but don\\'t update your provider packages to compatible versions, they continue using the older \"v1\" specification, causing this error.\\n\\n## Solution\\n\\n### Update provider packages to AI SDK 5 compatible versions\\n\\nUpdate all your `@ai-sdk/*` provider packages to compatible version `2.0.0` or later. These versions implement the v2 specification required by AI SDK 5.\\n\\n```bash\\npnpm install ai@latest @ai-sdk/openai@latest @ai-sdk/anthropic@latest\\n```\\n\\nFor AI SDK 5 compatibility, you need:\\n\\n- `ai` package: `5.0.0` or later\\n- `@ai-sdk/*` packages: `2.0.0` or later (for example, `@ai-sdk/openai`, `@ai-sdk/anthropic`, `@ai-sdk/google`)\\n- `@ai-sdk/provider` package: `2.0.0` or later\\n- `zod` package: `4.1.8` or later\\n\\n### Check provider compatibility\\n\\nIf you\\'re using a third-party or custom provider, verify that it has been updated to support AI SDK 5. Not all providers may have v2-compatible versions available yet.\\n\\nTo check if a provider supports AI SDK 5:\\n\\n1. Check the provider\\'s package.json for `@ai-sdk/provider` peer dependency version `2.0.0` or later\\n2. Review the provider\\'s changelog or migration guide\\n3. Check the provider\\'s repository for AI SDK 5 support\\n\\nFor more information on migrating to AI SDK 5, see the [AI SDK 5.0 migration guide](/docs/migration-guides/migration-guide-5-0).\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/20-no-object-generated-content-filter.mdx'), name='20-no-object-generated-content-filter.mdx', displayName='20-no-object-generated-content-filter.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Object generation failed with OpenAI\\ndescription: Troubleshooting NoObjectGeneratedError with finish-reason content-filter caused by incompatible Zod schema types when using OpenAI structured outputs\\n---\\n\\n# Object generation failed with OpenAI\\n\\n## Issue\\n\\nWhen using `generateObject` or `streamObject` with OpenAI\\'s structured output generation, you may encounter a `NoObjectGeneratedError` with the finish reason `content-filter`. This error occurs when your Zod schema contains incompatible types that OpenAI\\'s structured output feature cannot process.\\n\\n```typescript\\n// Problematic code - incompatible schema types\\nimport { generateObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\nconst result = await generateObject({\\n  model: openai(\\'gpt-4o-2024-08-06\\'),\\n  schema: z.object({\\n    name: z.string().nullish(), // ❌ .nullish() is not supported\\n    email: z.string().optional(), // ❌ .optional() is not supported\\n    age: z.number().nullable(), // ✅ .nullable() is supported\\n  }),\\n  prompt: \\'Generate a user profile\\',\\n});\\n\\n// Error: NoObjectGeneratedError: No object generated.\\n// Finish reason: content-filter\\n```\\n\\n## Background\\n\\nOpenAI\\'s structured output generation uses JSON Schema under the hood and has specific requirements for schema compatibility. The Zod methods `.nullish()` and `.optional()` generate JSON Schema patterns that are incompatible with OpenAI\\'s implementation, causing the model to reject the schema and return a content-filter finish reason.\\n\\n## Solution\\n\\nReplace `.nullish()` and `.optional()` with `.nullable()` in your Zod schemas when using structured output generation with OpenAI models.\\n\\n```typescript\\nimport { generateObject } from \\'ai\\';\\nimport { openai } from \\'@ai-sdk/openai\\';\\nimport { z } from \\'zod\\';\\n\\n// Correct approach - use .nullable()\\nconst result = await generateObject({\\n  model: openai(\\'gpt-4o-2024-08-06\\'),\\n  schema: z.object({\\n    name: z.string().nullable(), // ✅ Use .nullable() instead of .nullish()\\n    email: z.string().nullable(), // ✅ Use .nullable() instead of .optional()\\n    age: z.number().nullable(),\\n  }),\\n  prompt: \\'Generate a user profile\\',\\n});\\n\\nconsole.log(result.object);\\n// { name: \"John Doe\", email: \"john@example.com\", age: 30 }\\n// or { name: null, email: null, age: 25 }\\n```\\n\\n### Schema Type Comparison\\n\\n| Zod Type      | Compatible | JSON Schema Behavior                                   |\\n| ------------- | ---------- | ------------------------------------------------------ |\\n| `.nullable()` | ✅ Yes     | Allows `null` or the specified type                    |\\n| `.optional()` | ❌ No      | Field can be omitted (not supported)                   |\\n| `.nullish()`  | ❌ No      | Allows `null`, `undefined`, or omitted (not supported) |\\n\\n## Related Information\\n\\n- For more details on structured output generation, see [Generating Structured Data](/docs/ai-sdk-core/generating-structured-data)\\n- For OpenAI-specific structured output configuration, see [OpenAI Provider - Structured Outputs](/providers/ai-sdk-providers/openai#structured-outputs)\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/30-model-is-not-assignable-to-type.mdx'), name='30-model-is-not-assignable-to-type.mdx', displayName='30-model-is-not-assignable-to-type.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Model is not assignable to type \"LanguageModelV1\"\\ndescription: Troubleshooting errors related to incompatible models.\\n---\\n\\n# Model is not assignable to type \"LanguageModelV1\"\\n\\n## Issue\\n\\nI have updated the AI SDK and now I get the following error: `Type \\'SomeModel\\' is not assignable to type \\'LanguageModelV1\\'.`\\n\\n<Note>Similar errors can occur with `EmbeddingModelV3` as well.</Note>\\n\\n## Background\\n\\nSometimes new features are being added to the model specification.\\nThis can cause incompatibilities with older provider versions.\\n\\n## Solution\\n\\nUpdate your provider packages and the AI SDK to the latest version.\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/40-typescript-cannot-find-namespace-jsx.mdx'), name='40-typescript-cannot-find-namespace-jsx.mdx', displayName='40-typescript-cannot-find-namespace-jsx.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: TypeScript error \"Cannot find namespace \\'JSX\\'\"\\ndescription: Troubleshooting errors related to TypeScript and JSX.\\n---\\n\\n# TypeScript error \"Cannot find namespace \\'JSX\\'\"\\n\\n## Issue\\n\\nI am using the AI SDK in a project without React, e.g. an Hono server, and I get the following error:\\n`error TS2503: Cannot find namespace \\'JSX\\'.`\\n\\n## Background\\n\\nThe AI SDK has a dependency on `@types/react` which defines the `JSX` namespace.\\nIt will be removed in the next major version of the AI SDK.\\n\\n## Solution\\n\\nYou can install the `@types/react` package as a dependency to fix the error.\\n\\n```bash\\nnpm install @types/react\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx'), name='50-react-maximum-update-depth-exceeded.mdx', displayName='50-react-maximum-update-depth-exceeded.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: React error \"Maximum update depth exceeded\"\\ndescription: Troubleshooting errors related to the \"Maximum update depth exceeded\" error.\\n---\\n\\n# React error \"Maximum update depth exceeded\"\\n\\n## Issue\\n\\nI am using the AI SDK in a React project with the `useChat` or `useCompletion` hooks\\nand I get the following error when AI responses stream in: `Maximum update depth exceeded`.\\n\\n## Background\\n\\nBy default, the UI is re-rendered on every chunk that arrives.\\nThis can overload the rendering, especially on slower devices or when complex components\\nneed updating (e.g. Markdown). Throttling can mitigate this.\\n\\n## Solution\\n\\nUse the `experimental_throttle` option to throttle the UI updates:\\n\\n### `useChat`\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { messages, ... } = useChat({\\n  // Throttle the messages and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n\\n### `useCompletion`\\n\\n```tsx filename=\"page.tsx\" highlight=\"2-3\"\\nconst { completion, ... } = useCompletion({\\n  // Throttle the completion and data updates to 50ms:\\n  experimental_throttle: 50\\n})\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/60-jest-cannot-find-module-ai-rsc.mdx'), name='60-jest-cannot-find-module-ai-rsc.mdx', displayName='60-jest-cannot-find-module-ai-rsc.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: \"Jest: cannot find module \\'@ai-sdk/rsc\\'\"\\ndescription: \"Troubleshooting AI SDK errors related to the Jest: cannot find module \\'@ai-sdk/rsc\\' error\"\\n---\\n\\n# Jest: cannot find module \\'@ai-sdk/rsc\\'\\n\\n## Issue\\n\\nI am using AI SDK RSC and am writing tests for my RSC components with Jest.\\n\\nI am getting the following error: `Cannot find module \\'@ai-sdk/rsc\\'`.\\n\\n## Solution\\n\\nConfigure the module resolution via `jest config update` in `moduleNameMapper`:\\n\\n```json filename=\"jest.config.js\"\\n\"moduleNameMapper\": {\\n  \"^@ai-sdk/rsc$\": \"<rootDir>/node_modules/@ai-sdk/rsc/dist\"\\n}\\n```\\n', children=[]), DocItem(origPath=Path('09-troubleshooting/index.mdx'), name='index.mdx', displayName='index.mdx', digest='', short_digest='', essence='', relevant=True, usage=Usage(input=0, output=0, details=None), token_counts=TokenCounts(fulltext=0, digest=0, short_digest=0), fulltext='---\\ntitle: Troubleshooting\\ndescription: Troubleshooting information for common issues encountered with the AI SDK.\\ncollapsed: true\\n---\\n\\n# Troubleshooting\\n\\nThis section is designed to help you quickly identify and resolve common issues encountered with the AI SDK, ensuring a smoother and more efficient development experience.\\n\\n<Support />\\n', children=[])])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert tree\n",
    "tree.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "+3.429s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-foundations/01-overview.mdx\u001b[0m\n",
      "+0.298s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-foundations/02-providers-and-models.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-foundations/03-prompts.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-foundations/04-tools.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-foundations/05-streaming.mdx\u001b[0m\n",
      "+0.020s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-foundations/index.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/01-navigating-the-library.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/02-nextjs-app-router.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/03-nextjs-pages-router.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/04-svelte.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/05-nuxt.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/06-nodejs.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/07-expo.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 02-getting-started/index.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-agents/01-overview.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-agents/02-building-agents.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-agents/03-workflows.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-agents/04-loop-control.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-agents/05-configuring-call-options.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-agents/index.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/01-overview.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/05-generating-text.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/10-generating-structured-data.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/15-tools-and-tool-calling.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/16-mcp-tools.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/20-prompt-engineering.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/25-settings.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/30-embeddings.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/31-reranking.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/35-image-generation.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/36-transcription.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/37-speech.mdx\u001b[0m\n",
      "+0.022s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/40-middleware.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/45-provider-management.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/50-error-handling.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/55-testing.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/60-telemetry.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 03-ai-sdk-core/index.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/01-overview.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/02-chatbot.mdx\u001b[0m\n",
      "+0.024s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/03-chatbot-message-persistence.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/03-chatbot-resume-streams.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/03-chatbot-tool-usage.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/04-generative-user-interfaces.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/05-completion.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/08-object-generation.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/20-streaming-data.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/21-error-handling.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/21-transport.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/24-reading-ui-message-streams.mdx\u001b[0m\n",
      "+0.024s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/25-message-metadata.mdx\u001b[0m\n",
      "+0.024s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/50-stream-protocol.mdx\u001b[0m\n",
      "+0.024s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 04-ai-sdk-ui/index.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/01-overview.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/02-streaming-react-components.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/03-generative-ui-state.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/03-saving-and-restoring-states.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/04-multistep-interfaces.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/05-streaming-values.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/06-loading-state.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/08-error-handling.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/09-authentication.mdx\u001b[0m\n",
      "+0.020s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/10-migrating-to-ui.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 05-ai-sdk-rsc/index.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/01-prompt-engineering.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/02-stopping-streams.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/03-backpressure.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/04-caching.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/05-multiple-streamables.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/06-rate-limiting.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/07-rendering-ui-with-language-models.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/08-model-as-router.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/09-multistep-interfaces.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/09-sequential-generations.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/10-vercel-deployment-guide.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 06-advanced/index.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/01-azure-stream-slow.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/02-client-side-function-calls-not-invoked.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/03-server-actions-in-client-components.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/04-strange-stream-output.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/05-streamable-ui-errors.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/05-tool-invocation-missing-result.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/06-streaming-not-working-when-deployed.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/06-streaming-not-working-when-proxied.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/06-timeout-on-vercel.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/07-unclosed-streams.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/08-use-chat-failed-to-parse-stream.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/09-client-stream-error.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/10-use-chat-tools-no-response.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/11-use-chat-custom-request-options.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/12-typescript-performance-zod.mdx\u001b[0m\n",
      "+0.027s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/12-use-chat-an-error-occurred.mdx\u001b[0m\n",
      "+0.025s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/13-repeated-assistant-messages.mdx\u001b[0m\n",
      "+0.021s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/14-stream-abort-handling.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/14-tool-calling-with-structured-outputs.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/15-abort-breaks-resumable-streams.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/15-stream-text-not-working.mdx\u001b[0m\n",
      "+0.025s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/16-streaming-status-delay.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/17-use-chat-stale-body-data.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/18-ontoolcall-type-narrowing.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/19-unsupported-model-version.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/20-no-object-generated-content-filter.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/30-model-is-not-assignable-to-type.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/40-typescript-cannot-find-namespace-jsx.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/50-react-maximum-update-depth-exceeded.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/60-jest-cannot-find-module-ai-rsc.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 09-troubleshooting/index.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/01-generate-text.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/02-stream-text.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/03-generate-object.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/04-stream-object.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/05-embed.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/06-embed-many.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/06-rerank.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/10-generate-image.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/11-transcribe.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/12-generate-speech.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/15-agent.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/16-tool-loop-agent.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/17-create-agent-ui-stream.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/18-create-agent-ui-stream-response.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/18-pipe-agent-ui-stream-to-response.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/20-tool.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/22-dynamic-tool.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/23-create-mcp-client.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/24-mcp-stdio-transport.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/25-json-schema.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/26-zod-schema.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/27-valibot-schema.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/30-model-message.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/31-ui-message.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/32-validate-ui-messages.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/33-safe-validate-ui-messages.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/40-provider-registry.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/42-custom-provider.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/50-cosine-similarity.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/60-wrap-language-model.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/65-language-model-v2-middleware.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/66-extract-reasoning-middleware.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/67-simulate-streaming-middleware.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/68-default-settings-middleware.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/70-step-count-is.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/71-has-tool-call.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/75-simulate-readable-stream.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/80-smooth-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/90-generate-id.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/91-create-id-generator.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/01-ai-sdk-core/index.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/01-use-chat.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/02-use-completion.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/03-use-object.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/31-convert-to-model-messages.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/32-prune-messages.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/40-create-ui-message-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/41-create-ui-message-stream-response.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/42-pipe-ui-message-stream-to-response.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/43-read-ui-message-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/46-infer-ui-tools.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/47-infer-ui-tool.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/02-ai-sdk-ui/index.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/01-stream-ui.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/02-create-ai.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/03-create-streamable-ui.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/04-create-streamable-value.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/05-read-streamable-value.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/06-get-ai-state.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/07-get-mutable-ai-state.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/08-use-ai-state.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/09-use-actions.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/10-use-ui-state.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/11-use-streamable-value.mdx\u001b[0m\n",
      "+0.019s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/20-render.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/03-ai-sdk-rsc/index.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/01-ai-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/02-streaming-text-response.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/05-stream-to-response.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/07-openai-stream.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/08-anthropic-stream.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/09-aws-bedrock-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/10-aws-bedrock-anthropic-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/10-aws-bedrock-messages-stream.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/11-aws-bedrock-cohere-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/12-aws-bedrock-llama-2-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/13-cohere-stream.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/14-google-generative-ai-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/15-hugging-face-stream.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/16-langchain-adapter.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/16-llamaindex-adapter.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/17-mistral-stream.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/18-replicate-stream.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/19-inkeep-stream.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/04-stream-helpers/index.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-api-call-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-download-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-empty-response-body-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-argument-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-data-content-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-data-content.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-message-role-error.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-prompt-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-response-data-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-invalid-tool-input-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-json-parse-error.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-load-api-key-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-load-setting-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-message-conversion-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-content-generated-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-image-generated-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-object-generated-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-speech-generated-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-such-model-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-such-provider-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-such-tool-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-no-transcript-generated-error.mdx\u001b[0m\n",
      "+0.015s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-retry-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-too-many-embedding-values-for-call-error.mdx\u001b[0m\n",
      "+0.017s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-tool-call-repair-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-type-validation-error.mdx\u001b[0m\n",
      "+0.018s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/ai-unsupported-functionality-error.mdx\u001b[0m\n",
      "+0.016s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/05-ai-sdk-errors/index.mdx\u001b[0m\n",
      "+22.062s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 02-foundations\u001b[0m\n",
      "+48.798s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 03-agents\u001b[0m\n",
      "+45.358s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 07-reference/03-ai-sdk-rsc\u001b[0m\n",
      "+0.399s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 07-reference/05-ai-sdk-errors\u001b[0m\n",
      "+1.877s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 09-troubleshooting\u001b[0m\n",
      "+0.983s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 07-reference/04-stream-helpers\u001b[0m\n",
      "+1.815s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 07-reference/02-ai-sdk-ui\u001b[0m\n",
      "+1.071s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 06-advanced\u001b[0m\n",
      "+8.442s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 05-ai-sdk-rsc\u001b[0m\n",
      "+0.672s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 02-getting-started\u001b[0m\n",
      "+1.169s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 04-ai-sdk-ui\u001b[0m\n",
      "+2.266s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 03-ai-sdk-core\u001b[0m\n",
      "+5.893s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 07-reference/01-ai-sdk-core\u001b[0m\n",
      "+30.534s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 07-reference/index.mdx\u001b[0m\n",
      "+3.623s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing 07-reference\u001b[0m\n",
      "+65.776s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 00-introduction/index.mdx\u001b[0m\n",
      "+0.014s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:157\u001b[0m llm_process_page Processing 01-announcing-ai-sdk-6-beta/index.mdx\u001b[0m\n",
      "+21.836s \u001b[36mDEBUG\u001b[0m \u001b[34mlovely_docs/docs.py:259\u001b[0m llm_process_directory Processing .\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "processed_tree = await process_tree_depth_first(settings, tree, source.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Usage:\n",
      "  Input tokens: 839,200\n",
      "  Output tokens: 253,439\n",
      "  Total tokens: 1,092,639\n",
      "Cost:\n",
      "  Total: $2.11\n",
      "  Input: $0.84\n",
      "  Output: $1.27\n"
     ]
    }
   ],
   "source": [
    "usage = calculate_total_usage(processed_tree)\n",
    "print(f\"\\nTotal Usage:\")\n",
    "print(f\"  Input tokens: {usage.input:,}\")\n",
    "print(f\"  Output tokens: {usage.output:,}\")\n",
    "print(f\"  Total tokens: {(usage.input + usage.output):,}\")\n",
    "\n",
    "cost, input_cost, output_cost = calculate_cost(usage, 1, 5)\n",
    "\n",
    "print(f\"Cost:\")\n",
    "print(f\"  Total: ${cost:.2f}\")\n",
    "print(f\"  Input: ${input_cost:.2f}\")\n",
    "print(f\"  Output: ${output_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_processed_documents(source, settings.output_dir / source.name.replace(\"/\", \"_\"), processed_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
