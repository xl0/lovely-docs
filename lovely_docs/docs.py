# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_docs.ipynb.

# %% auto 0
__all__ = ['logger', 'TokenCounts', 'DocItem', 'DocPage', 'DocDirectory', 'build_markdown_doc_tree', 'PageReplySchema',
           'anthropic_count_tokens', 'llm_process_page', 'DirReplySchema', 'llm_process_directory',
           'process_tree_depth_first', 'calculate_total_usage', 'calculate_cost', 'save_doc_files', 'file_map',
           'build_metadata', 'save_processed_documents']

# %% ../nbs/02_docs.ipynb 3
from pathlib import Path
from typing import Literal
import logging

from pydantic import Field, BaseModel
from jinja2 import Environment, FileSystemLoader


import llm
from llm.models import Usage
import anthropic
import langsmith as ls

from lovely_docs.settings import Settings, settings
import asyncio

from tenacity import AsyncRetrying, stop_after_attempt, wait_exponential


# %% ../nbs/02_docs.ipynb 4
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# %% ../nbs/02_docs.ipynb 5
class TokenCounts(BaseModel):
    fulltext: int = 0
    digest: int = 0
    short_digest: int = 0

class DocItem(BaseModel):
    path: Path
    name: str = ""
    digest: str = ""
    short_digest: str = ""
    essence: str = ""
    relevant: bool = True
    usage: Usage|None = Field(default_factory=Usage)
    type: Literal["page", "dir"]
    token_counts: TokenCounts = Field(default_factory=TokenCounts)


class DocPage(DocItem):
    """Represents a single documentation page"""
    fulltext: str
    type: str = "page"


class DocDirectory(DocItem):
    """Represents a directory in the documentation structure"""
    pages: list[DocPage] = Field(default_factory=list)
    subdirs: list['DocDirectory'] = Field(default_factory=list)
    fulltext: str = ""
    type: str = "dir"

# %% ../nbs/02_docs.ipynb 7
def build_markdown_doc_tree(root:Path, path:Path = Path()) -> DocDirectory:
    """Recursively build a documentation tree from markdown files.

    Args:
        root: Root directory containing the documentation
        path: Relative path from root to process (default: root itself)

    Returns:
        DocDirectory containing pages and subdirectories
    """
    assert root.exists() and root.is_dir()
    assert (root/path).exists() and (root/path).is_dir()

    doc_dir = DocDirectory(path=path)

    # Get immediate children only
    for item in sorted((root/path).iterdir()):
        if item.is_file() and item.suffix == '.md':
            # We'll process files later, just record them
            doc_dir.pages.append(DocPage(path=item.relative_to(root), fulltext=item.read_text()))
        if item.is_dir():
            doc_dir.subdirs.append(build_markdown_doc_tree(root, item.relative_to(root)))

    return doc_dir

# %% ../nbs/02_docs.ipynb 11
class PageReplySchema(BaseModel):
    better_name: str = Field(description="")
    digest: str = Field(title="Digest, format: markdown", )
    short_digest: str = Field(title="Short digest, format:markdown")
    essence: str = Field(title="Essence, format:txt")
    relevant: bool


async def anthropic_count_tokens(client: anthropic.AsyncAnthropic, model: str, text: str):
    res = await client.messages.count_tokens(
        model=model,
        messages=[{
            "role": "user",
            "content": text
        }],
    )
    return res.input_tokens

async def llm_process_page(settings: Settings, page: DocPage, libname: str) -> DocPage:
    with ls.trace(name=f"Process page: {page.path}", run_type="chain") as trace:
        logger.debug(f"Processing {page.path}")


        model = llm.get_async_model(settings.model)
        model.key = settings.api_key

        # We need to use anthropic client directly to count tokens.
        anthropic_client = anthropic.AsyncAnthropic(api_key=settings.api_key)


        template = Environment(loader=FileSystemLoader(settings.templates_dir)).get_template("process_page.j2")
        inputs = {
            "text": page.fulltext,
            "filename": page.path.name,
            "path": page.path.parent.name + "/",
            "libname": libname
        }
        with ls.trace("Template", "prompt", inputs=inputs) as template_trace:
            prompt = template.render(**inputs)
            template_trace.end(outputs=prompt)

        with ls.trace("LLM call", "llm", inputs={"prompt": prompt}) as llm_trace:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=3, max=60)):
                with attempt:
                    try:
                        res = await model.prompt(prompt=prompt, schema=PageReplySchema, max_tokens=32768, temperature=0)
                        llm_trace.end(outputs=await res.text())
                    except Exception as e:
                        logger.warning(f"Retry {attempt.retry_state.attempt_number}: {str(e)}")
                        raise

        with ls.trace("Parse", "parser", inputs={"input": await res.text()}) as parse_trace:
            reply = PageReplySchema.model_validate_json(await res.text())
            # Normalize better_name: lowercase, replace spaces with hyphens, remove .md extension
            reply.better_name = (reply.better_name.lower()  .replace(' ', '-')
                                                            .removesuffix('.md')
                                                            .replace("/", "∕")) # No / in filenames!
            parse_trace.end(outputs=reply)
            usage = await res.usage()

        # Count tokens for fulltext, digest, and short_digest in parallel
        fulltext_tokens, digest_tokens, short_digest_tokens = await asyncio.gather(
            anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", page.fulltext),
            anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", reply.digest),
            anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", reply.short_digest)
        )
        token_counts = TokenCounts(
            fulltext=fulltext_tokens,
            digest=digest_tokens,
            short_digest=short_digest_tokens
        )

        result = DocPage(
            path=page.path,
            fulltext=page.fulltext,
            name=reply.better_name,
            digest=reply.digest,
            short_digest=reply.short_digest,
            essence=reply.essence,
            relevant=reply.relevant,
            token_counts=token_counts,
            usage=usage)

        trace.end(outputs=result)
        return result

# %% ../nbs/02_docs.ipynb 19
class DirReplySchema(BaseModel):
    better_name: str
    digest: str = Field(title="Directory digest, fmt:markdown")
    short_digest: str = Field(title="Short digest, format:markdown")
    essence: str
    relevant: bool

async def llm_process_directory(settings: Settings, directory: DocDirectory,  libname: str) -> DocDirectory:
    """Create a summary for a directory based on its relevant pages and subdirectories"""

    with ls.trace(name=f"Process directory: {directory.path}", run_type="chain") as trace:
        logger.debug(f"Processing {directory.path}")

        # If the directory did not have any relevant pages / subdirs, we should not be called.
        assert any(x for x in directory.pages+directory.subdirs if x.relevant)

        model = llm.get_async_model(settings.model)
        model.key = settings.api_key

        # We need to use anthropic client directly to count tokens.
        anthropic_client = anthropic.AsyncAnthropic(api_key=settings.api_key)

        template = Environment(loader=FileSystemLoader(settings.templates_dir)).get_template("process_directory.j2")

        input = {
            "dirname": directory.path.name,
            "path": directory.path.parent.name,
            "pages": [p for p in directory.pages if p.relevant],
            "subdirs": [s for s in directory.subdirs if s.relevant],
            "libname": libname
        }
        with ls.trace("Template", "prompt", inputs=input) as template_trace:
            prompt = template.render(**input)
            template_trace.end(outputs=prompt)


        with ls.trace("LLM call", "llm", inputs={"prompt": prompt}) as llm_trace:
            async for attempt in AsyncRetrying(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=3, max=60)):
                with attempt:
                    try:
                        res = await model.prompt(prompt=prompt, schema=DirReplySchema, max_tokens=32768, temperature=0)
                        llm_trace.end(outputs=await res.text())
                        usage = await res.usage()
                    except Exception as e:
                        logger.warning(f"Retry {attempt.retry_state.attempt_number}: {str(e)}")
                        raise

        with ls.trace("Parse", "parser", inputs={"input": await res.text()}) as parse_trace:
            reply = DirReplySchema.model_validate_json(await res.text())
            reply.better_name = (reply.better_name.lower()  .replace(' ', '-')
                                                            .removesuffix('.md')
                                                            .replace("/", "∕")) # No / in filenames!
            parse_trace.end(outputs=reply)


        # We save a generated fulltext for a directory which is the sum of digests of all the pages and subdirs within.
        fulltext_template = Environment(loader=FileSystemLoader(settings.templates_dir)).get_template("directory_fulltext.j2")
        fulltext = fulltext_template.render(**input)

        # Count tokens for fulltext, digest, and short_digest in parallel
        fulltext_tokens, digest_tokens, short_digest_tokens = await asyncio.gather(
            anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", fulltext),
            anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", reply.digest),
            anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", reply.short_digest)
        )
        token_counts = TokenCounts(
            fulltext=fulltext_tokens,
            digest=digest_tokens,
            short_digest=short_digest_tokens
        )

        result = directory.model_copy(deep=True)
        result.name = reply.better_name
        result.digest = reply.digest
        result.short_digest = reply.short_digest
        result.essence = reply.essence
        result.relevant = reply.relevant
        result.fulltext = fulltext
        result.token_counts = token_counts
        result.usage = usage

        trace.end(outputs=result)
        return result

# %% ../nbs/02_docs.ipynb 23
async def process_tree_depth_first(settings: Settings, doc_dir: DocDirectory, libname: str) -> DocDirectory:
    """
    Process documentation tree depth-first with parallel processing.
    Mutates the doc_dir object.
    """

    with ls.trace(name=f"Process tree: {libname}/{doc_dir.path}", run_type="chain") as trace:
        # First, recursively process all subdirectories in parallel
        subdirs: list[DocDirectory] = await asyncio.gather(*[
            process_tree_depth_first(settings, subdir, libname) for subdir in doc_dir.subdirs
        ])
        subdirs = sorted(subdirs, key=lambda s: s.path)

        # Then process all pages in this directory in parallel
        pages = await asyncio.gather(*[
            llm_process_page(settings, page, libname) for page in doc_dir.pages
        ])
        pages = sorted(pages, key=lambda s: s.path)

        # .name is llm-generated and might be not unique. Make it unique.
        names: set[str] = set()
        for x in subdirs + pages:
            name, i = x.name, 2
            while name in names:
                name = f"{x.name}_{str(i)}"
                x.name = name
            names.add(name)

        if not any(x for x in subdirs+pages if x.relevant):
            result = DocDirectory(path=doc_dir.path, pages=pages, relevant=False)
            trace.end(outputs=result)
            return result

        result = await llm_process_directory(settings, DocDirectory(path=doc_dir.path, pages=pages, subdirs=subdirs), libname)
        trace.end(outputs=result)
        return result

# %% ../nbs/02_docs.ipynb 28
def calculate_total_usage(doc_dir: DocDirectory) -> Usage:
    """Calculate total usage for a directory tree including all pages, subdirs, and summaries"""
    total_input = 0
    total_output = 0

    # Add usage from all pages in this directory
    for page in doc_dir.pages:
        total_input += page.usage.input or 0
        total_output += page.usage.output or 0

    # Add usage from directory summarization
    if doc_dir.usage:
        total_input += doc_dir.usage.input or 0
        total_output += doc_dir.usage.output or 0

    # Recursively add usage from subdirectories
    for subdir in doc_dir.subdirs:
        subdir_usage = calculate_total_usage(subdir)
        total_input += subdir_usage.input or 0
        total_output += subdir_usage.output or 0

    return Usage(input=total_input, output=total_output)


def calculate_cost(usage: Usage, input_cost: float, output_cost: float) -> tuple[float, float, float]:
    total_input = usage.input or 0
    total_output = usage.output or 0
    input_cost_total = (total_input / 1_000_000) * input_cost
    output_cost_total = (total_output / 1_000_000) * output_cost
    cost = input_cost_total + output_cost_total
    return cost, input_cost_total, output_cost_total

# %% ../nbs/02_docs.ipynb 31
import shutil

# %% ../nbs/02_docs.ipynb 32
def save_doc_files(path: Path, doc: DocDirectory):
    """Save a DocDirectory structure to disk at the specified path.

    Args:
        path: Directory path where the documentation will be saved
        doc: DocDirectory object containing the documentation structure to save
    """
    if path.exists(): shutil.rmtree(path)
    path.mkdir(parents=True, exist_ok=True)

    (path/"digest.md").write_text(doc.digest)
    (path/"short_digest.md").write_text(doc.short_digest)
    (path/"essence.md").write_text(doc.essence)
    (path/"fulltext.md").write_text(doc.fulltext)


    for page in doc.pages:
        (path/page.name).mkdir(parents=True, exist_ok=True)
        (path/page.name/"essence.md").write_text(page.essence)
        (path/page.name/"fulltext.md").write_text(page.fulltext)
        (path/page.name/"digest.md").write_text(page.digest)
        (path/page.name/"short_digest.md").write_text(page.short_digest)

    for subdir in doc.subdirs:
        save_doc_files(path/subdir.name, subdir)

# %% ../nbs/02_docs.ipynb 35
import git
import json
from datetime import datetime, timezone
from .settings import Source, WebSource, GitSource

# %% ../nbs/02_docs.ipynb 37
def file_map(doc: DocDirectory, prefix: Path):
    filemap = {}

    path = prefix / doc.name

    # The filemap is keyed by original name so it's easy to get the pages in the original order.
    logger.debug(f"Added {path.as_posix()} -> {doc.name} ({doc.path.as_posix()})")

    for subdir in doc.subdirs:
        filemap[(path/subdir.name).as_posix()] =  subdir.model_dump(mode="json", include=["path", "relevant", "usage", "token_counts"]) | {"type": "directory" }
        filemap |= file_map(subdir, path)

    for page in doc.pages:
        filemap[(path / page.name).as_posix()] = page.model_dump(mode="json", include=["path", "relevant", "usage", "token_counts"]) | {"type": "page"}
        logger.debug(f"Added page {(path / page.name).as_posix()} -> {page.name} ({page.path.as_posix()})")
    return filemap


def build_metadata(source: Source, doc: DocDirectory):

    if isinstance(source, GitSource):
        source_type = "git"
    elif isinstance(source, WebSource):
        source_type = "web"
    else:
        raise TypeError(f"Unknown source type: {type(source)}")

    # Get current git commit
    repo = git.Repo(search_parent_directories=True)
    commit = repo.head.commit.hexsha
    # Check if repo is dirty
    if repo.is_dirty(untracked_files=True):
        commit += "-dirty"

    return {
        "map": file_map(doc, Path("")),
        "source_type": source_type,
        "source": source.model_dump(mode="json"),
        "date": datetime.now(timezone.utc).isoformat(),
        "model": settings.model,
        "commit": commit,
    }


# %% ../nbs/02_docs.ipynb 40
def save_processed_documents(source: Source, path: Path, tree: DocDirectory):
    tree = tree.model_copy(deep=True)
    tree.name = "" # The top-level directry does not need a name.
    save_doc_files(path, tree)
    (path/"index.json").write_text(json.dumps(build_metadata(source, tree), indent=2))
