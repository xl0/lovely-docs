# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_docs.ipynb.

# %% auto 0
__all__ = ['logger', 'TokenCounts', 'DocItem', 'build_markdown_doc_tree', 'PageReplySchema', 'anthropic_count_tokens',
           'llm_process_page', 'DirReplySchema', 'llm_process_directory', 'process_tree_depth_first',
           'calculate_total_usage', 'calculate_cost', 'save_doc_files', 'file_map', 'build_metadata',
           'save_processed_documents']

# %% ../nbs/02_docs.ipynb 3
from pathlib import Path
from typing import Literal
import logging

from pydantic import Field, BaseModel, field_validator
from jinja2 import Environment, FileSystemLoader

import llm
from llm.models import Usage
import anthropic
import langsmith as ls

from lovely_docs.settings import Settings, settings
import asyncio

from tenacity import AsyncRetrying, stop_after_attempt, wait_exponential


# %% ../nbs/02_docs.ipynb 4
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# %% ../nbs/02_docs.ipynb 5
class TokenCounts(BaseModel):
    fulltext: int = 0
    digest: int = 0
    short_digest: int = 0

class DocItem(BaseModel):
    origPath: Path
    name: str

    @field_validator('name')
    @classmethod
    def validate_name(cls, v):
        if '/' in v or ' ' in v:
            raise ValueError(f"name must not contain '/' or spaces, got: {v!r}")
        return v

    displayName: str
    digest: str = ""
    short_digest: str = ""
    essence: str = ""
    relevant: bool = True
    usage: Usage = Field(default_factory=lambda: Usage(0, 0))
    token_counts: TokenCounts = Field(default_factory=TokenCounts)
    fulltext: str = ""
    children: list['DocItem'] = Field(default_factory=list)

DocItem.model_rebuild()  # Resolve forward refs.

# %% ../nbs/02_docs.ipynb 7
def build_markdown_doc_tree(root: Path, path: Path = Path()) -> DocItem:
    """Recursively build a documentation tree from markdown files.

    Args:
        root: Root directory containing the documentation
        path: Relative path from root to process (default: root itself)

    Returns:
        DocItem containing pages and subdirectories in children, if any
        None if there are no non-emprty pages or subdirectories.
    """
    assert root.exists() and root.is_dir()
    assert (root / path).exists() and (root / path).is_dir()

    children: list[DocItem] = []

    # Get immediate children only
    for item in sorted((root / path).iterdir()):
        if item.is_file() and item.suffix == '.md':
            # We'll process files later, just record them
            rel_path = item.relative_to(root)
            fulltext = item.read_text()
            if fulltext:
                name = str(rel_path.name)
                children.append(
                    DocItem(origPath=rel_path, name=name, displayName=name, fulltext=fulltext)
                )
        if item.is_dir():
            subtree = build_markdown_doc_tree(root, item.relative_to(root))
            if subtree:
                children.append(subtree)

    # Special case - directories with 1 child get folded.
    if len(children) == 1:
        return children[0]

    if children:
        name = str(path.name)
        return DocItem(origPath=path, name=name, displayName=name, children=children)

    return None

# %% ../nbs/02_docs.ipynb 10
class PageReplySchema(BaseModel):
    better_name: str = Field(description="")
    digest: str = Field(title="Digest, format: markdown", )
    short_digest: str = Field(title="Short digest, format:markdown")
    essence: str = Field(title="Essence, format:txt")
    relevant: bool

async def anthropic_count_tokens(client: anthropic.AsyncAnthropic, model: str, text: str):
    res = await client.messages.count_tokens(
        model=model,
        messages=[{
            "role": "user",
            "content": text
        }],
    )
    return res.input_tokens

async def llm_process_page(
    settings: Settings, page: DocItem, libname: str, extra_prompt: str | None = None
) -> DocItem:
    with ls.trace(
        name=f"Process page: {page.origPath}", run_type="chain", inputs={"input": page.fulltext}
    ) as trace:
        logger.debug(f"Processing {page.origPath}")
        assert not page.children, "A page should be a leaf node, no children allowed"

        if not page.fulltext.strip():
            logger.debug(f"Got an empry page {page.origPath}")
            return page.model_copy(update={"relevant": False})

        model = llm.get_async_model(settings.model)
        model.key = settings.api_key

        # We need to use anthropic client directly to count tokens.
        anthropic_client = anthropic.AsyncAnthropic(api_key=settings.api_key)

        template = Environment(loader=FileSystemLoader(settings.templates_dir)
                               ).get_template("process_page.j2")
        inputs = {
            "text": page.fulltext,
            "filename": str(page.origPath),
            "path": str(page.origPath.parent) + "/",
            "libname": libname,
            "extra": extra_prompt
        }
        with ls.trace("Template", "prompt", inputs=inputs) as template_trace:
            prompt = template.render(**inputs)
            template_trace.end(outputs=prompt)

        with ls.trace("LLM call", "llm", inputs={"prompt": prompt}) as llm_trace:
            async for attempt in AsyncRetrying(
                stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=3, max=60)
            ):
                with attempt:
                    try:
                        res = await model.prompt(
                            prompt=prompt, schema=PageReplySchema, max_tokens=32768, temperature=0
                        )
                        llm_trace.end(outputs=await res.text())
                    except Exception as e:
                        logger.warning(f"{page.origPath}: retry {attempt.retry_state.attempt_number}: {str(e)}")
                        raise

        with ls.trace("Parse", "parser", inputs={"input": await res.text()}) as parse_trace:
            reply = PageReplySchema.model_validate_json(await res.text())
            reply.better_name = reply.better_name.removesuffix('.md')
            parse_trace.end(outputs=reply)
            usage = await res.usage()

        # Count tokens for fulltext, digest, and short_digest in parallel
        async for attempt in AsyncRetrying(
            stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=3, max=60)
        ):
            with attempt:
                try:
                    fulltext_tokens, digest_tokens, short_digest_tokens = await asyncio.gather(
                        anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", page.fulltext),
                        anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", reply.digest),
                        anthropic_count_tokens(
                            anthropic_client, "claude-haiku-4-5", reply.short_digest
                        )
                    )
                    token_counts = TokenCounts(
                        fulltext=fulltext_tokens,
                        digest=digest_tokens,
                        short_digest=short_digest_tokens
                    )
                except Exception as e:
                    logger.warning(
                        f"{page.origPath}: retry token count {attempt.retry_state.attempt_number}: {str(e)}"
                    )
                    raise

        result = DocItem(
            origPath=page.origPath,
            fulltext=page.fulltext,
            displayName=reply.better_name,
            name=reply.better_name.lower().replace(" ", "_").replace("/", "_"),
            digest=reply.digest,
            short_digest=reply.short_digest,
            essence=reply.essence,
            relevant=reply.relevant,
            token_counts=token_counts,
            usage=usage
        )

        trace.end(outputs=result)
        return result

# %% ../nbs/02_docs.ipynb 22
class DirReplySchema(BaseModel):
    better_name: str
    digest: str = Field(title="Directory digest, fmt:markdown")
    short_digest: str = Field(title="Short digest, format:markdown")
    essence: str
    relevant: bool

async def llm_process_directory(
    settings: Settings, directory: DocItem, libname: str, extra: str | None = None
) -> DocItem:
    """Create a summary for a directory based on its relevant pages and subdirectories"""

    with ls.trace(name=f"Process directory: {directory.origPath}", run_type="chain") as trace:
        logger.debug(f"Processing {directory.origPath}")

        assert directory.children, "Expected a directory, got a single page"
        assert len(directory.children), "1-child directories are supposed to be folded as pages"
        # If the directory did not have any relevant pages / subdirs, we should not be called.
        assert any(
            x for x in directory.children if x.relevant
        ), "Expected relevant children, got none"

        pages = [p for p in directory.children if not p.children and p.relevant]
        subdirs = [s for s in directory.children if s.children and s.relevant]

        # Special case - if a directory has only 1 relevant child, fold the directory.
        if len(pages + subdirs) == 1:
            return (pages + subdirs)[0].model_copy(deep=True)

        model = llm.get_async_model(settings.model)
        model.key = settings.api_key

        # We need to use anthropic client directly to count tokens.
        anthropic_client = anthropic.AsyncAnthropic(api_key=settings.api_key)

        template = Environment(loader=FileSystemLoader(settings.templates_dir)
                               ).get_template("process_directory.j2")

        input = {
            "dirname": directory.origPath.name + "/",
            "path": directory.origPath.parent.name + "/",
            "pages": pages,
            "subdirs": subdirs,
            "libname": libname,
            "extra": extra
        }
        with ls.trace("Template", "prompt", inputs=input) as template_trace:
            prompt = template.render(**input)
            template_trace.end(outputs=prompt)

        with ls.trace("LLM call", "llm", inputs={"prompt": prompt}) as llm_trace:
            async for attempt in AsyncRetrying(
                stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=3, max=60)
            ):
                with attempt:
                    try:
                        res = await model.prompt(
                            prompt=prompt, schema=DirReplySchema, max_tokens=32768, temperature=0
                        )
                        llm_trace.end(outputs=await res.text())
                        usage = await res.usage()
                    except Exception as e:
                        logger.warning(
                            f"{directory.origPath}: retry {attempt.retry_state.attempt_number}: {str(e)}"
                        )
                        raise

        with ls.trace("Parse", "parser", inputs={"input": await res.text()}) as parse_trace:
            reply = DirReplySchema.model_validate_json(await res.text())
            reply.better_name = reply.better_name.removesuffix('.md')

            parse_trace.end(outputs=reply)

        # We save a generated fulltext for a directory which is the sum of digests of all the pages and subdirs within.
        fulltext_template = Environment(loader=FileSystemLoader(settings.templates_dir)
                                        ).get_template("directory_fulltext.j2")
        fulltext = fulltext_template.render(**input)

        # Count tokens for fulltext, digest, and short_digest in parallel
        async for attempt in AsyncRetrying(
            stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=3, max=60)
        ):
            with attempt:
                try:
                    fulltext_tokens, digest_tokens, short_digest_tokens = await asyncio.gather(
                        anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", fulltext),
                        anthropic_count_tokens(anthropic_client, "claude-haiku-4-5", reply.digest),
                        anthropic_count_tokens(
                            anthropic_client, "claude-haiku-4-5", reply.short_digest
                        )
                    )
                    token_counts = TokenCounts(
                        fulltext=fulltext_tokens,
                        digest=digest_tokens,
                        short_digest=short_digest_tokens
                    )
                except Exception as e:
                    logger.warning(
                        f"{directory.origPath}: retry token count {attempt.retry_state.attempt_number}: {str(e)}"
                    )
                    raise

        result = directory.model_copy(deep=True)
        result.displayName = reply.better_name
        result.name = reply.better_name.lower().replace(" ", "_").replace("/", "_")
        result.digest = reply.digest
        result.short_digest = reply.short_digest
        result.essence = reply.essence
        result.relevant = reply.relevant
        result.fulltext = fulltext
        result.token_counts = token_counts
        result.usage = usage

        trace.end(outputs=result)
        return result

# %% ../nbs/02_docs.ipynb 26
async def process_tree_depth_first(
    settings: Settings,
    doc_dir: DocItem,
    libname: str,
    extra_dir: str | None = None,
    extra_page: str | None = None
) -> DocItem:
    """
    Process documentation tree depth-first with parallel processing.
    Mutates the doc_dir object.
    """

    with ls.trace(name=f"Process tree: {libname}/{doc_dir.origPath}", run_type="chain") as trace:
        # First, recursively process all subdirectories in parallel
        subdirs = [c for c in doc_dir.children if c.children]
        subdirs = await asyncio.gather(
            *[
                process_tree_depth_first(settings, subdir, libname, extra_dir, extra_page)
                for subdir in subdirs
            ]
        )
        subdirs = sorted(subdirs, key=lambda s: s.origPath)

        # Then process all pages in this directory in parallel
        pages = [c for c in doc_dir.children if not c.children]
        pages = await asyncio.gather(
            *[llm_process_page(settings, page, libname, extra_page) for page in pages]
        )
        pages = sorted(pages, key=lambda s: s.origPath)

        # .name is llm-generated and might be not unique. Make it unique.
        names: set[str] = set()
        for x in subdirs + pages:
            name, i = x.displayName, 2
            while name in names:
                name = f"{x.displayName}_{str(i)}"
                i += 1
            x.displayName = name
            names.add(name)

        if not any(x for x in subdirs + pages if x.relevant):
            result = DocItem(
                origPath=doc_dir.origPath,
                displayName=doc_dir.displayName,
                children=pages,
                relevant=False
            )
            trace.end(outputs=result)
            return result

        # Update children with processed items
        doc_dir.children = subdirs + pages
        result = await llm_process_directory(settings, doc_dir, libname, extra_dir)
        trace.end(outputs=result)
        return result

# %% ../nbs/02_docs.ipynb 31
def calculate_total_usage(doc_dir: DocItem) -> Usage:
    """Calculate total usage for a directory tree including all pages, subdirs, and summaries"""
    total_input = 0
    total_output = 0

    # Add usage from all pages in this directory
    for child in doc_dir.children:
        child_usage = calculate_total_usage(child)
        total_input += child_usage.input or 0
        total_output += child_usage.output or 0

    # Add usage from directory summarization
    if doc_dir.usage:
        total_input += doc_dir.usage.input or 0
        total_output += doc_dir.usage.output or 0

    return Usage(input=total_input, output=total_output)

def calculate_cost(usage: Usage, input_cost: float,
                   output_cost: float) -> tuple[float, float, float]:
    total_input = usage.input or 0
    total_output = usage.output or 0
    input_cost_total = (total_input/1_000_000) * input_cost
    output_cost_total = (total_output/1_000_000) * output_cost
    cost = input_cost_total + output_cost_total
    return cost, input_cost_total, output_cost_total

# %% ../nbs/02_docs.ipynb 34
import shutil

# %% ../nbs/02_docs.ipynb 35
def save_doc_files(path: Path, doc: DocItem):
    """Save a DocItem structure to disk at the specified path.

    Args:
        path: Directory path where the documentation will be saved
        doc: DocItem object containing the documentation structure to save
    """
    if path.exists(): shutil.rmtree(path)
    path.mkdir(parents=True, exist_ok=True)

    (path / "digest.md").write_text(doc.digest)
    (path / "short_digest.md").write_text(doc.short_digest)
    (path / "essence.md").write_text(doc.essence)
    (path / "fulltext.md").write_text(doc.fulltext)

    for child in doc.children:
        save_doc_files(path / child.name, child)


# %% ../nbs/02_docs.ipynb 38
import git
import json
from datetime import datetime, timezone
from .settings import Source, WebSource, GitSource, LLMTxtSource

# %% ../nbs/02_docs.ipynb 40
def file_map(doc: DocItem):
    # if not doc.children:
    #     # Leaf node (page)
    #     return doc.model_dump(mode="json", include=["path", "relevant", "usage", "token_counts"]) | {"type": "page"}

    # Directory node
    children_map = {}
    for child in doc.children:
        children_map[child.name] = file_map(child)

    return doc.model_dump(
        mode="json", include=["origPath", "displayName", "relevant", "usage", "token_counts"]
    ) | {
        "children": children_map
    }

def build_metadata(source: Source, doc: DocItem):

    if isinstance(source, GitSource):
        source_type = "git"
    elif isinstance(source, WebSource):
        source_type = "web"
    elif isinstance(source, LLMTxtSource):
        source_type = "llms.txt"
    else:
        raise TypeError(f"Unknown source type: {type(source)}")

    # Get current git commit
    repo = git.Repo(search_parent_directories=True)
    commit = repo.head.commit.hexsha
    # Check if repo is dirty
    if repo.is_dirty(untracked_files=True):
        commit += "-dirty"

    return {
        "map": file_map(doc),
        "name": source.name,
        "ecosystems": source.ecosystems,
        "source_type": source_type,
        "source": source.model_dump(mode="json"),
        "date": datetime.now(timezone.utc).isoformat(),
        "model": settings.model,
        "commit": commit,
    }

# %% ../nbs/02_docs.ipynb 43
def save_processed_documents(source: Source, path: Path, tree: DocItem):
    tree = tree.model_copy(deep=True)
    tree.displayName = ""  # The top-level directry does not need a name.
    save_doc_files(path, tree)
    (path / "index.json").write_text(json.dumps(build_metadata(source, tree), indent=2))
